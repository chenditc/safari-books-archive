<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0006.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0006.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0006.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0006.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Chapter 6. Implementations - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html"><meta name="description" content="CHAPTER 6 Implementations: Real Machine Learning Schemes We have seen the basic ideas of several machine learning methods and studied in detail how to assess their performance on practical data ... "><meta property="og:title" content="Chapter 6. Implementations"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Chapter 6. Implementations"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0006.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content="CHAPTER 6 Implementations: Real Machine Learning Schemes We have seen the basic ideas of several machine learning methods and studied in detail how to assess their performance on practical data ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/xhtml/c0006.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0006.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0006.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0006.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Chapter%206.%20Implementations&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0006.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/Library/view/data-mining-practical/9780123748560/xhtml/p3.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">PART II. Advanced Data Mining</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006a.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">6.8. Clustering</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="c0006"></a><h1 class="chapterlabel" id="c0006tit1">CHAPTER 6</h1>
<h1 class="chaptertitle" id="c0006tit">Implementations: Real Machine Learning Schemes</h1><p id="p0010" class="noindent"><a id="p191"></a>We have seen the basic ideas of several machine learning methods and studied in detail how to assess their performance on practical data mining problems. Now we are well prepared to look at real, industrial-strength, machine learning algorithms. Our aim is to explain these algorithms both at a conceptual level and with a fair amount of technical detail so that you can understand them fully and appreciate the key implementation issues that arise.</p>
<p id="p0015" class="para_indented">In truth, there is a world of difference between the simplistic methods described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a> and the actual algorithms that are widely used in practice. The principles are the same. So are the inputs and outputs—methods of knowledge representation. But the algorithms are far more complex, principally because they have to deal robustly and sensibly with real-world problems such as numeric attributes, missing values, and—most challenging of all—noisy data. To understand how the various schemes cope with noise, we will have to draw on some of the statistical knowledge that we learned in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#c0005">Chapter 5</a>.</p>
<p id="p0020" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a> opened with an explanation of how to infer rudimentary rules and then examined statistical modeling and decision trees. Then we returned to rule induction and continued with association rules, linear models, the nearest-neighbor method of instance-based learning, and clustering. This chapter develops all these topics.</p>
<p id="p0025" class="para_indented">We begin with decision tree induction and work up to a full description of the C4.5 system, a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date. Then we describe decision rule induction. Despite the simplicity of the idea, inducing decision rules that perform comparably with state-of-the-art decision trees turns out to be quite difficult in practice. Most high-performance rule inducers find an initial rule set and then refine it using a rather complex optimization stage that discards or adjusts individual rules to make them work better together. We describe the ideas that underlie rule learning in the presence of noise and then go on to cover a scheme that operates by forming partial decision trees, an approach that has been demonstrated to perform well while avoiding complex and ad hoc heuristics. Following this, we take a brief look at how to generate rules with exceptions, which were described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, and examine fast data structures for learning association rules.</p>
<p id="p0030" class="para_indented">There has been a resurgence of interest in linear models with the introduction of <em>support vector machines</em>, a blend of linear modeling and instance-based learning. <a id="p192"></a>Support vector machines select a small number of critical boundary instances called <em>support vectors</em> from each class and build a linear discriminant function that separates them as widely as possible. This instance-based approach transcends the limitations of linear boundaries by making it practical to include extra nonlinear terms in the function, making it possible to form quadratic, cubic, and higher-order decision boundaries. The same techniques can be applied to the perceptron described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> to implement complex decision boundaries, and also to least squares regression. An older technique for extending the perceptron is to connect units together into multilayer “neural networks.” All of these ideas are described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>.</p>
<p id="p0035" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0180">Section 6.5</a> describes classic instance-based learners, developing the simple nearest-neighbor method introduced in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a> and showing some more powerful alternatives that perform explicit generalization. Following that we extend linear regression for numeric prediction to a more sophisticated procedure that comes up with the tree representation introduced in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0020">Section 3.3</a> and go on to describe locally weighted regression, an instance-based strategy for numeric prediction. Then we examine Bayesian networks, a potentially very powerful way of extending the Naïve Bayes method to make it less “naïve” by dealing with datasets that have internal dependencies. Next we return to clustering and review some methods that are more sophisticated than simple <em>k</em>-means, methods that produce hierarchical clusters and probabilistic clusters. We also look at semi-supervised learning, which can be viewed as combining clustering and classification. Finally, we discuss more advanced schemes for multi-instance learning than those covered in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0175">Section 4.9</a>.</p>
<p id="p0040" class="para_indented">Because of the nature of the material it contains, this chapter differs from the others in the book. Sections can be read independently, and each is self-contained, including the references to further reading, which are gathered together in Discussion sections.</p>
<div id="s0010">
<h2 id="st0010">6.1 Decision trees</h2>
<p id="p0045" class="noindent">The first machine learning scheme that we will develop in detail, the C4.5 algorithm, derives from the simple divide-and-conquer algorithm for producing decision trees that was described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0045">Section 4.3</a>. It needs to be extended in several ways before it is ready for use on real-world problems. First, we consider how to deal with numeric attributes and, after that, missing values. Then we look at the all-important problem of pruning decision trees, because although trees constructed by the divide-and-conquer algorithm as described perform well on the training set, they are usually overfitted to the training data and do not generalize well to independent test sets. We then briefly consider how to convert decision trees to classification rules and examine the options provided by the C4.5 algorithm itself. Finally, we look at an alternative pruning strategy that is implemented in the famous CART system for learning classification and regression trees.</p>
<div id="s0015">
<h3 id="st0015"><a id="p193"></a>Numeric Attributes</h3>
<p id="p0050" class="noindent">The method we described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0045">Section 4.3</a> only works when all the attributes are nominal, whereas, as we have seen, most real datasets contain some numeric attributes. It is not too difficult to extend the algorithm to deal with these. For a numeric attribute we will restrict the possibilities to a two-way, or binary, split. Suppose we use the version of the weather data that has some numeric features (see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>). Then, when temperature is being considered for the first split, the temperature values involved are</p><a id="p0055"></a>
<p id="t0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0010.jpg" alt="Image" width="559" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0010.jpg"></p>
<p></p>
<p id="p0060" class="para_indented">Repeated values have been collapsed together, and there are only 11 possible positions for the breakpoint—8 if the breakpoint is not allowed to separate items of the same class. The information gain for each can be calculated in the usual way. For example, the test <em>temperature</em> &lt; 71.5 produces four <em>yes</em>’s and two <em>no</em>’s, whereas <em>temperature</em> &gt; 71.5 produces five <em>yes</em>’s and three <em>no</em>’s, and so the information value of this test is</p>
<p class="figure" id="e0010"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si1.jpg" alt="image" width="619" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si1.jpg"></p>
<p></p>
<p id="p0065" class="para_indented">It is common to place numeric thresholds halfway between the values that delimit the boundaries of a concept, although something might be gained by adopting a more sophisticated policy. For example, we will see in the following that although the simplest form of instance-based learning puts the dividing line between concepts in the middle of the space between them, other methods that involve more than just the two nearest examples have been suggested.</p>
<p id="p0070" class="para_indented">When creating decision trees using the divide-and-conquer method, once the first attribute to split on has been selected, a top-level tree node is created that splits on that attribute, and the algorithm proceeds recursively on each of the child nodes. For each numeric attribute, it appears that the subset of instances at each child node must be re-sorted according to that attribute’s values—and, indeed, this is how programs for inducing decision trees are usually written. However, it is not actually necessary to re-sort because the sort order at a parent node can be used to derive the sort order for each child, leading to a speedier implementation. Consider the temperature attribute in the weather data, whose sort order (this time including duplicates) is</p><a id="p0075"></a>
<p id="t0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0015.jpg" alt="Image" width="560" height="34" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0015.jpg"></p>
<p></p>
<p id="p0080" class="para_indented">The italicized numbers below each temperature value give the number of the instance that has that value. Thus, instance number 7 has temperature value 64, instance 6 has temperature 65, and so on. Suppose we decide to split at the top level <a id="p194"></a>on the attribute <em>outlook</em>. Consider the child node for which <em>outlook</em> = <em>sunny</em>—in fact, the examples with this value of <em>outlook</em> are numbers 1, 2, 8, 9, and 11. If the italicized sequence is stored with the example set (and a different sequence must be stored for each numeric attribute)—that is, instance 7 contains a pointer to instance 6, instance 6 points to instance 5, instance 5 points to instance 9, and so on—then it is a simple matter to read off the examples for which <em>outlook</em> = <em>sunny</em> in order. All that is necessary is to scan through the instances in the indicated order, checking the <em>outlook</em> attribute for each and writing down the ones with the appropriate value:</p><a id="p0085"></a>
<p id="t0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0020.jpg" alt="Image" width="214" height="14" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0020.jpg"></p>
<p></p>
<p id="p0090" class="noindent">Thus, repeated sorting can be avoided by storing with each subset of instances the sort order for that subset according to each numeric attribute. The sort order must be determined for each numeric attribute at the beginning; no further sorting is necessary thereafter.</p>
<p id="p0095" class="para_indented">When a decision tree tests a nominal attribute as described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0045">Section 4.3</a>, a branch is made for each possible value of the attribute. However, we have restricted splits on numeric attributes to be binary. This creates an important difference between numeric attributes and nominal ones: Once you have branched on a nominal attribute, you have used all the information that it offers; however, successive splits on a numeric attribute may continue to yield new information. Whereas a nominal attribute can only be tested once on any path from the root of a tree to the leaf, a numeric one can be tested many times. This can yield trees that are messy and difficult to understand because the tests on any single numeric attribute are not located together but can be scattered along the path. An alternative, which is harder to accomplish but produces a more readable tree, is to allow a multiway test on a numeric attribute, testing against several different constants at a single node of the tree. A simpler but less powerful solution is to prediscretize the attribute as described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0030">Section 7.2</a>.</p>
</div>
<div id="s0020">
<h3 id="st0020">Missing Values</h3>
<p id="p0100" class="noindent">The next enhancement to the decision tree–building algorithm deals with the problems of missing values. Missing values are endemic in real-world datasets. As explained in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0002.html#c0002">Chapter 2</a> (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0002.html#p58">page 58</a>), one way of handling them is to treat them as just another possible value of the attribute; this is appropriate if the fact that the attribute is missing is significant in some way. In that case, no further action need be taken. But if there is no particular significance in the fact that a certain instance has a missing attribute value, a more subtle solution is needed. It is tempting to simply ignore all instances in which some of the values are missing, but this solution is often too draconian to be viable. Instances with missing values often provide a good deal of information. Sometimes the attributes with values that are missing play no part in the decision, in which case these instances are as good as any other.</p>
<p id="p0105" class="para_indented">One question is how to apply a given decision tree to an instance in which some of the attributes to be tested have missing values. We outlined a solution in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0020">Section 3.3</a><a id="p195"></a>that involves notionally splitting the instance into pieces, using a numeric weighting scheme, and sending part of it down each branch in proportion to the number of training instances going down that branch. Eventually, the various parts of the instance will each reach a leaf node, and the decisions at these leaf nodes must be recombined using the weights that have percolated to the leaves. The information gain and gain ratio calculations described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0045">Section 4.3</a> can also be applied to partial instances. Instead of having integer counts, the weights are used when computing both gain figures.</p>
<p id="p0110" class="para_indented">Another question is how to partition the training set once a splitting attribute has been chosen, to allow recursive application of the decision tree formation procedure on each of the daughter nodes. The same weighting procedure is used. Instances for which the relevant attribute value is missing are notionally split into pieces, one piece for each branch, in the same proportion as the known instances go down the various branches. Pieces of the instance contribute to decisions at lower nodes in the usual way through the information gain calculation, except that they are weighted accordingly. They may be further split at lower nodes, of course, if the values of other attributes are unknown as well.</p>
</div>
<div id="s0025">
<h3 id="st0025">Pruning</h3>
<p id="p0115" class="noindent">Fully expanded decision trees often contain unnecessary structure, and it is generally advisable to simplify them before they are deployed. Now it is time to learn how to prune decision trees.</p>
<p id="p0120" class="para_indented">By building the complete tree and pruning it afterward we are adopting a strategy of <em>postpruning</em> (sometimes called <em>backward pruning</em>) rather than <em>prepruning</em> (or <em>forward pruning</em>). Prepruning would involve trying to decide during the tree-building process when to stop developing subtrees—quite an attractive prospect because that would avoid all the work of developing subtrees only to throw them away afterward. However, postpruning does seem to offer some advantages. For example, situations occur in which two attributes individually seem to have nothing to contribute but are powerful predictors when combined—a sort of combination-lock effect in which the correct combination of the two attribute values is very informative but the attributes taken individually are not. Most decision tree builders postprune; however, prepruning can be a viable alternative when runtime is of particular concern.</p>
<p id="p0125" class="para_indented">Two rather different operations have been considered for postpruning: <em>subtree replacement</em> and <em>subtree raising</em>. At each node, a learning scheme might decide whether it should perform subtree replacement, subtree raising, or leave the subtree as it is, unpruned. Subtree replacement is the primary pruning operation, and we look at it first. The idea is to select some subtrees and replace them with single leaves. For example, the whole subtree in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(a)</a>, involving two internal nodes and four leaf nodes, has been replaced by the single leaf <em>bad</em>. This will certainly cause the accuracy on the training set to decrease if the original tree was produced by the decision tree algorithm described previously, because that continued to build <a id="p196"></a>the tree until all leaf nodes were pure (or until all attributes had been tested). However, it may increase the accuracy on an independently chosen test set.</p>
<p id="p0130" class="para_indented">When subtree replacement is implemented, it proceeds from the leaves and works back up toward the root. In the <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a> example, the whole subtree in (a) would not be replaced at once. First, consideration would be given to replacing the three daughter nodes in the <em>health plan contribution</em> subtree with a single leaf node. Assume that a decision is made to perform this replacement—we will explain how this decision is made shortly. Then, continuing to work back from the leaves, consideration would be given to replacing the <em>working hours per week</em> subtree, which now has just two daughter nodes, by a single leaf node. In the <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a> example, this replacement was indeed made, which accounts for the entire subtree in (a) being replaced by a single leaf marked <em>bad</em>. Finally, consideration would be given to replacing the two daughter nodes in the <em>wage increase 1st year</em> subtree with a single leaf node. In this case, that decision was not made, so the tree remains as shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(a)</a>. Again, we will examine how these decisions are actually made shortly.</p>
<p id="p0135" class="para_indented">The second pruning operation, subtree raising, is more complex, and it is not clear that it is necessarily always worthwhile. However, because it is used in the influential decision tree–building system C4.5, we describe it here. Subtree raising does not occur in the <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a> example, so we use the artificial example of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0010">Figure 6.1</a> for illustration. Here, consideration is given to pruning the tree in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0010">Figure 6.1(a)</a>, and the result is shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0010">Figure 6.1(b)</a>. The entire subtree from C downward has been “raised” to replace the B subtree. Note that although the daughters of B and C are shown as leaves, they can be entire subtrees. Of course, if we perform this raising operation, it is necessary to reclassify the examples at the nodes marked 4 and 5 into the new subtree headed by C. This is why the daughters of that node are marked with primes—1′, 2′, and 3′—to indicate that they are not the same as the original <a id="p197"></a>daughters 1, 2, and 3 but differ by the inclusion of the examples originally covered by 4 and 5.</p>
<p id="f0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-001ab-9780123748560.jpg" alt="image" width="514" height="250" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-001ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.1</span> Example of subtree raising, where (a) node C is “raised” to subsume node B (b).</p>
<p id="p0140" class="para_indented">Subtree raising is a potentially time-consuming operation. In actual implementations it is generally restricted to raising the subtree of the most popular branch. That is, we consider doing the raising illustrated in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0010">Figure 6.1</a> provided that the branch from B to C has more training examples than the branches from B to node 4 or from B to node 5. Otherwise, if (for example) node 4 were the majority daughter of B, we would consider raising node 4 to replace B and reclassifying all examples under C, as well as the examples from node 5, into the new node.</p>
</div>
<div id="s0030">
<h3 id="st0030">Estimating Error Rates</h3>
<p id="p0145" class="noindent">So much for the two pruning operations. Now we must address the question of how to decide whether to replace an internal node by a leaf (for subtree replacement) or whether to replace an internal node by one of the nodes below it (for subtree raising). To make this decision rationally, it is necessary to estimate the error rate that would be expected at a particular node given an independently chosen test set. We need to estimate the error at internal nodes as well as at leaf nodes. If we had such an estimate, it would be clear whether to replace, or raise, a particular subtree simply by comparing the estimated error of the subtree with that of its proposed replacement. Before estimating the error for a subtree proposed for raising, examples that lie under siblings of the current node—the examples at 4 and 5 of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0010">Figure 6.1</a>—would have to be temporarily reclassified into the raised tree.</p>
<p id="p0150" class="para_indented">It is no use taking the training set error as the error estimate: That would not lead to any pruning because the tree has been constructed expressly for that particular training set. One way of coming up with an error estimate is the standard verification technique: Hold back some of the data originally given and use it as an independent test set to estimate the error at each node. This is called <em>reduced-error</em> pruning. It suffers from the disadvantage that the actual tree is based on less data.</p>
<p id="p0155" class="para_indented">The alternative is to try to make some estimate of error based on the training data itself. That is what C4.5 does, and we will describe its method here. It is a heuristic based on some statistical reasoning, but the statistical underpinning is rather weak. However, it seems to work well in practice. The idea is to consider the set of instances that reach each node and imagine that the majority class is chosen to represent that node. That gives us a certain number of “errors,” <em>E</em>, out of the total number of instances, <em>N</em>. Now imagine that the true probability of error at the node is <em>q</em>, and that the <em>N</em> instances are generated by a Bernoulli process with parameter <em>q</em>, of which <em>E</em> turn out to be errors.</p>
<p id="p0160" class="para_indented">This is almost the same situation as we considered when looking at the holdout method in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0015">Section 5.2</a>, where we calculated confidence intervals on the true success probability <em>p</em> given a certain observed success rate. There are two differences. One is trivial: Here we are looking at the error rate <em>q</em> rather than the success rate <em>p</em>; these are simply related by <em>p</em> + <em>q</em> = 1. The second is more serious: Here the figures <em>E</em> and <em>N</em> are measured from the training data, whereas in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0015">Section 5.2</a> we were considering <a id="p198"></a>independent test data. Because of this difference we make a pessimistic estimate of the error rate by using the upper confidence limit rather than stating the estimate as a confidence range.</p><a id="p0165"></a><div class="boxg" id="b0010">
<p id="p0170" class="noindent">The mathematics involved is just the same as before. Given a particular confidence <em>c</em> (the default figure used by C4.5 is <em>c</em> = 25%), we find confidence limits <em>z</em> such that</p>
<p class="figure" id="e0015"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si2.jpg" alt="image" width="190" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si2.jpg"></p>
<p>where <em>N</em> is the number of samples, <em>f</em> = <em>E</em>/<em>N</em> is the observed error rate, and <em>q</em> is the true error rate. As before, this leads to an upper confidence limit for <em>q</em>. Now we use that upper confidence limit as a (pessimistic) estimate for the error rate <em>e</em> at the node:</p>
<p class="figure" id="e0020"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si3.jpg" alt="image" width="225" height="92" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si3.jpg"></p>
<p>Note the use of the + sign before the square root in the numerator to obtain the upper confidence limit. Here, <em>z</em> is the number of standard deviations corresponding to the confidence <em>c</em>, which for <em>c</em> = 25% is <em>z</em> = 0.69.</p>
<p id="p0175" class="para_indented">To see how all this works in practice, let’s look again at the labor negotiations decision tree of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a>, salient parts of which are reproduced in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0015">Figure 6.2</a> with the number of training examples that reach the leaves added. We use the previous formula with a 25% confidence figure—that is, with <em>z</em> = 0.69. Consider the lower left leaf, for which <em>E</em> = 2, <em>N</em> = 6, and so <em>f</em> = 0.33. Plugging these figures into the formula, the upper confidence limit is calculated as <em>e</em> = 0.47. That means that instead of using the training set error rate for this leaf, which is 33%, we will use the pessimistic estimate of 47%. This is pessimistic indeed, considering that it would be a bad mistake to let the error rate exceed 50% for a two-class problem. But things are worse for the neighboring leaf, where <em>E</em> = 1 and <em>N</em> = 2, because the upper confidence limit becomes <em>e</em> = 0.72. The third leaf has the same value of <em>e</em> as the first. The next step is to combine the error estimates for these three leaves in the ratio of the number of examples they cover, 6:2:6, which leads to a combined error estimate of 0.51. Now we consider the error estimate for the parent node, <em>health plan contribution</em>. This covers nine bad examples and five good ones, so the training set error rate is <em>f</em> = 5/14. For these values, the previous formula yields a pessimistic error estimate of <em>e</em> = 0.46. Because this is less than the combined error estimate of the three children, they are pruned away.</p>
<p id="f0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-002-9780123748560.jpg" alt="image" width="652" height="769" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-002-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.2</span> Pruning the labor negotiations decision tree.</p>
<p id="p0180" class="para_indented">The next step is to consider the <em>working hours per week</em> node, which now has two children that are both leaves. The error estimate for the first, with <em>E</em> = 1 and <em>N</em> = 2, is <em>e</em> = 0.72, while for the second it is <em>e</em> = 0.46, as we have just seen. Combining these in the appropriate ratio of 2:14 leads to a value that is higher than the error estimate for the <em>working hours</em> node, so the subtree is pruned away and replaced by a leaf node.</p>
<p id="p0185" class="para_indented">The estimated error figures obtained in these examples should be taken with a grain of salt because the estimate is only a heuristic one and is based on a number of shaky assumptions: the use of the upper confidence limit; the assumption of a normal distribution; and the fact that statistics from the training set are used. However, the qualitative behavior of the error formula is correct and the method seems to work reasonably well in practice. If necessary, the underlying confidence level, which we have taken to be 25%, can be tweaked to produce more satisfactory results.</p>
</div>
<p></p>
</div>
<div id="s0035">
<h3 id="st0035"><a id="p199"></a>Complexity of Decision Tree Induction</h3>
<p id="p0190" class="noindent">Now that we have learned how to accomplish the pruning operations, we have finally covered all the central aspects of decision tree induction. Let’s take stock and examine the computational complexity of inducing decision trees. We will use the standard order notation: O(<em>n</em>) stands for a quantity that grows at most linearly with <em>n</em>, O(<em>n</em><sup>2</sup>) grows at most quadratically with <em>n</em>, and so on.</p>
<p id="p0195" class="para_indented">Suppose the training data contains <em>n</em> instances and <em>m</em> attributes. We need to make some assumption about the size of the tree, and we will assume that its depth is on the order of log <em>n</em>, that is O(log <em>n</em>). This is the standard rate of growth of a tree with <em>n</em> leaves, provided that it remains “bushy” and doesn’t degenerate into a few very long, stringy branches. Note that we are tacitly assuming that most of the instances are different from each other and—this is almost the same thing—that the <em>m</em> attributes provide enough tests to allow the instances to be differentiated. For example, if there were only a few binary attributes, they would allow only so many instances to be differentiated and the tree could not grow past a certain point, rendering an “in the limit” analysis meaningless.</p>
<p id="p0200" class="para_indented">The computational cost of building the tree in the first place is O(<em>mn</em>log <em>n</em>). Consider the amount of work done for one attribute over all nodes of the tree. Not all the examples need to be considered at each node, of course. But at each possible tree depth, the entire set of <em>n</em> instances must be considered in the worst case. And because there are log <em>n</em> different depths in the tree, the amount of work for this one attribute is O(<em>n</em> log <em>n</em>). At each node all attributes are considered, so the total amount of work is O(<em>mn</em> log <em>n</em>).</p>
<p id="p0205" class="para_indented"><a id="p200"></a>This reasoning makes some assumptions. If some attributes are numeric, they must be sorted, but once the initial sort has been done there is no need to re-sort at each tree depth if the appropriate algorithm is used (described previously—see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p193">page 193</a>). The initial sort takes O(<em>n</em> log <em>n</em>) operations for each of up to <em>m</em> attributes; thus, the above complexity figure is unchanged. If the attributes are nominal, all attributes do <em>not</em> have to be considered at each tree node because attributes that are used further up the tree cannot be reused. However, if attributes are numeric, they can be reused and so they have to be considered at every tree level.</p>
<p id="p0210" class="para_indented">Next, consider pruning by subtree replacement. First an error estimate must be made for every tree node. Provided that counts are maintained appropriately, this is linear in the number of nodes in the tree. Then each node needs to be considered for replacement. The tree has at most <em>n</em> leaves, one for each instance. If it were a binary tree, each attribute being numeric or two-valued, that would give it 2<em>n</em> – 1 nodes; multiway branches would only serve to decrease the number of internal nodes. Thus, the complexity of subtree replacement is O(<em>n</em>).</p>
<p id="p0215" class="para_indented">Finally, subtree lifting has a basic complexity equal to subtree replacement. But there is an added cost because instances need to be reclassified during the lifting operation. During the whole process, each instance may have to be reclassified at every node between its leaf and the root—that is, as many as O(log <em>n</em>) times. That makes the total number of reclassifications O(<em>n</em> log <em>n</em>). And reclassification is not a single operation: One that occurs near the root will take O(log <em>n</em>) operations, and one of average depth will take half of this. Thus, the total complexity of subtree lifting is as follows: O(<em>n</em>(log <em>n</em>)<sup>2</sup>).</p>
<p id="p0220" class="para_indented">Taking into account all these operations, the full complexity of decision tree induction is</p>
<p class="figure" id="e0025"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si4.jpg" alt="image" width="227" height="33" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si4.jpg"></p>
<p></p>
</div>
<div id="s0040">
<h3 id="st0040">From Trees to Rules</h3>
<p id="p0225" class="noindent">It is possible to read a set of rules directly off a decision tree, as noted in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, by generating a rule for each leaf and making a conjunction of all the tests encountered on the path from the root to that leaf. This produces rules that are unambiguous in that it doesn’t matter in what order they are executed. However, the rules are more complex than necessary.</p>
<p id="p0230" class="para_indented">The estimated error rate described previously provides exactly the mechanism necessary to prune the rules. Given a particular rule, each condition in it is considered for deletion by tentatively removing it, working out which of the training examples are now covered by the rule, calculating from this a pessimistic estimate of the error rate of the new rule, and comparing this with the pessimistic estimate for the original rule. If the new rule is better, delete that condition and carry on, looking for other conditions to delete. Leave the rule when there are no remaining conditions that will improve it if they are removed. Once all rules have been pruned in this way, it is necessary to see if there are any duplicates and remove them from the rule set.</p>
<p id="p0235" class="para_indented"><a id="p201"></a>This is a greedy approach to detecting redundant conditions in a rule, and there is no guarantee that the best set of conditions will be removed. An improvement would be to consider all subsets of conditions, but this is usually prohibitively expensive. Another solution might be to use an optimization technique such as simulated annealing or a genetic algorithm to select the best version of this rule. However, the simple greedy solution seems to produce quite good rule sets.</p>
<p id="p0240" class="para_indented">The problem, even with the greedy method, is computational cost. For every condition that is a candidate for deletion, the effect of the rule must be reevaluated on all the training instances. This means that rule generation from trees tends to be very slow. The next section describes much faster methods that generate classification rules directly without forming a decision tree first.</p>
</div>
<div id="s0045">
<h3 id="st0045">C4.5: Choices and Options</h3>
<p id="p0245" class="noindent">The decision tree program C4.5 and its successor C5.0 were devised by Ross Quinlan over a 20-year period beginning in the late 1970s. A complete description of C4.5, the early 1990s version, appears as an excellent and readable book (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib257">Quinlan, 1993</a>), along with the full source code. The more recent version, C5.0, is available commercially. Its decision tree induction seems to be essentially the same as that used by C4.5, and tests show some differences but negligible improvements. However, its rule generation is greatly sped up and clearly uses a different technique, although this has not been described in the open literature.</p>
<p id="p0250" class="para_indented">C4.5 works essentially as described in the previous sections. The default confidence value is set at 25% and works reasonably well in most cases; possibly it should be altered to a lower value, which causes more drastic pruning, if the actual error rate of pruned trees on test sets is found to be much higher than the estimated error rate. There is one other important parameter whose effect it is to eliminate tests for which almost all of the training examples have the same outcome. Such tests are often of little use. Consequently, tests are not incorporated into the decision tree unless they have at least two outcomes that have at least a minimum number of instances. The default value for this minimum is 2, but it is controllable and should perhaps be increased for tasks that have a lot of noisy data.</p>
<p id="p0255" class="para_indented">Another heuristic in C4.5 is that candidate splits on numeric attributes are only considered if they cut off a certain minimum number of instances: at least 10% of the average number of instances per class at the current node, or 25 instances—whichever value is smaller (but the minimum just mentioned, 2 by default, is also enforced).</p>
<p id="p0260" class="para_indented">C4.5 Release 8, the last noncommercial version of C4.5, includes an MDL-based adjustment to the information gain for splits on numeric attributes. More specifically, if there are <em>S</em> candidate splits on a certain numeric attribute at the node currently considered for splitting, log<span class="sub">2</span>(<em>S</em>)/<em>N</em> is subtracted from the information gain, where <em>N</em> is the number of instances at the node. This heuristic, described by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib255">Quinlan (1986)</a>, is designed to prevent overfitting. The information gain may be negative after subtraction, and tree growing will stop if there are no attributes with positive information gain—a form of prepruning. We mention this here because it can be surprising <a id="p202"></a>to obtain a pruned tree even if postpruning has been turned off! This heuristic is also implemented in the software described in Part 3 of this book.</p>
</div>
<div id="s0050">
<h3 id="st0050">Cost-Complexity Pruning</h3>
<p id="p0265" class="noindent">As mentioned, the postpruning method in C4.5 is based on shaky statistical assumptions, and it turns out that it often does not prune enough. On the other hand, it is very fast and thus popular in practice. However, in many applications it is worthwhile expending more computational effort to obtain a more compact decision tree. Experiments have shown that C4.5’s pruning method can yield unnecessary additional structure in the final tree: Tree size continues to grow when more instances are added to the training data even when this does not further increase performance on independent test data. In that case, the more conservative <em>cost-complexity pruning</em> method from the Classification and Regression Trees (CART) learning system may be more appropriate.</p>
<p id="p0270" class="para_indented">Cost-complexity pruning is based on the idea of first pruning those subtrees that, relative to their size, lead to the smallest increase in error on the training data. The increase in error is measured by a quantity <em>α</em> that is defined to be the average error increase per leaf of the subtree concerned. By monitoring this quantity as pruning progresses, the algorithm generates a sequence of successively smaller pruned trees. In each iteration it prunes all subtrees that exhibit the smallest value of <em>α</em> among the remaining subtrees in the current version of the tree.</p>
<p id="p0275" class="para_indented">Each candidate tree in the resulting sequence of pruned trees corresponds to one particular threshold value, <em>α<span class="sub">i</span>
</em>. The question becomes, which tree should be chosen as the final classification model? To determine the most predictive tree, cost-complexity pruning either uses a holdout set to estimate the error rate of each tree, or, if data is limited, employs cross-validation.</p>
<p id="p0280" class="para_indented">Using a holdout set is straightforward. However, cross-validation poses the problem of relating the <em>α</em> values observed in the sequence of pruned trees for training fold <em>k</em> of the cross-validation to the <em>α</em> values from the sequence of trees for the full dataset: These values are usually different. This problem is solved by first computing the geometric average of <em>α<span class="sub">i</span>
</em> and <em>α<span class="sub">i</span>
</em>
<span class="sub">+1</span> for tree <em>i</em> from the full dataset. Then, for each fold <em>k</em> of the cross-validation, the tree that exhibits the largest <em>α</em> value smaller than this average is picked. The average of the error estimates for these trees from the <em>k</em> folds, estimated from the corresponding test datasets, is the cross-validation error for tree <em>i</em> from the full dataset.</p>
</div>
<div id="s0055">
<h3 id="st0055">Discussion</h3>
<p id="p0285" class="noindent">Top-down induction of decision trees is probably the most extensively researched method of machine learning used in data mining. Researchers have investigated a panoply of variations for almost every conceivable aspect of the learning process—for example, different criteria for attribute selection or modified pruning methods. However, they are rarely rewarded by substantial improvements in accuracy over a spectrum of diverse datasets. As discussed, the pruning method used by the CART system for learning decision trees (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib41">Breiman et al., 1984</a>) can often produce smaller <a id="p203"></a>trees than C4.5’s pruning method. This has been investigated empirically by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib244">Oates and Jensen (1997)</a>.</p>
<p id="p0290" class="para_indented">In our description of decision trees, we have assumed that only one attribute is used to split the data into subsets at each node of the tree. However, it is possible to allow tests that involve several attributes at a time. For example, with numeric attributes each test can be on a linear combination of attribute values. Then the final tree consists of a hierarchy of linear models of the kind we described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, and the splits are no longer restricted to being axis-parallel. Trees with tests involving more than one attribute are called <em>multivariate</em> decision trees, in contrast to the simple <em>univariate</em> trees that we normally use. The CART system has the option of generating multivariate tests. They are often more accurate and smaller than univariate trees but take much longer to generate and are also more difficult to interpret. We briefly mention one way of generating them in the Principal Components Analysis section in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>.</p>
</div>
</div>
<div id="s0060">
<h2 id="st0060">6.2 Classification rules</h2>
<p id="p0295" class="noindent">We call the basic covering algorithm for generating rules that was described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0065">Section 4.4</a> a separate-and-conquer technique because it identifies a rule that covers instances in a class (and excludes ones not in the class), separates them out, and continues on those that are left. Such algorithms have been used as the basis of many systems that generate rules. There, we described a simple correctness-based measure for choosing what test to add to the rule at each stage. However, there are many other possibilities, and the particular criterion that is used has a significant effect on the rules produced. We examine different criteria for choosing tests in this section. We also look at how the basic rule-generation algorithm can be extended to more practical situations by accommodating missing values and numeric attributes.</p>
<p id="p0300" class="para_indented">But the real problem with all these rule-generation schemes is that they tend to overfit the training data and do not generalize well to independent test sets, particularly on noisy data. To be able to generate good rule sets for noisy data, it is necessary to have some way of measuring the real worth of individual rules. The standard approach to assessing the worth of rules is to evaluate their error rate on an independent set of instances, held back from the training set, and we explain this next. After that, we describe two industrial-strength rule learners: one that combines the simple separate-and-conquer technique with a global optimization step, and another that works by repeatedly building partial decision trees and extracting rules from them. Finally, we consider how to generate rules with exceptions, and exceptions to the exceptions.</p>
<div id="s0065">
<h3 id="st0065">Criteria for Choosing Tests</h3>
<p id="p0305" class="noindent">When we introduced the basic rule learner in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0065">Section 4.4</a>, we had to figure out a way of deciding which of many possible tests to add to a rule to prevent it from covering any negative examples. For this we used the test that maximizes the ratio <em>p</em>/<em>t</em>, where <em>t</em> is the total number of instances that the new rule will cover, and <em>p</em> is <a id="p204"></a>the number of these that are positive—that is, belong to the class in question. This attempts to maximize the “correctness” of the rule on the basis that the higher the proportion of positive examples it covers, the more correct a rule is. One alternative is to calculate an information gain:</p>
<p class="figure" id="e0030"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si5.jpg" alt="image" width="152" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si5.jpg"></p>
<p>where <em>p</em> and <em>t</em> are the number of positive instances and the total number of instances covered by the new rule, as before, and <em>P</em> and <em>T</em> are the corresponding number of instances that satisfied the rule <em>before</em> the new test was added. The rationale for this is that it represents the total information gained regarding the current positive examples, which is given by the number of them that satisfy the new test, multiplied by the information gained regarding each one.</p>
<p id="p0310" class="para_indented">The basic criterion for choosing a test to add to a rule is to find one that covers as many positive examples as possible while covering as few negative examples as possible. The original correctness-based heuristic, which is just the percentage of positive examples among all examples covered by the rule, attains a maximum when no negative examples are covered regardless of the number of positive examples covered by the rule. Thus, a test that makes the rule exact will be preferred to one that makes it inexact, no matter how few positive examples the former rule covers nor how many positive examples the latter covers. For example, if we consider a test that covers one example that is positive, this criterion will prefer it over a test that covers 1000 positive examples along with one negative one.</p>
<p id="p0315" class="para_indented">The information-based heuristic, on the other hand, places far more emphasis on covering a large number of positive examples regardless of whether the rule so created is exact. Of course, both algorithms continue adding tests until the final rule produced is exact, which means that the rule will be finished earlier using the correctness measure whereas more terms will have to be added if the information-based measure is used. Thus, the correctness-based measure might find special cases and eliminate them completely, saving the larger picture for later (when the more general rule might be simpler because awkward special cases have already been dealt with), whereas the information-based one will try to generate high-coverage rules first and leave the special cases until later. It is by no means obvious that either strategy is superior to the other at producing an exact rule set. Moreover, the whole situation is complicated by the fact that, as described in the following, rules may be pruned and inexact ones tolerated.</p>
</div>
<div id="s0070">
<h3 id="st0070">Missing Values, Numeric Attributes</h3>
<p id="p0320" class="noindent">As with divide-and-conquer decision tree algorithms, the nasty practical considerations of missing values and numeric attributes need to be addressed. In fact, there is not much more to say. Now that we know how these problems can be solved for decision tree induction, appropriate solutions for rule induction are easily given.</p>
<p id="p0325" class="para_indented"><a id="p205"></a>When producing rules using covering algorithms, missing values can be best treated as though they don’t match any of the tests. This is particularly suitable when a decision list is being produced, because it encourages the learning algorithm to separate out positive instances using tests that are known to succeed. It has the effect either that instances with missing values are dealt with by rules involving other attributes that are not missing, or that any decisions about them are deferred until most of the other instances have been taken care of, at which time tests will probably emerge that involve other attributes. Covering algorithms for decision lists have a decided advantage over decision tree algorithms in this respect: Tricky examples can be left until late in the process, at which time they will appear less tricky because most of the other examples have already been classified and removed from the instance set.</p>
<p id="p0330" class="para_indented">Numeric attributes can be dealt with in exactly the same way as they are dealt with for trees. For each numeric attribute, instances are sorted according to the attribute’s value and, for each possible threshold, a binary less-than/greater-than test is considered and evaluated in exactly the same way that a binary attribute would be.</p>
</div>
<div id="s0075">
<h3 id="st0075">Generating Good Rules</h3>
<p id="p0335" class="noindent">Suppose you don’t want to generate perfect rules that guarantee to give the correct classification on all instances in the training set, but would rather generate “sensible” ones that avoid overfitting the training set and thereby stand a better chance of performing well on new test instances. How do you decide which rules are worthwhile? How do you tell when it becomes counterproductive to continue adding terms to a rule to exclude a few pesky instances of the wrong type, all the while excluding more and more instances of the correct type?</p>
<p id="p0340" class="para_indented">Let’s look at a few examples of possible rules—some good and some bad—for the contact lens problem in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0010">Table 1.1</a>. Consider first the rule</p>
<p id="p0345" class="noindent"><span class="monospace">If astigmatism = yes and tear production rate = normal</span></p>
<p id="p0350" class="para_indented"><span class="monospace"> then recommendation = hard</span></p>
<p id="p0355" class="noindent">This gives a correct result for four out of the six cases that it covers; thus, its success fraction is 4/6. Suppose we add a further term to make the rule a “perfect” one:</p>
<p id="p0360" class="noindent"><span class="monospace">If astigmatism = yes and tear production rate = normal</span></p>
<p id="p0365" class="para_indented"><span class="monospace"> and age = young then recommendation = hard</span></p>
<p id="p0370" class="noindent">This improves accuracy to 2/2. Which rule is better? The second one is more accurate on the training data but covers only two cases, whereas the first one covers six. It may be that the second version is just overfitting the training data. For a practical rule learner we need a principled way of choosing the appropriate version of a rule, preferably one that maximizes accuracy on future test data.</p>
<p id="p0375" class="para_indented">Suppose we split the training data into two parts that we will call a <em>growing set</em> and a <em>pruning set</em>. The growing set is used to form a rule using the basic covering <a id="p206"></a>algorithm. Then a test is deleted from the rule, and the effect is evaluated by trying out the truncated rule on the pruning set and seeing whether it performs better than the original rule. This pruning process repeats until the rule cannot be improved by deleting any further tests. The whole procedure is repeated for each class, obtaining one best rule for each class, and the overall best rule is established by evaluating the rules on the pruning set. This rule is then added to the rule set, the instances it covers are removed from the training data—from both growing and pruning sets—and the process is repeated.</p>
<p id="p0380" class="para_indented">Why not do the pruning as we build up the rule, rather than building up the whole thing and then throwing parts away? That is, why not preprune rather than postprune? Just as when pruning decision trees it is often best to grow the tree to its maximum size and then prune back, so with rules it is often best to make a perfect rule and then prune it. Who knows?—adding that last term may make a really good rule, a situation that we might never have noticed had we adopted an aggressive prepruning strategy.</p>
<p id="p0385" class="para_indented">It is essential that the growing and pruning sets are separate because it is misleading to evaluate a rule on the very data that was used to form it: That would lead to serious errors by preferring rules that were overfitted. Usually the training set is split so that two-thirds of instances are used for growing and one-third for pruning. A disadvantage, of course, is that learning occurs from instances in the growing set only, so the algorithm might miss important rules because some key instances had been assigned to the pruning set. Moreover, the wrong rule might be preferred because the pruning set contains only one-third of the data and may not be completely representative. These effects can be ameliorated by resplitting the training data into growing and pruning sets at each cycle of the algorithm—that is, after each rule is finally chosen.</p>
<p id="p0390" class="para_indented">The idea of using a separate pruning set for pruning—which is applicable to decision trees as well as rule sets—is called <em>reduced-error pruning</em>. The variant previously described prunes a rule immediately after it has been grown; it is called <em>incremental reduced-error pruning</em>. Another possibility is to build a full, unpruned, rule set first, pruning it afterwards by discarding individual tests. However, this method is much slower.</p>
<p id="p0395" class="para_indented">Of course, there are many different ways to assess the worth of a rule based on the pruning set. A simple measure is to consider how well the rule would do at discriminating the predicted class from other classes if it were the only rule in the theory, operating under the closed-world assumption. Suppose it gets <em>p</em> instances right out of the <em>t</em> instances that it covers, and there are <em>P</em> instances of this class out a total of <em>T</em> instances altogether. The instances that it does not cover include <em>N</em> – <em>n</em> negative ones, where <em>n</em> = <em>t</em> – <em>p</em> is the number of negative instances that the rule covers and <em>N</em> = <em>T</em> – <em>P</em> is the total number of negative instances. Thus, in total the rule makes correct decisions on <em>p</em> + (<em>N</em> – <em>n</em>) instances, and so has an overall success ratio of</p>
<p class="figure" id="e0035"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si6.jpg" alt="image" width="135" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si6.jpg"></p>
<p></p>
<p id="p0400" class="noindent"><a id="p207"></a>This quantity, evaluated on the test set, has been used to evaluate the success of a rule when using reduced-error pruning.</p>
<p id="p0405" class="para_indented">This measure is open to criticism because it treats noncoverage of negative examples as being as important as coverage of positive ones, which is unrealistic in a situation where what is being evaluated is one rule that will eventually serve alongside many others. For example, a rule that gets <em>p</em> = 2000 instances right out of a total coverage of 3000 (i.e., it gets <em>n</em> = 1000 wrong) is judged as more successful than one that gets <em>p</em> = 1000 out of a total coverage of 1001 (i.e., <em>n</em> = 1 wrong), because [<em>p</em> + (<em>N</em> – <em>n</em>)]/<em>T</em> is [1000 + <em>N</em>]/<em>T</em> in the first case but only [999 + <em>N</em>]/<em>T</em> in the second. This is counterintuitive: The first rule is clearly less predictive than the second because it has a 33.3% as opposed to only a 0.1% chance of being incorrect.</p>
<p id="p0410" class="para_indented">Using the success rate <em>p</em>/<em>t</em> as a measure, as was done in the original formulation of the covering algorithm (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0045">Figure 4.8</a>), is not the perfect solution either because it would prefer a rule that got a single instance right (<em>p</em> = 1) out of a total coverage of 1 (so <em>n</em> = 0) to the far more useful rule that got 1000 right out of 1001. Another heuristic that has been used is (<em>p</em> – <em>n</em>)/<em>t</em>, but that suffers from exactly the same problem because (<em>p</em> – <em>n</em>)/<em>t</em> = 2<em>p</em>/<em>t</em> – 1 and so the result, when comparing one rule with another, is just the same as with the success rate. It seems hard to find a simple measure of the worth of a rule that corresponds with intuition in all cases.</p>
<p id="p0415" class="para_indented">Whatever heuristic is used to measure the worth of a rule, the incremental reduced-error pruning algorithm is the same. A possible rule-learning algorithm based on this idea is given in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0020">Figure 6.3</a>. It generates a decision list, creating rules for each class in turn and choosing at each stage the best version of the rule according to its worth on the pruning data. The basic covering algorithm for rule generation (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0045">Figure 4.8</a>) is used to come up with good rules for each class, choosing conditions to add to the rule using the accuracy measure <em>p</em>/<em>t</em> that we described earlier.</p>
<p id="f0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-003-9780123748560.jpg" alt="image" width="514" height="221" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-003-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.3</span> Algorithm for forming rules by incremental reduced-error pruning.</p>
<p id="p0420" class="para_indented"><a id="p208"></a>This method has been used to produce rule-induction schemes that can process vast amounts of data and operate very quickly. It can be accelerated by generating rules for the classes in order rather than generating a rule for each class at every stage and choosing the best. A suitable ordering is the increasing order in which they occur in the training set so that the rarest class is processed first and the most common ones are processed later. Another significant speedup is obtained by stopping the whole process when a rule of sufficiently low accuracy is generated, so as not to spend time generating a lot of rules at the end with very small coverage. However, very simple terminating conditions (such as stopping when the accuracy for a rule is lower than the default accuracy for the class it predicts) do not give the best performance. One criterion that seems to work well is a rather complicated one based on the MDL principle, described later.</p>
</div>
<div id="s0080">
<h3 id="st0080">Using Global Optimization</h3>
<p id="p0425" class="noindent">In general, rules generated using incremental reduced-error pruning in this manner seem to perform quite well, particularly on large datasets. However, it has been found that a worthwhile performance advantage can be obtained by performing a global optimization step on the set of rules induced. The motivation is to increase the accuracy of the rule set by revising or replacing individual rules. Experiments show that both the size and the performance of rule sets are significantly improved by postinduction optimization. On the other hand, the process itself is rather complex.</p>
<p id="p0430" class="para_indented">To give an idea of how elaborate—and heuristic—industrial-strength rule learners become, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0025">Figure 6.4</a> shows an algorithm called RIPPER, an acronym for <em>repeated incremental pruning to produce error reduction</em>. Classes are examined in increasing size and an initial set of rules for a class is generated using incremental reduced-error pruning. An extra stopping condition is introduced that depends on the description length of the examples and rule set. The description-length <em>DL</em> is a complex formula that takes into account the number of bits needed to send a set of examples with respect to a set of rules, the number of bits required to send a rule with <em>k</em> conditions, and the number of bits needed to send the integer <em>k</em>—times an arbitrary factor of 50% to compensate for possible redundancy in the attributes.</p><a id="p209"></a><p id="f0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-004a-9780123748560.jpg" alt="image" width="514" height="550" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-004a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-004b-9780123748560.jpg" alt="image" width="514" height="271" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-004b-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.4</span> RIPPER: (a) algorithm for rule learning and (b) meaning of symbols.</p>
<p id="p0435" class="para_indented">Having produced a rule set for the class, each rule is reconsidered and two variants produced, again using reduced-error pruning—but at this stage, instances covered by other rules for the class are removed from the pruning set, and success rate on the remaining instances is used as the pruning criterion. If one of the two variants yields a better description length, it replaces the rule. Next we reactivate the original building phase to mop up any newly uncovered instances of the class. A final check is made, to ensure that each rule contributes to the reduction of description length, before proceeding to generate rules for the next class.</p>
</div>
<div id="s0085">
<h3 id="st0085">Obtaining Rules from Partial Decision Trees</h3>
<p id="p0440" class="noindent">There is an alternative approach to rule induction that avoids global optimization but nevertheless produces accurate, compact rule sets. The method combines the divide-and-conquer strategy for decision tree learning with the separate-and-conquer <a id="p210"></a>one for rule learning. It adopts the separate-and-conquer strategy in that it builds a rule, removes the instances it covers, and continues creating rules recursively for the remaining instances until none are left. However, it differs from the standard approach in the way that each rule is created. In essence, to make a single rule a pruned decision tree is built for the current set of instances, the leaf with the largest coverage is made into a rule, and the tree is discarded.</p>
<p id="p0445" class="para_indented">The prospect of repeatedly building decision trees only to discard most of them is not as bizarre as it first seems. Using a pruned tree to obtain a rule instead of pruning a rule incrementally by adding conjunctions one at a time avoids a tendency to overprune, which is a characteristic problem of the basic separate-and-conquer rule learner. Using the separate-and-conquer methodology in conjunction with decision trees adds flexibility and speed. It is indeed wasteful to build a full decision tree just to obtain a single rule, but the process can be accelerated significantly without sacrificing the advantages.</p>
<p id="p0450" class="para_indented">The key idea is to build a partial decision tree instead of a fully explored one. A partial decision tree is an ordinary decision tree that contains branches to undefined subtrees. To generate such a tree, the construction and pruning operations are integrated in order to find a “stable” subtree that can be simplified no further. Once this subtree has been found, tree building ceases and a single rule is read off.</p>
<p id="p0455" class="para_indented">The tree-building algorithm is summarized in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0030">Figure 6.5</a>: It splits a set of instances recursively into a partial tree. The first step chooses a test and divides the instances into subsets accordingly. The choice is made using the same information-gain heuristic that is normally used for building decision trees (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0045">Section 4.3</a>). Then the subsets are expanded in increasing order of their average entropy. The reason for this is that the later subsets will most likely not end up being expanded, and a subset with low-average entropy is more likely to result in a small subtree and therefore produce a more general rule. This proceeds recursively until a subset is expanded into a leaf, and then continues further by backtracking. But as soon as an internal node appears that has all its children expanded into leaves, the algorithm checks whether that node is better replaced by a single leaf. This is just the standard subtree replacement <a id="p211"></a>operation of decision tree pruning (see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>). If replacement is performed the algorithm backtracks in the standard way, exploring siblings of the newly replaced node. However, if during backtracking a node is encountered all of whose children expanded so far are not leaves—and this will happen as soon as a potential subtree replacement is <em>not</em> performed—then the remaining subsets are left unexplored and the corresponding subtrees are left undefined. Due to the recursive structure of the algorithm, this event automatically terminates tree generation.</p>
<p id="f0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-005-9780123748560.jpg" alt="image" width="514" height="158" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-005-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.5</span> Algorithm for expanding examples into a partial tree</p>
<p id="p0460" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0035">Figure 6.6</a> shows a step-by-step example. During the stages in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0035">Figure 6.6(a–c)</a>, tree building continues recursively in the normal way—except that at each point the lowest-entropy sibling is chosen for expansion: node 3 between stages (a) and (b). Gray elliptical nodes are as yet unexpanded; rectangular ones are leaves. Between stages (b) and (c), the rectangular node will have lower entropy than its sibling, node 5, but cannot be expanded further because it is a leaf. Backtracking occurs and node 5 is chosen for expansion. Once stage of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0035">Figure 6.6(c)</a> is reached, there is a node—node 5—that has all its children expanded into leaves, and this triggers pruning. Subtree replacement for node 5 is considered and accepted, leading to stage (d). Next node 3 is considered for subtree replacement, and this operation is again accepted. Backtracking continues, and node 4, having lower entropy than node 2, is expanded into two leaves. Now subtree replacement is considered for node 4, but suppose that node 4 is not replaced. At this point, the process terminates with the three-leaf partial tree of stage (e).</p>
<p id="f0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-006ae-9780123748560.jpg" alt="image" width="514" height="344" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-006ae-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.6</span> Example of building a partial tree.</p>
<p id="p0465" class="para_indented"><a id="p212"></a>If the data is noise-free and contains enough instances to prevent the algorithm from doing any pruning, just one path of the full decision tree has to be explored. This achieves the greatest possible performance gain over the naïve method that builds a full decision tree each time. The gain decreases as more pruning takes place. For datasets with numeric attributes, the asymptotic time complexity of the algorithm is the same as building the full decision tree because in this case the complexity is dominated by the time required to sort the attribute values in the first place.</p>
<p id="p0470" class="para_indented">Once a partial tree has been built, a single rule is extracted from it. Each leaf corresponds to a possible rule, and we seek the “best” leaf of those subtrees (typically a small minority) that have been expanded into leaves. Experiments show that it is best to aim at the most general rule by choosing the leaf that covers the greatest number of instances.</p>
<p id="p0475" class="para_indented">When a dataset contains missing values, they can be dealt with exactly as they are when building decision trees. If an instance cannot be assigned to any given branch because of a missing attribute value, it is assigned to each of the branches with a weight proportional to the number of training instances going down that branch, normalized by the total number of training instances with known values at the node. During testing, the same procedure is applied separately to each rule, thus associating a weight with the application of each rule to the test instance. That weight is deducted from the instance’s total weight before it is passed to the next rule in the list. Once the weight has reduced to 0, the predicted class probabilities are combined into a final classification according to the weights.</p>
<p id="p0480" class="para_indented">This yields a simple but surprisingly effective method for learning decision lists for noisy data. Its main advantage over other comprehensive rule-generation schemes is simplicity, because other methods appear to require a complex global optimization stage to achieve the same level of performance.</p>
</div>
<div id="s0090">
<h3 id="st0090">Rules with Exceptions</h3>
<p id="p0485" class="noindent">In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a> (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#p73">page 73</a>) we learned that a natural extension of rules is to allow them to have exceptions, and exceptions to the exceptions, and so on—indeed, the whole rule set can be considered as exceptions to a default classification rule that is used when no other rules apply. The method of generating a “good” rule, using one of the measures described previously, provides exactly the mechanism needed to generate rules with exceptions.</p>
<p id="p0490" class="para_indented">First, a default class is selected for the top-level rule: It is natural to use the class that occurs most frequently in the training data. Then, a rule is found pertaining to any class other than the default one. Of all such rules it is natural to seek the one with the most discriminatory power—for example, the one with the best evaluation on a test set. Suppose this rule has the form</p>
<p id="p0495" class="noindent"><span class="monospace">if &lt;condition&gt; then class = &lt;new class&gt;</span></p>
<p id="p0500" class="para_indented">It is used to split the training data into two subsets: one containing instances for which the rule’s condition is <em>true</em> and the other containing those for which it is <em>false</em>. <a id="p213"></a>If either subset contains instances of more than one class, the algorithm is invoked recursively on that subset. For the subset for which the condition is <em>true</em>, the “default class” is the new class as specified by the rule; for the subset where the condition is <em>false</em>, the default class remains as it was before.</p>
<p id="p0505" class="para_indented">Let’s examine how this algorithm would work for the rules with exceptions that were given in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a> for the iris data of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0025">Table 1.4</a>. We will represent the rules in the graphical form shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0040">Figure 6.7</a>, which is in fact equivalent to the textual rules noted in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#f0045">Figure 3.8</a>. The default of <em>Iris setosa</em> is the entry node at the top left. Horizontal, dotted paths show exceptions, so the next box, which contains a rule that concludes <em>Iris versicolor</em>, is an exception to the default. Below this is an alternative, a second exception—alternatives are shown by vertical, solid lines—leading to the conclusion <em>Iris virginica</em>. Following the upper path horizontally leads to an exception to the <em>Iris versicolor</em> rule that overrides it whenever the condition in the top right box holds, with the conclusion <em>Iris virginica</em>. Below this is an alternative, leading (as it happens) to the same conclusion. Returning to the box at bottom center, this has its own exception, the lower right box, which gives the conclusion <em>Iris versicolor</em>. The numbers at the lower right of each box give the “coverage” of the rule, expressed as the number of examples that satisfy it divided by the number that satisfy its condition but not its conclusion. For example, the condition in the top center box applies to 52 of the examples, and 49 of them are <em>Iris versicolor</em>. The strength of this representation is that you can get a very good feeling for the effect of the rules from the boxes toward the left side; the boxes at the right cover just a few exceptional cases.</p>
<p id="f0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-007-9780123748560.jpg" alt="image" width="477" height="280" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-007-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.7</span> Rules with exceptions for the iris data.</p>
<p id="p0510" class="para_indented">To create these rules, the default is first set to <em>Iris setosa</em> by taking the most frequently occurring class in the dataset. This is an arbitrary choice because, for this <a id="p214"></a>dataset, all classes occur exactly 50 times; as shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0040">Figure 6.7</a> this default “rule” is correct in 50 out of 150 cases. Then the best rule that predicts another class is sought. In this case it is</p>
<p id="p0515" class="noindent"><span class="monospace">if petal-length ≥ 2.45 and petal-length &lt; 5.355</span></p>
<p id="p0520" class="para_indented"><span class="monospace"> and petal-width &lt; 1.75 then Iris-versicolor</span></p>
<p id="p0525" class="noindent">This rule covers 52 instances, of which 49 are <em>Iris versicolor</em>. It divides the dataset into two subsets: the 52 instances that satisfy the condition of the rule and the remaining 98 that do not.</p>
<p id="p0530" class="para_indented">We work on the former subset first. The default class for these instances is <em>Iris versicolor</em>: There are only three exceptions, all of which happen to be <em>Iris virginica</em>. The best rule for this subset that does not predict <em>Iris versicolor</em> is</p>
<p id="p0535" class="noindent"><span class="monospace">if petal-length ≥ 4.95 and petal-width &lt; 1.55 then Iris-virginica</span></p>
<p id="p0540" class="noindent">It covers two of the three <em>Iris virginicas</em> and nothing else. Again, it divides the subset into two: those instances that satisfy its condition and those that do not. Fortunately, in this case, all those instances that satisfy the condition do indeed have class <em>Iris virginica</em>, so there is no need for a further exception. However, the remaining instances still include the third <em>Iris virginica</em>, along with 49 <em>Iris versicolors</em>, which are the default at this point. Again the best rule is sought:</p>
<p id="p0545" class="noindent"><span class="monospace">if sepal-length &lt; 4.95 and sepal-width ≥ 2.45 then Iris-virginica</span></p>
<p id="p0550" class="noindent">This rule covers the remaining <em>Iris virginica</em> and nothing else, so it also has no exceptions. Furthermore, all remaining instances in the subset that do not satisfy its condition have the class <em>Iris versicolor</em>, which is the default, so no more needs to be done.</p>
<p id="p0555" class="para_indented">Return now to the second subset created by the initial rule, the instances that do not satisfy the condition</p>
<p id="p0560" class="noindent"><span class="monospace">petal-length ≥ 2.45 and petal-length &lt; 5.355 and petal-width &lt; 1.75</span></p>
<p id="p0565" class="noindent">Of the rules for these instances that do not predict the default class <em>Iris setosa</em>, the best is</p>
<p id="p0570" class="noindent"><span class="monospace">if petal-length ≥ 3.35 then Iris-virginica</span></p>
<p id="p0575" class="noindent">It covers all 47 <em>Iris virginicas</em> that are in the example set (3 were removed by the first rule, as explained previously). It also covers 1 <em>Iris versicolor</em>. This needs to be taken care of as an exception, by the final rule:</p>
<p id="p0580" class="noindent"><span class="monospace">if petal-length &lt; 4.85 and sepal-length &lt; 5.95 then Iris-versicolor</span></p>
<p id="p0585" class="noindent">Fortunately, the set of instances that do <em>not</em> satisfy its condition are all the default, <em>Iris setosa</em>. Thus, the procedure is finished.</p>
<p id="p0590" class="para_indented">The rules that are produced have the property that most of the examples are covered by the high-level rules and the lower-level ones really do represent exceptions. For example, the last exception clause and the deeply nested <em>else</em> clause both <a id="p215"></a>cover a solitary example, and removing them would have little effect. Even the remaining nested exception rule covers only two examples. Thus, one can get an excellent feeling for what the rules do by ignoring all the deeper structure and looking only at the first level or two. That is the attraction of rules with exceptions.</p>
</div>
<div id="s0095">
<h3 id="st0095">Discussion</h3>
<p id="p0595" class="noindent">All algorithms for producing classification rules that we have described use the basic covering or separate-and-conquer approach. For the simple, noise-free case this produces PRISM (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib53">Cendrowska, 1987</a>), an algorithm that is simple and easy to understand. When applied to two-class problems with the closed-world assumption, it is only necessary to produce rules for one class: Then the rules are in disjunctive normal form and can be executed on test instances without any ambiguity arising. When applied to multiclass problems, a separate rule set is produced for each class; thus, a test instance may be assigned to more than one class, or to no class, and further heuristics are necessary if a unique prediction is sought.</p>
<p id="p0600" class="para_indented">To reduce overfitting in noisy situations, it is necessary to produce rules that are not “perfect” even on the training set. To do this it is necessary to have a measure for the “goodness,” or worth, of a rule. With such a measure it is then possible to abandon the class-by-class approach of the basic covering algorithm and start by generating the very best rule, regardless of which class it predicts, and then remove all examples covered by this rule and continue the process. This yields a method for producing a decision list rather than a set of independent classification rules, and decision lists have the important advantage that they do not generate ambiguities when interpreted.</p>
<p id="p0605" class="para_indented">The idea of incremental reduced-error pruning is from <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib129">Fürnkranz and Widmer (1994)</a> and forms the basis for fast and effective rule induction. The RIPPER rule learner is from <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib63">Cohen (1995)</a>, although the published description appears to differ from the implementation in precisely how the description length (DL) affects the stopping condition. What we have presented here is the basic idea of the algorithm; there are many more details in the implementation.</p>
<p id="p0610" class="para_indented">The whole question of measuring the value of a rule has not yet been satisfactorily resolved. Many different measures have been proposed, some blatantly heuristic and others based on information-theoretical or probabilistic grounds. However, there seems to be no consensus on the best measure to use. An extensive theoretical study of various criteria has been performed by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib128">Fürnkranz and Flach (2005)</a>.</p>
<p id="p0615" class="para_indented">The rule-learning scheme based on partial decision trees was developed by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib112">Frank and Witten (1998)</a>. On standard benchmark datasets it produces rule sets that are as accurate as rules generated by C4.5 and more accurate than those of RIPPER; however, it produces larger rule sets than RIPPER. Its main advantage over other schemes is not performance but simplicity: By combining top-down decision tree induction with separate-and-conquer rule learning, it produces good rule sets without any need for global optimization.</p>
<p id="p0620" class="para_indented"><a id="p216"></a>The procedure for generating rules with exceptions was developed as an option in the Induct system by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib130">Gaines and Compton (1995)</a>, who called them <em>ripple-down</em> rules. In an experiment with a large medical dataset (22,000 instances, 32 attributes, and 60 classes), they found that people can understand large systems of rules with exceptions more readily than equivalent systems of regular rules because that is the way they think about the complex medical diagnoses that are involved. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib264">Richards and Compton (1998)</a> describe their role as an alternative to classic knowledge engineering.</p>
</div>
</div>
<div id="s0100">
<h2 id="st0100">6.3 Association rules</h2>
<p id="p0625" class="noindent">In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a> we studied the Apriori algorithm for generating association rules that meet minimum support and confidence thresholds. Apriori follows a generate-and-test methodology for finding frequent item sets, generating successively longer candidate item sets from shorter ones that are known to be frequent. Each different size of candidate item set requires a scan through the dataset to determine whether its frequency exceeds the minimum support threshold. Although some improvements to the algorithm have been suggested to reduce the number of scans of the dataset, the combinatorial nature of this generation process can prove costly, particularly if there are many item sets or item sets are large. Both conditions readily occur even for modest datasets when low support thresholds are used. Moreover, no matter how high the threshold, if the data is too large to fit in main memory, it is undesirable to have to scan it repeatedly—and many association rule applications involve truly massive datasets.</p>
<p id="p0630" class="para_indented">These effects can be ameliorated by using appropriate data structures. We describe a method called FP-growth that uses an extended prefix tree—a frequent-pattern tree, or FP-tree—to store a compressed version of the dataset in main memory. Only two passes are needed to map a dataset into an FP-tree. The algorithm then processes the tree in a recursive fashion to grow large item sets directly, instead of generating candidate item sets and then having to test them against the entire database.</p>
<div id="s0105">
<h3 id="st0105">Building a Frequent-Pattern Tree</h3>
<p id="p0635" class="noindent">Like Apriori, the FP-growth algorithm begins by counting the number of times individual items (i.e., attribute–value pairs) occur in the dataset. After this initial pass, a tree structure is created in a second pass. Initially, the tree is empty and the structure emerges as each instance in the dataset is inserted into it.</p>
<p id="p0640" class="para_indented">The key to obtaining a compact tree structure that can be quickly processed to find large item sets is to sort the items in each instance in descending order of their frequency of occurrence in the dataset, which has already been recorded in the first pass, before inserting them into the tree. Individual items in each instance that do not meet the minimum support threshold are not inserted into the tree, effectively removing them from the dataset. The hope is that many instances will share those <a id="p217"></a>items that occur most frequently individually, resulting in a high degree of compression close to the tree’s root.</p>
<p id="p0645" class="para_indented">We illustrate the process with the weather data, reproduced in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(a)</a>, using a minimum support threshold of 6. The algorithm is complex, and its complexity far exceeds what would be reasonable for such a trivial example, but a small illustration is the best way of explaining it. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(b)</a> shows the individual items, with their frequencies, that are collected in the first pass. They are sorted into descending order and ones whose frequency exceeds the minimum threshold are bolded. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(c)</a> shows the original instances, numbered as in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(a)</a>, with the items in each instance sorted into descending frequency order. Finally, to give <a id="p218"></a>an advance peek at the final outcome, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(d)</a> shows the only two multiple-item sets whose frequency satisfies the minimum support threshold. Along with the six single-item sets shown in bold in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(b)</a>, these form the final answer: a total of eight item sets. We are going to have to do a lot of work to find the two multiple-item sets in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(d)</a> using the FP-tree method.</p>
<p class="table_caption"><span class="tab_num">Table 6.1. </span> Preparing Weather Data for Insertion into an FP-Tree</p>
<p id="t0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0025.jpg" alt="Image" width="561" height="593" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0025.jpg"></p>
<p id="t0025a" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0025a.jpg" alt="Image" width="561" height="636" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0025a.jpg"></p>
<p class="table_legend">(a) The original data, (b) frequency ordering of items with frequent item sets in bold, (c) the data with each instance sorted into frequency order, and (d) the two multiple-item frequent item sets.</p>
<p id="p0650" class="para_indented"><a id="p219"></a><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a> shows the FP-tree structure that results from this data with a minimum support threshold of 6. The tree itself is shown with solid arrows. The numbers at each node show how many times the sorted prefix of items, up to and including the item at that node, occur in the dataset. For example, following the third branch from the left in the tree we can see that, after sorting, two instances begin with the prefix <em>humidity = high</em>—that is, the second and last instances of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(c)</a>. Continuing down that branch, the next node records that the same two instances also have <em>windy = true</em> as their next most frequent item. The lowest node in the branch shows that one of these two instances—that is, the last in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(c)</a>—contains <em>temperature = mild</em> as well. The other instance—that is, the second in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(c)</a>—drops out at this stage because its next most frequent item does not meet the minimum support constraint and is therefore omitted from the tree.</p><a id="p220"></a><p id="f0045" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-008a-9780123748560.jpg" alt="image" width="750" height="246" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-008a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-008bc-9780123748560.jpg" alt="image" width="800" height="450" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-008bc-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.8</span><a id="p221"></a>Extended prefix trees for the weather data: (a) the full data, (b) the data conditional on <em>temperature = mild</em>, and (c) the data conditional on <em>humidity = normal.</em></p>
<p id="p0655" class="para_indented">On the left side of the diagram a “header table” shows the frequencies of the individual items in the dataset (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(b)</a>). These items appear in descending frequency order, and only those with at least minimum support are included. Each item in the header table points to its first occurrence in the tree, and subsequent items in the tree with the same name are linked together to form a list. These lists, emanating from the header table, are shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a> by dashed arrows.</p>
<p id="p0660" class="para_indented">It is apparent from the tree that only two nodes have counts that satisfy the minimum support threshold, corresponding to the item sets <em>play = yes</em> (count of 9) and <em>play = yes</em> and <em>windy = false</em> (count of 6) in the leftmost branch. Each entry in the header table is itself a single-item set that also satisfies the threshold. This identifies as part of the final answer all the bold items in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(b)</a> and the first item set in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(d)</a>. Since we know the outcome in advance we can see that there is only one more item set to go—the second in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(d)</a>. But there is no hint of it in the data structure of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a>, and we will have to do a lot of work to discover it!</p>
</div>
<div id="s0110">
<h3 id="st0110">Finding Large Item Sets</h3>
<p id="p0665" class="noindent">The purpose of the links from the header table into the tree structure is to facilitate traversal of the tree to find other large item sets, apart from the two that are already in the tree. This is accomplished by a divide-and-conquer approach that recursively processes the tree to grow large item sets. Each header table list is followed in turn, starting from the bottom of the table and working upward. Actually, the header table can be processed in any order, but it is easier to think about processing the longest paths in the tree first, and these correspond to the lower-frequency items.</p>
<p id="p0670" class="para_indented">Starting from the bottom of the header table, we can immediately add <em>temperature = mild</em> to the list of large item sets. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(b)</a> shows the result of the next stage, which is an FP-tree for just those instances in the dataset that include <em>temperature = mild</em>. <a id="p222"></a>This tree was not created by rescanning the dataset but by further processing of the tree in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a>, as follows.</p>
<p id="p0675" class="para_indented">To see if a larger item set containing <em>temperature = mild</em> can be grown, we follow its link from the header table. This allows us to find all instances that contain <em>temperature = mild</em>. From here the new tree in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(b)</a> is created, with counts projected from the original tree corresponding to the set of instances that are conditional on the presence of <em>temperature = mild</em>. This is done by propagating the counts from the <em>temperature = mild</em> nodes up the tree, each node receiving the sum of its children’s counts.</p>
<p id="p0680" class="para_indented">A quick glance at the header table for this new FP-tree shows that the <em>temperature = mild</em> pattern cannot be grown any larger because there are no individual items, conditional on <em>temperature = mild</em>, that meet the minimum support threshold. Note, however, that it is necessary to create the whole <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(b)</a> tree in order to discover this because it is effectively being created bottom up and the counts in the header table to the left are computed from the numbers in the tree. The recursion exits at this point, and processing continues on the remaining header table items in the original FP-tree.</p>
<p id="p0685" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(c)</a> shows a second example, the FP-tree that results from following the header table link for <em>humidity = normal</em>. Here the <em>windy = false</em> node has a count of 4, corresponding to the four instances that had <em>humidity = normal</em> in the node’s left branch in the original tree. Similarly, <em>play = yes</em> has a count of 6, corresponding to the four instances from <em>windy = false</em> and the two instances that contain <em>humidity = normal</em> from the middle branch of the subtree rooted at <em>play</em> = <em>yes</em> in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a>.</p>
<p id="p0690" class="para_indented">Processing the header list for this FP-tree shows that the <em>humidity = normal</em> item set can be grown to include <em>play = yes</em> since these two occur together six times, which meets the minimum support constraint. This corresponds to the second item set in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0025">Table 6.1(d)</a>, which in fact completes the output. However, in order to be sure that there are no other eligible item sets it is necessary to continue processing the entire header link table in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0045">Figure 6.8(a)</a>.</p>
<p id="p0695" class="para_indented">Once the recursive tree mining process is complete all large item sets that meet the minimum support threshold have been found. Then association rules are created using the approach explained in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>. Studies have claimed that the FP-growth algorithm is up to an order of magnitude faster than Apriori at finding large item sets, although this depends on the details of the implementation and the nature of the dataset.</p>
</div>
<div id="s0115">
<h3 id="st0115">Discussion</h3>
<p id="p0700" class="noindent">The process of recursively creating projected FP-trees can be efficiently implemented within a single prefix tree structure by having a list of frequencies, indexed by recursion depth, at each node in the tree and each element of the header table. The tree structure itself is usually far smaller than the original dataset, and if the dataset is dense it achieves a high level of compression. This outweighs the overhead imposed by the pointers and counters that must be maintained at each node. Only when the support threshold is set very low does the FP-tree’s ability to compress the dataset degrade. Under these conditions, the tree becomes bushy, with little node sharing. On massive datasets for which the frequent-pattern tree exceeds main memory, <a id="p223"></a>disk-resident trees can be constructed using indexing techniques that have been developed for relational database systems.</p>
<p id="p0705" class="para_indented">The FP-tree data structure and FP-growth algorithm for finding large item sets without candidate generation were introduced by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib149">Han et al. (2000)</a> following pioneering work by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib333">Zaki et al. (1997)</a>; <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib150">Han et al. (2004)</a> give a more comprehensive description. It has been extended in various ways. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib306">Wang et al. (2003)</a> develop an algorithm called CLOSET+ to mine closed item sets—that is, sets for which there is no proper superset that has the same support. Finding large closed item sets provides essentially the same information as finding the complete set of large item sets, but produces few redundant rules and thus eases the task that users face when examining the output of the mining process. GSP (Generalized Sequential Patterns) is a method based on the Apriori algorithm for mining patterns in databases of event sequences (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib288">Srikant and Agrawal, 1996</a>). A similar approach to FP-growth is used for event sequences by algorithms called PrefixSpan (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib249">Pei et al., 2004</a>) and CloSpan (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib328">Yan et al., 2003</a>), and for graph patterns by algorithms called gSpan (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib326">Yan and Han, 2002</a>) and CloseGraph (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib327">Yan and Han, 2003</a>).</p>
<p id="p0710" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib52">Ceglar and Roddick (2006)</a> provide a comprehensive survey of association rule mining. Some authors have worked on integrating association rule mining with classification. For example, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib211">Liu et al. (1998)</a> mine a kind of association rule that they call a “class association rule,” and build a classifier on the rules that are found using a technique they call CBA (Classification Based on Associations). <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib235">Mutter et al. (2004)</a> use classification to evaluate the output of confidence-based association rule mining, and find that standard learners for classification rules are generally preferable to CBA when runtime and size of the rule sets is of concern.</p>
</div>
</div>
<div id="s0120">
<h2 id="st0120">6.4 Extending linear models</h2>
<p id="p0715" class="noindent"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> described how simple linear models can be used for classification in situations where all attributes are numeric. Their biggest disadvantage is that they can only represent linear boundaries between classes, which makes them too simple for many practical applications. Support vector machines use linear models to implement nonlinear class boundaries. (Although it is a widely used term, <em>support vector machines</em> is something of a misnomer: These are algorithms, not machines.) How can this be possible? The trick is easy: Transform the input using a nonlinear mapping. In other words, transform the instance space into a new space. With a nonlinear mapping, a straight line in the new space doesn’t look straight in the original instance space. A linear model constructed in the new space can represent a nonlinear decision boundary in the original space.</p>
<p id="p0720" class="para_indented">Imagine applying this idea directly to the ordinary linear models in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>. For example, the original set of attributes could be replaced by one giving all products of <em>n</em> factors that can be constructed from these attributes. An example for two attributes, including all products with three factors, is</p>
<p class="figure" id="e0040"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si7.jpg" alt="image" width="317" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si7.jpg"></p>
<p></p>
<p id="p0725" class="para_indented"><a id="p224"></a>Here, <em>x</em> is the outcome, <em>a</em>
<span class="sub">1</span> and <em>a</em>
<span class="sub">2</span> are the two attribute values, and there are four weights <em>w<span class="sub">i</span>
</em> to be learned. As described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, the result can be used for classification by training one linear system for each class and assigning an unknown instance to the class that gives the greatest output <em>x</em>—the standard technique of multiresponse linear regression. Then, <em>a</em>
<span class="sub">1</span> and <em>a</em>
<span class="sub">2</span> will be the attribute values for the test instance.</p>
<p id="p0730" class="para_indented">To generate a linear model in the space that is spanned by these products, each training instance is mapped into the new space by computing all possible three-factor products of its two attribute values. The learning algorithm is then applied to the transformed instances. To classify an instance, it is processed by the same transformation prior to classification. There is nothing to stop us from adding in more synthetic attributes. For example, if a constant term were included, the original attributes and all two-factor products of them would yield a total of 10 weights to be learned. (Alternatively, adding an additional attribute with a value that was always a constant would have the same effect.) Indeed, polynomials of sufficiently high degree can approximate arbitrary decision boundaries to any required accuracy.</p>
<p id="p0735" class="para_indented">It seems too good to be true—and it is. As you will probably have guessed, problems arise with this procedure due to the large number of coefficients introduced by the transformation in any realistic setting. The first snag is computational complexity. With 10 attributes in the original dataset, suppose we want to include all products with five factors: then the learning algorithm will have to determine more than 2000 coefficients. If its runtime is cubic in the number of attributes, as it is for linear regression, training will be infeasible. That is a problem of practicality. The second problem is one of principle: overfitting. If the number of coefficients is large relative to the number of training instances, the resulting model will be “too nonlinear”—it will overfit the training data. There are just too many parameters in the model.</p>
<div id="s0125">
<h3 id="st0125">Maximum-Margin Hyperplane</h3>
<p id="p0740" class="noindent">Support vector machines address both problems. They are based on an algorithm that finds a special kind of linear model: the <em>maximum-margin hyperplane</em>. We already know what a hyperplane is—it’s just another term for a linear model. To visualize a maximum-margin hyperplane, imagine a two-class dataset whose classes are linearly separable—that is, there is a hyperplane in instance space that classifies all training instances correctly. The maximum-margin hyperplane is the one that gives the greatest separation between the classes—it comes no closer to either than it has to. An example is shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0050">Figure 6.9</a>, where the classes are represented by open and filled circles, respectively. Technically, the <em>convex hull</em> of a set of points is the tightest enclosing convex polygon: Its outline emerges when you connect every point of the set to every other point. Because we have supposed that the two classes are linearly separable, their convex hulls cannot overlap. Among all hyperplanes that separate the classes, the maximum-margin hyperplane is the one that is as far as <a id="p225"></a>possible from both convex hulls—it is the perpendicular bisector of the shortest line connecting the hulls (shown dashed in the figure).</p>
<p id="f0050" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-009-9780123748560.jpg" alt="image" width="701" height="562" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-009-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.9</span> A maximum-margin hyperplane.</p>
<p id="p0745" class="para_indented">The instances that are closest to the maximum-margin hyperplane—the ones with the minimum distance to it—are called <em>support vectors</em>. There is always at least one support vector for each class, and often there are more. The important thing is that the set of support vectors uniquely defines the maximum-margin hyperplane for the learning problem. Given the support vectors for the two classes, we can easily construct the maximum-margin hyperplane. All other training instances are irrelevant—they can be deleted without changing the position and orientation of the hyperplane.</p><a id="p0750"></a><div class="boxg" id="b0015">
<p id="p0755" class="noindent">A hyperplane separating the two classes might be written as</p>
<p class="figure" id="e0045"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si8.jpg" alt="image" width="152" height="27" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si8.jpg"></p>
<p>in the two-attribute case, where <em>a</em>
<span class="sub">1</span> and <em>a</em>
<span class="sub">2</span> are the attribute values and there are three weights <em>w<span class="sub">i</span>
</em> to be learned. However, the equation defining the maximum-margin hyperplane can be written in another form, in terms of the support vectors. Write the class value <em>y</em> of a training instance as either 1 (for <em>yes</em>, it is in this class) or –1 (for <em>no</em>, it is not). Then the maximum-margin hyperplane can be written as</p>
<p class="figure" id="e0050"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si9.jpg" alt="image" width="221" height="44" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si9.jpg"></p>
<p></p>
<p id="p0760" class="para_indented">Here, <em>y<span class="sub">i</span>
</em> is the class value of training instance <strong>a(i)</strong>, while <em>b</em> and <em>α<span class="sub">i</span>
</em> are numeric parameters that have to be determined by the learning algorithm. Note that <strong>a(i)</strong> and <strong>a</strong> are vectors. The vector <strong>a</strong> represents a test instance—just as the vector [<em>a</em>
<span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>] represented a test instance in the earlier formulation. The vectors <strong>a(i)</strong> are the support vectors, those circled in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0050">Figure 6.9</a>; they are selected members of the training set. The term <strong>a(i)</strong> • <strong>a</strong> represents the dot product of the test instance with one of the support vectors: <strong>a(i)</strong> • <strong>a</strong> = Σ<em>
<span class="sub">j</span> a</em>(<em>i</em>)<em>
<span class="sub">j</span>a<span class="sub">j</span>
</em>. If you are not familiar with dot product notation, you should still be able to understand the gist of what follows: Just think of <strong>a(i)</strong> as the whole set of attribute values for the <em>i</em>th support vector. Finally, <em>b</em> and <em>α<span class="sub">i</span>
</em> are parameters that determine the hyperplane, just as the weights <em>w</em>
<span class="sub">0</span>, <em>w</em>
<span class="sub">1</span>, and <em>w</em>
<span class="sub">2</span> are parameters that determine the hyperplane in the earlier formulation.</p>
<p id="p0765" class="para_indented">It turns out that finding the support vectors for the training instances and determining the parameters <em>b</em> and <em>α<span class="sub">i</span>
</em> belongs to a standard class of optimization problems known as <em>constrained quadratic optimization</em>. There are off-the-shelf software packages for solving these problems (see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib103">Fletcher, 1987</a>, for a comprehensive and practical account of solution methods). However, the computational complexity can be reduced, and learning accelerated, if special-purpose algorithms for training support vector machines are applied—but the details of these algorithms lie beyond the scope of this book (see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib251">Platt, 1998</a>).</p>
</div>
<p></p>
</div>
<div id="s0130">
<h3 id="st0130"><a id="p226"></a>Nonlinear Class Boundaries</h3>
<p id="p0770" class="noindent">We motivated the introduction of support vector machines by claiming that they can be used to model nonlinear class boundaries. However, so far we have only described the linear case. Consider what happens when an attribute transformation, as described before, is applied to the training data before determining the maximum-margin hyperplane. Recall that there are two problems with the straightforward application of such transformations to linear models: computational complexity on the one hand and overfitting on the other.</p>
<p id="p0775" class="para_indented">With support vectors, overfitting is unlikely to occur. The reason is that it is inevitably associated with instability: With an algorithm that overfits, changing one or two instance vectors will make sweeping changes to large sections of the decision boundary. But the maximum-margin hyperplane is relatively stable: It only moves if training instances are added or deleted that are support vectors—and this is true even in the high-dimensional space spanned by the nonlinear transformation. Overfitting is caused by too much flexibility in the decision boundary. The support vectors are global representatives of the whole set of training points, and there are usually few of them, which gives little flexibility. Thus, overfitting is less likely to occur.</p>
<p id="p0780" class="para_indented">What about computational complexity? This is still a problem. Suppose that the transformed space is a high-dimensional one so that the transformed support vectors and test instance have many components. According to the preceding equation, every time an instance is classified its dot product with all support vectors must be calculated. In the high-dimensional space produced by the nonlinear mapping this is rather expensive. Obtaining the dot product involves one multiplication and one addition for each attribute, and the number of attributes in the new space can be huge. This problem occurs not only during classification but also during training because the optimization algorithms have to calculate the same dot products very frequently. Fortunately, it turns out that it is possible to calculate the dot product <em>before</em> the nonlinear mapping is performed, on the original attribute set, using a so-called kernel function based on the dot product.</p><a id="p0785"></a><div class="boxg" id="b0020">
<p id="p0790" class="noindent">A high-dimensional version of the preceding equation is simply</p>
<p class="figure" id="e0055"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si10.jpg" alt="image" width="179" height="42" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si10.jpg"></p>
<p>where <em>n</em> is chosen as the number of factors in the transformation (three in the example we used earlier). If you expand the term (<strong>a(i)</strong> • <strong>a</strong>)<em><sup>n</sup></em>, you will find that it contains all the high-dimensional terms that would have been involved if the test and training vectors were first transformed by including all products of <em>n</em> factors and the dot product of the result was taken. (If you actually do the calculation, you will notice that some constant factors—binomial coefficients—are introduced. However, these do not matter: It is the dimensionality of the space that concerns us; the constants merely scale the axes.)</p>
<p id="p0795" class="para_indented">Because of this mathematical equivalence, the dot products can be computed in the original low-dimensional space, and the problem becomes feasible. In implementation terms, you take a software package for constrained quadratic optimization and every time <strong>a(i)</strong> • <strong>a</strong> is evaluated you evaluate (<strong>a(i)</strong> • <strong>a</strong>)<em>
<sup>n</sup>
</em> instead. It’s as simple as that because in both the optimization and the classification algorithms these vectors are only used in this dot product form. The training vectors, including the support vectors, and the test instance all remain in the original low-dimensional space throughout the calculations.</p>
</div>
<p></p>
<p id="p0800" class="para_indented"><a id="p227"></a>The function (<strong>x</strong> • <strong>y</strong>)<em>
<sup>n</sup>
</em>, which computes the dot product of two vectors <strong>x</strong> and <strong>y</strong> and raises the result to the power <em>n</em>, is called a <em>polynomial kernel.</em> A good way of choosing the value of <em>n</em> is to start with 1 (a linear model) and increment it until the estimated error ceases to improve. Usually, quite small values suffice. To include lower-order terms, we can use the kernel (<strong>x</strong> • <strong>y</strong> + 1)<em>
<sup>n</sup>
</em>.</p>
<p id="p0805" class="para_indented">Other kernel functions can be used instead to implement different nonlinear mappings. Two that are often suggested are the <em>radial basis function (RBF) kernel</em> and the <em>sigmoid kernel.</em> Which one produces the best results depends on the application, although the differences are rarely large in practice. It is interesting to note that a support vector machine with the RBF kernel is simply a type of neural network called an <em>RBF network</em> (which we describe later)<em>,</em> and one with the sigmoid kernel implements another type of neural network, a multilayer perceptron with one hidden layer (also described later).</p>
<p id="p0810" class="para_indented">Mathematically, any function <em>K</em>(<strong>x</strong>, <strong>y</strong>) is a kernel function if it can be written as <em>K</em>(<strong>x</strong>, <strong>y</strong>) = Φ(<strong>x</strong>) • Φ(<strong>y</strong>), where Φ is a function that maps an instance into a (potentially high-dimensional) feature space. In other words, the kernel function represents a dot product in the feature space created by Φ. Practitioners sometimes apply functions that are not proper kernel functions (the sigmoid kernel with certain parameter settings is an example). Despite the lack of theoretical guarantees, this can nevertheless produce accurate classifiers.</p>
<p id="p0815" class="para_indented">Throughout this section, we have assumed that the training data is linearly separable—either in the instance space or in the new space spanned by the nonlinear mapping. It turns out that support vector machines can be generalized to the case where the training data is not separable. This is accomplished by placing an upper bound on the coefficients <em>α<span class="sub">i</span>
</em>. Unfortunately, this parameter must be chosen by the user, and the best setting can only be determined by experimentation. Also, except in trivial cases it is not possible to determine a priori whether the data is linearly separable or not.</p>
<p id="p0820" class="para_indented">Finally, we should mention that compared with other methods such as decision tree learners, even the fastest training algorithms for support vector machines are slow when applied in the nonlinear setting. However, they often produce very accurate classifiers because subtle and complex decision boundaries can be obtained.</p>
</div>
<div id="s0135">
<h3 id="st0135">Support Vector Regression</h3>
<p id="p0825" class="noindent">The maximum-margin hyperplane concept only applies to classification. However, support vector machine algorithms have been developed for numeric prediction that share many of the properties encountered in the classification case: They <a id="p228"></a>produce a model that can usually be expressed in terms of a few support vectors and can be applied to nonlinear problems using kernel functions. As with regular support vector machines, we will describe the concepts involved, but will not attempt to describe the algorithms that actually perform the work.</p>
<p id="p0830" class="para_indented">As with linear regression, covered in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, the basic idea is to find a function that approximates the training points well by minimizing the prediction error. The crucial difference is that all deviations up to a user-specified parameter <em>ε</em> are simply discarded. Also, when minimizing the error, the risk of overfitting is reduced by simultaneously trying to maximize the flatness of the function. Another difference is that what is minimized is normally the predictions’ absolute error instead of the squared error used in linear regression. (There are, however, versions of the algorithm that use the squared error instead.)</p>
<p id="p0835" class="para_indented">A user-specified parameter <em>ε</em> defines a tube around the regression function in which errors are ignored: For linear support vector regression, the tube is a cylinder. If all training points can fit within a tube of width 2<em>ε</em>, the algorithm outputs the function in the middle of the flattest tube that encloses them. In this case the total perceived error is 0. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0055">Figure 6.10(a)</a> shows a regression problem with one attribute, <a id="p229"></a>a numeric class, and eight instances. In this case <em>ε</em> was set to 1, so the width of the tube around the regression function (indicated by dotted lines) is 2. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0055">Figure 6.10(b)</a> shows the outcome of the learning process when <em>ε</em> is set to 2. As you can see, the wider tube makes it possible to learn a flatter function.</p>
<p id="f0055" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-010ac-9780123748560.jpg" alt="image" width="512" height="416" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-010ac-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.10</span> Support vector regression: (a) <em>ε</em> = 1, (b) <em>ε</em> = 2, and (c) <em>ε</em> = 0.5.</p>
<p id="p0840" class="para_indented">The value of <em>ε</em> controls how closely the function will fit the training data. Too large a value will produce a meaningless predictor—in the extreme case, when 2<em>ε</em> exceeds the range of class values in the training data, the regression line is horizontal and the algorithm just predicts the mean class value. On the other hand, for small values of <em>ε</em> there may be no tube that encloses all the data. In that case, some training points will have nonzero error, and there will be a tradeoff between the prediction error and the tube’s flatness. In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0055">Figure 6.10(c)</a>, <em>ε</em> was set to 0.5 and there is no tube of width 1 that encloses all the data.</p>
<p id="p0845" class="para_indented">For the linear case, the support vector regression function can be written as</p>
<p class="figure" id="e0060"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si11.jpg" alt="image" width="240" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si11.jpg"></p>
<p></p>
<p id="p0850" class="para_indented">As with classification, the dot product can be replaced by a kernel function for nonlinear problems. The support vectors are all those points that do not fall strictly within the tube—that is, the points outside the tube and on its border. As with classification, all other points have coefficient 0 and can be deleted from the training data without changing the outcome of the learning process. In contrast to the classification case, the <em>α<span class="sub">i</span>
</em> may be negative.</p>
<p id="p0855" class="para_indented">We have mentioned that as well as minimizing the error, the algorithm simultaneously tries to maximize the flatness of the regression function. In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0055">Figures 6.10(a) and (b)</a>, where there is a tube that encloses all the training data, the algorithm simply outputs the flattest tube that does so. However, in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0055">Figure 6.10(c)</a>, there is no tube with error 0, and a tradeoff is struck between the prediction error and the tube’s flatness. This tradeoff is controlled by enforcing an upper limit <em>C</em> on the absolute value of the coefficients <em>α<span class="sub">i</span>
</em>. The upper limit restricts the influence of the support vectors on the shape of the regression function and is a parameter that the user must specify in addition to <em>ε</em>. The larger <em>C</em> is, the more closely the function can fit the data. In the degenerate case <em>ε</em> = 0, the algorithm simply performs least-absolute-error regression under the coefficient size constraint and all training instances become support vectors. Conversely, if <em>ε</em> is large enough that the tube can enclose all the data, the error becomes 0, there is no tradeoff to make, and the algorithm outputs the flattest tube that encloses the data irrespective of the value of <em>C</em>.</p>
</div>
<div id="s0140">
<h3 id="st0140">Kernel Ridge Regression</h3>
<p id="p0860" class="noindent"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a> introduced classic least-squares linear regression as a technique for predicting numeric quantities. In the previous section we saw how the powerful idea of support vector machines can be applied to regression and, furthermore, how nonlinear problems can be tackled by replacing the dot product in the support vector formulation by a kernel function—this is often known as the “kernel trick.” For <a id="p230"></a>classic linear regression using squared loss, only simple matrix operations are needed to find the model, but this is not the case for support vector regression with the user-specified loss parameter <em>ε</em>. It would be nice to combine the power of the kernel trick with the simplicity of standard least-squares regression. Kernel ridge regression does just that. In contrast to support vector regression, it does not ignore errors smaller than <em>ε</em>, and the squared error is used instead of the absolute error.</p><a id="p0865"></a><div class="boxg" id="b0025">
<p id="p0870" class="noindent">Instead of expressing the linear regression model’s predicted class value for a given test instance <strong>a</strong> as a weighted sum of the attribute values, as in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>, it can be expressed as a weighted sum over the dot products of each training instance <strong>a</strong><em><span class="sub">j</span></em> and the test instance in question:</p>
<p class="figure" id="e0065"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si12.jpg" alt="image" width="81" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si12.jpg"></p>
<p>where we assume that the function goes through the origin and an intercept is not required. This involves a coefficient <em>α<span class="sub">j</span>
</em> for each training instance, which resembles the situation with support vector machines—except that here <em>j</em> ranges over <em>all</em> instances in the training data, not just the support vectors. Again, the dot product can be replaced by a kernel function to yield a nonlinear model.</p>
<p id="p0875" class="para_indented">The sum of the squared errors of the model’s predictions on the training data is given by</p>
<p class="figure" id="e0070"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si13.jpg" alt="image" width="162" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si13.jpg"></p>
<p></p>
<p id="p0880" class="para_indented">This is the squared loss, just as in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>, and again we seek to minimize it by choosing appropriate <em>α<span class="sub">j</span>
</em>’s. But now there is a coefficient for each training instance, not just for each attribute, and most data sets have far more instances than attributes. This means that there is a serious risk of overfitting the training data when a kernel function is used instead of the dot product to obtain a nonlinear model.</p>
<p id="p0885" class="para_indented">That is where the <em>ridge</em> part of kernel ridge regression comes in. Instead of minimizing the squared loss, we trade closeness of fit against model complexity by introducing a penalty term:</p>
<p class="figure" id="e0075"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si14.jpg" alt="image" width="283" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si14.jpg"></p>
<p></p>
<p id="p0890" class="para_indented">The second sum penalizes large coefficients. This prevents the model from placing too much emphasis on individual training instances by giving them large coefficients, unless this yields a correspondingly large drop in error. The parameter <em>λ</em> controls the tradeoff between closeness of fit and model complexity. When matrix operations are used to solve for the coefficients of the model, the ridge penalty also has the added benefit of stabilizing degenerate cases. For this reason, it is often applied in standard least-squares linear regression as well.</p>
</div>
<p></p>
<p id="p0895" class="para_indented">Although kernel ridge regression has the advantage over support vector machines of computational simplicity, one disadvantage is that there is no sparseness in the vector of coefficients—in other words, no concept of “support vectors.” This makes a difference at prediction time because support vector machines have to sum only over the set of support vectors, not the entire training set.</p>
<p id="p0900" class="para_indented"><a id="p231"></a>In a typical situation with more instances than attributes, kernel ridge regression is more computationally expensive than standard linear regression, even when using the dot product rather than a kernel. This is because of the complexity of the matrix inversion operation used to find the model’s coefficient vector. Standard linear regression requires inverting an <em>m</em> × <em>m</em> matrix, which has complexity O(<em>m</em><sup>3</sup>), where <em>m</em> is the number of attributes in the data<em>.</em> Kernel ridge regression, on the other hand, involves an <em>n</em> × <em>n</em> matrix, with complexity O(<em>n</em><sup>3</sup>) where <em>n</em> is the number of instances in the training data. Nevertheless, it is advantageous to use kernel ridge regression in cases where a nonlinear fit is desired, or where there are more attributes than training instances.</p>
</div>
<div id="s0145">
<h3 id="st0145">Kernel Perceptron</h3>
<p id="p9000" class="noindent">In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> we introduced the perceptron algorithm for learning a linear classifier. It turns out that the kernel trick can also be used to upgrade this algorithm to learn nonlinear decision boundaries.</p><a id="p9005"></a><div class="boxg" id="b9000">
<p id="p0905" class="noindent">To see this, we first revisit the linear case. The perceptron algorithm repeatedly iterates through the training data instance by instance and updates the weight vector every time one of these instances is misclassified based on the weights learned so far. The weight vector is updated simply by adding or subtracting the instance’s attribute values to or from it. This means that the final weight vector is just the sum of the instances that have been misclassified. The perceptron makes its predictions based on whether</p>
<p class="figure" id="e0080"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si15.jpg" alt="image" width="60" height="33" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si15.jpg"></p>
<p>is greater or less than 0, where <em>w<span class="sub">i</span></em> is the weight for the <em>i</em>th attribute and <em>a<span class="sub">i</span></em> the corresponding attribute value of the instance that we wish to classify. Instead, we could use</p>
<p class="figure" id="e0085"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si16.jpg" alt="image" width="140" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si16.jpg"></p>
<p></p>
<p id="p0910" class="para_indented">Here, <strong>a′(j)</strong> is the <em>j</em>th misclassified training instance, <em>a</em>′(<em>j</em>)<em><span class="sub">i</span></em> its <em>i</em>th attribute value, and <em>y</em>(<em>j</em>) its class value (either +1 or –1). To implement this we no longer keep track of an explicit weight vector: We simply store the instances that have been misclassified so far and use the previous expression to make a prediction.</p>
<p id="p0915" class="para_indented">It looks like we’ve gained nothing—in fact, the algorithm is much slower because it iterates through all misclassified training instances every time a prediction is made. However, closer inspection of this formula reveals that it can be expressed in terms of dot products between instances. First, swap the summation signs to yield</p>
<p class="figure" id="e0090"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si17.jpg" alt="image" width="138" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si17.jpg"></p>
<p></p>
<p id="p0920" class="para_indented">The second sum is just a dot product between two instances and can be written as</p>
<p class="figure" id="e0095"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si18.jpg" alt="image" width="117" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si18.jpg"></p>
<p></p>
<p id="p0925" class="para_indented">This rings a bell! A similar expression for support vector machines enabled the use of kernels. Indeed, we can apply exactly the same trick here and use a kernel function instead of the dot product. Writing this function as <em>K</em>(…) gives</p>
<p class="figure" id="e0100"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si19.jpg" alt="image" width="129" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si19.jpg"></p>
<p></p>
<p id="p0930" class="para_indented">In this way the perceptron algorithm can learn a nonlinear classifier simply by keeping track of the instances that have been misclassified during the training process and using this expression to form each prediction.</p>
</div>
<p></p>
<p id="p0935" class="para_indented"><a id="p232"></a>If a separating hyperplane exists in the high-dimensional space implicitly created by the kernel function, this algorithm will learn one. However, it won’t learn the maximum-margin hyperplane found by a support vector machine classifier. This means that classification performance is usually worse. On the plus side, the algorithm is easy to implement and supports incremental learning.</p>
<p id="p0940" class="para_indented">This classifier is called the <em>kernel perceptron</em>. It turns out that all sorts of algorithms for learning linear models can be upgraded by applying the kernel trick in a similar fashion. For example, logistic regression can be turned into <em>kernel logistic regression</em>. As we saw before, the same applies to regression problems: Linear regression can also be upgraded using kernels. Again, a drawback of these advanced methods for linear and logistic regression (if they are done in a straightforward manner) is that the solution is not “sparse”: Every training instance contributes to the solution vector. In support vector machines and the kernel perceptron, only some of the training instances affect the solution, and this can make a big difference in computational efficiency.</p>
<p id="p0945" class="para_indented">The solution vector found by the perceptron algorithm depends greatly on the order in which the instances are encountered. One way to make the algorithm more stable is to use all the weight vectors encountered during learning, not just the final one, letting them vote on a prediction. Each weight vector contributes a certain number of votes. Intuitively, the “correctness” of a weight vector can be measured roughly as the number of successive trials after its inception in which it correctly classified subsequent instances and thus didn’t have to be changed. This measure can be used as the number of votes given to the weight vector, giving an algorithm known as the <em>voted perceptron</em> that performs almost as well as a support vector machine. (Note that, as mentioned earlier, the various weight vectors in the voted perceptron don’t need to be stored explicitly, and the kernel trick can be applied here too.)</p>
</div>
<div id="s0150">
<h3 id="st0150">Multilayer Perceptrons</h3>
<p id="p0950" class="noindent">Using a kernel is not the only way to create a nonlinear classifier based on the perceptron. In fact, kernel functions are a recent development in machine learning. Previously, neural network proponents used a different approach for nonlinear classification: They connected many simple perceptron-like models in a hierarchical structure. This can represent nonlinear decision boundaries.</p>
<p id="p0955" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> explained that a perceptron represents a hyperplane in instance space. We mentioned there that it is sometimes described as an artificial “neuron.” Of course, human and animal brains successfully undertake very complex classification tasks—for example, image recognition. The functionality of each individual neuron that is in a brain is certainly not sufficient to perform these feats. How can they be solved by brainlike structures? The answer must lie in the fact that the neurons in the brain are massively interconnected, allowing a problem to be decomposed into subproblems that can be solved at the neuron level. <a id="p233"></a>This observation inspired the development of artificial networks of neurons—neural nets.</p>
<p id="p0960" class="para_indented">Consider the simple dataset in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11</a>. Part (a) shows a two-dimensional instance space with four instances having classes 0 and 1, represented by white and black dots, respectively. No matter how you draw a straight line through this space, you will not be able to find one that separates all the black points from all the white ones. In other words, the problem is not linearly separable, and the simple perceptron algorithm will fail to generate a separating hyperplane (in this two-dimensional instance space a hyperplane is just a straight line). The situation is different in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(b)</a> and <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(c)</a>: Both these problems are linearly separable. The same holds for <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(d)</a>, which shows two points in a one-dimensional instance space (in the case of one dimension the separating hyperplane degenerates to a separating point).</p><a id="p234"></a><p id="f0060" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-011ah-9780123748560.jpg" alt="image" width="515" height="686" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-011ah-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.11</span> Example datasets and corresponding perceptrons.</p>
<p id="p0965" class="para_indented">If you are familiar with propositional logic, you may have noticed that the four situations in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11</a> correspond to four types of logical connectives. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(a)</a> represents a logical XOR (exclusive-OR), where the class is 1 if and only if exactly one of the attributes has value 1. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(b)</a> represents logical AND, where the class is 1 if and only if both attributes have value 1. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(c)</a> represents OR, where the class is 0 only if both attributes have value 0. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(d)</a> represents NOT, where the class is 0 if and only if the attribute has value 1. Because the last three are linearly separable, a perceptron can represent AND, OR, and NOT. Indeed, perceptrons for the corresponding datasets are shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figures 6.11(f–h)</a>, respectively. However, a simple perceptron cannot represent XOR because that is not linearly separable. To build a classifier for this type of problem a single perceptron is not sufficient—we need several of them.</p>
<p id="p0970" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0060">Figure 6.11(e)</a> shows a network with three perceptrons, or <em>units</em>, labeled A, B, and C. The first two are connected to what is sometimes called the <em>input layer</em> of the network, representing the attributes in the data. As in a simple perceptron, the input layer has an additional constant input called the <em>bias</em>. However, the third unit does not have any connections to the input layer. Its input consists of the output of units A and B (either 0 or 1) and another constant bias unit. These three units make up the <em>hidden layer</em> of the multilayer perceptron. They are called “hidden” because the units have no direct connection to the environment. This layer is what enables the system to represent XOR. You can verify this by trying all four possible combinations of input signals. For example, if attribute <em>a</em>
<span class="sub">1</span> has value 1 and <em>a</em>
<span class="sub">2</span> has value 1, then unit A will output 1 (because 1 × 1 + 1 × 1 + −0.5 × 1 &gt; 0), unit B will output 0 (because –1 × 1 + –1 × 1 + –1.5 × 1 &lt; 0), and unit C will output 0 (because 1 × 1 + 1 × 0 + –1.5 × 1 &lt; 0). This is the correct answer. Closer inspection of the behavior of the three units reveals that the first one represents OR, the second represents NAND (NOT combined with AND), and the third represents AND. Together they represent the expression (<em>a</em>
<span class="sub">1</span> OR <em>a</em>
<span class="sub">2</span>) AND (<em>a</em>
<span class="sub">1</span> NAND <em>a</em>
<span class="sub">2</span>), which is precisely the definition of XOR.</p>
<p id="p0975" class="para_indented">As this example illustrates, any expression from propositional calculus can be converted into a multilayer perceptron, because the three connectives AND, OR, and <a id="p235"></a>NOT are sufficient for this and we have seen how each can be represented using a perceptron. Individual units can be connected together to form arbitrarily complex expressions. Hence, a multilayer perceptron has the same expressive power as, say, a decision tree. In fact, it turns out that a two-layer perceptron (not counting the input layer) is sufficient. In this case, each unit in the hidden layer corresponds to a variant of AND—because we assume that it may negate some of the inputs before forming the conjunction—joined by an OR that is represented by a single unit in the output layer. In other words, each node in the hidden layer has the same role as a leaf in a decision tree or a single rule in a set of decision rules.</p>
<p id="p0980" class="para_indented">The big question is how to learn a multilayer perceptron. There are two aspects to the problem: learning the structure of the network and learning the connection weights. It turns out that there is a relatively simple algorithm for determining the weights given a fixed network structure. This algorithm is called <em>backpropagation</em> and is described in the next section. However, although there are many algorithms that attempt to identify network structure, this aspect of the problem is commonly solved by experimentation—perhaps combined with a healthy dose of expert knowledge. Sometimes the network can be separated into distinct modules that represent identifiable subtasks (e.g., recognizing different components of an object in an image recognition problem), which opens up a way of incorporating domain knowledge into the learning process. Often a single hidden layer is all that is necessary, and an appropriate number of units for that layer is determined by maximizing the estimated accuracy.</p>
<div id="s0155">
<h4 class="h4" id="st0155">Backpropagation</h4>
<p id="p0985" class="noindent">Suppose we have some data and seek a multilayer perceptron that is an accurate predictor for the underlying classification problem. Given a fixed network structure, we must determine appropriate weights for the connections in the network. In the absence of hidden layers, the perceptron learning rule from <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> can be used to find suitable values. But suppose there are hidden units. We know what the output unit should predict and could adjust the weights of the connections leading to that unit based on the perceptron rule. But the correct outputs for the hidden units are unknown, so the rule cannot be applied there.</p>
<p id="p0995" class="para_indented">It turns out that, roughly speaking, the solution is to modify the weights of the connections leading to the hidden units based on the strength of each unit’s contribution to the final prediction. There is a standard mathematical optimization algorithm, called <em>gradient descent</em>, which achieves exactly that. Unfortunately, it requires taking derivatives, and the step function that the simple perceptron uses to convert the weighted sum of the inputs into a 0/1 prediction is not differentiable. We need to see whether the step function can be replaced by something else.</p>
<p id="p1000" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0065">Figure 6.12(a)</a> shows the step function: If the input is smaller than 0, it outputs 0; otherwise, it outputs 1. We want a function that is similar in shape but differentiable. A commonly used replacement is shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0065">Figure 6.12(b)</a>. In neural networks terminology it is called the <em>sigmoid</em> function, and the version we consider here is defined by</p>
<p class="figure" id="e0105"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si20.jpg" alt="image" width="125" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si20.jpg"></p>
<p></p>
<p id="f0065" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-012ab-9780123748560.jpg" alt="image" width="513" height="192" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-012ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.12</span> Step versus sigmoid: (a) step function and (b) sigmoid function.</p>
<p id="p1005" class="para_indented"><a id="p236"></a>We encountered it in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> when we described the logit transform used in logistic regression. In fact, learning a multilayer perceptron is closely related to logistic regression.</p>
<p id="p1010" class="para_indented">To apply the gradient descent procedure, the error function—the thing that is to be minimized by adjusting the weights—must also be differentiable. The number of misclassifications—measured by the discrete 0 – 1 loss mentioned in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0045">Section 5.6</a>—does not fulfill this criterion. Instead, multilayer perceptrons are usually trained by minimizing the squared error of the network’s output, essentially treating it as an estimate of the class probability. (Other loss functions are also applicable. For example, if the negative log-likelihood is used instead of the squared error, learning a sigmoid-based perceptron is identical to logistic regression.)</p>
<p id="p1015" class="para_indented">We work with the squared-error loss function because it is most widely used. For a single training instance, it is</p>
<p class="figure" id="e0110"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si21.jpg" alt="image" width="150" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si21.jpg"></p>
<p>where <em>f</em>(<em>x</em>) is the network’s prediction obtained from the output unit and <em>y</em> is the instance’s class label (in this case, it is assumed to be either 0 or 1). The factor <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-001-9780123748560.jpg" alt="image" width="11" height="10" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-001-9780123748560.jpg"> is included just for convenience and will drop out when we start taking derivatives.</p>
<p id="p1020" class="para_indented">Gradient descent exploits information given by the derivative of the function that is to be minimized—in this case, the error function. As an example, consider a hypothetical error function that happens to be identical to <em>w</em><sup>2</sup> + 1, shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0070">Figure 6.13</a>. The <em>x</em>-axis represents a hypothetical parameter <em>w</em> that is to be optimized. The derivative of <em>w</em><sup>2</sup> + 1 is simply 2<em>w</em>. The crucial observation is that, based on the derivative, we can figure out the slope of the function at any particular point. If the derivative is negative, the function slopes downward to the right; if it is positive, <a id="p237"></a>it slopes downward to the left; and the size of the derivative determines how steep the decline is. Gradient descent is an iterative optimization procedure that uses this information to adjust a function’s parameters. It takes the value of the derivative, multiplies it by a small constant called the <em>learning rate</em>, and subtracts the result from the current parameter value. This is repeated for the new parameter value, and so on, until a minimum is reached.</p>
<p id="f0070" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-013-9780123748560.jpg" alt="image" width="750" height="551" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-013-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.13</span> Gradient descent using the error function <em>w</em><sup>2</sup> + 1.</p>
<p id="p1025" class="para_indented">Returning to the example, assume that the learning rate is set to 0.1 and the current parameter value <em>w</em> is 4. The derivative is double this—8 at this point. Multiplying by the learning rate yields 0.8, and subtracting this from 4 gives 3.2, which becomes the new parameter value. Repeating the process for 3.2, we get 2.56, then 2.048, and so on. The little crosses in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0070">Figure 6.13</a> show the values encountered in this process. The process stops once the change in parameter value becomes too small. In the example this happens when the value approaches 0, the value corresponding to the location on the <em>x</em>-axis where the minimum of the hypothetical error function is located.</p>
<p id="p1030" class="para_indented">The learning rate determines the step size and hence how quickly the search converges. If it is too large and the error function has several minima, the search may overshoot and miss a minimum entirely, or it may oscillate wildly. If it is too small, progress toward the minimum may be slow. Note that gradient descent can only find a <em>local</em> minimum. If the function has several minima—and error functions for multilayer perceptrons usually have many—it may not find the best one. This is a significant drawback of standard multilayer perceptrons compared with, for example, support vector machines.</p><a id="p0990"></a><a id="p238"></a><div class="boxg" id="b0030">
<p id="p1035" class="noindent">To use gradient descent to find the weights of a multilayer perceptron, the derivative of the squared error must be determined with respect to each parameter—that is, each weight in the network. Let’s start with a simple perceptron without a hidden layer. Differentiating the error function with respect to a particular weight <em>w<span class="sub">i</span>
</em> yields</p>
<p class="figure" id="e0115"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si22.jpg" alt="image" width="154" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si22.jpg"></p>
<p></p>
<p id="p1040" class="para_indented">Here, <em>f</em>(<em>x</em>) is the perceptron’s output and <em>x</em> is the weighted sum of the inputs.</p>
<p id="p1045" class="para_indented">To compute the second factor on the right side, the derivative of the sigmoid function <em>f</em>(<em>x</em>) is needed. It turns out that this has a particularly simple form that can be written in terms of <em>f</em>(<em>x</em>) itself:</p>
<p class="figure" id="e0120"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si23.jpg" alt="image" width="156" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si23.jpg"></p>
<p></p>
<p id="p1050" class="para_indented">We use <em>f′</em>(<em>x</em>) to denote this derivative. But we seek the derivative with respect to <em>w<span class="sub">i</span>
</em>, not <em>x</em>. Because</p>
<p class="figure" id="e0125"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si24.jpg" alt="image" width="92" height="33" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si24.jpg"></p>
<p>the derivative of <em>f</em>(<em>x</em>) with respect to <em>w<span class="sub">i</span>
</em> is</p>
<p class="figure" id="e0130"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si25.jpg" alt="image" width="113" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si25.jpg"></p>
<p></p>
<p id="p1055" class="para_indented">Plugging this back into the derivative of the error function yields</p>
<p class="figure" id="e0135"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si26.jpg" alt="image" width="167" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si26.jpg"></p>
<p></p>
<p id="p1060" class="para_indented">This expression gives all that is needed to calculate the change of weight <em>w<span class="sub">i</span>
</em> caused by a particular example vector <strong>a</strong> (extended by 1 to represent the bias, as explained previously). Having repeated this computation for each training instance, we add up the changes associated with a particular weight <em>w<span class="sub">i</span>
</em>, multiply by the learning rate, and subtract the result from <em>w<span class="sub">i</span>
</em>’s current value.</p>
<p id="p1065" class="para_indented">So far so good. But all this assumes that there is no hidden layer. With a hidden layer, things get a little trickier. Suppose <em>f</em>(<em>x<span class="sub">i</span>
</em>) is the output of the <em>i</em>th hidden unit, <em>w<span class="sub">ij</span>
</em> is the weight of the connection from input <em>j</em> to the <em>i</em>th hidden unit, and <em>w<span class="sub">i</span>
</em> is the weight of the <em>i</em>th hidden unit to the output unit. The situation is depicted in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0075">Figure 6.14</a>. As before, <em>f</em>(<em>x</em>) is the output of the single unit in the output layer. The update rule for the weights <em>w<span class="sub">i</span>
</em> is essentially the same as above, except that <em>a<span class="sub">i</span>
</em> is replaced by the output of the <em>i</em>th hidden unit:</p>
<p class="figure" id="e0140"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si27.jpg" alt="image" width="183" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si27.jpg"></p>
<p></p>
<p id="f0075" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-014-9780123748560.jpg" alt="image" width="750" height="633" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-014-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.14</span> Multilayer perceptron with a hidden layer.</p>
<p id="p1070" class="para_indented">However, to update the weights <em>w<span class="sub">ij</span>
</em> the corresponding derivatives must be calculated. Applying the chain rule gives</p>
<p class="figure" id="e0145"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si28.jpg" alt="image" width="269" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si28.jpg"></p>
<p></p>
<p id="p1075" class="para_indented">The first two factors are the same as in the previous equation. To compute the third factor, differentiate further. Because</p>
<p class="figure" id="e0150"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si29.jpg" alt="image" width="110" height="33" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si29.jpg"></p>
<p>then</p>
<p class="figure" id="e0155"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si30.jpg" alt="image" width="121" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si30.jpg"></p>
<p></p>
<p id="p1080" class="para_indented">Furthermore,</p>
<p class="figure" id="e0160"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si31.jpg" alt="image" width="100" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si31.jpg"></p>
<p>so</p>
<p class="figure" id="e0165"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si32.jpg" alt="image" width="210" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si32.jpg"></p>
<p></p>
<p id="p1085" class="para_indented">This means that we are finished. Putting everything together yields an equation for the derivative of the error function with respect to the weights <em>w<span class="sub">ij</span>
</em>:</p>
<p class="figure" id="e0170"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si33.jpg" alt="image" width="223" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si33.jpg"></p>
<p></p>
<p id="p1090" class="para_indented">As before, we calculate this value for every training instance, add up the changes associated with a particular weight <em>w<span class="sub">ij</span>
</em>, multiply by the learning rate, and subtract the outcome from the current value of <em>w<span class="sub">ij</span>
</em>.</p>
<p id="p1095" class="para_indented">This derivation applies to a perceptron with one hidden layer. If there are two hidden layers, the same strategy can be applied a second time to update the weights pertaining to the input connections of the first hidden layer, propagating the error from the output unit through the second hidden layer to the first one. Because of this error propagation mechanism, this version of the generic gradient descent strategy is called <em>backpropagation</em>.</p>
</div>
<p></p>
<p id="p1100" class="para_indented"><a id="p239"></a>We have tacitly assumed that the network’s output layer has just one unit, which is appropriate for two-class problems. For more than two classes, a separate network could be learned for each class that distinguishes it from the remaining classes. A more compact classifier can be obtained from a single network by creating an output unit for each class, connecting every unit in the hidden layer to every output unit. <a id="p240"></a>The squared error for a particular training instance is the sum of squared errors taken over all output units. The same technique can be applied to predict several targets, or attribute values, simultaneously by creating a separate output unit for each one. Intuitively, this may give better predictive accuracy than building a separate classifier for each class attribute if the underlying learning tasks are in some way related.</p>
<p id="p1105" class="para_indented">We have assumed that weights are only updated after all training instances have been fed through the network and all the corresponding weight changes have been accumulated. This is <em>batch</em> learning because all the training data is processed together. But exactly the same formulas can be used to update the weights incrementally after each training instance has been processed. This is called <em>stochastic backpropagation</em> because the overall error does not necessarily decrease after every update. It can be used for online learning, in which new data arrives in a continuous stream and every training instance is processed just once. In both variants of backpropagation, it is often helpful to standardize the attributes to have zero mean and unit standard deviation. Before learning starts, each weight is initialized to a small, randomly chosen value based on a normal distribution with zero mean.</p>
<p id="p1110" class="para_indented">Like any other learning scheme, multilayer perceptrons trained with backpropagation may suffer from overfitting, especially if the network is much larger than what is actually necessary to represent the structure of the underlying learning problem. Many modifications have been proposed to alleviate this. A very simple one, called <em>early stopping</em>, works like reduced-error pruning in rule learners: A holdout set is used to decide when to stop performing further iterations of the backpropagation algorithm. The error on the holdout set is measured and the algorithm is terminated once the error begins to increase because that indicates overfitting to the training data. Another method, called <em>weight decay</em>, adds to the error function a penalty term that consists of the squared sum of all weights in the network, as in ridge regression. This attempts to limit the influence of irrelevant connections on the network’s predictions by penalizing large weights that do not contribute a correspondingly large reduction in the error.</p>
<p id="p1115" class="para_indented">Although standard gradient descent is the simplest technique for learning the weights in a multilayer perceptron, it is by no means the most efficient one. In practice, it tends to be rather slow. A trick that often improves performance is to include a <em>momentum</em> term when updating weights: Add to the new weight change a small proportion of the update value from the previous iteration. This smoothes the search process by making changes in direction less abrupt. More sophisticated methods make use of information obtained from the second derivative of the error function as well; they can converge much more quickly. However, even those algorithms can be very slow compared with other methods of classification learning.</p>
<p id="p1120" class="para_indented">A serious disadvantage of multilayer perceptrons that contain hidden units is that they are essentially opaque. There are several techniques that attempt to extract rules from trained neural networks. However, it is unclear whether they offer any advantages over standard rule learners that induce rule sets directly from data, especially considering that this can generally be done much more quickly than learning a multilayer perceptron in the first place.</p>
<p id="p1125" class="para_indented"><a id="p241"></a>Although multilayer perceptrons are the most prominent type of neural network, many others have been proposed. Multilayer perceptrons belong to a class of networks called <em>feed-forward networks</em> because they do not contain any cycles and the network’s output depends only on the current input instance. <em>Recurrent</em> neural networks do have cycles. Computations derived from earlier input are fed back into the network, which gives them a kind of memory.</p>
</div>
</div>
<div id="s0160">
<div id="s0165">
<h4 class="h4" id="st0160">Radial Basis Function Networks</h4>
<p id="p1130" class="noindent">Another popular type of feed-forward network is the <em>radial basis function</em> (RBF) network. It has two layers, not counting the input layer, and differs from a multilayer perceptron in the way that the hidden units perform computations. Each hidden unit essentially represents a particular point in input space, and its output, or <em>activation</em>, for a given instance depends on the distance between its point and the instance, which is just another point. Intuitively, the closer these two points, the stronger the activation. This is achieved by using a nonlinear transformation function to convert the distance into a similarity measure. A bell-shaped Gaussian <em>activation function</em>, of which the width may be different for each hidden unit, is commonly used for this purpose. The hidden units are called RBFs because the points in instance space for which a given hidden unit produces the same activation form a hypersphere or hyperellipsoid. (In a multilayer perceptron, this is a hyperplane.)</p>
<p id="p1135" class="para_indented">The output layer of an RBF network is the same as that of a multilayer perceptron: It takes a linear combination of the outputs of the hidden units and—in classification problems—pipes it through the sigmoid function (or something with a similar shape).</p>
<p id="p1140" class="para_indented">The parameters that such a network learns are (a) the centers and widths of the RBFs and (b) the weights used to form the linear combination of the outputs obtained from the hidden layer. A significant advantage over multilayer perceptrons is that the first set of parameters can be determined independently of the second set and still produce accurate classifiers.</p>
<p id="p1145" class="para_indented">One way to determine the first set of parameters is to use clustering. The simple <em>k</em>-means clustering algorithm described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0155">Section 4.8</a> can be applied, clustering each class independently to obtain <em>k</em>-basis functions for each class. Intuitively, the resulting RBFs represent prototype instances. The second set of parameters is then learned by keeping the first parameters fixed. This involves learning a simple linear classifier using one of the techniques we have discussed (e.g., linear or logistic regression). If there are far fewer hidden units than training instances, this can be done very quickly.</p>
<p id="p1150" class="para_indented">A disadvantage of RBF networks is that they give every attribute the same weight because all are treated equally in the distance computation, unless attribute weight parameters are included in the overall optimization process. Thus, they cannot deal effectively with irrelevant attributes, in contrast to multilayer perceptrons. Support vector machines share the same problem. In fact, support vector machines with Gaussian kernels (i.e., “RBF kernels”) are a particular type of RBF network, in which one basis function is centered on every training instance, all basis functions <a id="p242"></a>have the same width, and the outputs are combined linearly by computing the maximum-margin hyperplane. This has the effect that only some of the RBFs have a nonzero weight—the ones that represent the support vectors.</p>
</div>
</div>
<div id="s0170">
<h3 id="st0165">Stochastic Gradient Descent</h3>
<p id="p1155" class="noindent">We have introduced gradient descent and stochastic backpropagation as optimization methods for learning the weights in a neural network. Gradient descent is, in fact, a general-purpose optimization technique that can be applied whenever the objective function is differentiable. Actually, it turns out that it can even be applied in cases where the objective function is not completely differentiable through use of a device called <em>subgradients</em>.</p>
<p id="p1160" class="para_indented">One application is the use of gradient descent to learn linear models such as linear support vector machines or logistic regression. Learning such models using gradient descent is easier than optimizing nonlinear neural networks because the objective function has a global minimum rather than many local minima, which is usually the case for nonlinear networks. For linear problems, a stochastic gradient descent procedure can be designed that is computationally simple and converges very rapidly, allowing models such as linear support vector machines and logistic regression to be learned from large datasets. Moreover, stochastic gradient descent allows models to be learned incrementally, in an online setting.</p>
<p id="p1165" class="para_indented">For support vector machines, the error function—the thing that is to be minimized—is called the <em>hinge loss</em>. Illustrated in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0080">Figure 6.15</a>, this is so named because it comprises a downwards sloping linear segment joined to a horizontal part at <em>z</em> = 1—more formally, <em>E</em>(<em>z</em>) = max{0, 1 – <em>z</em>}. For comparison, the figure also shows the 0 – 1 loss, which is discontinuous, and the squared loss, which is both continuous <a id="p243"></a>and differentiable. These functions are plotted as a function of the margin <em>z</em> = <em>y f</em>(<em>x</em>), where the class <em>y</em> is either –1 or +1 and <em>f</em>(<em>x</em>) is the output of the linear model. Misclassification occurs when <em>z</em> &lt; 0, so all loss functions incur their most serious penalties in the negative region. In the linearly separable case, the hinge loss is 0 for a function that successfully separates the data. The maximum-margin hyperplane is given by the smallest weight vector that achieves a zero hinge loss.</p>
<p id="f0080" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-015-9780123748560.jpg" alt="image" width="800" height="586" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-015-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.15</span> Hinge, squared, and 0 – 1 loss functions.</p><a id="p1170"></a><div class="boxg" id="b0035">
<p id="p1175" class="noindent">The hinge loss is continuous, unlike the 0 – 1 loss, but is not differentiable at <em>z</em> = 1, unlike the squared loss, which is differentiable everywhere. This lack of differentiability presents a problem if gradient descent is used to update the model’s weights after a training example has been processed, because the loss function’s derivative is needed for this. That is where subgradients come in. The basic idea is that even though the gradient cannot be computed, the minimum will still be found if something resembling a gradient can be substituted. In the case of the hinge loss, the gradient is taken to be 0 at the point of nondifferentiability. In fact, since the hinge loss is 0 for <em>z</em> ≥ 1, we can focus on that part of the function that is differentiable (<em>z</em> &lt; 1) and proceed as usual.</p>
<p id="p1180" class="para_indented">Ignoring the weight decay necessary to find the smallest weight vector, the weight update for a linear support vector machine using the hinge loss is Δ<em>w<span class="sub">i</span>
</em> = <em>ηa<span class="sub">i</span>y</em>, where <em>η</em> is the learning rate. For stochastic gradient descent, all that is needed to compute <em>z</em> for each training instance is to take the dot product between the current weight vector and the instance, multiply the result by the instance’s class value, and check to see if the resulting value is less than 1. If so, the weights are updated accordingly. As with perceptrons, a bias term can be included by extending the weight vector by one element and including an additional attribute with each training instance that always has the value 1.</p>
</div>
<p></p>
</div>
<div id="s0175">
<h3 id="st0170">Discussion</h3>
<p id="p1185" class="noindent">Support vector machines originated from research in statistical learning theory (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib303">Vapnik, 1999</a>), and a good starting point for exploration is a tutorial by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib47">Burges (1998)</a>. A general description, including generalization to the case in which the data is not linearly separable, has been published by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib65">Cortes and Vapnik (1995)</a>. We have introduced the standard version of support vector regression; <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib276">Schölkopf et al. (1999)</a> present a different version that has one parameter instead of two. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib286">Smola and Schölkopf (2004)</a> provide an extensive tutorial on support vector regression.</p>
<p id="p1190" class="para_indented">Ridge regression was introduced in statistics by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib9003">Hoerl and Kennard (1970</a>) and can now be found in standard statistics texts. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib156">Hastie et al. (2009)</a> give a good description of kernel ridge regression. Kernel ridge regression is equivalent to a technique called Gaussian process regression in terms of point estimates produced, but a discussion of Gaussian processes is beyond the scope of this book. The complexity of the most efficient general matrix inversion algorithm is in fact O(<em>n</em><sup>2.807</sup>) rather than O(<em>n</em><sup>3</sup>).</p>
<p id="p1195" class="para_indented">The (voted) kernel perceptron is due to <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib119">Freund and Schapire (1999)</a>. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib67">Cristianini and Shawe-Taylor (2000)</a> provide a nice introduction to support vector machines <a id="p244"></a>and other kernel-based methods, including the optimization theory underlying the support vector learning algorithms. We have barely skimmed the surface of these learning schemes, mainly because advanced mathematics lies just beneath. The idea of using kernels to solve nonlinear problems has been applied to many algorithms, for example, principal components analysis (described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>). A kernel is essentially a similarity function with certain mathematical properties, and it is possible to define kernel functions over all sorts of structures—for example, sets, strings, trees, and probability distributions. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib284">Shawe-Taylor and Cristianini (2004)</a> and <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib278">Schölkopf and Smola (2002)</a> cover kernel-based learning in detail.</p>
<p id="p1200" class="para_indented">There is extensive literature on neural networks, and <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib26">Bishop (1995)</a> provides an excellent introduction to both multilayer perceptrons and RBF networks. Interest in neural networks appears to have declined since the arrival of support vector machines, perhaps because the latter generally require fewer parameters to be tuned to achieve the same (or greater) accuracy. However, multilayer perceptrons have the advantage that they can learn to ignore irrelevant attributes, and RBF networks trained using <em>k</em>-means can be viewed as a quick-and-dirty method for finding a nonlinear classifier. Recent studies have shown that multilayer perceptrons achieve performance competitive with more modern learning techniques on many practical datasets.</p>
<p id="p1205" class="para_indented">Recently there has been renewed interest in gradient methods for learning classifiers. In particular, stochastic gradient methods have been explored because they are applicable to large data sets and online learning scenarios. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib181">Kivinen et al. (2002)</a>, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib336">Zhang (2004)</a>, and <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib283">Shalev-Shwartz et al. (2007)</a> explore such methods when applied to learning support vector machines. Kivinen et al. and Shalev-Shwartz et al. provide heuristics for setting the learning rate for gradient descent based on the current iteration, which only require the user to provide a value for a single parameter that determines the closeness of fit to the training data (a so-called regularization parameter). In the vanilla approach, regularization is performed by limiting the number of updates that can be performed.</p>
</div>
</div>
<div id="s0180">
<h2 id="st0175">6.5 Instance-based learning</h2>
<p id="p1210" class="noindent">In <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a> we saw how the nearest-neighbor rule can be used to implement a basic form of instance-based learning. There are several practical problems with this simple scheme. First, it tends to be slow for large training sets because the entire set must be searched for each test instance—unless sophisticated data structures such as <em>k</em>D-trees or ball trees are used. Second, it performs badly with noisy data because the class of a test instance is determined by its single nearest neighbor without any “averaging” to help eliminate noise. Third, it performs badly when different attributes affect the outcome to different extents—in the extreme case, when some attributes are completely irrelevant—because all attributes contribute equally to the distance formula. Fourth, it does not perform explicit generalization, although we <a id="p245"></a>intimated in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0050">Section 3.5</a> (and illustrated in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#f0055">Figure 3.10</a>) that some instance-based learning systems do indeed perform explicit generalization.</p>
<div id="s0185">
<h3 id="st0180">Reducing the Number of Exemplars</h3>
<p id="p1215" class="noindent">The plain nearest-neighbor rule stores a lot of redundant exemplars. Yet it is almost always completely unnecessary to save all the examples seen so far. A simple variant is to classify each example with respect to the examples already seen and to save only ones that are misclassified. We use the term <em>exemplars</em> to refer to the already-seen instances that are used for classification. Discarding correctly classified instances reduces the number of exemplars and proves to be an effective way to prune the exemplar database. Ideally, only a single exemplar is stored for each important region of the instance space. However, early in the learning process examples may be discarded that later turn out to be important, possibly leading to some decrease in predictive accuracy. As the number of stored instances increases, the accuracy of the model improves, and so the system makes fewer mistakes.</p>
<p id="p1220" class="para_indented">Unfortunately, the strategy of only storing misclassified instances does not work well in the face of noise. Noisy examples are very likely to be misclassified, and so the set of stored exemplars tends to accumulate those that are least useful. This effect is easily observed experimentally. Thus, this strategy is only a stepping-stone on the way toward more effective instance-based learners.</p>
</div>
<div id="s0190">
<h3 id="st0185">Pruning Noisy Exemplars</h3>
<p id="p1225" class="noindent">Noisy exemplars inevitably lower the performance of any nearest-neighbor scheme that does not suppress them, because they have the effect of repeatedly misclassifying new instances. There are two ways of dealing with this. One is to locate, instead of the single nearest neighbor, the <em>k-</em>nearest neighbors for some predetermined constant <em>k</em>, and assign the majority class to the unknown instance. The only problem here is determining a suitable value of <em>k</em>. Plain nearest-neighbor learning corresponds to <em>k</em> = 1. The more noise, the greater the optimal value of <em>k</em>. One way to proceed is to perform cross-validation tests with several different values and choose the best. Although this is expensive in computation time, it often yields excellent predictive performance.</p>
<p id="p1230" class="para_indented">A second solution is to monitor the performance of each exemplar that is stored and discard ones that do not perform well. This can be done by keeping a record of the number of correct and incorrect classification decisions that each exemplar makes. Two predetermined thresholds are set on the success ratio. When an exemplar’s performance drops below the lower one, it is deleted from the exemplar set. If its performance exceeds the upper threshold, it is used for predicting the class of new instances. If its performance lies between the two, it is not used for prediction but, whenever it is the closest exemplar to the new instance (and thus would have <a id="p246"></a>been used for prediction if its performance record had been good enough), its success statistics are updated as though it had been used to classify that new instance.</p>
<p id="p1235" class="para_indented">To accomplish this, we use the confidence limits on the success probability of a Bernoulli process that we derived in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0015">Section 5.2</a>. Recall that we took a certain number of successes <em>S</em> out of a total number of trials <em>N</em> as evidence on which to base confidence limits on the true underlying success rate <em>p</em>. Given a certain confidence level of, say, 5%, we can calculate upper and lower bounds and be 95% sure that <em>p</em> lies between them.</p>
<p id="p1240" class="para_indented">To apply this to the problem of deciding when to accept a particular exemplar, suppose that it has been used <em>n</em> times to classify other instances and that <em>s</em> of these have been successes. That allows us to estimate bounds, at a particular confidence level, on the true success rate of this exemplar. Now suppose that the exemplar’s class has occurred <em>c</em> times out of a total number <em>N</em> of training instances. This allows us to estimate bounds on the default success rate—that is, the probability of successfully classifying an instance of this class without any information about other instances. We insist that the <em>lower</em> confidence bound on an exemplar’s success rate exceeds the <em>upper</em> confidence bound on the default success rate. We use the same method to devise a criterion for rejecting a poorly performing exemplar, requiring that the <em>upper</em> confidence bound on its success rate lies below the <em>lower</em> confidence bound on the default success rate.</p>
<p id="p1245" class="para_indented">With suitable choices of thresholds, this scheme works well. In a particular implementation, called <em>IB3</em> for <em>Instance-Based Learner version 3</em>, a confidence level of 5% is used to determine acceptance whereas a level of 12.5% is used for rejection. The lower percentage figure produces a wider confidence interval, which makes for a more stringent criterion because it is harder for the lower bound of one interval to lie above the upper bound of the other. The criterion for acceptance is more stringent than for rejection, making it more difficult for an instance to be accepted. The reason for a less stringent rejection criterion is that there is little to be lost by dropping instances with only moderately poor classification accuracies: They will probably be replaced by similar instances later. Using these thresholds has been found to improve the performance of instance-based learning and, at the same time, dramatically reduce the number of exemplars—particularly noisy exemplars—that are stored.</p>
</div>
<div id="s0195">
<h3 id="st0190">Weighting Attributes</h3>
<p id="p1250" class="noindent">The Euclidean distance function, modified to scale all attribute values to between 0 and 1, works well in domains in which the attributes are equally relevant to the outcome. Such domains, however, are the exception rather than the rule. In most domains some attributes are irrelevant and some relevant ones are less important than others. The next improvement in instance-based learning is to learn the relevance of each attribute incrementally by dynamically updating feature weights.</p>
<p id="p1255" class="para_indented">In some schemes, the weights are class specific in that an attribute may be more important to one class than to another. To cater for this, a description is produced <a id="p247"></a>for each class that distinguishes its members from members of all other classes. This leads to the problem that an unknown test instance may be assigned to several different classes, or no classes at all—a problem that is all too familiar from our description of rule induction. Heuristic solutions are applied to resolve these situations.</p>
<p id="p1260" class="para_indented">The distance metric incorporates the feature weights <em>w</em>
<span class="sub">1</span>, <em>w</em>
<span class="sub">2</span>, …, <em>w<span class="sub">m</span>
</em> on each dimension:</p>
<p class="figure" id="e0175"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si34.jpg" alt="image" width="417" height="38" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si34.jpg"></p>
<p></p>
<p id="p1265" class="para_indented">In the case of class-specific feature weights, there will be a separate set of weights for each class.</p>
<p id="p1270" class="para_indented">All attribute weights are updated after each training instance is classified, and the most similar exemplar (or the most similar exemplar of each class) is used as the basis for updating. Call the training instance <em>x</em> and the most similar exemplar <em>y</em>. For each attribute <em>i</em>, the difference |<em>x<span class="sub">i</span>
</em> − <em>y<span class="sub">i</span>
</em>| is a measure of the contribution of that attribute to the decision. If this difference is small then the attribute contributes positively, whereas if it is large it may contribute negatively. The basic idea is to update the <em>i</em>th weight on the basis of the size of this difference and whether the classification was indeed correct. If the classification is correct the associated weight is increased, and if it is incorrect it is decreased, the amount of increase or decrease being governed by the size of the difference: large if the difference is small and vice versa. The weight change is generally followed by a renormalization step. A simpler strategy, which may be equally effective, is to leave the weights alone if the decision is correct, and if it is incorrect to increase the weights for those attributes that differ most greatly, accentuating the difference. Details of these weight adaptation algorithms are described by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib6">Aha (1992)</a>.</p>
<p id="p1275" class="para_indented">A good test of whether an attribute weighting scheme works is to add irrelevant attributes to all examples in a dataset. Ideally, the introduction of irrelevant attributes should not affect either the quality of predictions or the number of exemplars stored.</p>
</div>
<div id="s0200">
<h3 id="st0195">Generalizing Exemplars</h3>
<p id="p1280" class="noindent">Generalized exemplars are rectangular regions of instance space, called <em>hyperrectangles</em> because they are high-dimensional. When classifying new instances it is necessary to modify the distance function as described below to allow the distance to a hyperrectangle to be computed. When a new exemplar is classified correctly, it is generalized by simply merging it with the nearest exemplar of the same class. The nearest exemplar may be either a single instance or a hyperrectangle. In the former case, a new hyperrectangle is created that covers the old and the new instance. In the latter, the hyperrectangle is enlarged to encompass the new instance. Finally, if the prediction is incorrect and it was a hyperrectangle that was responsible for the incorrect prediction, the hyperrectangle’s boundaries are altered so that it shrinks away from the new instance.</p>
<p id="p1285" class="para_indented"><a id="p248"></a>It is necessary to decide at the outset whether overgeneralization caused by nesting or overlapping hyperrectangles is to be permitted or not. If it is to be avoided, a check is made before generalizing a new example to see whether any regions of feature space conflict with the proposed new hyperrectangle. If they do, the generalization is aborted and the example is stored verbatim. Note that overlapping hyperrectangles are precisely analogous to situations in which the same example is covered by two or more rules in a rule set.</p>
<p id="p1290" class="para_indented">In some schemes, generalized exemplars can be nested in that they may be completely contained within one another, in the same way that in some representations rules may have exceptions. To do this, whenever an example is incorrectly classified, a fallback heuristic is tried using the second nearest neighbor if it produces a correct prediction in a further attempt to perform generalization. This second-chance mechanism promotes nesting of hyperrectangles. If an example falls within a rectangle of the wrong class that already contains an exemplar of the same class, the two are generalized into a new “exception” hyperrectangle nested within the original one. For nested generalized exemplars, the learning process frequently begins with a small number of seed instances to prevent all examples of the same class from being generalized into a single rectangle that covers most of the problem space.</p>
</div>
<div id="s0205">
<h3 id="st0200">Distance Functions for Generalized Exemplars</h3>
<p id="p1295" class="noindent">With generalized exemplars it is necessary to generalize the distance function to compute the distance from an instance to a generalized exemplar, as well as to another instance. The distance from an instance to a hyperrectangle is defined to be zero if the point lies within the hyperrectangle. The simplest way to generalize the distance function to compute the distance from an exterior point to a hyperrectangle is to choose the closest instance within it and to measure the distance to that. However, this reduces the benefit of generalization because it reintroduces dependence on a particular single example. More precisely, whereas new instances that happen to lie within a hyperrectangle continue to benefit from generalizations, ones that lie outside do not. It might be better to use the distance from the nearest part of the hyperrectangle instead.</p>
<p id="p1300" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0085">Figure 6.16</a> shows the implicit boundaries that are formed between two rectangular classes if the distance metric is adjusted to measure distance to the nearest point of a rectangle. Even in two dimensions the boundary contains a total of nine regions (they are numbered for easy identification); the situation will be more complex for higher-dimensional hyperrectangles.</p>
<p id="f0085" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-016-9780123748560.jpg" alt="image" width="601" height="944" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-016-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.16</span> A boundary between two rectangular classes.</p>
<p id="p1305" class="para_indented">Proceeding from the lower left, the first region, in which the boundary is linear, lies outside the extent of both rectangles—to the left of both borders of the larger one and below both borders of the smaller one. The second is within the extent of one rectangle—to the right of the leftmost border of the larger rectangle—but outside that of the other—below both borders of the smaller one. In this region the boundary is parabolic because the locus of a point that is the same distance from a given line <a id="p249"></a>as from a given point is a <em>parabola</em>. The third region is where the boundary meets the lower border of the larger rectangle when projected upward and the left border of the smaller one when projected to the right. The boundary is linear in this region because it is equidistant from these two borders. The fourth is where the boundary lies to the right of the larger rectangle but below the bottom of that rectangle. In this case the boundary is parabolic because it is the locus of points equidistant from the lower right corner of the larger rectangle and the left side of the smaller one. The fifth region lies between the two rectangles: Here the boundary is vertical. The pattern is repeated in the upper right part of the diagram: first parabolic, then linear, then parabolic (although this particular parabola is almost indistinguishable from a straight line), and finally linear as the boundary escapes from the scope of both rectangles.</p>
<p id="p1310" class="para_indented">This simple situation certainly defines a complex boundary! Of course, it is not necessary to represent the boundary explicitly; it is generated implicitly by the nearest-neighbor calculation. Nevertheless, the solution is still not a very good one. Whereas taking the distance from the nearest instance within a hyperrectangle is overly dependent on the position of that particular instance, taking the distance to the nearest point of the hyperrectangle is overly dependent on that corner of the rectangle—the nearest example might be far from the corner.</p>
<p id="p1315" class="para_indented">A final problem concerns measuring the distance to hyperrectangles that overlap or are nested. This complicates the situation because an instance may fall within more than one hyperrectangle. A suitable heuristic for use in this case is to choose the class of the most specific hyperrectangle containing the instance—that is, the one covering the smallest area of instance space.</p>
<p id="p1320" class="para_indented">Whether or not overlap or nesting is permitted, the distance function should be modified to take account of both the observed prediction accuracy of exemplars and the relative importance of different features, as described in the sections above on pruning noisy exemplars and attribute weighting.</p>
</div>
<div id="s0210">
<h3 id="st0205">Generalized Distance Functions</h3>
<p id="p1325" class="noindent">There are many different ways of defining a distance function, and it is hard to find rational grounds for any particular choice. An elegant solution is to consider one <a id="p250"></a>instance being transformed into another through a sequence of predefined elementary operations and to calculate the probability of such a sequence occurring if operations are chosen randomly. Robustness is improved if all possible transformation paths are considered, weighted by their probabilities, and the scheme generalizes naturally to the problem of calculating the distance between an instance and a set of other instances by considering transformations to all instances in the set. Through such a technique it is possible to consider each instance as exerting a “sphere of influence,” but a sphere with soft boundaries rather than the hard-edged cutoff implied by the <em>k</em>-nearest-neighbor rule, in which any particular example is either “in” or “out” of the decision.</p>
<p id="p1330" class="para_indented">With such a measure, given a test instance that has a class that is unknown, its distance to the set of all training instances in each class in turn is calculated and the closest class is chosen. It turns out that nominal and numeric attributes can be treated in a uniform manner within this transformation-based approach by defining different transformation sets, and it is even possible to take account of unusual attribute types—such as degrees of arc or days of the week, which are measured on a circular scale.</p>
</div>
<div id="s0215">
<h3 id="st0210">Discussion</h3>
<p id="p1335" class="noindent">Nearest-neighbor methods gained popularity in machine learning through the work of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib6">Aha (1992)</a>, who showed that, when combined with noisy exemplar pruning and attribute weighting, instance-based learning performs well in comparison with other methods. It is worth noting that although we have described it solely in the context of classification rather than numeric prediction problems, it applies to these equally well: Predictions can be obtained by combining the predicted values of the <em>k-</em>nearest neighbors and weighting them by distance.</p>
<p id="p1340" class="para_indented">Viewed in instance space, the standard rule- and tree-based representations are only capable of representing class boundaries that are parallel to the axes defined by the attributes. This is not a handicap for nominal attributes, but it is for numeric ones. Non-axis-parallel class boundaries can only be approximated by covering the region above or below the boundary with several axis-parallel rectangles, the number of rectangles determining the degree of approximation. In contrast, the instance-based method can easily represent arbitrary linear boundaries. Even with just one example of each of two classes, the boundary implied by the nearest-neighbor rule is a straight line of arbitrary orientation, namely the perpendicular bisector of the line joining the examples.</p>
<p id="p1345" class="para_indented">Plain instance-based learning does not produce explicit knowledge representations except by selecting representative exemplars. However, when combined with exemplar generalization, a set of rules can be obtained that may be compared with those produced by other machine learning schemes. The rules tend to be more conservative because the distance metric, modified to incorporate generalized exemplars, can be used to process examples that do not fall within the rules. This reduces the pressure to produce rules that cover the whole example space or even all of the <a id="p251"></a>training examples. On the other hand, the incremental nature of most instance-based learning schemes means that rules are formed eagerly, after only part of the training set has been seen; this inevitably reduces their quality.</p>
<p id="p1350" class="para_indented">We have not given precise algorithms for variants of instance-based learning that involve generalization, because it is not clear what the best way to do generalization is. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib273">Salzberg (1991)</a> suggested that generalization with nested exemplars can achieve a high degree of classification of accuracy on a variety of different problems, a conclusion disputed by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib316">Wettschereck and Dietterich (1995)</a>, who argued that these results were fortuitous and did not hold in other domains. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib220">Martin (1995)</a> explored the idea that it is not generalization but the overgeneralization that occurs when hyperrectangles nest or overlap that is responsible for poor performance, and demonstrated that if nesting and overlapping are avoided, excellent results are achieved in a large number of domains. The generalized distance function based on transformations is described by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib61">Cleary and Trigg (1995)</a>.</p>
<p id="p1355" class="para_indented">Exemplar generalization is a rare example of a learning strategy in which the search proceeds from specific to general rather than from general to specific as in the case of tree or rule induction. There is no particular reason why specific-to-general searching should necessarily be handicapped by forcing the examples to be considered in a strictly incremental fashion, and batch-oriented methods exist that generate rules using a basic instance-based approach. Moreover, it seems that the idea of producing conservative generalizations and coping with instances that are not covered by choosing the “closest” generalization may be generally useful for tree and rule inducers.</p>
</div>
</div>
<div id="s0220">
<h2 id="st0215">6.6 Numeric prediction with local linear models</h2>
<p id="p1360" class="noindent">Trees that are used for numeric prediction are just like ordinary decision trees, except that at each leaf they store either a class value that represents the average value of instances that reach the leaf, in which case the tree is called a <em>regression tree</em>, or a linear regression model that predicts the class value of instances that reach the leaf, in which case it is called a <em>model tree</em>. In what follows we will talk about model trees because regression trees are really a special case.</p>
<p id="p1365" class="para_indented">Regression and model trees are constructed by first using a decision tree induction algorithm to build an initial tree. However, whereas most decision tree algorithms choose the splitting attribute to maximize the information gain, it is appropriate for numeric prediction to instead minimize the intrasubset variation in the class values down each branch. Once the basic tree has been formed, consideration is given to pruning the tree back from each leaf, just as with ordinary decision trees. The only difference between regression tree and model tree induction is that, for the latter, each node is replaced by a regression plane instead of a constant value. The attributes that serve to define that plane are generally those that participate in decisions in the subtree that will be pruned—that is, in nodes beneath the current one and perhaps those that occur on the path to the root node.</p>
<p id="p1370" class="para_indented"><a id="p252"></a>Following an extensive description of model trees, we briefly explain how to generate rules from model trees, and then describe another approach to numeric prediction based on generating local linear models: locally weighted linear regression. Whereas model trees derive from the basic divide-and-conquer decision tree methodology, locally weighted regression is inspired by the instance-based methods for classification that we described in the previous section. Like instance-based learning, it performs all “learning” at prediction time. Although locally weighted regression resembles model trees in that it uses linear regression to fit models locally to particular areas of instance space, it does so in quite a different way.</p>
<div id="s0225">
<h3 id="st0220">Model Trees</h3>
<p id="p1375" class="noindent">When a model tree is used to predict the value for a test instance, the tree is followed down to a leaf in the normal way, using the instance’s attribute values to make routing decisions at each node. The leaf will contain a linear model based on some of the attribute values, and this is evaluated for the test instance to yield a raw predicted value.</p>
<p id="p1380" class="para_indented">Instead of using this raw value directly, however, it turns out to be beneficial to use a smoothing process to reduce the sharp discontinuities that will inevitably occur between adjacent linear models at the leaves of the pruned tree. This is a particular problem for models constructed from a small number of training instances. Smoothing can be accomplished by producing linear models for each internal node, as well as for the leaves, at the time the tree is built. Then, once the leaf model has been used to obtain the raw predicted value for a test instance, that value is filtered along the path back to the root, smoothing it at each node by combining it with the value predicted by the linear model for that node.</p>
<p id="p1385" class="para_indented">An appropriate smoothing calculation is</p>
<p class="figure" id="e0180"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si35.jpg" alt="image" width="113" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si35.jpg"></p>
<p>where <em>p</em>′ is the prediction passed up to the next higher node, <em>p</em> is the prediction passed to this node from below, <em>q</em> is the value predicted by the model at this node, <em>n</em> is the number of training instances that reach the node below, and <em>k</em> is a smoothing constant. Experiments show that smoothing substantially increases the accuracy of predictions.</p>
<p id="p1390" class="para_indented">However, discontinuities remain and the resulting function is not smooth. In fact, exactly the same smoothing process can be accomplished by incorporating the interior models into each leaf model after the tree has been built. Then, during the classification process, only the leaf models are used. The disadvantage is that the leaf models tend to be larger and more difficult to comprehend because many coefficients that were previously zero become nonzero when the interior nodes’ models are incorporated.</p>
</div>
<div id="s0230">
<h3 id="st0225"><a id="p253"></a>Building the Tree</h3>
<p id="p1395" class="noindent">The splitting criterion is used to determine which attribute is the best to split that portion <em>T</em> of the training data that reaches a particular node. It is based on treating the standard deviation of the class values in <em>T</em> as a measure of the error at that node, and calculating the expected reduction in error as a result of testing each attribute at that node. The attribute that maximizes the expected error reduction is chosen for splitting at the node.</p>
<p id="p1405" class="para_indented">The expected error reduction, which we call SDR for <em>standard deviation reduction</em>, is calculated by</p>
<p class="figure" id="e0185"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si36.jpg" alt="image" width="265" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si36.jpg"></p>
<p>where <em>T</em>
<span class="sub">1</span>, <em>T</em>
<span class="sub">2</span>, … are the sets that result from splitting the node according to the chosen attribute, and <em>sd</em>(<em>T</em>) is the standard deviation of the class values.</p>
<p id="p1410" class="para_indented">The splitting process terminates when the class values of the instances that reach a node vary just slightly—that is, when their standard deviation is only a small fraction (say less than 5%) of the standard deviation of the original instance set. Splitting also terminates when just a few instances remain (say four or fewer). Experiments show that the results obtained are not very sensitive to the exact choice of these parameters.</p>
</div>
<div id="s0235">
<h3 id="st0230">Pruning the Tree</h3>
<p id="p1415" class="noindent">As noted earlier, a linear model is needed for each interior node of the tree, not just at the leaves, for use in the smoothing process. Before pruning, a model is calculated for each node of the unpruned tree. The model takes the form</p>
<p class="figure" id="e0190"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si37.jpg" alt="image" width="242" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si37.jpg"></p>
<p>where <em>a</em>
<span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>, …, <em>a<span class="sub">k</span>
</em> are attribute values. The weights <em>w</em>
<span class="sub">1</span>, <em>w</em>
<span class="sub">2</span>, …, <em>w<span class="sub">k</span>
</em> are calculated using standard regression. However, only a subset of the attributes are generally used here—for example, those that are tested in the subtree below this node and perhaps those occurring along the path to the root node. Note that we have tacitly assumed that attributes are numeric; we describe the handling of nominal attributes in the next section.</p>
<p id="p1420" class="para_indented">The pruning procedure makes use of an estimate, at each node, of the expected error for test data. First, the absolute difference between the predicted value and the actual class value is averaged over each of the training instances that reach that node. Because the tree has been built expressly for this dataset, this average will underestimate the expected error for unseen cases. To compensate, it is multiplied by the factor (<em>n</em> + <em>ν</em>)/(<em>n</em> – <em>ν</em>), where <em>n</em> is the number of training instances that reach the node and <em>ν</em> is the number of parameters in the linear model that gives the class value at that node.</p>
<p id="p1425" class="para_indented"><a id="p254"></a>The expected error for test data at a node is calculated as described previously, using the linear model for prediction. Because of the compensation factor (<em>n</em> + <em>ν</em>)/(<em>n</em> – <em>ν</em>), it may be that the linear model can be further simplified by dropping terms to minimize the estimated error. Dropping a term decreases the multiplication factor, which may be enough to offset the inevitable increase in average error over the training instances. Terms are dropped one by one, greedily, as long as the error estimate decreases.</p>
<p id="p1430" class="para_indented">Finally, once a linear model is in place for each interior node, the tree is pruned back from the leaves as long as the expected estimated error decreases. The expected error for the linear model at that node is compared with the expected error from the subtree below. To calculate the latter, the error from each branch is combined into a single, overall value for the node by weighting the branch by the proportion of the training instances that go down it and combining the error estimates linearly using those weights. Alternatively, one can calculate the training error of the subtree and multiply it by the above modification factor based on an ad hoc estimate of the number of parameters in the tree—perhaps adding one for each split point.</p>
</div>
<div id="s0240">
<h3 id="st0235">Nominal Attributes</h3>
<p id="p1435" class="noindent">Before constructing a model tree, all nominal attributes are transformed into binary variables that are then treated as numeric. For each nominal attribute, the average class value corresponding to each possible value in the set is calculated from the training instances, and the values are sorted according to these averages. Then, if the nominal attribute has <em>k</em> possible values, it is replaced by <em>k</em> – 1 synthetic binary attributes, the <em>i</em>th being 0 if the value is one of the first <em>i</em> in the ordering and 1 otherwise. Thus, all splits are binary: They involve either a numeric attribute or a synthetic binary attribute that is treated as numeric.</p>
<p id="p1440" class="para_indented">It is possible to prove analytically that the best split at a node for a nominal variable with <em>k</em> values is one of the <em>k</em> – 1 positions obtained by ordering the average class values for each value of the attribute. This sorting operation should really be repeated at each node; however, there is an inevitable increase in noise due to small numbers of instances at lower nodes in the tree (and in some cases nodes may not represent all values for some attributes), and not much is lost by performing the sorting just once before starting to build a model tree.</p>
</div>
<div id="s0245">
<h3 id="st0240">Missing Values</h3>
<p id="p1450" class="noindent">To take account of missing values, a modification is made to the SDR formula. The final formula, including the missing value compensation, is</p>
<p class="figure" id="e0195"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si38.jpg" alt="image" width="360" height="69" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si38.jpg"></p>
<p>where <em>m</em> is the number of instances without missing values for that attribute, and <em>T</em> is the set of instances that reach this node. <em>T<span class="sub">L</span>
</em>, <em>T<span class="sub">R</span>
</em> are sets that result from splitting on this attribute because all tests on attributes are now binary.</p>
<p id="p1455" class="para_indented"><a id="p255"></a>When processing both training and test instances, once an attribute is selected for splitting it is necessary to divide the instances into subsets according to their value for this attribute. An obvious problem arises when the value is missing. An interesting technique called <em>surrogate splitting</em> has been developed to handle this situation. It involves finding another attribute to split on in place of the original one and using it instead. The attribute is chosen as the one most highly correlated with the original attribute. However, this technique is both complex to implement and time consuming to execute.</p>
<p id="p1460" class="para_indented">A simpler heuristic is to use the class value as the surrogate attribute, in the belief that, a priori, this is the attribute most likely to be correlated with the one being used for splitting. Of course, this is only possible when processing the training set because for test examples the class is not known. A simple solution for test examples is simply to replace the unknown attribute value by the average value of that attribute for the training examples that reach the node, which has the effect, for a binary attribute, of choosing the most populous subnode. This simple approach seems to work well in practice.</p>
<p id="p1465" class="para_indented">Let’s consider in more detail how to use the class value as a surrogate attribute during the training process. We first deal with all instances for which the value of the splitting attribute is known. We determine a threshold for splitting in the usual way, by sorting the instances according to the splitting attribute’s value and, for each possible split point, calculating the SDR according to the preceding formula, choosing the split point that yields the greatest reduction in error. Only the instances for which the value of the splitting attribute is known are used to determine the split point.</p>
<p id="p1470" class="para_indented">Next we divide these instances into the two sets <em>L</em> and <em>R</em> according to the test. We determine whether the instances in <em>L</em> or <em>R</em> have the greater average class value, and we calculate the average of these two averages. Then an instance for which this attribute value is unknown is placed into <em>L</em> or <em>R</em> according to whether its class value exceeds this overall average or not. If it does, it goes into whichever of <em>L</em> and <em>R</em> has the greater average class value; otherwise, it goes into the one with the smaller average class value. When the splitting stops, all the missing values will be replaced by the average values of the corresponding attributes of the training instances reaching the leaves.</p>
</div>
<div id="s0250">
<h3 id="st0245">Pseudocode for Model Tree Induction</h3>
<p id="p1475" class="noindent"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0090">Figure 6.17</a> gives pseudocode for the model tree algorithm we have described. The two main parts are creating a tree by successively splitting nodes, performed by <em>split</em>, and pruning it from the leaves upward, performed by <em>prune</em>. The <em>node</em> data structure contains a type flag indicating whether it is an internal node or a leaf, pointers to the left and right child, the set of instances that reach that node, the attribute that is used for splitting at that node, and a structure representing the linear model for the node.</p>
<p id="f0090" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-017-9780123748560.jpg" alt="image" width="514" height="626" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-017-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.17</span> Pseudocode for model tree induction.</p>
<p id="p1480" class="para_indented">The <span class="monospace">sd</span> function called at the beginning of the main program and again at the beginning of <em>split</em> calculates the standard deviation of the class values of a set of instances. This is followed by the procedure for obtaining synthetic binary attributes that was described previously. Standard procedures for creating new nodes and printing the final tree are not shown. In <em>split</em>, <span class="monospace">sizeof</span> returns the number of elements in a <a id="p256"></a>set. Missing attribute values are dealt with as described earlier. The SDR is calculated according to the equation at the beginning of the previous section. Although not shown in the code, it is set to infinity if splitting on the attribute would create a leaf with less than two instances. In <em>prune</em>, the <span class="monospace">linearRegression</span> routine recursively descends the subtree collecting attributes, performs a linear regression on the instances <a id="p257"></a>at that node as a function of those attributes, and then greedily drops terms if doing so improves the error estimate, as described earlier. Finally, the <em>error</em> function returns</p>
<p class="figure" id="e0200"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si39.jpg" alt="image" width="442" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si39.jpg"></p>
<p>where <em>n</em> is the number of instances at the node and ν is the number of parameters in the node’s linear model.</p>
<p id="p1485" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0095">Figure 6.18</a> gives an example of a model tree formed by this algorithm for a problem with two numeric and two nominal attributes. What is to be predicted is the rise time of a simulated servo system involving a servo amplifier, motor, lead screw, and sliding carriage. The nominal attributes play important roles. Four synthetic binary attributes have been created for each of the five-valued nominal attributes <em>motor</em> and <em>screw</em> and are shown in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0030">Table 6.2</a> in terms of the two sets of values to which they correspond. The ordering of these values—D, E, C, B, A for <em>motor</em> and coincidentally D, E, C, B, A for <em>screw</em> also—is determined from the training data: the rise time averaged over all examples for which <em>motor</em> = D is less than that averaged over examples for which <em>motor</em> = E, which is less than when <em>motor</em> = C, and so on. It is apparent from the magnitude of the coefficients in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#t0030">Table 6.2</a> that <em>motor</em> = D versus E, C, B, A and <em>screw</em> = D, E, C, B versus A play leading roles in the LM2, LM3, and LM4 models (among others). Both <em>motor</em> and <em>screw</em> also play a minor role in several of the models.</p>
<p id="f0095" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-018-9780123748560.jpg" alt="image" width="437" height="365" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-018-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.18</span> Model tree for a dataset with nominal attributes.</p><a id="p258"></a><p class="table_caption"><span class="tab_num">Table 6.2. </span> Linear Models in the Model Tree</p>
<p id="t0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000067tabt0030.jpg" alt="Image" width="872" height="392" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000067tabt0030.jpg"></p>
</div>
<div id="s0255">
<h3 id="st0250"><a id="p259"></a>Rules from Model Trees</h3>
<p id="p1490" class="noindent">Model trees are essentially decision trees with linear models at the leaves. Like decision trees, they may suffer from the replicated subtree problem explained in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, and sometimes the structure can be expressed much more concisely using a set of rules instead of a tree. Can we generate <em>rules</em> for numeric prediction? Recall the rule learner described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a> that uses separate-and-conquer in conjunction with partial decision trees to extract decision rules from trees. The same strategy can be applied to model trees to generate decision lists for numeric prediction.</p>
<p id="p1495" class="para_indented">First build a partial model tree from all the data. Pick one of the leaves and make it into a rule. Remove the data covered by that leaf, then repeat the process with the remaining data. The question is, how do we build the partial model tree—that is, a tree with unexpanded nodes? This boils down to the question of how to pick which node to expand next. The algorithm of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0030">Figure 6.5</a> (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>) picks the node of which the entropy for the class attribute is smallest. For model trees, the predictions of which are numeric, simply use the standard deviation instead. This is based on the same rationale: The lower the standard deviation, the shallower the subtree and the shorter the rule. The rest of the algorithm stays the same, with the model tree learner’s split selection method and pruning strategy replacing the decision tree learner’s. Because the model tree’s leaves are linear models, the corresponding rules will have linear models on the right side.</p>
<p id="p1500" class="para_indented">There is one caveat when using model trees in this fashion to generate rule sets. It turns out that using smoothed model trees does not reduce the error in the final rule set’s predictions. This may be because smoothing works best for contiguous data, but the separate-and-conquer scheme removes data covered by previous rules, leaving holes in the distribution. Smoothing, if it is done at all, must be performed after the rule set has been generated.</p>
</div>
<div id="s0260">
<h3 id="st0255">Locally Weighted Linear Regression</h3>
<p id="p1505" class="noindent">An alternative approach to numeric prediction is the method of locally weighted linear regression. With model trees, the tree structure divides the instance space into regions, and a linear model is found for each of them. In effect, the training data determines how the instance space is partitioned. Locally weighted regression, on the other hand, generates local models at prediction time by giving higher weight to instances in the neighborhood of the particular test instance. More specifically, it weights the training instances according to their distance to the test instance and performs a linear regression on the weighted data. Training instances close to the test instance receive a high weight; those far away, a low one. In other words, a linear model is tailor-made for the particular test instance at hand and used to predict the instance’s class value.</p>
<p id="p1510" class="para_indented">To use locally weighted regression, you need to decide on a distance-based weighting scheme for the training instances. A common choice is to weight the <a id="p260"></a>instances according to the inverse of their Euclidean distance from the test instance. Another possibility is to use the Euclidean distance in conjunction with a Gaussian kernel function. However, there is no clear evidence that the choice of weighting function is critical. More important is the selection of a “smoothing parameter” that is used to scale the distance function—the distance is multiplied by the inverse of this parameter. If it is set to a small value, only instances very close to the test instance will receive significant weight; if it is large, more distant instances will also have a significant impact on the model.</p>
<p id="p9010" class="para_indented">One way of choosing the smoothing parameter is to set it to the distance of the <em>k</em>th-nearest training instance so that its value becomes smaller as the volume of training data increases. If the weighting function is linear, say 1 – <em>distance</em>, the weight is 0 for all instances further than the <em>k</em>th-nearest one. Then the weighting function has bounded support and only the (<em>k</em> – 1)th-nearest neighbors need to be considered for building the linear model. The best choice of <em>k</em> depends on the amount of noise in the data. The more noise there is, the more neighbors should be included in the linear model. Generally, an appropriate smoothing parameter is found using cross-validation.</p>
<p id="p1515" class="para_indented">Like model trees, locally weighted linear regression is able to approximate nonlinear functions. One of its main advantages is that it is ideally suited for incremental learning: All training is done at prediction time, so new instances can be added to the training data at any time. However, like other instance-based methods, it is slow at deriving a prediction for a test instance. First, the training instances must be scanned to compute their weights; then a weighted linear regression is performed on these instances. Also, like other instance-based methods, locally weighted regression provides little information about the global structure of the training dataset. Note that if the smoothing parameter is based on the <em>k</em>th-nearest neighbor and the weighting function gives zero weight to more distant instances, the <em>k</em>D-trees (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p132">page 132</a>) and ball trees described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a> can be used to accelerate the process of finding the relevant neighbors.</p>
<p id="p1520" class="para_indented">Locally weighted learning is not restricted to linear regression: It can be applied with any learning technique that can handle weighted instances. In particular, you can use it for classification. Most algorithms can be easily adapted to deal with weights. The trick is to realize that (integer) weights can be simulated by creating several copies of the same instance. Whenever the learning algorithm uses an instance when computing a model, just pretend that it is accompanied by the appropriate number of identical shadow instances. This also works if the weight is not an integer. For example, in the Naïve Bayes algorithm described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>, multiply the counts derived from an instance by the instance’s weight, and—Voilà!—you have a version of Naïve Bayes that can be used for locally weighted learning.</p>
<p id="p1525" class="para_indented">It turns out that locally weighted Naïve Bayes works quite well in practice, outperforming both Naïve Bayes itself and the <em>k</em>-nearest-neighbor technique. It also compares favorably with more sophisticated ways of enhancing Naïve Bayes by relaxing its intrinsic independence assumption. Locally weighted learning only assumes independence within a neighborhood, not globally in the whole instance space as standard Naïve Bayes does.</p>
<p id="p1530" class="para_indented"><a id="p261"></a>In principle, locally weighted learning can also be applied to decision trees and other models that are more complex than linear regression and Naïve Bayes. However, it is less beneficial here because it is primarily a way of allowing simple models to become more flexible by allowing them to approximate arbitrary targets. If the underlying learning algorithm can already do that, there is little point in applying locally weighted learning. Nevertheless, it may improve other simple models—for example, linear support vector machines and logistic regression.</p>
</div>
<div id="s0265">
<h3 id="st0260">Discussion</h3>
<p id="p1535" class="noindent">Regression trees were introduced in the CART system of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib41">Breiman et al. (1984)</a>. CART, for <em>classification and regression trees</em>, incorporated a decision tree inducer for discrete classes like that of C4.5, as well as a scheme for inducing regression trees. Many of the techniques described in this section, such as the method of handling nominal attributes and the surrogate device for dealing with missing values, were included in CART. However, model trees did not appear until much more recently, being first described by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib256">Quinlan (1992)</a>. Using model trees for generating rule sets (although not partial trees) has been explored by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib147">Hall et al. (1999)</a>.</p>
<p id="p1540" class="para_indented">A comprehensive description (and implementation) of model tree induction is given by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib307">Wang and Witten (1997)</a>. Neural nets are also commonly used for predicting numeric quantities, although they suffer from the disadvantage that the structures they produce are opaque and cannot be used to help understand the nature of the solution. There are techniques for producing understandable insights from the structure of neural networks, but the arbitrary nature of the internal representation means that there may be dramatic variations between networks of identical architecture trained on the same data. By dividing the function being induced into linear patches, model trees provide a representation that is reproducible and at least somewhat comprehensible.</p>
<p id="p1545" class="para_indented">There are many variations of locally weighted learning. For example, statisticians have considered using locally quadratic models instead of linear ones and have applied locally weighted logistic regression to classification problems. Also, many different potential weighting and distance functions can be found in the literature. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib14">Atkeson et al. (1997)</a> have written an excellent survey on locally weighted learning, primarily in the context of regression problems. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib9002">Frank et al. (2003)</a> evaluated the use of locally weighted learning in conjunction with Naïve Bayes.</p>
</div>
</div>
<div id="s0270">
<h2 id="st0265">6.7 Bayesian networks</h2>
<p id="p1550" class="noindent">The Naïve Bayes classifier of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a> and the logistic regression models of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a> both produce probability estimates rather than hard classifications. For each class value, they estimate the probability that a given instance belongs to that class. Most other types of classifiers can be coerced into yielding this kind of information if necessary. For example, probabilities can be obtained from a decision tree <a id="p262"></a>by computing the relative frequency of each class in a leaf and from a decision list by examining the instances that a particular rule covers.</p>
<p id="p1555" class="para_indented">Probability estimates are often more useful than plain predictions. They allow predictions to be ranked and their expected cost to be minimized (see <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>). In fact, there is a strong argument for treating classification learning as the task of learning class probability estimates from data. What is being estimated is the conditional probability distribution of the values of the class attribute given the values of the other attributes. Ideally, the classification model represents this conditional distribution in a concise and easily comprehensible form.</p>
<p id="p1560" class="para_indented">Viewed in this way, Naïve Bayes classifiers, logistic regression models, decision trees, and so on, are just alternative ways of representing a conditional probability distribution. Of course, they differ in representational power. Naïve Bayes classifiers and logistic regression models can only represent simple distributions, whereas decision trees can represent—or at least approximate—arbitrary distributions. However, decision trees have their drawbacks: They fragment the training set into smaller and smaller pieces, which inevitably yields less reliable probability estimates, and they suffer from the replicated subtree problem described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>. Rule sets go some way toward addressing these shortcomings, but the design of a good rule learner is guided by heuristics with scant theoretical justification.</p>
<p id="p1565" class="para_indented">Does this mean that we have to accept our fate and live with these shortcomings? No! There is a statistically based alternative: a theoretically well-founded way of representing probability distributions concisely and comprehensibly in a graphical manner; the structures are called <em>Bayesian networks</em>. They are drawn as a network of nodes, one for each attribute, connected by directed edges in such a way that there are no cycles—a <em>directed acyclic graph</em>.</p>
<p id="p1570" class="para_indented">In our explanation of how to interpret Bayesian networks and how to learn them from data, we will make some simplifying assumptions. We assume that all attributes are nominal and that there are no missing values. Some advanced learning algorithms can create new attributes in addition to the ones present in the data—so-called hidden attributes with values that cannot be observed. These can support better models if they represent salient features of the underlying problem, and Bayesian networks provide a good way of using them at prediction time. However, they make both learning and prediction far more complex and time consuming, so we will not consider them here.</p>
<div id="s0275">
<h3 id="st0270">Making Predictions</h3>
<p id="p1575" class="noindent"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0100">Figure 6.19</a> shows a simple Bayesian network for the weather data. It has a node for each of the four attributes <em>outlook</em>, <em>temperature</em>, <em>humidity</em>, and <em>windy</em> and one for the class attribute <em>play</em>. An edge leads from the <em>play</em> node to each of the other nodes. However, in Bayesian networks the structure of the graph is only half the story. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0100">Figure 6.19</a> shows a table inside each node. The information in the tables defines a probability distribution that is used to predict the class probabilities for any given instance.</p>
<p id="f0100" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-019-9780123748560.jpg" alt="image" width="509" height="501" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-019-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.19</span> A simple Bayesian network for the weather data.</p>
<p id="p1580" class="para_indented"><a id="p263"></a>Before looking at how to compute this probability distribution, consider the information in the tables. The lower four tables (for <em>outlook</em>, <em>temperature</em>, <em>humidity</em>, and <em>windy</em>) have two parts separated by a vertical line. On the left are the values of <em>play</em>, and on the right are the corresponding probabilities for each value of the attribute represented by the node. In general, the left side contains a column for every edge pointing to the node, in this case just the <em>play</em> attribute. That is why the table associated with <em>play</em> itself does not have a left side: It has no parents. Each row of probabilities corresponds to one combination of values of the parent attributes, and the entries in the row show the probability of each value of the node’s attribute given this combination. In effect, each row defines a probability distribution over the values of the node’s attribute. The entries in a row always sum to 1.</p>
<p id="p1585" class="para_indented"><a id="p264"></a><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">Figure 6.20</a> shows a more complex network for the same problem, where three nodes (<em>windy</em>, <em>temperature</em>, and <em>humidity</em>) have two parents. Again, there is one column on the left for each parent and as many columns on the right as the attribute has values. Consider the first row of the table associated with the <em>temperature</em> node. The left side gives a value for each parent attribute, <em>play</em> and <em>outlook</em>; the right gives a probability for each value of <em>temperature</em>. For example, the first number (0.143) is the probability of <em>temperature</em> taking on the value <em>hot</em>, given that <em>play</em> and <em>outlook</em> have values <em>yes</em> and <em>sunny</em>, respectively.</p><a id="p265"></a><p id="f0105" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-020-9780123748560.jpg" alt="image" width="510" height="657" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-020-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.20</span> Another Bayesian network for the weather data.</p>
<p id="p1590" class="para_indented">How are the tables used to predict the probability of each class value for a given instance? This turns out to be very easy because we are assuming that there are no missing values. The instance specifies a value for each attribute. For each node in the network, look up the probability of the node’s attribute value based on the row determined by its parents’ attribute values. Then just multiply all these probabilities together.</p>
<p id="p1595" class="para_indented">For example, consider an instance with values <em>outlook = rainy</em>, <em>temperature = cool</em>, <em>humidity = high</em>, and <em>windy = true</em>. To calculate the probability for <em>play = no</em>, observe that the network in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">Figure 6.20</a> gives probability 0.367 from node <em>play,</em> 0.385 from <em>outlook</em>, 0.429 from <em>temperature</em>, 0.250 from <em>humidity</em>, and 0.167 from <em>windy</em>. The product is 0.0025. The same calculation for <em>play = yes</em> yields 0.0077. However, these are clearly not the final answers: The final probabilities must sum to 1, whereas 0.0025 and 0.0077 don’t. They are actually the joint probabilities Pr[<em>play</em> = <em>no</em>, <em>E</em>] and Pr[<em>play</em> = <em>yes</em>, <em>E</em>], where <em>E</em> denotes all the evidence given by the instance’s attribute values. Joint probabilities measure the likelihood of observing an instance that exhibits the attribute values in <em>E</em> as well as the respective class value. They only sum to 1 if they exhaust the space of all possible attribute–value combinations, including the class attribute. This is certainly not the case in our example.</p>
<p id="p1600" class="para_indented">The solution is quite simple (we already encountered it in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>). To obtain the conditional probabilities Pr[<em>play</em> = <em>no</em> | <em>E</em>] and Pr[<em>play</em> = <em>yes</em> | <em>E</em>], normalize the joint probabilities by dividing them by their sum. This gives probability 0.245 for <em>play = no</em> and 0.755 for <em>play = yes</em>.</p>
<p id="p1605" class="para_indented">Just one mystery remains: Why multiply all those probabilities together? It turns out that the validity of the multiplication step hinges on a single assumption—namely that, given values for each of a node’s parents, knowing the values for any other set of nondescendants does not change the probability associated with each of the node’s possible values. In other words, other sets of nondescendants do not provide any information about the likelihood of the node’s values over and above the information provided by the parents. This can be written as</p>
<p class="figure" id="e0205"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si40.jpg" alt="image" width="581" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si40.jpg"></p>
<p>which must hold for all values of the nodes and attributes involved. In statistics this property is called <em>conditional independence</em>. Multiplication is valid provided that each node is conditionally independent of its grandparents, great-grandparents, and <a id="p266"></a>indeed any other set of nondescendants, given its parents. The multiplication step follows as a direct result of the chain rule in probability theory, which states that the joint probability of <em>m</em> attributes <em>a<span class="sub">i</span>
</em> can be decomposed into this product:</p>
<p class="figure" id="e0210"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si41.jpg" alt="image" width="338" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si41.jpg"></p>
<p></p>
<p id="p1610" class="para_indented">The decomposition holds for any order of the attributes. Because our Bayesian network is an acyclic graph, its nodes can be ordered to give all ancestors of a node <em>a<span class="sub">i</span>
</em> indices smaller than <em>i</em>. Then, because of the conditional independence assumption,</p>
<p class="figure" id="e0215"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si42.jpg" alt="image" width="540" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si42.jpg"></p>
<p>which is exactly the multiplication rule that we applied earlier.</p>
<p id="p1615" class="para_indented">The two Bayesian networks in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0100">Figures 6.19</a> and <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">6.20</a> are fundamentally different. The first (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0100">Figure 6.19</a>) makes stronger independence assumptions because for each of its nodes the set of parents is a subset of the corresponding set of parents in the second (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">Figure 6.20</a>). In fact, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0100">Figure 6.19</a> is almost identical to the simple Naïve Bayes classifier of <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>. (The probabilities are slightly different but only because each count has been initialized to 0.5 to avoid the zero-frequency problem.) The network in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">Figure 6.20</a> has more rows in the conditional probability tables and hence more parameters; it may be a more accurate representation of the underlying domain.</p>
<p id="p1620" class="para_indented">It is tempting to assume that the directed edges in a Bayesian network represent causal effects. But be careful! In our case, a particular value of <em>play</em> may enhance the prospects of a particular value of <em>outlook</em>, but it certainly doesn’t cause it—it is more likely to be the other way around. Different Bayesian networks can be constructed for the same problem, representing exactly the same probability distribution. This is done by altering the way in which the joint probability distribution is factorized to exploit conditional independencies. The network that has directed edges model causal effects is often the simplest one with the fewest parameters. Thus, human experts who construct Bayesian networks for a particular domain often benefit by representing causal effects by directed edges. However, when machine learning techniques are applied to induce models from data whose causal structure is unknown, all they can do is construct a network based on the correlations that are observed in the data. Inferring causality from correlation is always a dangerous business.</p>
</div>
<div id="s0280">
<h3 id="st0275">Learning Bayesian Networks</h3>
<p id="p1625" class="noindent">The main way to construct a learning algorithm for Bayesian networks is to define two components: a function for evaluating a given network based on the data and a <a id="p267"></a>method for searching through the space of possible networks. The quality of a given network is measured by the probability of the data given the network. We calculate the probability that the network accords to each instance and multiply these probabilities together over all instances. In practice, this quickly yields numbers too small to be represented properly (called <em>arithmetic underflow</em>), so we use the sum of the logarithms of the probabilities rather than their product. The resulting quantity is the log-likelihood of the network given the data.</p>
<p id="p1630" class="para_indented">Assume that the structure of the network—the set of edges—is given. It’s easy to estimate the numbers in the conditional probability tables: Just compute the relative frequencies of the associated combinations of attribute values in the training data. To avoid the zero-frequency problem each count is initialized with a constant as described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>. For example, to find the probability that <em>humidity = normal</em> given that <em>play = yes</em> and <em>temperature = cool</em> (the last number of the third row of the <em>humidity</em> node’s table in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0105">Figure 6.20</a>), observe from <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a> (<a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p10">page 10</a>) that there are three instances with this combination of attribute values in the weather data and no instances with <em>humidity = high</em> and the same values for <em>play</em> and <em>temperature</em>. Initializing the counts for the two values of <em>humidity</em> to 0.5 yields the probability (3 + 0.5)/(3 + 0 + 1) = 0.875 for <em>humidity = normal</em>.</p>
<p id="p1635" class="para_indented">The nodes in the network are predetermined, one for each attribute (including the class). Learning the network structure amounts to searching through the space of possible sets of edges, estimating the conditional probability tables for each set, and computing the log-likelihood of the resulting network based on the data as a measure of the network’s quality. Bayesian network learning algorithms differ mainly in the way in which they search through the space of network structures. Some algorithms are introduced below.</p>
<p id="p1640" class="para_indented">There is one caveat. If the log-likelihood is maximized based on the training data, it will always be better to add more edges: The resulting network will simply overfit. Various methods can be employed to combat this problem. One possibility is to use cross-validation to estimate the goodness of fit. A second is to add a penalty for the complexity of the network based on the number of parameters—that is, the total number of independent estimates in all the probability tables. For each table, the number of independent probabilities is the total number of entries minus the number of entries in the last column, which can be determined from the other columns because all rows must sum to 1. Let <em>K</em> be the number of parameters, <em>LL</em> the log-likelihood, and <em>N</em> the number of instances in the data. Two popular measures for evaluating the quality of a network are the <em>Akaike Information Criterion</em> (AIC):</p>
<p class="figure" id="e0220"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si43.jpg" alt="image" width="187" height="23" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si43.jpg"></p>
<p>and the following <em>MDL metric</em> based on the MDL principle:</p>
<p class="figure" id="e0225"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si44.jpg" alt="image" width="250" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si44.jpg"></p>
<p></p>
<p id="p1645" class="para_indented">In both cases the log-likelihood is negated, so the aim is to minimize these scores.</p>
<p id="p1650" class="para_indented"><a id="p268"></a>A third possibility is to assign a prior distribution over network structures and find the most likely network by combining its prior probability with the probability accorded to the network by the data. This is the “Bayesian” approach to network scoring. Depending on the prior distribution used, it can take various forms. However, true Bayesians would average over all possible network structures rather than singling out one particular network for prediction. Unfortunately, this generally requires a great deal of computation. A simplified approach is to average over all network structures that are substructures of a given network. It turns out that this can be implemented very efficiently by changing the method for calculating the conditional probability tables so that the resulting probability estimates implicitly contain information from all subnetworks. The details of this approach are rather complex and will not be described here.</p>
<p id="p1655" class="para_indented">The task of searching for a good network structure can be greatly simplified if the right metric is used for scoring. Recall that the probability of a single instance based on a network is the product of all the individual probabilities from the various conditional probability tables. The overall probability of the dataset is the product of these products for all instances. Because terms in a product are interchangable, the product can be rewritten to group together all factors relating to the same table. The same holds for the log-likelihood, using sums instead of products. This means that the likelihood can be optimized separately for each node of the network. This can be done by adding, or removing, edges from other nodes to the node that is being optimized—the only constraint is that cycles must not be introduced. The same trick also works if a local scoring metric such as AIC or MDL is used instead of plain log-likelihood, because the penalty term splits into several components, one for each node, and each node can be optimized independently.</p>
</div>
<div id="s0285">
<h3 id="st0280">Specific Algorithms</h3>
<p id="p1660" class="noindent">Now we move on to actual algorithms for learning Bayesian networks. One simple and very fast learning algorithm, called <em>K2</em>, starts with a given ordering of the attributes (i.e., nodes). Then it processes each node in turn and greedily considers adding edges from previously processed nodes to the current one. In each step it adds the edge that maximizes the network’s score. When there is no further improvement, attention turns to the next node. As an additional mechanism for overfitting avoidance, the number of parents for each node can be restricted to a predefined maximum. Because only edges from previously processed nodes are considered and there is a fixed ordering, this procedure cannot introduce cycles. However, the result depends on the initial ordering, so it makes sense to run the algorithm several times with different random orderings.</p>
<p id="p1665" class="para_indented">The Naïve Bayes classifier is a network with an edge leading from the class attribute to each of the other attributes. When building networks for classification, it sometimes helps to use this network as a starting point for the search. This can be done in K2 by forcing the class variable to be the first one in the ordering and initializing the set of edges appropriately.</p>
<p id="p1670" class="para_indented"><a id="p269"></a>Another potentially helpful trick is to ensure that every attribute in the data is in the <em>Markov blanket</em> of the node that represents the class attribute. A node’s Markov blanket includes all its parents, children, and children’s parents. It can be shown that a node is conditionally independent of all other nodes given values for the nodes in its Markov blanket. Thus, if a node is absent from the class attribute’s Markov blanket, its value is completely irrelevant to the classification. Conversely, if K2 finds a network that does not include a relevant attribute in the class node’s Markov blanket, it might help to add an edge that rectifies this shortcoming. A simple way of doing this is to add an edge from the attribute’s node to the class node or from the class node to the attribute’s node, depending on which option avoids a cycle.</p>
<p id="p1675" class="para_indented">A more sophisticated but slower version of K2 is not to order the nodes but to greedily consider adding or deleting edges between arbitrary pairs of nodes (all the while ensuring acyclicity, of course). A further step is to consider inverting the direction of existing edges as well. As with any greedy algorithm, the resulting network only represents a <em>local</em> maximum of the scoring function: It is always advisable to run such algorithms several times with different random initial configurations. More sophisticated optimization strategies such as simulated annealing, tabu search, or genetic algorithms can also be used.</p>
<p id="p1680" class="para_indented">Another good learning algorithm for Bayesian network classifiers is called <em>tree-augmented Naïve Bayes</em> (TAN). As the name implies, it takes the Naïve Bayes classifier and adds edges to it. The class attribute is the single parent of each node of a Naïve Bayes network. TAN considers adding a second parent to each node. If the class node and all corresponding edges are excluded from consideration, and assuming that there is exactly one node to which a second parent is not added, the resulting classifier has a tree structure rooted at the parentless node—this is where the name comes from. For this restricted type of network there is an efficient algorithm for finding the set of edges that maximizes the network’s likelihood based on computing the network’s maximum weighted spanning tree. This algorithm is linear in the number of instances and quadratic in the number of attributes.</p>
<p id="p1685" class="para_indented">The type of network learned by the TAN algorithm is called a <em>one-dependence estimator</em>. An even simpler type of network is the <em>superparent one-dependence estimator</em>. Here, exactly one other node, apart from the class node, is elevated to parent status and becomes the parent of every other nonclass node. It turns out that a simple ensemble of these one-dependence estimators yields very accurate classifiers: In each of these estimators, a different attribute becomes the extra parent node. Then, at prediction time, class probability estimates from the different one-dependence estimators are simply averaged. This scheme is known as <em>AODE</em>, for <em>averaged one-dependence estimator</em>. Normally, only estimators with certain supports in the data are used in the ensemble, but more sophisticated selection schemes are possible. Because no structure learning is involved for each superparent one-dependence estimator, AODE is a very efficient classifier.</p>
<p id="p1690" class="para_indented">All the scoring metrics that we have described so far are likelihood-based in the sense that they are designed to maximize the joint probability Pr[<em>a</em>
<span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>, …, <em>a<span class="sub">n</span>
</em>] for each instance. However, in classification, what we really want to maximize is the <a id="p270"></a>conditional probability of the class given the values of the other attributes—in other words, the conditional likelihood. Unfortunately, there is no closed-form solution for the maximum conditional-likelihood probability estimates that are needed for the tables in a Bayesian network. On the other hand, computing the conditional likelihood for a given network and dataset is straightforward—after all, this is what logistic regression does. Thus, it has been proposed to use standard maximum-likelihood probability estimates in the network, but to use the conditional likelihood to evaluate a particular network structure.</p>
<p id="p1695" class="para_indented">Another way of using Bayesian networks for classification is to build a separate network for each class value, based on the data pertaining to that class, and combine their predictions using Bayes’ rule. The set of networks is called a <em>Bayesian multinet</em>. To obtain a prediction for a particular class value, take the corresponding network’s probability and multiply it by the class’s prior probability. Do this for each class and normalize the result as we did previously. In this case we would not use the conditional likelihood to learn the network for each class value.</p>
<p id="p1700" class="para_indented">All the network learning algorithms we have introduced are score-based. A different strategy, which we will not explain here, is to piece a network together by testing individual conditional independence assertions based on subsets of the attributes. This is known as <em>structure learning by conditional independence tests</em>.</p>
</div>
<div id="s0290">
<h3 id="st0285">Data Structures for Fast Learning</h3>
<p id="p1705" class="noindent">Learning Bayesian networks involves a lot of counting. For each network structure considered in the search, the data must be scanned afresh to obtain the counts needed to fill out the conditional probability tables. Instead, could they be stored in a data structure that eliminated the need for scanning the data over and over again? An obvious way is to precompute the counts and store the nonzero ones in a table—say, the hash table mentioned in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>. Even so, any nontrivial dataset will have a huge number of nonzero counts.</p>
<p id="p1710" class="para_indented">Again, consider the weather data from <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a>. There are five attributes, two with three values and three with two values. This gives 4 × 4 × 3 × 3 × 3 = 432 possible counts. Each component of the product corresponds to an attribute, and its contribution to the product is one more than the number of its values because the attribute may be missing from the count. All these counts can be calculated by treating them as item sets, as explained in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>, and setting the minimum coverage to 1. But even without storing counts that are 0, this simple scheme runs into memory problems very quickly. The FP-growth data structure described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0100">Section 6.3</a> was designed for efficient representation of data in the case of item set mining. In the following, we describe a structure that has been used for Bayesian networks.</p>
<p id="p1715" class="para_indented">It turns out that the counts can be stored effectively in a structure called an <em>all-dimensions (AD) tree</em>, which is analogous to the <em>k</em>D-trees used for the nearest-neighbor search described in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a>. For simplicity, we illustrate this using a reduced version of the weather data that only has the attributes <em>humidity</em>, <em>windy</em>, and <em>play</em>. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(a)</a> summarizes the data. The number of possible counts is 3 × 3 × <a id="p271"></a>3 = 27, although only eight of them are shown. For example, the count for <em>play</em> = <em>no</em> is 5 (count them!).</p>
<p id="f0110" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-021ab-9780123748560.jpg" alt="image" width="512" height="435" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-021ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.21</span> The weather data: (a) reduced version and (b) corresponding AD-tree.</p>
<p id="p1720" class="para_indented"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(b)</a> shows an AD-tree for this data. Each node says how many instances exhibit the attribute values that are tested along the path from the root to that node. For example, the leftmost leaf says that there is one instance with values <em>humidity = normal, windy = true,</em> and <em>play = no</em>, and the rightmost leaf says that there are five instances with <em>play = no</em>.</p>
<p id="p1725" class="para_indented">It would be trivial to construct a tree that enumerates all 27 counts explicitly. However, that would gain nothing over a plain table and is obviously not what the tree in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(b)</a> does because it contains only 8 counts. There is, for example, no branch that tests <em>humidity = high</em>. How was the tree constructed, and how can all counts be obtained from it?</p>
<p id="p1730" class="para_indented">Assume that each attribute in the data has been assigned an index. In the reduced version of the weather data we give <em>humidity</em> index 1, <em>windy</em> index 2, and <em>play</em> index 3. An AD-tree is generated by expanding each node corresponding to an attribute <em>i</em> with the values of all attributes that have indices <em>j</em> &gt; <em>i</em>, with two important <a id="p272"></a>restrictions: The most populous expansion for each attribute is omitted (breaking ties arbitrarily) as are expansions with counts that are zero. The root node is given index 0, so for this node all attributes are expanded, subject to the same restrictions.</p>
<p id="p1735" class="para_indented">For example, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(b)</a> contains no expansion for <em>windy = false</em> from the root node because with eight instances it is the most populous expansion: The value <em>false</em> occurs more often in the data than the value <em>true</em>. Similarly, from the node labeled <em>humidity = normal</em> there is no expansion for <em>windy = false</em> because <em>false</em> is the most common value for <em>windy</em> among all instances with <em>humidity = normal.</em> In fact, in our example the second restriction—namely that expansions with zero counts are omitted—never kicks in because the first restriction precludes any path that starts with the tests <em>humidity = normal</em> and <em>windy = false</em>, which is the only way to reach the solitary 0 in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(a)</a>.</p>
<p id="p1740" class="para_indented">Each node of the tree represents the occurrence of a particular combination of attribute values. It is straightforward to retrieve the count for a combination that occurs in the tree. However, the tree does not explicitly represent many nonzero counts because the most populous expansion for each attribute is omitted. For example, the combination <em>humidity = high</em> and <em>play = yes</em> occurs three times in the data but has no node in the tree. Nevertheless, it turns out that any count can be calculated from those that the tree stores explicitly.</p>
<p id="p1745" class="para_indented">Here’s a simple example. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#f0110">Figure 6.21(b)</a> contains no node for <em>humidity = normal</em>, <em>windy = true</em>, and <em>play = yes</em>. However, it shows three instances with <em>humidity = normal</em> and <em>windy = true</em>, and one of them has a value for <em>play</em> that is different from <em>yes</em>. It follows that there must be two instances for <em>play = yes</em>. Now for a trickier case: How many times does <em>humidity = high</em>, <em>windy = true</em>, and <em>play = no</em> occur? At first glance it seems impossible to tell because there is no branch for <em>humidity = high</em>. However, we can deduce the number by calculating the count for <em>windy = true</em> and <em>play = no</em> (3) and subtracting the count for <em>humidity = normal</em>, <em>windy = true</em>, and <em>play = no</em> (1). This gives 2, the correct value.</p>
<p id="p1750" class="para_indented">This idea works for any subset of attributes and any combination of attribute values, but it may have to be applied recursively. For example, to obtain the count for <em>humidity = high</em>, <em>windy = false</em>, and <em>play = no</em>, we need the count for <em>windy = false</em> and <em>play = no</em> and the count for <em>humidity = normal</em>, <em>windy = false</em>, and <em>play = no</em>. We obtain the former by subtracting the count for <em>windy = true</em> and <em>play = no</em> (3) from the count for <em>play = no</em> (5), giving 2, and we obtain the latter by subtracting the count for <em>humidity = normal</em>, <em>windy = true</em>, and <em>play = no</em> (1) from the count for <em>humidity = normal</em> and <em>play = no</em> (1), giving 0. Thus, there must be 2 – 0 = 2 instances with <em>humidity = high</em>, <em>windy = false</em>, and <em>play = no</em>, which is correct.</p>
<p id="p1755" class="para_indented">AD-trees only pay off if the data contains many thousands of instances. It is pretty obvious that they do not help on the weather data. The fact that they yield no benefit on small datasets means that, in practice, it makes little sense to expand the tree all the way down to the leaf nodes. Usually, a cutoff parameter <em>k</em> is employed, and nodes covering fewer than <em>k</em> instances hold a list of pointers to these instances rather than a list of pointers to other nodes. This makes the trees smaller and more efficient to use.</p>
</div>
<div id="s0295">
<h3 id="st0290"><a id="p273"></a>Discussion</h3>
<p id="p1760" class="noindent">The K2 algorithm for learning Bayesian networks was introduced by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib64">Cooper and Herskovits (1992)</a>. Bayesian scoring metrics are covered by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib157">Heckerman et al. (1995)</a>. The TAN algorithm was introduced by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib124">Friedman et al. (1997)</a>, who also describe multinets. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib141">Grossman and Domingos (2004)</a> show how to use the conditional likelihood for scoring networks. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib143">Guo and Greiner (2004)</a> present an extensive comparison of scoring metrics for Bayesian network classifiers. <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib31">Bouckaert (1995)</a> describes averaging over subnetworks. Averaged one-dependence estimators are described by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib311">Webb et al. (2005)</a>. AD-trees were introduced and analyzed by <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib233">Moore and Lee (1998)</a>—the same Andrew Moore whose work on <em>k</em>D-trees and ball trees was mentioned in <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a>. In a more recent paper, <a href="/Library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib191">Komarek and Moore (2000)</a> introduce AD-trees for incremental learning that are also more efficient for datasets with many attributes.</p>
<p id="p1765" class="para_indented">We have only skimmed the surface of the subject of learning Bayesian networks. We left open questions of missing values, numeric attributes, and hidden attributes. We did not describe how to use Bayesian networks for regression tasks. Bayesian networks are a special case of a wider class of statistical models called <em>graphical models</em>, which include networks with undirected edges called <em>Markov networks</em>. Graphical models are attracting great attention in the machine learning community today.</p>
</div>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/Library/view/data-mining-practical/9780123748560/xhtml/p3.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">PART II. Advanced Data Mining</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006a.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">6.8. Clustering</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.7577409327368896"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.8014822747601538" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=0e74db19-dd91-57cf-51a1-75c9ee2f5534&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Chapter%206.%20Implementations%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fxhtml%2Fc0006.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=162668"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 201.01px; left: 1081.01px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/Library/view/data-mining-practical/9780123748560/xhtml/c0006.html#">Reset</a>
</div>
</div></body></html>