<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0011.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0011.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0011.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0011.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Chapter 11. The Explorer - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html"><meta name="description" content=" CHAPTER 11 The Explorer Weka’s main graphical user interface, the Explorer, gives access to all its facilities using menu selection and form filling. It is illustrated in Figure 11 ... "><meta property="og:title" content="Chapter 11. The Explorer"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Chapter 11. The Explorer"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0011.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content=" CHAPTER 11 The Explorer Weka’s main graphical user interface, the Explorer, gives access to all its facilities using menu selection and form filling. It is illustrated in Figure 11 ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/xhtml/c0011.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0011.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0011.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0011.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Chapter%2011.%20The%20Explorer&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0011.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0010.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 10. Introduction to Weka</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0012.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 12. The Knowledge Flow Interface</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="c0011"></a><h1 class="chapterlabel" id="c0011tit1">
<strong>CHAPTER</strong> 11</h1>
<h1 class="chaptertitle" id="c0011tit">The Explorer</h1>
<p id="p0010" class="noindent"><a id="p407"></a>Weka’s main graphical user interface, the Explorer, gives access to all its facilities using menu selection and form filling. It is illustrated in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0010">Figure 11.1</a>. There are six different panels, selected by the tabs at the top, corresponding to the various data mining tasks that Weka supports.</p>
<p id="f0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-001-9780123748560.jpg" alt="image" width="513" height="385" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-001-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.1</span> The Explorer interface.</p>
<div id="s0010">
<h2 id="st0010">11.1 Getting started</h2>
<p id="p0015" class="noindent">Suppose you have some data and you want to build a decision tree from it. First, you need to prepare the data, then fire up the Explorer and load it in. Next, you select a decision tree construction method, build a tree, and interpret the output. It’s easy to do it again with a different tree construction algorithm or a different evaluation method. In the Explorer you can flip back and forth between the results you have obtained, evaluate the models that have been built on different datasets, and visualize graphically both the models and the datasets themselves, including any classification errors the models make.</p>
<div id="s0015">
<h3 id="st0015">Preparing the Data</h3>
<p id="p0020" class="noindent">The data is often presented in a spreadsheet or database. However, Weka’s native data storage method is the ARFF format (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#s0035">Section 2.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#p52">page 52</a>). You can easily convert from a spreadsheet to ARFF. The bulk of an ARFF file consists of a list of the instances, and the attribute values for each instance are separated by commas (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#f0015">Figure 2.2</a>). Most spreadsheet and database programs allow you to export data into a file in comma-separated value (CSV) format as a list of records with commas between items. Having done this, you need only load the file into a text editor or word processor; add the dataset’s name using the <em>@relation</em> tag, the attribute information using <em>@attribute</em>, and an <em>@data</em> line; then save the file as raw text. For example, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0015">Figure 11.2</a> shows an Excel spreadsheet containing the weather data from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0030">Section 1.2</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p9">page 9</a>), the data in CSV form loaded into Microsoft Word, and the result of converting it manually into an ARFF file. However, you don’t actually have to go through these steps to create the ARFF file yourself because the Explorer can read CSV spreadsheet files directly, as described later.</p>
<p id="f0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-002-9780123748560.jpg" alt="image" width="473" height="552" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-002-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.2</span> Weather data: (a) spreadsheet, (b) CSV format, and (c) ARFF.</p>
</div>
<div id="s0020">
<h3 id="st0020"><a id="p408"></a>Loading the Data into the Explorer</h3>
<p id="p0025" class="noindent">Let’s load this data into the Explorer and start analyzing it. Fire up Weka to get the <em>GUI Chooser</em> panel in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(a)</a>. Select <em>Explorer</em> from the four choices on the right side. (The others were mentioned earlier: <em>Simple CLI</em> is the old-fashioned command-line interface.)</p>
<p id="f0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-003ab-9780123748560.jpg" alt="image" width="568" height="285" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-003ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.3</span> The Weka Explorer: (a) choosing the Explorer interface and (b) reading in the weather data.</p>
<p id="p0030" class="para_indented">What you see next is the main Explorer screen, shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a>. Actually, the figure shows what it will look like <em>after</em> you have loaded in the weather data. The six tabs along the top are the basic operations that the Explorer supports: Right now we are on <em>Preprocess</em>. Click the <em>Open file</em> button to bring up a standard dialog through which you can select a file. Choose the <em>weather.arff</em> file. If you have it in CSV format, change from <em>ARFF data files</em> to <em>CSV data files</em>. When you specify a .csv file it is automatically converted into ARFF format.</p>
<p id="p0035" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> shows the screen once you have loaded the file. This tells you about the dataset: It has 14 instances and 5 attributes (center left); the attributes are called <em>outlook</em>, <em>temperature</em>, <em>humidity</em>, <em>windy</em>, and <em>play</em> (lower left). The first attribute, <em>outlook</em>, is selected by default (you can choose others by clicking them) and has no missing values, three distinct values, and no unique values; the actual <a id="p409"></a>values are <em>sunny</em>, <em>overcast</em>, and <em>rainy</em> and they occur five, four, and five times, respectively (center right). A histogram at the lower right shows how often each of the two values of the <em>play</em> class occurs for each value of the <em>outlook</em> attribute. The <em>outlook</em> attribute is used because it appears in the box above the histogram, but you can draw a histogram of any other attribute instead. Here, <em>play</em> is selected as the class attribute; it is used to color the histogram, and any filters that require a class value use it too.</p>
<p id="p0040" class="para_indented">The <em>outlook</em> attribute in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> is nominal. If you select a numeric attribute, you see its minimum and maximum values, mean, and standard deviation. In <a id="p410"></a>this case the histogram will show the distribution of the class as a function of this attribute (an example appears later in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0055">Figure 11.10</a>).</p>
<p id="p0045" class="para_indented">You can delete an attribute by clicking its checkbox and using the <em>Remove</em> button. <em>All</em> selects all the attributes, <em>None</em> selects none, <em>Invert</em> inverts the current selection, and <em>Pattern</em> selects those attributes of which the names match a user-supplied regular expression. You can undo a change by clicking the <em>Undo</em> button. The <em>Edit</em> button brings up an editor that allows you to inspect the data, search for particular values and edit them, and delete instances and attributes. Right-clicking on values and column headers brings up corresponding context menus.</p>
</div>
<div id="s0025">
<h3 id="st0025">Building a Decision Tree</h3>
<p id="p0050" class="noindent">To see what the C4.5 decision tree learner described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p201">page 201</a>) does with this dataset, use the <em>J4.8</em> algorithm, which is Weka’s implementation of this decision tree learner. (<em>J4.8</em> actually implements a later and slightly improved version called C4.5 revision 8, which was the last public version of this family of algorithms before the commercial implementation C5.0 was released.) Click the <em>Classify</em> tab to get a screen that looks like <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>. Actually, the figure shows what it will look like <em>after</em> you have analyzed the weather data.</p>
<p id="f0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-004ab-9780123748560.jpg" alt="image" width="568" height="257" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-004ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.4</span> Using <em>J4.8</em>: (a) finding it in the classifiers list and (b) the <em>Classify</em> tab.</p>
<p id="p0055" class="para_indented">First select the classifier by clicking the <em>Choose</em> button at the top left, opening up the <em>trees</em> section of the hierarchical menu in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(a)</a>, and finding <em>J48</em>. The menu structure represents the organization of the Weka code into modules, which is described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0014.html#c0014">Chapter 14</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0014.html#p519">page 519</a>). For now, just open up the hierarchy as <a id="p411"></a>necessary—the items you need to select are always at the lowest level. Once selected, <em>J48</em> appears in the line beside the <em>Choose</em> button as shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>, along with its default parameter values. If you click that line, the <em>J4.8</em> classifier’s object editor opens up and you can see what the parameters mean and alter their values if you wish. The Explorer generally chooses sensible defaults.</p>
<p id="p0060" class="para_indented">Having chosen the classifier, invoke it by clicking the <em>Start</em> button. Weka works for a brief period—when it is working, the little bird at the lower right of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> jumps up and dances—and then produces the output shown in the main panel of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>.</p>
</div>
<div id="s0030">
<h3 id="st0030">Examining the Output</h3>
<p id="p0065" class="noindent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0030">Figure 11.5</a> shows the full output (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> only gives the lower half). At the beginning is a summary of the dataset and the fact that tenfold cross-validation was used to evaluate it. That is the default, and if you look closely at <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> you will see that the <em>Cross-validation</em> box on the left is checked. Then comes a pruned decision tree in textual form. The model that is shown here is always one generated from the full dataset available from the <em>Preprocess</em> panel. The first split is on the <em>outlook</em> attribute, and then, at the second level, the splits are on <em>humidity</em> and <em>windy</em>, respectively.</p><a id="p412"></a><p id="f0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-005-9780123748560.jpg" alt="image" width="449" height="678" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-005-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.5</span> Output from the <em>J4.8</em> decision tree learner.</p>
<p id="p0070" class="para_indented">In the tree structure, a colon introduces the class label that has been assigned to a particular leaf, followed by the number of instances that reach that leaf, expressed as a decimal number because of the way the algorithm uses fractional instances to handle missing values. If there were incorrectly classified instances (there aren’t in <a id="p413"></a>this example) their number would appear too; thus, <em>2.0/1.0</em> means that two instances reached that leaf, of which one is classified incorrectly. Beneath the tree structure the number of leaves is printed; then the total number of nodes (<em>Size of the tree</em>). There is a way to view decision trees more graphically, which will be shown later in this chapter.</p>
<p id="p0075" class="para_indented">The next part of the output gives estimates of the tree’s predictive performance. In this case they are obtained using stratified cross-validation with 10 folds, the default in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>. As you can see, more than 30% of the instances (5 out of 14) have been misclassified in the cross-validation. This indicates that the results obtained from the training data are optimistic compared with what might be obtained from an independent test set from the same source. From the confusion matrix at the end (described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p164">page 164</a>) observe that 2 instances of class <em>yes</em> have been assigned to class <em>no</em> and 3 of class <em>no</em> are assigned to class <em>yes</em>.</p>
<p id="p0080" class="para_indented">As well as the classification error, the evaluation module also outputs the Kappa statistic (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p166">page 166</a>), the mean absolute error, and the root mean-squared error of the class probability estimates assigned by the tree. The root mean-squared error is the square root of the average squared loss (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0045">Section 5.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p160">page 160</a>). The mean absolute error is calculated in a similar way using the absolute instead of the squared difference. It also outputs relative errors, which are based on the prior probabilities (i.e., those obtained by the <em>ZeroR</em> learning scheme described later). Finally, for each class it also outputs some statistics described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>. Also reported is the per-class average of each statistic, weighted by the number of instances from each class.</p>
</div>
<div id="s0035">
<h3 id="st0035">Doing It Again</h3>
<p id="p0085" class="noindent">You can easily run <em>J4.8</em> again with a different evaluation method. Select <em>Use training set</em> (near the top left in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>) and click <em>Start</em>. The classifier output is quickly replaced to show how well the derived model performs on the training set, instead of showing the cross-validation results. This evaluation is highly optimistic (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0010">Section 5.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p148">page 148</a>). It may still be useful because it generally represents an upper bound to the model’s performance on fresh data. In this case, all 14 training instances are classified correctly. In some cases a classifier may decide to leave some instances unclassified, in which case these will be listed as <em>Unclassified Instances</em>. This does not happen for most learning schemes in Weka.</p>
<p id="p0090" class="para_indented">The panel in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> has further test options: <em>Supplied test set</em>, in which you specify a separate file containing the test set, and <em>Percentage split</em>, with which you can hold out a certain percentage of the data for testing. You can output the predictions for each instance by clicking the <em>More options</em> button and checking the appropriate entry. There are other useful options, such as suppressing some output and including other statistics such as entropy evaluation measures and cost-sensitive evaluation. For the latter, you must enter a cost matrix: Type the number of classes into the <em>Classes</em> box (and terminate it with the <em>Enter</em> or <em>Return</em> key) to get a default cost matrix (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p166">page 166</a>), then edit the values as required.</p>
<p id="p0095" class="para_indented"><a id="p414"></a>The small pane at the lower left of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>, which contains one highlighted line, is a history list of the results. The Explorer adds a new line whenever you run a classifier. Because you have now run the classifier twice, the list will contain two items. To return to a previous result set, click the corresponding line and the output for that run will appear in the <em>Classifier Output</em> pane. This makes it easy to explore different classifiers or evaluation schemes and revisit the results to compare them.</p>
</div>
<div id="s0040">
<h3 id="st0040">Working with Models</h3>
<p id="p0100" class="noindent">The result history list is the entry point to some powerful features of the Explorer. When you right-click an entry, a menu appears that allows you to view the results in a separate window or to save the result buffer. More important, you can save the model that Weka has generated in the form of a Java object file. You can reload a model that was saved previously, which generates a new entry in the result list. If you now supply a test set, you can reevaluate the old model on that new set.</p>
<p id="p0105" class="para_indented">Several items on the right-click menu allow you to visualize the results in various ways. At the top of the Explorer interface is a separate <em>Visualize</em> tab, but that is different: It shows the dataset, not the results for a particular model. By right-clicking an entry in the history list you can see the classifier errors. If the model is a tree or a Bayesian network you can see its structure. You can also view the margin curve and various cost and threshold curves, including the cost–benefit analyzer tool (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p170">page 170</a>). For all of these you must choose a class value from a submenu. The <em>Visualize threshold curve</em> menu item allows you to see the effect of varying the probability threshold above which an instance is assigned to that class. You can select from a wide variety of curves that include the ROC and recall–precision curves (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#t0040">Table 5.7</a>). To see these, choose the <em>x</em>-axis and <em>y</em>-axis appropriately from the menus given. For example, set X to <em>False Positive Rate</em> and Y to <em>True Positive Rate</em> for an ROC curve or X to <em>Recall</em> and Y to <em>Precision</em> for a recall–precision curve.</p>
<p id="p0110" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6</a> shows two ways of looking at the result of using <em>J4.8</em> to classify the iris dataset (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0030">Section 1.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p13">page 13</a>)—we use this rather than the weather data because it produces more interesting pictures. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(a)</a> shows the tree; right-click a blank space in this window to bring up a menu enabling you to automatically scale the view or force the tree into the window. Drag the mouse to pan around the space. It’s also possible to visualize the instance data at any node if it has been saved by the learning algorithm.</p>
<p id="f0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-006ab-9780123748560.jpg" alt="image" width="367" height="540" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-006ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.6</span> Visualizing the result of <em>J4.8</em> on the iris dataset: (a) the tree and (b) the classifier errors.</p>
<p id="p0115" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(b)</a> shows the classifier errors on a two-dimensional plot. You can choose which attributes to use for X and Y using the selection boxes at the top. Alternatively, click one of the speckled horizontal strips to the right of the plot: Left-click for X and right-click for Y. Each strip shows the spread of instances along that attribute. X and Y appear beside the ones you have chosen for the axes.</p>
<p id="p0120" class="para_indented">The data points are colored according to their class: blue, red, and green for <em>Iris setosa</em>, <em>Iris versicolor</em>, and <em>Iris virginica</em>, respectively (there is a key at the bottom of the screen). Correctly classified instances are shown as crosses; incorrectly classified ones appear as boxes (of which there are three in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(b)</a>). You can <a id="p415"></a>click on an instance to bring up relevant details: its instance number, the values of the attributes, its class, and the predicted class.</p>
</div>
<div id="s0045">
<h3 id="st0045">When Things Go Wrong</h3>
<p id="p0125" class="noindent">Beneath the result history list, at the bottom of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>, is a status line that says, simply, <em>OK</em>. Occasionally this changes to <em>See error log</em>, an indication that something has gone wrong. For example, there may be constraints among the various selections you can make in a panel. Most of the time the interface grays out inappropriate selections and refuses to let you choose them. But, occasionally, <a id="p416"></a>the interactions are more complex, and you can end up selecting an incompatible set of options. In this case, the status line changes when Weka discovers the incompatibility—typically when you press <em>Start</em>. To see the error, click the <em>Log</em> button to the left of the bird in the lower right corner of the interface. Weka also writes a detailed log to a file in the user’s home directory called <em>weka.log</em>, which often contains more information about the causes of problems than the Explorer’s Log window because it captures debugging output directed to the standard out and error channels (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0050">Section 11.2</a>).</p>
</div>
</div>
<div id="s0050">
<h2 id="st0050">11.2 Exploring the explorer</h2>
<p id="p0130" class="noindent">We have briefly investigated two of the six tabs at the top of the Explorer window in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a>. In summary, here’s what all of the tabs do:</p>
<div class="none">
<p class="hang" id="o0010">1. <a id="p0135"></a><em>Preprocess:</em> Choose the dataset and modify it in various ways.</p>
<p class="hang" id="o0015">2. <a id="p0140"></a><em>Classify:</em> Train learning schemes that perform classification or regression and evaluate them.</p>
<p class="hang" id="o0020">3. <a id="p0145"></a><em>Cluster:</em> Learn clusters for the dataset.</p>
<p class="hang" id="o0025">4. <a id="p0150"></a><em>Associate:</em> Learn association rules for the data and evaluate them.</p>
<p class="hang" id="o0030">5. <a id="p0155"></a><em>Select attributes:</em> Select the most relevant aspects in the dataset.</p>
<p class="hang" id="o0035">6. <a id="p0160"></a><em>Visualize:</em> View different two-dimensional plots of the data and interact with them.</p>
</div>
<p id="p0165" class="noindent">Each tab gives access to a whole range of facilities. In our tour so far, we have barely scratched the surface of the <em>Preprocess</em> and <em>Classify</em> panels.</p>
<p id="p0170" class="para_indented">At the bottom of every panel is a <em>status</em> box and a <em>Log</em> button. The status box displays messages that keep you informed about what’s going on. For example, if the Explorer is busy loading a file, the status box will say so. Right-clicking anywhere inside this box brings up a menu with two options: display the amount of memory available to Weka and run the Java garbage collector. Note that the garbage collector runs constantly as a background task anyway.</p>
<p id="p0175" class="para_indented">Clicking the <em>Log</em> button opens a textual log of the actions that Weka has performed in this session, with timestamps.</p>
<p id="p0180" class="para_indented">As noted earlier, the little bird at the lower right of the window jumps up and dances when Weka is active. The number beside the × shows how many concurrent processes are running. If the bird is standing but stops moving, it’s sick! Something has gone wrong, and you may have to restart the Explorer.</p>
<div id="s0055">
<h3 id="st0055">Loading and Filtering Files</h3>
<p id="p0185" class="noindent">Along the top of the <em>Preprocess</em> panel in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> are buttons for opening files, URLs, and databases. Initially, only files of which the names end in .arff appear in the file browser; to see others, change the <em>Format</em> item in the file selection box.</p>
<div id="s0060">
<h4 class="h4" id="st0060"><a id="p417"></a>Converting Files to ARFF</h4>
<p id="p0190" class="noindent">Weka has converters for the following file formats:</p>
<div class="none">
<p class="hang" id="u0040">• <a id="p0195"></a>Spreadsheet files with extension .csv.</p>
<p class="hang" id="u0045">• <a id="p0200"></a>C4.5’s native file format with extensions .names and .data.</p>
<p class="hang" id="u0050">• <a id="p0205"></a>Serialized instances with extension .bsi.</p>
<p class="hang" id="u0055">• <a id="p0210"></a>LIBSVM format files with extension .libsvm.</p>
<p class="hang" id="u0060">• <a id="p0215"></a>SVM-Light format files with extension .dat.</p>
<p class="hang" id="u0065">• <a id="p0220"></a>XML-based ARFF format files with extension .xrff.</p>
</div>
<p id="p0225" class="para_indented">The appropriate converter is used based on the file extension. If Weka cannot load the data, it tries to interpret it as ARFF. If that fails, it pops up the box shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0040">Figure 11.7(a)</a>.</p>
<p id="f0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-007ac-9780123748560.jpg" alt="image" width="513" height="423" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-007ac-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.7</span> Generic Object Editor: (a) the editor, (b) more information (click <em>More</em>), and (c) choosing a converter (click <em>Choose</em>).</p>
<p id="p0230" class="para_indented">This is a generic object editor, used throughout Weka for selecting and configuring an object. For example, when you set parameters for a classifier, you use the <a id="p418"></a>same kind of box. The <em>CSVLoader</em> for .csv files is selected by default, and the <em>More</em> button gives you more information about it, shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0040">Figure 11.7(b)</a>. It is always worth looking at the documentation! In this case, it explains that the spreadsheet’s first row determines the attribute names and gives a brief description of the <em>CSVLoader</em>’s options. Click <em>OK</em> to use this converter. For a different one, click <em>Choose</em> to select from the list in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0040">Figure 11.7(c)</a>.</p>
<p id="p0235" class="para_indented">The <em>ArffLoader</em> is the first option, and we reached this point only because it failed. The second option is for the C4.5 format, in which there are two files for a dataset, one giving field names and the other giving the actual data. The third option, <em>CSVLoader</em>, is the default, and we clicked <em>Choose</em> because we want a different one. The fourth option is for reading from a database rather than a file; however, the <em>SQLViewer</em> tool, shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0045">Figure 11.8</a> and accessible by pressing the <em>Open DB</em> button on the <em>Preprocess</em> panel, is a more user-friendly route for accessing a database. The <em>serialized instances</em> option is for reloading datasets that have been saved as a Java-serialized object. Any Java object can be saved in this format and reloaded. As a native Java format, it is quicker to load than an ARFF file, which must be <a id="p419"></a>parsed and checked. When repeatedly reloading a large dataset it may be worth saving it in this form.</p>
<p id="f0045" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-008-9780123748560.jpg" alt="image" width="513" height="422" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-008-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.8</span> The SQLViewer tool.</p>
<p id="p0240" class="para_indented">The eighth menu item is for importing a directory containing plaintext files for the purposes of text mining. The imported directory is expected to have a specific structure—namely, a set of subdirectories, each containing one or more text files with the extension .txt. Each text file becomes one instance in the dataset, with a string attribute holding the contents of the file and a nominal class attribute holding the name of the subdirectory that it came from. This dataset can then be further processed into word frequencies (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p328">page 328</a>) using the <em>StringToWordVector</em> filter (covered in the next section). The last option is for loading data files in XRFF, the XML Attribute Relation File format. As the name suggests, this gives ARFF header and instance information in the XML markup language.</p>
<p id="p0245" class="para_indented">Further features of the generic object editor in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0040">Figure 11.7(a)</a> are <em>Save</em>, which saves a configured object, and <em>Open</em>, which opens a previously saved one. These are not useful for this particular kind of object. However, other generic object editor panels have many editable properties, and having gone to some trouble to set them up you may want to save the configured object to reuse later.</p>
<p id="p0250" class="para_indented">Files on your computer are not the only source of datasets for Weka. You can open a URL, and Weka will use the hypertext transfer protocol (HTTP) to download an ARFF file from the Web. Or you can open a database (<em>Open DB</em>)—any database that has a Java database connectivity (JDBC) driver—and retrieve instances using the SQL <em>Select</em> statement. This returns a relation that Weka reads in as an ARFF file. To make this work with your database, you may need to modify the file <em>weka/experiment/DatabaseUtils.props</em> in the Weka distribution by adding your database driver to it. (To access this file, expand the <em>weka.jar</em> file in the Weka distribution.) <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0045">Figure 11.8</a> shows the <em>SQLViewer</em> tool that appears when <em>Open DB</em> is clicked. In this example, the iris dataset has been extracted from a single database table.</p>
<p id="p0255" class="para_indented">Data can be saved in all these formats (with the exception of the directory containing text files) using the <em>Save</em> button in the <em>Preprocess</em> panel (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a>). It is also possible to generate artificial data using the <em>Generate</em> button. Artificial data suitable for classification can be generated from decision lists, radial-basis function networks, and Bayesian networks, as well as from the classic LED24 domain. Artificial regression data can be generated according to mathematical expressions. There are also several generators for producing artificial data for clustering purposes. Apart from loading and saving datasets, the <em>Preprocess</em> panel also allows you to filter them. Filters are an important component of Weka.</p>
</div>
<div id="s0065">
<h4 class="h4" id="st0065">Using Filters</h4>
<p id="p0260" class="noindent">Clicking <em>Choose</em> (near the top left) in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> gives a list of filters like that in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(a)</a>. Actually, you get a collapsed version—click on an arrow to open up its contents. We will describe how to use a simple filter to delete specified attributes from a dataset—in other words, to perform manual attribute selection. The same effect can be achieved more easily by selecting the relevant attributes using <a id="p421"></a>the tick boxes and pressing the <em>Remove</em> button. Nevertheless, we describe the equivalent filtering operation explicitly, as an example.</p><a id="p420"></a><p id="f0050" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-009ae-9780123748560.jpg" alt="image" width="513" height="669" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-009ae-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.9</span> Choosing a filter: (a) the <em>filters</em> menu, (b) an object editor, (c) more information (click <em>More</em>), (d) information about the filter’s capabilities (click <em>Capabilities</em>), and (e) constraints on capabilities.</p>
<p id="p0265" class="para_indented"><em>Remove is</em> an unsupervised attribute filter, and to see it you must scroll further down the list. When selected, it appears in the line beside the <em>Choose</em> button, along with its parameter values—in this case, the line reads simply “Remove.” Click that line to bring up a generic object editor with which you can examine and alter the filter’s properties. (You did the same thing earlier by clicking the <em>J48</em> line in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> to open the <em>J4.8</em> classifier’s object editor.) The object editor for the <em>Remove</em> filter is shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(b)</a>.</p>
<p id="p0270" class="para_indented">To learn about it, click <em>More</em> to show the information in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(c)</a>. This explains that the filter removes a range of attributes from the dataset. It has an option, <em>attributeIndices</em>, that specifies the range to act on and another called <em>invertSelection</em> that determines whether the filter selects attributes or deletes them. There are boxes for both of these in the object editor shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(b)</a>, and in fact we have already set them to <em>1,2</em> (to affect attributes 1 and 2, namely <em>outlook</em> and <em>temperature</em>) and <em>False</em> (to remove rather than retain them). Click <em>OK</em> to set these properties and close the box. Notice that the line beside the <em>Choose</em> button now reads <em>Remove –R 1,2</em>. In the command-line version of the <em>Remove</em> filter, the option <em>–R</em> is used to specify which attributes to remove. After configuring an object it’s often worth glancing at the resulting command-line formulation that the Explorer sets up.</p>
<p id="p0275" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9</a> demonstrates a further feature of the generic object editor, namely capabilities. Algorithms in Weka may provide information about what data characteristics they can handle and, if they do, a <em>Capabilities</em> button appears underneath <em>More</em> in the generic object editor (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(b)</a>). Clicking it brings up <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(d)</a>, which gives information about what the method can do. Here, it states that <em>Remove</em> can handle many attribute characteristics, such as different types (nominal, numeric, relational, etc.) and missing values. It shows the minimum number of instances that are required for <em>Remove</em> to operate on.</p>
<p id="p0280" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(e)</a> shows a list of selected constraints on the capabilities which is obtained by clicking the <em>Filter</em> button at the bottom of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(a)</a>. If the current dataset exhibits some characteristic that is ticked in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(e)</a> but missing from the capabilities for the <em>Remove</em> filter (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(d)</a>), the <em>Apply</em> button to the right of <em>Choose</em> in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> will be grayed out, as will the entry in the list in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(a)</a>. Although you cannot apply it, you can nevertheless select a grayed-out entry to inspect its options, documentation, and capabilities using the generic object editor. You can release individual constraints by deselecting them in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0050">Figure 11.9(e)</a>, or click the <em>Remove filter</em> button to clear all the constraints.</p>
<p id="p0285" class="para_indented">Apply the filter by clicking <em>Apply</em> (at the right side of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a>). Immediately, the screen in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0055">Figure 11.10</a> appears—just like the one in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0020">Figure 11.3(b)</a> but with only three attributes, <em>humidity</em>, <em>windy</em>, and <em>play</em>. At this point the fifth button in the row near the top becomes active. <em>Undo</em> reverses the filtering operation and restores the original dataset, which is useful when you experiment with different filters.</p>
<p id="f0055" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-010-9780123748560.jpg" alt="image" width="513" height="385" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-010-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.10</span> The weather data with two attributes removed.</p>
<p id="p0290" class="para_indented"><a id="p422"></a>The first attribute, <em>humidity</em>, is selected and a summary of its values appears on the right. As a numeric attribute, the minimum and maximum values, mean, and standard deviation are shown. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0055">Figure 11.10</a> is a histogram that shows the distribution of the <em>play</em> attribute. Unfortunately, this display is impoverished because the attribute has so few different values that they fall into two equal-size bins. More realistic datasets yield more informative histograms.</p>
</div>
</div>
<div id="s0070">
<h3 id="st0070">Training and Testing Learning Schemes</h3>
<p id="p0295" class="noindent">The <em>Classify</em> panel lets you train and test learning schemes that perform classification or regression. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0010">Section 11.1</a> explained how to interpret the output of a decision tree learner and showed the performance figures that are automatically generated by the evaluation module. The interpretation of these is the same for all models that predict a categorical class. However, when evaluating models for numeric prediction, Weka produces a different set of performance measures.</p>
<p id="p0300" class="para_indented">As an example, in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0060">Figure 11.11(a)</a> the CPU performance dataset from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0030">Table 1.5</a> has been loaded into Weka. You can see the histogram of values of the first attribute, <em>MYCT</em>, at the lower right. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0060">Figure 11.11(b)</a> the model tree inducer M5′ has been <a id="p424"></a>chosen as the classifier by going to the <em>Classify</em> panel, clicking the <em>Choose</em> button at the top left, opening up the <em>trees</em> section of the hierarchical menu shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(a)</a>, finding <em>M5P</em>, and clicking <em>Start</em>. The hierarchy helps to locate particular classifiers by grouping items with common functionality.</p><a id="p423"></a><p id="f0060" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-011ab-9780123748560.jpg" alt="image" width="513" height="813" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-011ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.11</span> Processing the CPU performance data with M5′.</p>
<p id="p0305" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0065">Figure 11.12</a> shows the output. The pruned model tree contains splits on three of the six attributes in the data. The root splits on the <em>CHMIN</em> attribute, yielding a linear model at the leaf on the left branch and the remaining structure in the right branch. There are five leaves in all, each with a corresponding linear model. The first number in parentheses at each leaf is the number of instances that reach it; the second is the root mean-squared error of the predictions from the leaf’s linear model for those instances, expressed as a percentage of the standard deviation of the class attribute computed over all the training data. The description of the tree is followed by several figures that measure its performance. These are derived from the test option chosen in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0060">Figure 11.11(b)</a>, tenfold cross-validation (not stratified because stratification doesn’t make sense for numeric prediction). <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0105">Section 5.8</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#t0045">Table 5.8</a>) explains the meaning of the various measures.</p><a id="p425"></a>
<p id="f0065" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-012-9780123748560.jpg" alt="image" width="449" height="702" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-012-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.12</span> Output from the M5′ program for numeric prediction.</p>
<p id="p0310" class="para_indented">Ordinary linear regression (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p124">page 124</a>), another scheme for numeric prediction, is found under <em>LinearRegression</em> in the <em>functions</em> section of the menu in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(a)</a>. It builds a single linear regression model rather than the five in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0065">Figure 11.12</a>; not surprisingly, its performance is slightly worse.</p>
<p id="p0315" class="para_indented">To get a feel for their relative performance, let’s visualize the errors these schemes make, as we did for the iris dataset in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(b)</a>. Right-click the entry in the history list and select <em>Visualize classifier errors</em> to bring up the two-dimensional plot of the data in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0070">Figure 11.13</a>. The points are color-coded by class—in this case the color varies continuously because the class is numeric. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0070">Figure 11.13</a> the <em>MMAX</em> attribute has been selected for the <em>x</em>-axis and the instance number has been chosen for the <em>y</em>-axis because this gives a good spread of points. Each data point is marked by a cross of which the size indicates the absolute value of the error for that instance. The smaller crosses in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0070">Figure 11.13(a)</a> (for M5′), when compared with those in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0070">Figure 11.13(b)</a> (for linear regression), show that M5′ is superior.</p><a id="p426"></a><p id="f0070" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-013ab-9780123748560.jpg" alt="image" width="477" height="679" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-013ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.13</span> Visualizing the errors: (a) from M5′ and (b) from linear regression.</p>
</div>
<div id="s0075">
<h3 id="st0075">Do It Yourself: The User Classifier</h3>
<p id="p0320" class="noindent">The User Classifier (mentioned in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0015">Section 3.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#p65">page 65</a>) allows Weka users to build their own classifiers interactively. It resides in the <em>trees</em> section of the hierarchical menu in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(a)</a> under <em>UserClassifier</em>. We illustrate its operation on a new problem, segmenting visual image data into classes such as <em>grass</em>, <em>sky</em>, <em>foliage</em>, <em>brick</em>, and <em>cement</em> based on attributes giving average intensity, hue, size, position, and various simple textural features. The training data file is supplied with the Weka distribution and called <em>segment-challenge.arff</em>. Having loaded it, select the User Classifier. For evaluation use the special test set called <em>segment-test.arff</em> as the <em>Supplied test set</em> on the <em>Classify</em> panel. Evaluation by cross-validation is impossible when you have to construct a classifier manually for each fold.</p>
<p id="p0325" class="para_indented"><a id="p427"></a>Following <em>Start</em>, a new window appears and Weka waits for you to build the classifier. The <em>Tree Visualizer</em> and <em>Data Visualizer</em> tabs switch between different views. The former shows the current state of the classification tree, and each node gives the number of instances of each class at that node. The aim is to come up with a tree in which the leaf nodes are as pure as possible. Initially, there is only one node, the root, which contains all the data. Switch to the <em>Data Visualizer</em> to create a split. This shows the same two-dimensional plot that we saw in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(b)</a> for the iris dataset and in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0070">Figure 11.13</a> for the CPU performance data. The attributes to use for X and Y are selected as before, and the goal here is to find a combination that separates the classes as cleanly as possible. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0075">Figure 11.14(a)</a> shows a good choice: <em>region-centroid-row</em> for X and <em>intensity-mean</em> for Y.</p><a id="p428"></a><p id="f0075" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-014ab-9780123748560.jpg" alt="image" width="477" height="800" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-014ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.14</span> Working on segment-challenge data with the User Classifier: (a) data and (b) tree visualizers.</p>
<p id="p0330" class="para_indented">Having found a good separation, you must specify a region in the graph. Four tools for this appear in the pull-down menu below the <em>y</em>-axis selector. <em>Select Instance</em> identifies a particular instance. <em>Rectangle</em> (shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0075">Figure 11.14(a)</a>) allows you to drag out a rectangle on the graph. With <em>Polygon</em> and <em>Polyline</em> you build a free-form polygon or draw a free-form polyline (left-click to add a vertex and right-click to complete the operation). Once an area has been selected, it turns gray. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0075">Figure 11.14(a)</a> the user has defined a rectangle. The <em>Submit</em> button creates two new nodes in the tree, one holding the selected instances and the other holding all the rest. <em>Clear</em> clears the selection; <em>Save</em> saves the instances in the current tree node as an ARFF file.</p>
<p id="p0335" class="para_indented">At this point, the <em>Tree Visualizer</em> shows the tree in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0075">Figure 11.14(b)</a>. There is a pure node for the <em>sky</em> class, but the other node is mixed and should be split further. Clicking on different nodes determines which subset of data is shown by the <em>Data Visualizer</em>. Continue adding nodes until you are satisfied with the result—that is, until the leaf nodes are mostly pure. Then right-click on any blank space in the <em>Tree Visualizer</em> and choose <em>Accept the Tree</em>. Weka evaluates your tree on the test set and outputs performance statistics (90% is a good score on this problem).</p>
<p id="p0340" class="para_indented">Building trees manually is very tedious. But Weka can complete the task for you by building a subtree under any node: Just right-click the node.</p>
</div>
<div id="s0080">
<h3 id="st0080">Using a Metalearner</h3>
<p id="p0345" class="noindent">Metalearners (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#c0008">Chapter 8</a>) take simple classifiers and turn them into more powerful learners. For example, to boost decision stumps in the Explorer, go to the <em>Classify</em> panel and choose the classifier <em>AdaboostM1</em> from the <em>meta</em> section of the hierarchical menu. When you configure this classifier by clicking it, the object editor shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0080">Figure 11.15</a> appears. This has its own classifier field, which we set to <em>DecisionStump</em> (as shown). This method could itself be configured by clicking (except that <em>DecisionStump</em> happens to have no editable properties). Click <em>OK</em> to return to the main <em>Classify</em> panel and <em>Start</em> to try out boosting decision stumps up to 10 times. It turns out that this mislabels only 7 of the 150 instances in the iris data—good performance considering the rudimentary nature of decision stumps and the rather small number of boosting iterations.</p>
<p id="f0080" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-015-9780123748560.jpg" alt="image" width="750" height="658" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-015-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.15</span> Configuring a metalearner for boosting decision stumps.</p>
</div>
<div id="s0085">
<h3 id="st0085"><a id="p429"></a>Clustering and Association Rules</h3>
<p id="p0350" class="noindent">Use the <em>Cluster</em> and <em>Associate</em> panels to invoke clustering algorithms (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p272">page 272</a>) and methods for finding association rules (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p116">page 116</a>). When clustering, Weka shows the number of clusters and how many instances each cluster contains. For some algorithms the number of clusters can be specified by setting a parameter in the object editor. For probabilistic clustering methods, Weka measures the log-likelihood of the clusters on the training data: The larger this quantity, the better the model fits the data. Increasing the number of clusters normally increases the likelihood, but may overfit.</p>
<p id="p0355" class="para_indented">The controls on the <em>Cluster</em> panel are similar to those for <em>Classify</em>. You can specify some of the same evaluation methods—use training set, supplied test set, and percentage split (the last two are used with the log-likelihood). A further method, classes to clusters evaluation, compares how well the chosen clusters match a preassigned class in the data. You select an attribute (which must be nominal) that represents the “true” class. Having clustered the data, Weka determines the majority class in each cluster and prints a confusion matrix showing how many errors there would be if the clusters were used instead of the true class. If your dataset has a class attribute, you can ignore it during clustering by selecting it from a pull-down list of attributes and see how well the clusters correspond to actual class values. Finally, you can choose whether or not to store the clusters for visualization. The only reason not to do so is to conserve space. As with classifiers, you visualize the results by right-clicking on the result list, which allows you to view two-dimensional scatter plots like the one in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0035">Figure 11.6(b)</a>. If you have chosen classes to clusters evaluation, the class assignment errors are shown. For the <em>Cobweb</em> clustering scheme, you can also visualize the tree.</p>
<p id="p0360" class="para_indented"><a id="p430"></a>The <em>Associate</em> panel is simpler than <em>Classify</em> or <em>Cluster</em>. Weka contains six algorithms for determining association rules and no methods for evaluating such rules. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0085">Figure 11.16</a> shows the output from the <em>Apriori</em> program for association rules (described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>) on the nominal version of the weather data. Despite the simplicity of the data, several rules are found. The number before the arrow is the number of instances for which the antecedent is true; that after the arrow is the number of instances for which the consequent is true also; the confidence (in parentheses) is the ratio between the two. Ten rules are found by default: You can ask for more by using the object editor to change <em>numRules</em>.</p>
<p id="f0085" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-016-9780123748560.jpg" alt="image" width="514" height="164" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-016-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.16</span> Output from the <em>Apriori</em> program for association rules.</p>
</div>
<div id="s0090">
<h3 id="st0090">Attribute Selection</h3>
<p id="p0365" class="noindent">The <em>Select attributes</em> panel gives access to several methods for attribute selection. As explained in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p307">page 307</a>), this involves an attribute evaluator and a search method. Both are chosen in the usual way and configured with the object editor. You must also decide which attribute to use as the class. Attribute selection can be performed using the full training set or using cross-validation. In the latter case it is done separately for each fold, and the output shows how many times—that is, in how many of the folds—each attribute was selected. The results are stored in the history list. When you right-click an entry here you can visualize the dataset in terms of the selected attributes (choose <em>Visualize reduced data</em>).</p>
</div>
<div id="s0095">
<h3 id="st0095">Visualization</h3>
<p id="p0370" class="noindent">The <em>Visualize</em> panel helps you visualize a dataset—not the result of a classification or clustering model, but the dataset itself. It displays a matrix of two-dimensional scatter plots of every pair of attributes. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0090">Figure 11.17(a)</a> shows the iris dataset. You can select an attribute—normally the class—for coloring the data points using the controls at the bottom. If it is nominal, the coloring is discrete; if it is numeric, the color spectrum ranges continuously from blue (low values) to orange (high values). <a id="p432"></a>Data points with no class value are shown in black. You can change the size of each plot, the size of the points, and the amount of jitter, which is a random displacement applied to X and Y values to separate points that lie on top of one another. Without jitter, a thousand instances at the same data point would look just the same as one instance. You can reduce the size of the matrix of plots by selecting certain attributes, and you can subsample the data for efficiency. Changes in the controls do not take effect until the <em>Update</em> button is clicked.</p><a id="p431"></a><p id="f0090" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-017ab-9780123748560.jpg" alt="image" width="337" height="771" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-017ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.17</span> Visualizing the iris dataset.</p>
<p id="p0375" class="para_indented">Click one of the plots in the matrix to enlarge it. For example, clicking on the top left plot brings up the panel in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0090">Figure 11.17(b)</a>. You can zoom in on any area of this panel by choosing <em>Rectangle</em> from the menu near the top right and dragging out a rectangle on the viewing area like that shown. The <em>Submit</em> button near the top left rescales the rectangle into the viewing area.</p>
</div>
</div>
<div id="s0100">
<h2 id="st0100">11.3 Filtering algorithms</h2>
<p id="p0380" class="noindent">Now we take a detailed look at the filtering algorithms implemented within Weka. These are accessible from the Explorer and from the Knowledge Flow and Experimenter interfaces described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0012.html#c0012">Chapters 12</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0013.html#c0013">13</a>. All filters transform the input dataset in some way. When a filter is selected using the <em>Choose</em> button, its name appears in the line beside that button. Click that line to get a generic object editor to specify its properties. What appears in the line is the command-line version of the filter, and the parameters are specified with minus signs. This is a good way of learning how to use the Weka commands directly.</p>
<p id="p0385" class="para_indented">There are two kinds of filter: unsupervised and supervised (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0030">Section 7.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p316">page 316</a>). This seemingly innocuous distinction masks an issue that is rather fundamental. Filters are often applied to a training dataset and then also applied to the test file. If the filter is supervised—for example, if it uses class values to derive good intervals for discretization—applying it to the test data will bias the results. It is the discretization intervals derived from the <em>training</em> data that must be applied to the test data. When using supervised filters, you must to be careful to ensure that the results are evaluated fairly, which is an issue that does not arise with unsupervised filters.</p>
<p id="p0390" class="para_indented">We treat Weka’s unsupervised and supervised filtering methods separately. Within each type there is a further distinction between <em>attribute filters</em>, which work on the attributes in the datasets, and <em>instance filters</em>, which work on the instances. To learn more about a particular filter, select it in the Weka Explorer and look at its associated object editor, which defines what the filter does and the parameters it takes.</p>
<div id="s0105">
<h3 id="st0105">Unsupervised Attribute Filters</h3>
<p id="p0395" class="noindent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0010">Table 11.1</a> lists Weka’s unsupervised attribute filters. Many of the operations were introduced in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#c0007">Chapter 7</a>.</p><a id="p433"></a><a id="p434"></a><a id="p435"></a><p class="table_caption"><span class="tab_num">Table 11.1. </span> Unsupervised Attribute Filters</p>
<table id="t0010" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>Add</em></td>
<td class="tb">Add a new attribute, the values of which are all marked as missing</td></tr>
<tr><td class="tb"><em>AddCluster</em></td>
<td class="tb">Add a new nominal attribute representing the cluster assigned to each instance by a given clustering algorithm</td></tr>
<tr><td class="tb"><em>AddExpression</em></td>
<td class="tb">Create a new attribute by applying a specified mathematical function to existing attributes</td></tr>
<tr><td class="tb"><em>AddID</em></td>
<td class="tb">Add an attribute that contains a unique ID for each instance in the dataset</td></tr>
<tr><td class="tb"><em>AddNoise</em></td>
<td class="tb">Change a percentage of a given nominal attribute’s values</td></tr>
<tr><td class="tb"><em>AddValues</em></td>
<td class="tb">Add the labels from a user-supplied list to an attribute if they are missing. The labels can be optionally sorted.</td></tr>
<tr><td class="tb"><em>Center</em></td>
<td class="tb">Center all numeric attributes in the given dataset to have zero mean (apart from the class attribute, if set)</td></tr>
<tr><td class="tb"><em>ChangeDateFormat</em></td>
<td class="tb">Change the string used to format a date attribute</td></tr>
<tr><td class="tb"><em>ClassAssigner</em></td>
<td class="tb">Set or unset the class attribute</td></tr>
<tr><td class="tb"><em>ClusterMembership</em></td>
<td class="tb">Use a cluster to generate cluster membership values, which then form new attributes</td></tr>
<tr><td class="tb"><em>Copy</em></td>
<td class="tb">Copy a range of attributes in the dataset</td></tr>
<tr><td class="tb"><em>Discretize</em></td>
<td class="tb">Convert numeric attributes to nominal: Specify which attributes, number of bins, whether to optimize the number of bins, output binary attributes, use equal-width (default) or equal-frequency binning</td></tr>
<tr><td class="tb"><em>FirstOrder</em></td>
<td class="tb">Apply a first-order differencing operator to a range of numeric attributes</td></tr>
<tr><td class="tb"><em>InterquartileRange</em></td>
<td class="tb">Create new attributes to indicate outliers and extreme values. Interquartile ranges are used to define what constitutes an outlier or an extreme value.</td></tr>
<tr><td class="tb"><em>KernelFilter</em></td>
<td class="tb">Produce a kernel matrix for a dataset. The new dataset contains one attribute and one instance for every instance in the source dataset. Attribute values for a particular instance in the new dataset hold the result of evaluating a kernel function on the instance in question and the original instance corresponding to each attribute.</td></tr>
<tr><td class="tb"><em>MakeIndicator</em></td>
<td class="tb">Replace a nominal attribute by a Boolean attribute. Assign value 1 to instances with a particular range of attribute values, else 0. By default, the Boolean attribute is coded as numeric.</td></tr>
<tr><td class="tb"><em>MathExpression</em></td>
<td class="tb">Similar to <em>AddExpression</em>, but modifies existing attributes in situ rather than creating a new attribute</td></tr>
<tr><td class="tb"><em>MergeTwoValues</em></td>
<td class="tb">Merge two values of a given attribute: Specify the index of the two values to be merged</td></tr>
<tr><td class="tb"><em>MultiInstanceToPropositional</em></td>
<td class="tb">Convert a multi-instance dataset into a single-instance one by giving each instance its bag’s class value and optionally setting its weight</td></tr>
<tr><td class="tb"><em>NominalToBinary</em></td>
<td class="tb">Change a nominal attribute to several binary ones, one for each value</td></tr>
<tr><td class="tb"><em>NominalToString</em></td>
<td class="tb">Convert nominal attributes into string attributes</td></tr>
<tr><td class="tb"><em>Normalize</em></td>
<td class="tb">Scale all numeric values in the dataset to lie within the interval [0,1]</td></tr>
<tr><td class="tb"><em>NumericCleaner</em></td>
<td class="tb">Replace values of numeric attributes that are too small, too large, or too close to a particular value with user-supplied default values</td></tr>
<tr><td class="tb"><em>NumericToBinary</em></td>
<td class="tb">Convert all numeric attributes into binary ones: Nonzero values become 1</td></tr>
<tr><td class="tb"><em>NumericToNominal</em></td>
<td class="tb">Convert numeric attributes to nominal by simply adding all observed values for a numeric attribute into the list of nominal values</td></tr>
<tr><td class="tb"><em>NumericTransform</em></td>
<td class="tb">Transform a numeric attribute using any Java function</td></tr>
<tr><td class="tb"><em>Obfuscate</em></td>
<td class="tb">Obfuscate the dataset by renaming the relation, all attribute names, and nominal and string attribute values</td></tr>
<tr><td class="tb"><em>PartitionedMultiFilter</em></td>
<td class="tb">A meta filter that applies a set of filters to a corresponding set of attribute ranges and assembles the output into a new dataset</td></tr>
<tr><td class="tb"><em>PKIDiscretize</em></td>
<td class="tb">Discretize numeric attributes using equal-frequency binning, where the number of bins is equal to the square root of the number of values (excluding missing values)</td></tr>
<tr><td class="tb"><em>PrincipalComponents</em></td>
<td class="tb">Perform a principal components analysis and transformation of the input data</td></tr>
<tr><td class="tb"><em>PropositionalToMultiInstance</em></td>
<td class="tb">Convert a single-instance dataset (with a bag ID attribute) to multi-instance format using relational attributes</td></tr>
<tr><td class="tb"><em>RandomProjection</em></td>
<td class="tb">Project the data onto a lower-dimensional subspace using a random matrix</td></tr>
<tr><td class="tb"><em>RandomSubset</em></td>
<td class="tb">Create a new dataset that includes a percentage of the original number of attributes chosen at random</td></tr>
<tr><td class="tb"><em>RELAGGS</em></td>
<td class="tb">Use the RELAGGS algorithm to convert multi-instance data to single-instance format</td></tr>
<tr><td class="tb"><em>Remove</em></td>
<td class="tb">Remove attributes</td></tr>
<tr><td class="tb"><em>RemoveType</em></td>
<td class="tb">Remove attributes of a given type (nominal, numeric, string, or date)</td></tr>
<tr><td class="tb"><em>RemoveUseless</em></td>
<td class="tb">Remove constant attributes, along with nominal attributes that vary too much</td></tr>
<tr><td class="tb"><em>Reorder</em></td>
<td class="tb">Change the order of the attributes</td></tr>
<tr><td class="tb"><em>ReplaceMissingValues</em></td>
<td class="tb">Replace all missing values for nominal and numeric attributes with the modes and means of the training data</td></tr>
<tr><td class="tb"><em>Standardize</em></td>
<td class="tb">Standardize all numeric attributes to have zero mean and unit variance</td></tr>
<tr><td class="tb"><em>StringToNominal</em></td>
<td class="tb">Convert a string attribute to nominal</td></tr>
<tr><td class="tb"><em>StringToWordVector</em></td>
<td class="tb">Convert a string attribute to a vector that represents word occurrence frequencies; you can choose the delimiter(s)—and there are many more options</td></tr>
<tr><td class="tb"><em>SwapValues</em></td>
<td class="tb">Swap two values of an attribute</td></tr>
<tr><td class="tb"><em>TimeSeriesDelta</em></td>
<td class="tb">Replace attribute values in the current instance with the difference between the current value and the value in some previous (or future) instance</td></tr>
<tr><td class="tb"><em>TimeSeriesTranslate</em></td>
<td class="tb">Replace attribute values in the current instance with the equivalent value in some previous (or future) instance</td></tr>
<tr><td class="tb"><em>Wavelet</em></td>
<td class="tb">Perform a Haar wavelet transformation</td></tr>
</tbody>
</table>
<div id="s0110">
<h4 class="h4" id="st0110"><a id="p436"></a>Adding and Removing Attributes</h4>
<p id="p0400" class="noindent"><em>Add</em> inserts an attribute at a given position, the value of which is declared to be missing for all instances. Use the generic object editor to specify the attribute’s name, where it will appear in the list of attributes, and its possible values (for nominal attributes); for data attributes you can also specify the date format. <em>Copy</em> copies existing attributes so that you can preserve them when experimenting with filters that overwrite attribute values. Several attributes can be copied together using an expression such as <em>1–3</em> for the first three attributes, or <em>first-3,5,9-last</em> for attributes 1, 2, 3, 5, 9, 10, 11, 12, …. The selection can be inverted, affecting all attributes <em>except</em> those specified. These features are shared by many filters.</p>
<p id="p0405" class="para_indented"><em>AddID</em> inserts a numeric identifier attribute at the user-specified index in the list of attributes. An identifier attribute is useful for keeping track of individual instances after a dataset has been processed in various ways, such as being transformed by other filters or having the order of the instances randomized.</p>
<p id="p0410" class="para_indented"><em>Remove</em> has already been described. Similar filters are <em>RemoveType</em>, which deletes all attributes of a given type (nominal, numeric, string, date, or relational), and <em>RemoveUseless</em>, which deletes constant attributes and nominal attributes of which the values are different for almost all instances. You can decide how much variation is tolerated before an attribute is deleted by specifying the number of distinct values as a percentage of the total number of values. Note that some unsupervised attribute filters behave differently if the menu in the <em>Preprocess</em> panel has been used to set a class attribute (by default, the last attribute is the class attribute). For example, <em>RemoveType</em> and <em>RemoveUseless</em> both skip the class attribute.</p>
<p id="p0415" class="para_indented"><em>InterquartileRange</em> adds new attributes that indicate whether the values of instances can be considered outliers or extreme values. The definitions of outlier and extreme value are based on the difference between the 25th and 75th quartile of an attribute’s values. Values are flagged as extreme if they exceed the 75th quartile (or fall below the 25th quartile) by the product of the user-specified extreme value factor and the interquartile range. Values that are not extreme values but exceed the 75th quartile (or fall below the 25th quartile) by the product of the outlier factor and the interquartile range are flagged as outliers. The filter can be configured to flag an instance as an outlier or extreme if any of its attribute values are deemed outliers or extreme, or to generate an outlier–extreme indicator pair for each attribute. It is also possible to flag all extreme values as outliers and to output attributes that indicate by how many interquartile ranges an attribute’s value deviates from the median.</p>
<p id="p0420" class="para_indented"><em>AddCluster</em> applies a clustering algorithm to the data before filtering it. You use the object editor to choose the clustering algorithm. Clusterers are configured just as filters are (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0255">Section 11.6</a>). The <em>AddCluster</em> object editor contains its own <em>Choose</em> button for the clusterer, and you configure the clusterer by clicking its line and getting <em>another</em> object editor panel, which must be filled in before returning to the <em>AddCluster</em> object editor. This is probably easier to understand when you do it in practice than when you read about it in a book! At any rate, once you have chosen a clusterer, <em>AddCluster</em> uses it to assign a cluster number to each instance, as a new <a id="p437"></a>attribute. The object editor also allows you to ignore certain attributes when clustering, specified as described previously for <em>Copy</em>. <em>ClusterMembership</em> uses a clusterer, again specified in the filter’s object editor, to generate membership values. A new version of each instance is created of which the attributes are these values. The class attribute, if set, is ignored during clustering.</p>
<p id="p0425" class="para_indented"><em>AddExpression</em> creates a new attribute by applying a mathematical function to numeric attributes. The expression can contain attribute references and constants; arithmetic operators +, –, *, /, and ∧; the functions <em>log</em> and <em>exp</em>, <em>abs</em> and <em>sqrt</em>, <em>floor</em>, <em>ceil</em> and <em>rint</em>,<sup><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#fn0010" id="cc000110fn0010" class="totri-footnote">1</a></sup><em>sin</em>, <em>cos</em>, and <em>tan</em>; and parentheses. Attributes are specified by the prefix <em>a</em>—for example, <em>a7</em> is the seventh attribute. An example expression is</p>
<p class="figure" id="e0010"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110si1.jpg" alt="image" width="202" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110si1.jpg"></p>
<p></p>
<p id="p0430" class="noindent">There is a debug option that replaces the new attribute’s value with a postfix parse of the supplied expression.</p>
<p id="p0435" class="para_indented"><em>MathExpression</em> is similar to <em>AddExpression</em> but can be applied to multiple attributes. Rather than creating a new attribute, it replaces the original values with the result of the expression <em>in situ</em>; because of this, the expression cannot reference the value of other attributes. All the operators that apply to <em>AddExpression</em> can be used, as well as the minimum, maximum, mean, sum, sum-squared, and standard deviation of the attribute being processed. Furthermore, simple if–then–else expressions involving the operators and functions can be applied as well.</p>
<p id="p0440" class="para_indented">Whereas <em>AddExpression</em> and <em>MathExpression</em> apply mathematical functions specified in textual form, <em>NumericTransform</em> performs an arbitrary transformation by applying a given Java function to selected numeric attributes. The function can be anything that takes a <em>double</em> as its argument and returns another <em>double</em>—for example, <em>sqrt</em>() in <em>java.lang.Math.</em> One parameter is the name of the Java class that implements the function (which must be a fully qualified name); another is the name of the transformation method itself.</p>
<p id="p0445" class="para_indented"><em>Normalize</em> scales all numeric values in the dataset to lie between 0 and 1. The normalized values can be further scaled and translated with user-supplied constants. <em>Center</em> and <em>Standardize</em> transform them to have zero mean; <em>Standardize</em> gives them unit variance too. All three skip the class attribute, if set. <em>RandomSubset</em> randomly selects a subset of the attributes to include in the output; the user specifies how many (as a percentage of the number of attributes). The class attribute, if set, is always included.</p>
<p id="p0450" class="para_indented"><em>PartitionedMultiFilter</em> is a special filter that applies a set of filters to a corresponding set of attribute ranges in the input dataset. The user supplies and configures each filter, and defines the range of attributes for them to work with. There is an option to delete attributes that are not covered by any of the ranges. Only filters that operate on attributes are allowed. The output of the individual filters is assembled <a id="p438"></a>into a new dataset. <em>Reorder</em> alters the order of the attributes in the data; the new order is specified by supplying a list of attribute indices. By omitting or duplicating indices it is possible to delete attributes or make several copies of them.</p>
</div>
<div id="s0115">
<h4 class="h4" id="st0115">Changing Values</h4>
<p id="p0455" class="noindent"><em>SwapValues</em> swaps the positions of two values of a nominal attribute. The order of values is entirely cosmetic—it does not affect learning at all—but if the class is selected, changing the order affects the layout of the confusion matrix. <em>MergeTwoValues</em> merges values of a nominal attribute into a single category. The new value’s name is a concatenation of the two original ones, and every occurrence of either of the original values is replaced by the new one. The index of the new value is the smaller of the original indices. For example, if you merge the first two values of the <em>outlook</em> attribute in the weather data—in which there are five <em>sunny</em>, four <em>overcast</em>, and five <em>rainy</em> instances—the new <em>outlook</em> attribute will have values <em>sunny_overcast</em> and <em>rainy</em>; there will be nine <em>sunny_overcast</em> instances and the original five <em>rainy</em> ones.</p>
<p id="p0460" class="para_indented">One way of dealing with missing values is to replace them globally before applying a learning scheme. <em>ReplaceMissingValues</em> replaces each missing value by the mean for numeric attributes and the mode for nominal ones. If a class is set, missing values of that attribute are not replaced by default, but this can be changed.</p>
<p id="p0465" class="para_indented"><em>NumericCleaner</em> replaces the values of numeric attributes that are too small, too large, or too close to a particular value with default values. A different default can be specified for each case, along with thresholds for what is considered to be too large or small and a tolerance value for defining too close.</p>
<p id="p0470" class="para_indented"><em>AddValues</em> adds any values that are not already present in a nominal attribute from a user-supplied list. The labels can optionally be sorted. <em>ClassAssigner</em> can be used to set or unset a dataset’s class attribute. The user supplies the index of the new class attribute; a value of 0 unsets the existing class attribute.</p>
</div>
<div id="s0120">
<h4 class="h4" id="st0120">Conversions</h4>
<p id="p0475" class="noindent">Many filters convert attributes from one form to another. <em>Discretize</em> uses equal-width or equal-frequency binning (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0030">Section 7.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p316">page 316</a>) to discretize a range of numeric attributes, specified in the usual way. For the former method the number of bins can be specified or chosen automatically by maximizing the likelihood using leave-one-out cross-validation. It is also possible to create several binary attributes instead of one multivalued one. For equal-frequency discretization, the desired number of instances per interval can be changed. <em>PKIDiscretize</em> discretizes numeric attributes using equal-frequency binning; the number of bins is the square root of the number of values (excluding missing values). Both these filters skip the class attribute by default.</p>
<p id="p0480" class="para_indented"><em>MakeIndicator</em> converts a nominal attribute into a binary indicator attribute and can be used to transform a multiclass dataset into several two-class ones. It substitutes a binary attribute for the chosen nominal one, of which the values for each instance are 1 if a particular original value was present and 0 otherwise. The new attribute is declared to be numeric by default, but it can be made nominal if desired.</p>
<p id="p0485" class="para_indented"><a id="p439"></a>For some learning schemes, such as support vector machines, multivalued nominal attributes must be converted to binary ones. The <em>NominalToBinary</em> filter transforms all specified multivalued nominal attributes in a dataset into binary ones, replacing each attribute with <em>k</em> values by <em>k</em> binary attributes using a simple one-per-value encoding. The new attributes will be numeric by default. Attributes that are already binary are left untouched. <em>NumericToBinary</em> converts all numeric attributes into nominal binary ones (except the class, if set). If the value of the numeric attribute is exactly 0, the new attribute will be 0, and if it is missing, the new attribute will be missing; otherwise, the value of the new attribute will be 1. These filters also skip the class attribute. <em>NumericToNominal</em> converts numeric attributes to nominal ones by simply adding every distinct numeric value to the list of nominal values. This can be a useful filter to apply after importing a .csv file—Weka’s <em>csv</em> import facility creates a numeric attribute for any data column with values that can all be parsed as numbers, but it might make sense to interpret the values of an integer attribute as discrete instead.</p>
<p id="p0490" class="para_indented"><em>FirstOrder</em> takes a range of <em>N</em> numeric attributes and replaces them with <em>N</em> – 1 numeric attributes with values that are the differences between consecutive attribute values from the original instances. For example, if the original attribute values were 3, 2, and 1, the new ones will be –1 and –1.</p>
<p id="p0495" class="para_indented"><em>KernelFilter</em> converts the data to a kernel matrix: It outputs a new dataset, containing the same number of instances as before, in which each value is the result of evaluating a kernel function on a pair of original instances. By default, all values are transformed to center them around 0, although they are not rescaled to unit variance. However, different filters can be specified.</p>
<p id="p0500" class="para_indented"><em>PrincipalComponents</em> performs a principal components transformation on the dataset. First, any multivalued nominal attributes are converted to binary, and missing values are replaced by means. The data is standardized (by default). The number of components is normally determined based on the user-specified proportion of variance to be covered, but it is also possible to specify the number of components explicitly.</p>
<p id="p0505" class="para_indented">The <em>Wavelet</em> filter applies a Haar wavelet transformation to the data. A <em>MultiFilter</em> are used to replace missing values with means and modes, and to normalize the data. The user can adjust the configuration of <em>MultiFilter</em> in order to modify the preprocessing that is done.</p>
</div>
<div id="s0125">
<h4 class="h4" id="st0125">String Conversion</h4>
<p id="p0510" class="noindent">A string attribute has an unspecified number of values. <em>StringToNominal</em> converts it to nominal with a set number of values. You should ensure that all string values that will appear in potential test data are represented in the dataset. <em>NominalToString</em> converts the other way.</p>
<p id="p0515" class="para_indented"><em>StringToWordVector</em> produces numeric attributes that represent the frequency of words in the value of a string attribute. The set of words—that is, the new attribute set—is determined from the full set of values in the string attribute. The new <a id="p440"></a>attributes can be named with a user-determined prefix to keep attributes derived from different string attributes distinct.</p>
<p id="p0520" class="para_indented">There are many options that affect tokenization. Words can be formed from contiguous alphabetic sequences or separated by a given set of delimiter characters. In the latter case, they can be further split into <em>n</em>-grams (with user-supplied minimum and maximum length), or they can be processed by a stemming algorithm. They can be converted to lowercase before being added to the dictionary, or all words on a supplied list of stopwords can be ignored. Words that are not among the top <em>k</em> words ranked by frequency can be discarded (slightly more than <em>k</em> words will be retained if there are ties at the <em>k</em>th position). If a class attribute has been assigned, the top <em>k</em> words for each class will be kept (this can be turned off by the user). The value of each word attribute reflects its presence or absence in the string, but this can be changed. A count of the number of times the word appears in the string can be used instead. Word frequencies can be normalized to give each document’s attribute vector the same Euclidean length—the length chosen is not 1 to avoid the very small numbers that would entail, but is the average length of all documents that appear as values of the original string attribute. Alternatively, the frequencies <em>f<span class="sub">ij</span>
</em> for word <em>i</em> in document <em>j</em> can be transformed using log (1 + <em>f<span class="sub">ij</span>
</em>) or the TF × IDF measure (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p329">page 329</a>).</p>
<p id="p0525" class="para_indented"><em>ChangeDateFormat</em> alters the formatting string that is used to parse date attributes. Any format supported by Java’s <em>SimpleDateFormat</em> class can be specified.</p>
</div>
<div id="s0130">
<h4 class="h4" id="st0130">Multi-Instance Data</h4>
<p id="p0530" class="noindent">There are two filters that convert multi-instance data into single-instance format. <em>MultiInstanceToPropositional</em> takes the “aggregating the output” approach (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0175">Section 4.9</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p142">page 142</a>), converting multi-instance data involving a single relational attribute to single-instance format by assigning to each instance in a bag the bag’s class value. There are several options for setting the weight of the new instances in order to adjust for bags of different sizes. <em>RELAGGS</em> implements the “aggregating the input” approach by computing summary statistics (e.g., mean, minimum, and maximum) for the attribute values in each bag. It has an option to avoid computing statistics for nominal attributes with more than a user-specified maximum number of values, effectively removing them from the data.</p>
<p id="p0535" class="para_indented"><em>PropositionalToMultiInstance</em> is a simple filter for converting in the other direction—that is, from single- to multi-instance format involving a single relational attribute. It assumes that the first attribute in the data is an ID that indicates the bag to which the instance belongs.</p>
</div>
<div id="s0135">
<h4 class="h4" id="st0135">Time Series</h4>
<p id="p0540" class="noindent">Two filters work with time series data. <em>TimeSeriesTranslate</em> replaces the values of an attribute (or attributes) in the current instance with the equivalent value in some other (previous or future) instance. <em>TimeSeriesDelta</em> replaces attribute values in the current instance with the difference between the current value and the value in some other instance. In both cases, instances in which the time-shifted value is unknown may be removed or missing values used.</p>
</div>
<div id="s0140">
<h4 class="h4" id="st0140"><a id="p441"></a>Randomizing</h4>
<p id="p0545" class="noindent">Other attribute filters degrade the data. <em>AddNoise</em> takes a nominal attribute and changes a given percentage of its values. Missing values can be retained or changed along with the rest. <em>Obfuscate</em> anonymizes data by renaming the relation, the attribute names, and the nominal and string attribute values. <em>RandomProjection</em> projects the dataset onto a lower-dimensional subspace using a random matrix with columns of unit length (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p326">page 326</a>). The class attribute is not included in the projection.</p>
</div>
</div>
<div id="s0145">
<h3 id="st0145">Unsupervised Instance Filters</h3>
<p id="p0550" class="noindent">Weka’s instance filters, listed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0015">Table 11.2</a>, affect all instances in a dataset rather than all values of a particular attribute or attributes.</p>
<p class="table_caption"><span class="tab_num">Table 11.2. </span> Unsupervised Instance Filters</p>
<table id="t0015" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>NonSparseToSparse</em></td>
<td class="tb">Convert all incoming instances to sparse format (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#s0035">Section 2.4</a>, page 56)</td></tr>
<tr><td class="tb"><em>Normalize</em></td>
<td class="tb">Treat numeric attributes as a vector and normalize the vector to a given length</td></tr>
<tr><td class="tb"><em>Randomize</em></td>
<td class="tb">Randomize the order of instances in a dataset</td></tr>
<tr><td class="tb"><em>RemoveFolds</em></td>
<td class="tb">Output a specified cross-validation fold for the dataset</td></tr>
<tr><td class="tb"><em>RemoveFrequentValues</em></td>
<td class="tb">Remove instances containing the <em>n</em> most frequent or infrequent values of a nominal attribute</td></tr>
<tr><td class="tb"><em>RemoveMisclassified</em></td>
<td class="tb">Remove instances that are incorrectly classified according to a specified classifier—useful for removing outliers</td></tr>
<tr><td class="tb"><em>RemovePercentage</em></td>
<td class="tb">Remove a given percentage of a dataset</td></tr>
<tr><td class="tb"><em>RemoveRange</em></td>
<td class="tb">Remove a given range of instances from a dataset</td></tr>
<tr><td class="tb"><em>RemoveWithValues</em></td>
<td class="tb">Filter out instances with certain attribute values</td></tr>
<tr><td class="tb"><em>Resample</em></td>
<td class="tb">Produce a random subsample of a dataset, sampling with replacement</td></tr>
<tr><td class="tb"><em>ReservoirSample</em></td>
<td class="tb">Uniformly sample <em>n</em> instances from a dataset read incrementally</td></tr>
<tr><td class="tb"><em>SparseToNonSparse</em></td>
<td class="tb">Convert all incoming sparse instances into nonsparse format</td></tr>
<tr><td class="tb"><em>SubsetByExpression</em></td>
<td class="tb">Retain instances according to the evaluation of a logical expression involving mathematical and logical operators applied to attribute values</td></tr>
</tbody>
</table>
<div id="s0150">
<h4 class="h4" id="st0150"><a id="p442"></a>Randomizing and Subsampling</h4>
<p id="p0555" class="noindent">You can <em>Randomize</em> the order of instances in the dataset. <em>Normalize</em> treats all numeric attributes (excluding the class) as a vector and normalizes it to a given length. You can specify the vector length and the norm to be used.</p>
<p id="p0560" class="para_indented">There are various ways of generating subsets of the data. Use <em>Resample</em> to produce a random sample by sampling with or without replacement, or <em>RemoveFolds</em> to split it into a given number of cross-validation folds and reduce it to just one of them. If a random number seed is provided, the dataset will be shuffled before the subset is extracted. <em>ReservoirSample</em> uses the reservoir sampling algorithm described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0090">Section 7.4</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p330">page 330</a>) to produce a random sample (without replacement) from a dataset. When used from the Knowledge Flow interface (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0012.html#c0012">Chapter 12</a>) or from the command-line interface (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0014.html#c0014">Chapter 14</a>), the dataset is read incrementally so that datasets that exceed main memory can be sampled.</p>
<p id="p0565" class="para_indented"><em>RemovePercentage</em> removes a given percentage of instances, and <em>RemoveRange</em> removes a certain range of instance numbers. To remove all instances that have certain values for nominal attributes, or numeric values above or below a certain threshold, use <em>RemoveWithValues</em>. By default, all instances are deleted that exhibit one of a given set of nominal attribute values (if the specified attribute is nominal) or a numeric value below a given threshold (if it is numeric). However, the matching criterion can be inverted. The attribute information is normally left unchanged, but this can be altered so that corresponding values of a nominal attribute are deleted from its definition.</p>
<p id="p0570" class="para_indented"><em>RemoveFrequentValues</em> can be used to remove those instances containing the most- or least-frequent values of a particular nominal attribute; the user can specify how many frequent or infrequent values to remove.</p>
<p id="p0575" class="para_indented"><em>SubsetByExpression</em> selects all instances that satisfy a user-supplied logical expression. The expression can involve mathematical operators and functions, such as those used by <em>AddExpression</em> and <em>MathExpression</em>, and logical operators (e.g., <em>and</em>, <em>or</em>, and <em>not</em>) applied to attribute values. For example, the expression</p>
<p class="figure" id="e0015"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110si2.jpg" alt="image" width="342" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110si2.jpg"></p>
<p>selects only those instances of which the class attribute has the value <em>mammal</em> and the 14th attribute exceeds 2.</p>
<p id="p0580" class="para_indented">You can remove outliers by applying a classification method to the dataset (specifying it just as the clustering method was specified previously for <em>AddCluster</em>) and use <em>RemoveMisclassified</em> to delete the instances that it misclassifies. The process is normally repeated until the data is fully cleansed, but a maximum number of iterations can be specified instead. Cross-validation can be used rather than evaluation on the training data, and for numeric classes an error threshold can be specified.</p>
</div>
<div id="s0155">
<h4 class="h4" id="st0155">Sparse Instances</h4>
<p id="p0585" class="noindent">The <em>NonSparseToSparse</em> and <em>SparseToNonSparse</em> filters convert between the regular representation of a dataset and its sparse representation (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#s0035">Section 2.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#p56">page 56</a>).</p>
</div>
</div>
<div id="s0160">
<h3 id="st0160"><a id="p443"></a>Supervised Filters</h3>
<p id="p0590" class="noindent">Supervised filters are available from the Explorer’s <em>Preprocess</em> panel, just as unsupervised ones are. You need to be careful with them because, despite appearances, they are not really preprocessing operations. We noted this earlier with regard to discretization—the test data splits must not use the test data’s class values because these are supposed to be unknown—and it is true for supervised filters in general.</p>
<p id="p0595" class="para_indented">Because of popular demand, Weka allows you to invoke supervised filters as a preprocessing operation, just like unsupervised filters. However, if you intend to use them for classification, you should adopt a different methodology. A metalearner is provided that invokes a filter in a way that wraps the learning algorithm into the filtering mechanism. This filters the test data using the filter that has been created by the training data. It is also useful for some unsupervised filters. For example, in <em>StringToWordVector</em> the dictionary will be created from the training data alone: Words that are novel in the test data will be discarded. To use a supervised filter in this way, invoke the <em>FilteredClassifier</em> metalearning scheme from the <em>meta</em> section of the menu displayed by the <em>Classify</em> panel’s <em>Choose</em> button. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0095">Figure 11.18(a)</a> shows the object editor for this metalearning scheme. With it, you choose a classifier and a filter. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0095">Figure 11.18(b)</a> shows the menu of filters.</p>
<p id="f0095" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-018ab-9780123748560.jpg" alt="image" width="568" height="235" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-018ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.18</span> Using Weka’s metalearner for discretization: (a) configuring <em>FilteredClassifier</em> and (b) the menu of filters.</p>
<p id="p0600" class="para_indented">Supervised filters, like unsupervised ones, are divided into attribute and instance filters, listed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0020">Tables 11.3</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0025">11.4</a>.</p>
<p class="table_caption"><span class="tab_num">Table 11.3. </span> Supervised Attribute Filters</p>
<table id="t0020" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>AddClassification</em></td>
<td class="tb">Add predictions from a classifier (class labels or probability distributions) as new attributes</td></tr>
<tr><td class="tb"><em>AttributeSelection</em></td>
<td class="tb">Provides access to the same attribute selection methods as the <em>Select attributes</em> panel</td></tr>
<tr><td class="tb"><em>ClassOrder</em></td>
<td class="tb">Randomize, or otherwise alter, the ordering of class values</td></tr>
<tr><td class="tb"><em>Discretize</em></td>
<td class="tb">Convert numeric attributes to nominal</td></tr>
<tr><td class="tb"><em>NominalToBinary</em></td>
<td class="tb">Convert nominal attributes to binary, using a supervised method if the class is numeric</td></tr>
<tr><td class="tb"><em>PLSFilter</em></td>
<td class="tb">Compute partial least-squares directions from the input data and transform it into partial least-squares space</td></tr>
</tbody>
</table>
<p class="table_caption"><span class="tab_num">Table 11.4. </span> Supervised Instance Filters</p>
<table id="t0025" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>Resample</em></td>
<td class="tb">Produce a random subsample of a dataset, sampling with replacement</td></tr>
<tr><td class="tb"><em>SMOTE</em></td>
<td class="tb">Resample a dataset by applying the Synthetic Minority Oversampling Technique</td></tr>
<tr><td class="tb"><em>SpreadSubsample</em></td>
<td class="tb">Produce a random subsample with a given spread between class frequencies, sampling with replacement</td></tr>
<tr><td class="tb"><em>StratifiedRemoveFolds</em></td>
<td class="tb">Output a specified stratified cross-validation fold for the dataset</td></tr>
</tbody>
</table>
<div id="s0165">
<h4 class="h4" id="st0165">Supervised Attribute Filters</h4>
<p id="p0605" class="noindent"><em>Discretize</em>, highlighted in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0095">Figure 11.18</a>, uses the MDL method of supervised discretization (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0030">Section 7.2</a>). You can specify a range of attributes or force the <a id="p444"></a>discretized attribute to be binary. The class must be nominal. By default <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib96">Fayyad and Irani’s (1993)</a> criterion is used, but <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib192">Kononenko’s method (1995)</a> is an option.</p>
<p id="p0610" class="para_indented">There is a supervised version of the <em>NominalToBinary</em> filter that transforms all multivalued nominal attributes to binary ones. In this version, the transformation depends on whether the class is nominal or numeric. If nominal, the same method as before is used: An attribute with <em>k</em> values is transformed into <em>k</em> binary attributes. If the class is numeric, however, the method described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0220">Section 6.6</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p253">page 253</a>) is applied. In either case the class itself is not transformed.</p>
<p id="p0615" class="para_indented"><em>ClassOrder</em> changes the ordering of the class values. The user determines whether the new ordering is random or in ascending or descending class frequency. This filter must not be used with the <em>FilteredClassifier</em> metalearning scheme! <em>AttributeSelection</em> can be used for automatic attribute selection and provides the same functionality as the Explorer’s <em>Select attributes</em> panel (described later).</p>
<p id="p0620" class="para_indented"><a id="p445"></a><em>AddClassification</em> adds to the data the predictions of a given classifier, which can be either trained on the input dataset or loaded from a file as a serialized object. New attributes can be added that hold the predicted class value, the predicted probability distribution (if the class is nominal), or a flag that indicates misclassified instances (or, for numeric classes, the difference between the predicted and actual class values).</p>
<p id="p0625" class="para_indented"><em>PLSFilter</em> computes the partial least-squares directions for the input data and uses them to transform it into partial least-squares space. The result is the same as that produced by the method described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p326">page 326</a>). (Two different algorithms are implemented; they yield the same result modulo a constant factor.) The number of directions to compute can be specified, and it is possible to retain the original class attribute in the new dataset or to replace it with predictions. Missing values are replaced by defaults, and the input data can be either centered or standardized before computing the partial least squares. The dataset must be entirely numeric: If it contains nominal attributes, the user must remove them or transform them to numeric ones.</p>
</div>
<div id="s0170">
<h4 class="h4" id="st0170">Supervised Instance Filters</h4>
<p id="p0630" class="noindent">There are four supervised instance filters. <em>Resample</em> is like the eponymous unsupervised instance filter except that it maintains the class distribution in the subsample. Alternatively, it can be configured to bias the class distribution toward a uniform one. Sampling can be performed with (default) or without replacement. <em>SpreadSubsample</em> also produces a random subsample, but the frequency difference between the rarest and the most common class can be controlled—for example, you can specify at most a 2:1 difference in class frequencies. You can also limit the number of instances in any class by specifying an explicit maximum count.</p>
<p id="p0635" class="para_indented"><em>SMOTE</em> is another filter that samples the data and alters the class distribution (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib9001">Chawla et al., 2002</a>). Like <em>SpreadSubsample</em>, it can be used to adjust the relative frequency between minority and majority classes in the data—but it takes care not to undersample majority classes and it oversamples the minority class by creating synthetic instances using a <em>k</em>-nearest-neighbor approach. The user can specify the oversampling percentage and the number of neighbors to use when creating synthetic instances.</p>
<p id="p0640" class="para_indented">Like the unsupervised instance filter <em>RemoveFolds</em>, <em>StratifiedRemoveFolds</em> outputs a specified cross-validation fold for the dataset, except that this time the fold is stratified.</p>
</div>
</div>
</div>
<div id="s0175">
<h2 id="st0175">11.4 Learning algorithms</h2>
<p id="p0645" class="noindent">On the <em>Classify</em> panel, when you select a learning algorithm using the <em>Choose</em> button the command-line version of the classifier appears in the line beside the button, including the parameters specified with minus signs. To change the parameters, click that line to get an appropriate object editor. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> lists Weka’s classifiers. They are <a id="p451"></a>divided into Bayesian classifiers, trees, rules, functions, lazy classifiers, multi-instance classifiers, and a final miscellaneous category. We describe them briefly here, along with their parameters. To learn more, choose one in the Weka Explorer interface and examine its object editor. A further kind of classifier, the Metalearner, is described in the next section.</p><a id="p446"></a><a id="p447"></a><a id="p448"></a><a id="p449"></a><a id="p450"></a><p class="table_caption"><span class="tab_num">Table 11.5. </span> Classifier Algorithms in Weka</p>
<table id="t0030" frame="box" rules="all">
<thead>
<tr><td class="tch"></td><td class="tb">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb">Bayes</td>
<td class="tb"><em>AODE</em></td>
<td class="tb">Averaged one-dependence estimators</td></tr>
<tr><td class="tb"></td><td class="tb"><em>AODEsr</em></td>
<td class="tb">Averaged one-dependence estimators with subsumption resolution</td></tr>
<tr><td class="tb"></td><td class="tb"><em>BayesianLogisticRegression</em></td>
<td class="tb">Bayesian approach to learning a linear logistic regression model</td></tr>
<tr><td class="tb"></td><td class="tb"><em>BayesNet</em></td>
<td class="tb">Learns Bayesian nets</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ComplementNaiveBayes</em></td>
<td class="tb">Builds a Complement Naïve Bayes classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>DMNBText</em></td>
<td class="tb">Discriminative multinomial Naïve Bayes classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>HNB</em></td>
<td class="tb">Hidden Naïve Bayes classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NaiveBayes</em></td>
<td class="tb">Standard probabilistic Naïve Bayes classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NaiveBayesMultinomial</em></td>
<td class="tb">Multinomial version of Naïve Bayes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NaiveBayesMultinomial-Updateable</em></td>
<td class="tb">Incremental multinomial Naïve Bayes classifier that learns one instance at a time</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NaiveBayesSimple</em></td>
<td class="tb">Simple implementation of Naïve Bayes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NaiveBayesUpdateable</em></td>
<td class="tb">Incremental Naïve Bayes classifier that learns one instance at a time</td></tr>
<tr><td class="tb"></td><td class="tb"><em>WAODE</em></td>
<td class="tb">Weightily averaged one-dependence estimators</td></tr>
<tr><td class="tb">Trees</td>
<td class="tb"><em>ADTree</em></td>
<td class="tb">Builds alternating decision trees</td></tr>
<tr><td class="tb"></td><td class="tb"><em>BFTree</em></td>
<td class="tb">Builds a decision tree using a best-first search</td></tr>
<tr><td class="tb"></td><td class="tb"><em>DecisionStump</em></td>
<td class="tb">Builds one-level decision trees</td></tr>
<tr><td class="tb"></td><td class="tb"><em>FT</em></td>
<td class="tb">Builds a functional tree with oblique splits and linear functions at the leaves</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Id3</em></td>
<td class="tb">Basic divide-and-conquer decision tree algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>J48</em></td>
<td class="tb">C4.5 decision tree learner (implements C4.5 revision 8)</td></tr>
<tr><td class="tb"></td><td class="tb"><em>J48graft</em></td>
<td class="tb">C4.5 with grafting</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LADTree</em></td>
<td class="tb">Builds multiclass alternating decision trees using LogitBoost</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LMT</em></td>
<td class="tb">Builds logistic model trees</td></tr>
<tr><td class="tb"></td><td class="tb"><em>M5P</em></td>
<td class="tb">M5′ model tree learner</td></tr>
<tr><td class="tb"></td><td class="tb"><em>NBTree</em></td>
<td class="tb">Builds a decision tree with Naïve Bayes classifiers at the leaves</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RandomForest</em></td>
<td class="tb">Constructs random forests</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RandomTree</em></td>
<td class="tb">Constructs a tree that considers a given number of random features at each node</td></tr>
<tr><td class="tb"></td><td class="tb"><em>REPTree</em></td>
<td class="tb">Fast tree learner that uses reduced-error pruning</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SimpleCart</em></td>
<td class="tb">Decision tree learner using CART’s minimal cost complexity pruning</td></tr>
<tr><td class="tb"></td><td class="tb"><em>UserClassifier</em></td>
<td class="tb">Allows users to build their own decision tree</td></tr>
<tr><td class="tb">Rules</td>
<td class="tb"><em>ConjunctiveRule</em></td>
<td class="tb">Simple conjunctive rule learner</td></tr>
<tr><td class="tb"></td><td class="tb"><em>DecisionTable</em></td>
<td class="tb">Builds a simple decision table majority classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>DTNB</em></td>
<td class="tb">Hybrid classifier combining decision tables and Naïve Bayes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>JRip</em></td>
<td class="tb">Ripper algorithm for fast, effective rule induction</td></tr>
<tr><td class="tb"></td><td class="tb"><em>M5Rules</em></td>
<td class="tb">Obtains rules from model trees built using M5′</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Nnge</em></td>
<td class="tb">Nearest-neighbor method of generating rules using nonnested generalized exemplars</td></tr>
<tr><td class="tb"></td><td class="tb"><em>OneR</em></td>
<td class="tb">1R classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>PART</em></td>
<td class="tb">Obtains rules from partial decision trees built using <em>J4.8</em></td></tr>
<tr><td class="tb"></td><td class="tb"><em>Prism</em></td>
<td class="tb">Simple covering algorithm for rules</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Ridor</em></td>
<td class="tb">Ripple-down rule learner</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ZeroR</em></td>
<td class="tb">Predicts the majority class (if nominal) or the average value (if numeric).</td></tr>
<tr><td class="tb">Functions</td>
<td class="tb"><em>GaussianProcesses</em></td>
<td class="tb">Gaussian processes for regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>IsotonicRegression</em></td>
<td class="tb">Builds an isotonic regression model</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LeastMedSq</em></td>
<td class="tb">Robust regression using the median rather than the mean</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LibLINEAR</em></td>
<td class="tb">Wrapper classifier for using the third-party <em>LIBLINEAR</em> library for regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LibSVM</em></td>
<td class="tb">Wrapper classifier for using the third-party <em>LIBSVM</em> library for support vector machines</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LinearRegression</em></td>
<td class="tb">Standard multiple linear regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Logistic</em></td>
<td class="tb">Builds linear logistic regression models</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MultilayerPerceptron</em></td>
<td class="tb">Backpropagation neural network</td></tr>
<tr><td class="tb"></td><td class="tb"><em>PaceRegression</em></td>
<td class="tb">Builds linear regression models using Pace regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>PLSClassifier</em></td>
<td class="tb">Builds partial least-squares directions and uses them for prediction</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RBFNetwork</em></td>
<td class="tb">Implements a radial basis function network</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SimpleLinearRegression</em></td>
<td class="tb">Learns a linear regression model based on a single attribute</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SimpleLogistic</em></td>
<td class="tb">Builds linear logistic regression models with built-in attribute selection</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SMO</em></td>
<td class="tb">Sequential minimal optimization algorithm for support vector classification</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SMOreg</em></td>
<td class="tb">Sequential minimal optimization algorithm for support vector regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SPegasos</em></td>
<td class="tb">Binary class linear support vector machine or logistic regression learned via stochastic gradient descent</td></tr>
<tr><td class="tb"></td><td class="tb"><em>VotedPerceptron</em></td>
<td class="tb">Voted Perceptron algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Winnow</em></td>
<td class="tb">Mistake-driven perceptron with multiplicative updates</td></tr>
<tr><td class="tb">Lazy</td>
<td class="tb"><em>IB1</em></td>
<td class="tb">Basic nearest-neighbor instance-based learner</td></tr>
<tr><td class="tb"></td><td class="tb"><em>IBk</em></td>
<td class="tb"><em>k</em>-nearest-neighbors classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>KStar</em></td>
<td class="tb">Nearest neighbor with generalized distance function</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LBR</em></td>
<td class="tb">Lazy Bayesian Rules classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LWL</em></td>
<td class="tb">General algorithm for locally weighted learning</td></tr>
<tr><td class="tb">MI</td>
<td class="tb"><em>CitationKNN</em></td>
<td class="tb">Citation KNN distance-based method</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MDD</em></td>
<td class="tb">Diverse density using the collective assumption</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MIBoost</em></td>
<td class="tb">Boosting for multi-instance data using the collective assumption</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MIDD</em></td>
<td class="tb">Standard diverse density algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MIEMDD</em></td>
<td class="tb">EM-based diverse density algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MILR</em></td>
<td class="tb">Multi-instance logistic regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MINND</em></td>
<td class="tb">Nearest-neighbor method using Kullback-Leibler distance</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MIOptimalBall</em></td>
<td class="tb">Classifies multi-instance data using distance to a reference point</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MISMO</em></td>
<td class="tb">SMO using multi-instance kernels</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MISVM</em></td>
<td class="tb">Iteratively applies a single-instance SVM learner to multi-instance data</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MIWrapper</em></td>
<td class="tb">Applies single-instance learner using the aggregating-the-output approach</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SimpleMI</em></td>
<td class="tb">Applies single-instance learner using the aggregating-the-input approach</td></tr>
<tr><td class="tb">Misc</td>
<td class="tb"><em>HyperPipes</em></td>
<td class="tb">Extremely simple, fast learner based on hypervolumes in instance space</td></tr>
<tr><td class="tb"></td><td class="tb"><em>VFI</em></td>
<td class="tb">Voting feature intervals method, simple and fast</td></tr>
</tbody>
</table>
<div id="s0180">
<h3 id="st0180">Bayesian Classifiers</h3>
<p id="p0650" class="noindent"><em>NaiveBayes</em> implements the probabilistic Naïve Bayes classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p93">page 93</a>). <em>NaiveBayesSimple</em> uses the normal distribution to model numeric attributes. <em>NaiveBayes</em> can use kernel density estimators, which improve performance if the normality assumption is grossly incorrect; it can also handle numeric attributes using supervised discretization.</p>
<p id="p0655" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0100">Figure 11.19</a> shows the output of <em>NaiveBayes</em> on the weather data. The salient difference between this and the output in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0030">Figure 11.5</a> of <em>J48</em> on the same data is that instead of a tree in textual form, here the parameters of the Naïve Bayes model are presented in a table. The first column shows attributes and the other two show class values; entries are either frequency counts of nominal values or parameters of normal distributions for numeric attributes. For example, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0100">Figure 11.19</a> shows that the mean temperature value for instances of class <em>yes</em> is 72.9697, while for instances for which <em>windy</em> = <em>yes</em> the values <em>true</em> and <em>false</em> occur 4 and 7 times, respectively. The grand total of the <em>yes</em> and <em>no</em> counts for <em>windy</em> is, surprisingly, 18—more than the 14 instances in the weather data (the situation for <em>outlook</em> is even worse, totaling 20). The reason is that <em>NaiveBayes</em> avoids zero frequencies by applying the Laplace correction, which involves initializing each count to 1 rather than to 0.</p><a id="p452"></a><p id="f0100" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-019-9780123748560.jpg" alt="image" width="514" height="878" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-019-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.19</span> Output of <em>NaiveBayes</em> on the weather data.</p>
<p id="p0660" class="para_indented"><em>NaiveBayesUpdateable</em> is an incremental version that processes one instance at a time; it can use a kernel estimator but not discretization. <em>NaiveBayesMultinomial</em> implements the multinomial Bayes’ classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p97">page 97</a>); <em>NaiveBayesMultinomialUpdateable</em> is an incremental version. <em>ComplementNaiveBayes</em> builds a Complement Naïve Bayes classifier as described by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib262">Rennie et al. (2003)</a> (the TF × IDF and length normalization transforms used in this paper can be performed using the <em>StringToWordVector</em> filter).</p>
<p id="p0665" class="para_indented"><em>AODE</em> is the averaged one-dependence estimator discussed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p268">page 268</a>). <em>WAODE</em> is a version of AODE that creates a weighted ensemble of one-dependence estimators rather than a simple average (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib167">Jiang and Zhang, 2006</a>). The weight for each ensemble member is proportional to the mutual information between its superparent and the class attribute. <em>AODEsr</em> is a version of AODE that incorporates the lazy elimination of highly related attribute values at classification time (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib337">Zheng and Webb, 2006</a>). <em>HNB</em> learns a hidden Naïve Bayes model—a kind of simple Bayesian network where each attribute has the class as a parent node and another special “hidden” parent node that combines the influences of all the other attributes (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib334">Zhang et al., 2005</a>). Each attribute’s hidden node is constructed from the average of weighted one-dependence estimators, the weights of which are computed using conditional mutual information.</p>
<p id="p0670" class="para_indented"><a id="p453"></a>Another Naïve Bayes scheme for text classification is <em>DMNBtext</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib292">Su et al., 2008</a>). This learns a multinomial Naïve Bayes classifier in a combined generative and discriminative fashion. The parameters of a Bayesian network are traditionally learned in generative fashion, calculating frequency counts for the conditional probability tables from the training data—thereby maximizing the likelihood of the data given the model. In contrast, parameter values that maximize the generalization accuracy (or conditional likelihood) are desirable in classification settings. <em>DMNBText</em> injects a discriminative element into parameter learning by considering the current classifier’s prediction for a training instance before updating frequency counts. When processing a given training instance, the counts are incremented by one minus the predicted probability for the instance’s class value. <em>DMNBText</em> allows users to specify how many iterations over the training data the algorithm will make, and whether word frequency information should be ignored—in which case, the method learns a standard Naïve Bayes model rather than a multinomial one.</p>
<p id="p0675" class="para_indented"><em>BayesianLogisticRegression</em> takes a Bayesian approach to learning a binomial logistic regression function by allowing the user to place a prior distribution on the values for the model’s coefficients. Zero-mean Gaussian and Laplace distributions can be used for the prior. Both favor sparseness in the coefficients, Laplace more so than Gaussian, which makes this approach suitable for learning logistic regression models for high-dimensional problems—for example, text classification (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib133">Genkin et al., 2007</a>). The user can opt to set the variance of the prior to a specific value or have it chosen, within a specified range, through the use of cross-validation.</p>
<p id="p0680" class="para_indented"><em>BayesNet</em> learns Bayesian nets under the assumptions made in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a>: nominal attributes (numeric ones are prediscretized) and no missing values (any such values are replaced globally). There are four different algorithms for estimating the conditional probability tables of the network. Search is done using K2 or the TAN algorithm (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p267">page 267</a>) or more sophisticated methods based on hill-climbing, simulated annealing, tabu search, and genetic algorithms. Optionally, search speed can be improved using AD-trees (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p269">page 269</a>). There are also two algorithms that use conditional independence tests to learn the structure of the network; alternatively, the network structure can be loaded from an XML (eXtensible Markup Language) file. More details on the implementation of Bayesian networks in Weka can be found in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib32">Bouckaert (2004)</a>.</p>
<p id="p0685" class="para_indented">You can observe the network structure by right-clicking the history item and selecting <em>Visualize graph.</em>
<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0105">Figure 11.20(a)</a> shows the graph for the nominal version of the weather data, which in fact corresponds to the Naïve Bayes result with all probabilities conditioned on the class value. This is because the search algorithm defaults to K2 with the maximum number of parents of a node set to one. Reconfiguring this to three by clicking on <em>K2</em> in the configuration panel yields the more interesting network in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0105">Figure 11.20(b)</a>. Clicking on a node shows its probability distribution—<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0105">Figure 11.20(c)</a> is obtained by clicking on the <em>windy</em> node in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0105">Figure 11.20(b)</a>.</p>
<p id="f0105" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-020ac-9780123748560.jpg" alt="image" width="513" height="469" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-020ac-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.20</span> Visualizing a Bayesian network for the weather data (nominal version): (a) default output, (b) with the maximum number of parents set to three in the search algorithm, and (c) the probability distribution table for the <em>windy</em> node in (b).</p>
</div>
<div id="s0185">
<h3 id="st0185"><a id="p454"></a>Trees</h3>
<p id="p0690" class="noindent">Of the tree classifiers in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> we have already encountered the <em>UserClassifier</em> (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0050">Section 11.2</a>). We have also seen how to use <em>J4.8</em>, which reimplements C4.5 (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p201">page 201</a>). To see the options, click the line beside the <em>Choose</em> button in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0025">Figure 11.4(b)</a> to bring up the object editor in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0110">Figure 11.21</a>. You can build a binary tree instead of one with multiway branches. You can set the confidence threshold for pruning (default 0.25) and the minimum number of instances permissible at a leaf (default 2). Instead of standard C4.5 pruning you can choose reduced-error pruning (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p206">page 206</a>). The <em>numFolds</em> parameter (default 3) determines the size of the pruning set: The data is divided equally into that number of parts and the last one used for pruning. When visualizing the tree it is nice to be <a id="p455"></a>able to consult the original data points, which you can do if <em>saveInstanceData</em> has been turned on (it is off, or <em>False</em>, by default to reduce memory requirements). You can suppress subtree raising, yielding a more efficient algorithm; force the algorithm to use the unpruned tree instead of the pruned one; or use Laplace smoothing for predicted probabilities (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p93">page 93</a>).</p>
<p id="f0110" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-021-9780123748560.jpg" alt="image" width="341" height="402" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-021-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.21</span> Changing the parameters for <em>J4.8</em>.</p>
<p id="p0695" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> shows many other decision tree methods. <em>Id3</em> implements the basic algorithm explained in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>. <em>DecisionStump</em>, designed for use with the boosting methods described later, builds one-level binary decision trees for datasets with a categorical or numeric class, dealing with missing values by treating them as a separate value and extending a third branch from the stump. Trees built by <em>RandomTree</em> test a given number of random features at each node, performing no pruning. <em>RandomForest</em> constructs random forests by bagging ensembles of random trees (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0030">Section 8.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p356">page 356</a>).</p>
<p id="p0700" class="para_indented"><em>J48graft</em> is an extended version of <em>J48</em> that considers grafting additional branches onto the tree in a postprocessing phase (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib309">Webb, 1999</a>). The grafting process attempts to achieve some of the power of ensemble methods such as bagged and boosted trees while maintaining a single interpretable structure. It identifies regions of the instance space that are either empty or contain only misclassified examples and explores alternative classifications by considering different tests that could have been selected at nodes above the leaf containing the region in question.</p>
<p id="p0705" class="para_indented"><a id="p456"></a><em>REPTree</em> builds a decision or regression tree using information gain/variance reduction and prunes it using reduced-error pruning (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p206">page 206</a>). Optimized for speed, it only sorts values for numeric attributes once (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p192">page 192</a>). It deals with missing values by splitting instances into pieces, as C4.5 does. You can set the minimum number of instances per leaf, maximum tree depth (useful when boosting trees), minimum proportion of training set variance for a split (numeric classes only), and number of folds for pruning.</p>
<p id="p0710" class="para_indented"><em>BFTree</em> constructs a decision tree using a best-first expansion of nodes rather than the depth-first expansion used by standard decision tree learners (such as C4.5). Pre- and postpruning options are available that are based on finding the best number of expansions to use via cross-validation on the training data. While fully grown trees are the same for best-first and depth-first algorithms, the pruning mechanism used by <em>BFTree</em> will yield a different pruned tree structure than that produced by depth-first methods.</p>
<p id="p0715" class="para_indented"><em>SimpleCart</em> is a decision tree learner for classification that employs the minimal cost-complexity pruning strategy (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p202">page 202</a>). Though named after the CART (classification and regression tree) learner that pioneered this strategy (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib41">Breiman et al., 1984</a>), the similarity ends here: It provides none of CART’s other features. You can set the minimum number of instances per leaf, the percentage of training data used to construct the tree, and the number of cross-validation folds used in the pruning procedure.</p>
<p id="p0720" class="para_indented"><em>NBTree</em> is a hybrid between decision trees and Naïve Bayes. It creates trees with leaves that are Naïve Bayes classifiers for the instances that reach the leaf. When constructing the tree, cross-validation is used to decide whether a node should be split further or a Naïve Bayes model used instead (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib186">Kohavi, 1996</a>).</p>
<p id="p0725" class="para_indented"><em>M5P</em> is the model tree learner described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0220">Section 6.6</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p251">page 251</a>).</p>
<p id="p0730" class="para_indented"><em>LMT</em> builds logistic model trees (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0075">Section 8.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p368">page 368</a>). It can deal with binary and multiclass target variables, numeric and nominal attributes, and missing values. When fitting the logistic regression functions at a node using the LogitBoost algorithm, it uses cross-validation to determine how many iterations to run just once, and employs the same number throughout the tree instead of cross-validating at every node. This heuristic (which you can switch off) improves the runtime considerably, with little effect on accuracy. Alternatively, you can set the number of boosting iterations to be used throughout the tree manually, or use a fast heuristic based on the Akaike Information Criterion instead of cross-validation. Weight trimming can be used to further improve runtime. Normally, it is the misclassification error that cross-validation minimizes, but the root mean-squared error of the probabilities can be chosen instead. The splitting criterion can be based on C4.5’s information gain (the default) or on the LogitBoost residuals, striving to improve the purity of the residuals. <em>LMT</em> employs the minimal cost-complexity pruning mechanism (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p202">page 202</a>) to produce a compact tree structure.</p>
<p id="p0735" class="para_indented"><em>FT</em> builds functional trees (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib131">Gama, 2004</a>)—that is, trees for classification with linear functions at the leaves and, optionally, at interior nodes as well. It builds on the <em>LMT</em> implementation and expands the choice of attributes to split on at interior <a id="p457"></a>nodes by creating synthetic attributes that hold the class probabilities predicted by that node’s logistic regression model. Like <em>LMT</em>, C4.5’s splitting criterion is used to select an attribute to split on. If a synthetic attribute is chosen, the split is oblique, not axis-parallel. Unlike <em>LMT</em>, <em>FT</em> uses standard C4.5 pruning rather than minimal cost-complexity pruning. The user has the option of having the algorithm build trees with functions only at the leaves (in which case, <em>FT</em> behaves like <em>LMT</em>, but with C4.5 pruning), only at the interior nodes, or at both.</p>
<p id="p0740" class="para_indented"><em>ADTree</em> builds an alternating decision tree for two-class problems using boosting (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0075">Section 8.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p270">page 270</a>). The number of boosting iterations is a parameter that can be tuned to suit the dataset and the desired complexity–accuracy tradeoff. Each iteration adds three nodes to the tree (one split node and two prediction nodes) unless nodes can be merged. The default search method is the exhaustive search (<em>Expand all paths</em>); the others are heuristics and are much faster. You can determine whether to save instance data for visualization. <em>LADTree</em> is an alternating decision tree algorithm that can handle multiclass problems based on the LogitBoost algorithm (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib163">Holmes et al., 2002</a>). Like <em>ADTree</em>, the number of boosting iterations is a parameter that can be tuned for the data at hand and determines the size of the tree constructed.</p>
</div>
<div id="s0190">
<h3 id="st0190">Rules</h3>
<p id="p0745" class="noindent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> earlier in the chapter shows many methods for generating rules.</p>
<p id="p0750" class="para_indented"><em>DecisionTable</em> builds a decision table majority classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p314">page 314</a>). It evaluates feature subsets using best-first search and can use cross-validation for evaluation (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib185">Kohavi, 1995b</a>). An option uses the nearest-neighbor method to determine the class for each instance that is not covered by a decision table entry, instead of the table’s global majority, based on the same set of features.</p>
<p id="p0755" class="para_indented"><em>DTNB</em> is a hybrid classifier that combines a decision table with Naïve Bayes (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib148">Hall and Frank, 2008</a>). The algorithm divides the attributes into two distinct subsets, one modeled by Naïve Bayes and the other by the decision table. A greedy search, guided by leave-one-out cross-validation and starting with all attributes modeled by the decision table, is used to decide which attributes should be modeled by Naïve Bayes; consideration is also given to dropping an attribute entirely from the model. The predictions produced by the two methods are combined into an overall prediction using Bayes’ rule. Users can select the evaluation measure used for cross-validation: Options for classification problems include accuracy, root mean-squared error on the class probabilities, mean absolute error on the class probabilities, and area under the ROC curve. For numeric classes root mean-squared error is used.</p>
<p id="p0760" class="para_indented"><em>OneR</em> is the 1R classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0010">Section 4.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p86">page 86</a>) with one parameter—the minimum bucket size for discretization. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0115">Figure 11.22</a> shows its output for the labor negotiations data. The <em>Classifier model</em> part shows that <em>wage-increase-first-year</em> has been identified as the basis of the rule produced, with a split at the value 2.9 dividing <em>bad</em> outcomes from <em>good</em> ones (the class is also <em>good</em> if the value of that attribute is missing). Beneath the rule the fraction of training instances correctly classified by the rule is given in parentheses.</p><a id="p458"></a><p id="f0115" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-022-9780123748560.jpg" alt="image" width="514" height="781" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-022-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.22</span> Output of <em>OneR</em> on the labor negotiations data.</p>
<p id="p0765" class="para_indented"><a id="p459"></a><em>ConjunctiveRule</em> learns a single rule that predicts either a numeric or a nominal class value. Uncovered test instances are assigned the default class value (or distribution) of the uncovered training instances. The information gain (nominal class) or variance reduction (numeric class) of each antecedent is computed, and rules are pruned using reduced-error pruning. <em>ZeroR</em> is even simpler: It predicts the test data’s majority class (if nominal) or average value (if numeric). <em>Prism</em> implements the elementary covering algorithm for rules (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0065">Section 4.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p108">page 108</a>).</p>
<p id="p0770" class="para_indented"><em>PART</em> obtains rules from partial decision trees (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p208">page 208</a>). It builds the tree using C4.5’s heuristics with the same user-defined parameters as <em>J4.8</em>. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0120">Figure 11.23</a> shows the output of <em>PART</em> for the labor negotiations data. Three rules are found and are intended to be processed in order, the prediction generated for any test instance being the outcome of the first rule that fires. The last, “catch-all” rule will always fire. As with <em>J48</em>, the numbers in parentheses that follow each rule give the number of instances that are covered by the rule followed by the number that are misclassified (if any).</p><a id="p460"></a><p id="f0120" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-023a-9780123748560.jpg" alt="image" width="514" height="718" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-023a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-023b-9780123748560.jpg" alt="image" width="514" height="188" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-023b-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.23</span> Output of <em>PART</em> for the labor negotiations data.</p>
<p id="p0775" class="para_indented"><em>M5Rules</em> obtains regression rules from model trees built using M5′ (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>). <em>Ridor</em> learns rules with exceptions (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p212">page 212</a>) by generating the default rule, using incremental reduced-error pruning to find exceptions with the smallest error rate, finding the best exceptions for each exception, and iterating.</p>
<p id="p0780" class="para_indented"><em>JRip</em> implements RIPPER (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p208">page 208</a>), including heuristic global optimization of the rule set (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib63">Cohen, 1995</a>). <em>NNge</em> is a nearest-neighbor method for generating rules using non-nested generalized exemplars (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0180">Section 6.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p246">page 246</a>).</p>
</div>
<div id="s0195">
<h3 id="st0195">Functions</h3>
<p id="p0785" class="noindent">The <em>functions</em> category of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> includes an assorted group of classifiers that can be written down as mathematical equations in a reasonably natural way. Other methods, such as decision trees and rules, cannot (there are exceptions: Naïve Bayes has a simple mathematical formulation). Four of them implement linear regression (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p124">page 124</a>). <em>SimpleLinearRegression</em> learns a linear regression model based on a single attribute—it chooses the one that yields the smallest squared error. Missing values and nonnumeric attributes are not allowed. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0125">Figure 11.24</a> shows the output of <em>SimpleLinearRegression</em> for the CPU performance data. The attribute that has the smallest squared error in this case is <em>MMAX</em>.</p><a id="p461"></a><p id="f0125" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-024-9780123748560.jpg" alt="image" width="514" height="485" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-024-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.24</span> Output of <em>SimpleLinearRegression</em> for the CPU performance data.</p>
<p id="p0790" class="para_indented"><em>LinearRegression</em> performs standard least-squares multiple linear regression and can optionally perform attribute selection, either greedily using backward elimination (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p311">page 311</a>) or by building a full model from all attributes and dropping the terms one by one, in decreasing order of their standardized coefficients, until a stopping criteria is reached (this method was described in a slightly different context in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0220">Section 6.6</a> under Pruning the Tree). Both methods use a version of the AIC termination criterion discussed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p266">page 266</a>). The implementation has two further refinements: a heuristic mechanism for detecting collinear attributes (which can be turned off) and a <em>ridge</em> parameter that stabilizes degenerate cases and can reduce overfitting by penalizing large coefficients. Technically, <a id="p462"></a><em>LinearRegression</em> implements ridge regression, which is described in standard statistics texts.</p>
<p id="p0795" class="para_indented"><em>LeastMedSq</em> is a robust linear regression method that minimizes the median (rather than the mean) of the squares of divergences from the regression line (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0100">Section 7.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p333">page 333</a>) (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib269">Rousseeuw and Leroy, 1987</a>). It repeatedly applies standard linear regression to subsamples of the data and outputs the solution that has the smallest median-squared error.</p>
<p id="p0800" class="para_indented"><em>PaceRegression</em> builds linear regression models using the technique of Pace regression (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib308">Wang and Witten, 2002</a>). When there are many attributes, Pace regression is particularly good at determining which ones to discard—indeed, under certain regularity conditions it is provably optimal as the number of attributes tends to infinity.</p>
<p id="p0805" class="para_indented"><em>IsotonicRegression</em> implements the method for learning an isotonic regression function based on the pair-adjacent violators approach (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0155">Section 7.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p345">page 345</a>). <em>PLSClassifier</em> learns a partial least-squares regression model (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p328">page 328</a>). It uses the <em>PLSFilter</em> to transform the training data into partial least-squares space and then learns a linear regression from the transformed data. All the options of the <em>PLSFilter</em> are available to the user in the object editor for the <em>PLSClassifier</em>.</p>
<p id="p0810" class="para_indented"><em>SMO</em> implements the sequential minimal-optimization algorithm for training a support vector classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>), using kernel functions such as polynomial or Gaussian kernels (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib251">Platt, 1998</a>; <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib174">Keerthi et al., 2001</a>). Missing values are replaced globally, nominal attributes are transformed into binary ones, and attributes are normalized by default—note that the coefficients in the output are based on the normalized data. Normalization can be turned off, or the input standardized to zero mean and unit variance. Pairwise classification is used for multiclass problems. Logistic regression models can be fitted to the support vector machine output to obtain probability estimates. In the multiclass case, the predicted probabilities will be coupled pairwise (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib155">Hastie and Tibshirani, 1998</a>). When working with sparse instances, turn normalization off for faster operation.</p>
<p id="p0815" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0130">Figure 11.25</a> shows the output of <em>SMO</em> on the iris data. A polynomial kernel with an exponent of 1 has been used, making the model a linear support vector machine. Since the iris data contains three class values, three binary <em>SMO</em> models have been output—one hyperplane to separate each of the possible pair of class values. Furthermore, since the machine is linear, the hyperplanes are expressed as functions of the attribute values in the original (albeit normalized) space. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0135">Figure 11.26</a> shows the result when the exponent of the polynomial kernel is set to 2, making the support vector machine nonlinear. There are three binary <em>SMO</em> models as before, but this time the hyperplanes are expressed as functions of the support vectors (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p225">page 225</a>). Each of the support vectors is shown enclosed in angle brackets, along with the value of its coefficient <em>α</em>. The value of the offset parameter, <em>β</em> (which is the same as <em>α</em>
<span class="sub">0</span>), is shown as the last component of each function.</p><a id="p463"></a><p id="f0130" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-025a-9780123748560.jpg" alt="image" width="514" height="829" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-025a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-025b-9780123748560.jpg" alt="image" width="514" height="355" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-025b-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.25</span> Output of <em>SMO</em> on the iris data.</p><a id="p465"></a><a id="p466"></a><p id="f0135" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-026a-9780123748560.jpg" alt="image" width="514" height="830" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-026a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-026b-9780123748560.jpg" alt="image" width="514" height="733" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-026b-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-026c-9780123748560.jpg" alt="image" width="514" height="219" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-026c-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.26</span> Output of <em>SMO</em> with a nonlinear kernel on the iris data.</p>
<p id="p0820" class="para_indented"><em>SMOreg</em> implements the sequential minimal-optimization algorithm for learning a support vector regression model (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib286">Smola and Schölkopf, 2004</a>).</p>
<p id="p0825" class="para_indented"><a id="p464"></a><em>SPegasos</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib283">Shalev-Shwartz et al., 2007</a>) learns a linear support vector machine for two-class problems using stochastic gradient descent (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p240">page 240</a>). Nominal attributes are converted to binary ones automatically. By default, missing values are replaced with means/modes and attributes are normalized. The user can specify the number of epochs (iterations over the training data) to perform and the value of “lambda,” the regularization constant for controlling the closeness of fit. Two loss functions are provided: the hinge loss (the default setting) for learning a support vector machine, and the log loss, which results in a logistic regression rather than a support vector machine. <em>SPegasos</em> can be trained in batch mode or incrementally, one instance at a time. If trained incrementally, the epochs’ parameter has no effect because each instance is processed just once.</p>
<p id="p0830" class="para_indented"><em>VotedPerceptron</em> is the voted perceptron algorithm (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p232">page 232</a>). <em>Winnow</em> (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p129">page 129</a>) modifies the basic perceptron to use multiplicative updates. The implementation allows for a second multiplier <em>β</em>—different from 1/<em>α</em>—to be used in place of the divisions in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0060">Figure 4.11</a>, and also provides the balanced version of the algorithm.</p>
<p id="p0835" class="para_indented"><em>GaussianProcesses</em> implements the Bayesian Gaussian process technique for nonlinear regression. Users can specify the kernel function, along with a “noise” regularization parameter for controlling the closeness of fit. They can choose to have the training data normalized or standardized before learning the regression. For point estimates, this method is equivalent to kernel ridge regression (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p229">page 229</a>).</p>
<p id="p0840" class="para_indented"><a id="p467"></a><em>SimpleLogistic</em> builds logistic regression models (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0110">Section 4.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p126">page 126</a>), fitting them using LogitBoost with simple regression functions as base learners and determining how many iterations to perform using cross-validation, which supports automatic attribute selection (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib199">Landwehr et al., 2005</a>). <em>SimpleLogistic</em> generates a degenerate logistic model tree comprising a single node, and supports the options given earlier for <em><em>LMT</em>
</em>.</p>
<div class="boxg" id="b0010">
<p id="p0845" class="noindent"><em>Logistic</em> is an alternative implementation for building and using a multinomial logistic regression model with a ridge estimator to guard against overfitting by penalizing large coefficients, based on work by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib206">le Cessie and van Houwelingen (1992)</a>. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0140">Figure 11.27</a> shows its output on the iris data. The coefficients of the regression functions are shown in tabular form, one for each class value except for the last class. Given <em>m</em> input attributes and <em>k</em> classes, the probability predicted for class <em>j</em> (with the exception of the last class) is given by</p>
<p class="figure" id="e0020"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110si3.jpg" alt="image" width="465" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110si3.jpg"></p>
<p>where <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110if011-001-9780123748560.jpg" alt="image" width="22" height="26" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110if011-001-9780123748560.jpg"> is the intercept term for class <em>j</em>. The probability of the last class <em>k</em> is given by</p>
<p class="figure" id="e0025"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110si4.jpg" alt="image" width="204" height="40" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110si4.jpg"></p>
<p></p><a id="p468"></a><p id="f0140" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-027-9780123748560.jpg" alt="image" width="514" height="806" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-027-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.27</span> Output of <em>Logistic</em> on the iris data.</p>
</div>
<p></p>
<p id="p0850" class="para_indented">Beneath the table of regression coefficients is a second table giving an estimate of the odds ratio for each input attribute and class value. For a given attribute, this value gives an indication of its influence on the class when the values of the other attributes are held fixed.</p>
<p id="p0855" class="para_indented"><em>RBFNetwork</em> implements a Gaussian radial basis function network (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p239">page 239</a>), deriving the centers and widths of hidden units using <em>k</em>-means and <a id="p469"></a>combining the outputs obtained from the hidden layer using logistic regression if the class is nominal and linear regression if it is numeric. The activations of the basis functions are normalized to sum to 1 before they are fed into the linear models. You can specify <em>k</em>, the number of clusters; the maximum number of logistic regression iterations for nominal-class problems; the minimum standard deviation for the clusters; and the ridge value for regression. If the class is nominal, <em>k</em>-means is applied separately to each class to derive <em>k</em> clusters for each class.</p>
<p id="p0860" class="para_indented"><em>LibSVM</em> and <em>LibLINEAR</em> are both wrapper classifiers that allow third-party implementations of support vector machines and logistic regression to be used in Weka. To use them, the jar file for the library in question must be in the class path for the Java virtual machine. The former gives access to the LIBSVM library of support vector classification and regression algorithms (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib55">Chang and Lin, 2001</a>), which provides several types of support vector machines for multiclass classification, regression, and one-class problems, and gives a choice of linear, polynomial, radial-basis, and sigmoid kernels. The latter gives access to the LIBLINEAR library (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib95">Fan et al., 2008</a>), which includes fast implementations of linear support vector machines for classification and logistic regression.</p>
</div>
<div id="s0200">
<h3 id="st0200">Neural Networks</h3>
<p id="p0865" class="noindent"><em>MultilayerPerceptron</em> is a neural network that trains using backpropagation (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p235">page 235</a>). Although listed under <em>functions</em> in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a>, it differs from the other schemes because it has its own user interface. If you load up the numeric version of the weather data, invoke <em>MultilayerPerceptron</em>, set <em>GUI</em> to <em>True</em> in its object editor, and run the network by clicking <em>Start</em> on the <em>Classify</em> panel, the diagram in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28</a> appears in a separate window. This network has three layers: an input layer on the left with one rectangular box for each attribute (colored green); a hidden layer next to it (red) to which all the input nodes are connected; and an output layer at the right (orange). The labels at the far right show the classes that the output nodes represent. Output nodes for numeric classes are automatically converted to unthresholded linear units.</p><a id="p470"></a><p id="f0145" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-028ab-9780123748560.jpg" alt="image" width="440" height="704" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-028ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.28</span> Using Weka’s neural-network graphical user interface: (a) beginning the process of editing the network to add a second hidden layer and (b) the finished network with two hidden layers.</p>
<p id="p0870" class="para_indented">Before clicking <em>Start</em> to run the network, you can alter its structure by adding nodes and connections. Nodes can be selected or deselected. All six nodes in the hidden and output layers in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(a)</a> are deselected, indicated by the gray color of their center. To select a node, simply click on it. This changes the color of its center from gray to bright yellow. To deselect a node, right-click in an empty space. To add a node, ensure that none is selected and left-click anywhere in the panel; the new node will be selected automatically. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(a)</a>, new node has been added at the lower center. To connect two nodes, select the start node and then click on the end one. If several start nodes are selected, they are all connected to the end node. If you click in empty space instead, a new node is created as the end node. Notice that connections are directional (although the directions are not shown). The start nodes remain selected; thus, you can add an entire hidden layer with just a few clicks, as shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(b)</a>. To remove a node, ensure that no nodes are <a id="p471"></a>selected and right-click it; this also removes all connections to it. To remove a single connection, select one node and right-click the node at the other end.</p>
<p id="p0875" class="para_indented">As well as configuring the structure of the network, you can control the learning rate, the momentum (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p238">page 238</a>), and the number of passes that will be taken through the data, called <em>epochs</em>. The network begins to train when you click <em>Start</em>, and a running indication of the epoch and the error for that epoch is shown at the lower left of the panels in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28</a>. Note that the error is based on a network that changes as the value is computed. For numeric classes the error value depends on whether the class is normalized. The network stops when the specified number of epochs is reached, at which point you can accept the result or increase the desired number of epochs and press <em>Start</em> again to continue training.</p>
<p id="p0880" class="para_indented"><em>MultilayerPerceptron</em> need not be run through the graphical interface. Several parameters can be set from the object editor to control its operation. If you are using the graphical interface, they govern the initial network structure, which you can override interactively. With <em>autoBuild</em> set, hidden layers are added and connected up. The default is to have the one hidden layer shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(a)</a>; however, without <em>autoBuild</em> this would not appear and there would be no connections. The <em>hiddenLayers</em> parameter defines what hidden layers are present and how many nodes each one contains. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(a)</a> is generated by a value of <em>4</em> (one hidden layer with four nodes), and although <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(b)</a> was created by adding nodes interactively, it could have been generated by setting <em>hiddenLayers</em> to <em>4,5</em> (one hidden layer with four nodes and another with five). The value is a comma-separated list of integers; <em>0</em> gives no hidden layers. Furthermore, there are predefined values that can be used instead of integers: <em>i</em> is the number of attributes, <em>o</em> the number of class values, <em>a</em> the average of the two, and <em>t</em> their sum. The default, <em>a</em>, was used to generate <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0145">Figure 11.28(a)</a>.</p>
<p id="p0885" class="para_indented">The parameters <em>learningRate</em> and <em>momentum</em> set values for these variables, which can be overridden in the graphical interface. A <em>decay</em> parameter causes the learning rate to decrease with time: It divides the starting value by the epoch number to obtain the current rate. This sometimes improves performance and may stop the network from diverging. The <em>reset</em> parameter automatically resets the network with a lower learning rate and begins training again if it is diverging from the answer (this option is only available if the graphical interface is <em>not</em> used).</p>
<p id="p0890" class="para_indented">The <em>trainingTime</em> parameter sets the number of training epochs. Alternatively, a percentage of the data can be set aside for validation (using <em>validationSetSize</em>): Then training continues until performance on the validation set starts to deteriorate consistently, or until the specified number of epochs is reached. If the percentage is set to <em>0</em>, no validation set is used. The <em>validationThreshold</em> parameter determines how many consecutive times the validation set error can deteriorate before training is stopped.</p>
<p id="p0895" class="para_indented">The <em>NominalToBinaryFilter</em> filter is specified in the <em>MultilayerPerceptron</em> object editor by default; turning it off may improve performance on data in which the nominal attributes are actually ordinal. The attributes can be normalized <a id="p472"></a>(with <em>normalizeAttributes</em>); in addition, a numeric class can be normalized (with <em>normalizeNumericClass</em>). Both may improve performance; these options are turned on by default.</p>
</div>
<div id="s0205">
<h3 id="st0205">Lazy Classifiers</h3>
<p id="p0900" class="noindent">Lazy learners store the training instances and do no real work until classification time. <em>IB1</em> is a basic instance-based learner (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p131">page 131</a>) that finds the training instance closest in Euclidean distance to the given test instance and predicts the same class as this training instance. If several instances qualify as the closest, the first one found is used.</p>
<p id="p0905" class="para_indented"><em>IBk</em> is a <em>k</em>-nearest-neighbor classifier. A variety of different search algorithms can be used to speed up the task of finding the nearest neighbors. A linear search is the default, but other options include <em>k</em>D-trees, ball trees, and so-called “cover trees” (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib23">Beygelzimer et al., 2006</a>). The distance function used is a parameter of the search method. The default is the same as for <em>IB1</em>—that is, the Euclidean distance; other options include Chebyshev, Manhattan, and Minkowski distances. The number of nearest neighbors (default <em>k</em> = 1) can be specified explicitly in the object editor or determined automatically using leave-one-out cross-validation, subject to an upper limit given by the specified value. Predictions from more than one neighbor can be weighted according to their distance from the test instance, and two different formulas are implemented for converting the distance into a weight. The number of training instances kept by the classifier can be restricted by setting the window size option. As new training instances are added, the oldest ones are removed to maintain the number of training instances at this size.</p>
<p id="p0910" class="para_indented"><em>KStar</em> is a nearest-neighbor method with a generalized distance function based on transformations (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0180">Section 6.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p248">page 248</a>).</p>
<p id="p0915" class="para_indented"><em>LBR</em> (for <em>Lazy Bayesian Rules</em>) is a Bayesian classifier that defers all processing to classification time. For each test instance it selects a set of attributes for which the independence assumption should not be made; the others are treated as independent of one another given the class and the selected set of attributes. It works well, but is computationally quite expensive (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib338">Zheng and Webb, 2000</a>). Attributes need to be discretized before applying this classifier.</p>
<p id="p0920" class="para_indented"><em>LWL</em> is a general algorithm for locally weighted learning. It assigns weights using an instance-based method and builds a classifier from the weighted instances. The classifier is selected in <em>LWL</em>’s object editor: A good choice is Naïve Bayes for classification problems and linear regression for regression problems (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0220">Section 6.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p258">page 258</a>). You can set the number of neighbors used, which determines the kernel bandwidth, and the kernel shape to use for weighting—linear, inverse, or Gaussian. Attribute normalization is turned on by default.</p>
</div>
<div id="s0210">
<h3 id="st0210">Multi-Instance Classifiers</h3>
<p id="p0925" class="noindent">Classifiers in the MI category handle multi-instance data. <em>MIDD</em>, <em>MIEMDD</em>, and <em>MDD</em> are all variants of the diverse-density algorithm described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a><a id="p473"></a>(<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#p301">page 301</a>). <em>MIDD</em> is the standard version; it maximizes the bag-level likelihood using the noisy-or model. The data can be normalized or standardized first. <em>MIEMDD</em> combines the diverse-density algorithm with an EM-style iterative approach. As well as the option to normalize or standardize the data, the user can specify a random seed to initialize the iterative process. <em>MIDD</em> and <em>MIEMDD</em> operate on the standard multi-instance assumption that a bag is positive if and only if it contains at least one positive instance. <em>MDD</em>, on the other hand, makes the “collective” assumption that all individual instances in a bag contribute equally and independently to its class label.</p>
<p id="p0930" class="para_indented"><em>MILR</em> is an adaptation of standard single-instance logistic regression to the multi-instance setting (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#p299">page 299</a>). It has one mode of operation based on the standard multi-instance assumption and two based on the collective assumption. As in single-instance logistic regression, a ridge parameter can be used to guard against overfitting.</p>
<p id="p0935" class="para_indented"><em>MISMO</em> and <em>MISVM</em> both upgrade support vector machines to the multi-instance setting (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a>). The former uses the SMO algorithm in conjunction with a kernel function designed for multi-instance data. The user can select between a multi-instance polynomial or radial-basis function kernel. Alternatively, the bag-level data can be summarized by the maximum and minimum attribute values—in which case the multi-instance kernels degenerate to their single-instance equivalents and the standard SMO options apply. <em>MISVM</em> implements an alternative support vector machine method using a single-instance classifier in the iterative fashion described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#p300">page 300</a>).</p>
<p id="p0940" class="para_indented">Three multi-instance classifiers use distance-based approaches. Given a target bag to be classified, <em>CitationKNN</em> (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a>) considers not only its nearest neighbors—the “references”—but also those bags in the training set for which the target bag is the nearest one—the “citers.” There is an option for setting how many of each to use, and also an option for setting the rank of the Hausdorff distance—that is, use the <em>n</em>th largest distance instead of the largest. <em>MINND</em> represents each bag by a normal distribution with a diagonal covariance matrix and finds nearest neighbors using the Kullback-Leibler distance. <em>MIOptimalBall</em> classifies unknown bags based on the distance of their instances to a reference point (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0380">Section 6.10</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#p300">page 300</a>). Because of its simplicity, this is a good base classifier for use with a boosting algorithm such as <em>AdaBoostM1</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0220">Section 11.5</a>).</p>
<p id="p0945" class="para_indented"><em>SimpleMI</em> and <em>MIWrapper</em> apply standard single-instance learners to multi-instance data using the methods of aggregating the input and output, respectively (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0175">Section 4.9</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p142">page 142</a>). The former can aggregate bag-level data using the coordinate-wise geometric average, arithmetic average, or minimum and maximum values. The latter allows users to specify whether to aggregate the instance-level probability estimates from the single-instance model using the average, geometric average, or maximum. Users can also specify whether each instance in a bag receives a weight of 1, or whether weights are normalized to give each bag the same weight. <em>MIBoost</em> is a boosting algorithm inspired by AdaBoost that builds a series of weak classifiers using a single-instance learner (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib325">Xu and Frank, 2004</a>). Probability estimates for individual instances are combined using the geometric mean to form bag-level <a id="p474"></a>probability estimates. The single-instance base learner and the number of iterations to perform are configurable options.</p>
</div>
<div id="s0215">
<h3 id="st0215">Miscellaneous Classifiers</h3>
<p id="p0950" class="noindent">The “Misc.” category in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a> includes three classifiers, two of which were mentioned at the end of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p138">page 138</a>). <em>HyperPipes</em>, for discrete classification problems, records the range of values observed in the training data for each attribute and category and works out which ranges contain the attribute values of a test instance, choosing the category with the largest number of correct ranges. <em>VFI</em> (<em>voting feature intervals</em>) constructs intervals around each class by discretizing numeric attributes and using point intervals for nominal ones, records class counts for each interval on each attribute, and classifies test instances by voting (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib73">Demiroz and Guvenir, 1997</a>). A simple attribute weighting scheme assigns higher weight to more confident intervals, where confidence is a function of entropy. <em>VFI</em> is faster than Naïve Bayes but slower than <em>HyperPipes</em>. Neither method can handle missing values. <em>SerializedClassifier</em> loads a model that has been serialized to a file and uses it for prediction. Providing a new training dataset has no effect because it encapsulates a static model. Similarly, performing cross-validation using <em>SerializedClassifier</em> makes little sense.</p>
</div>
</div>
<div id="s0220">
<h2 id="st0220">11.5 Metalearning algorithms</h2>
<p id="p0955" class="noindent">Metalearning algorithms, listed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0035">Table 11.6</a>, take classifiers and turn them into more powerful learners. One parameter specifies the base classifier; others specify the number of iterations for iterative schemes such as bagging and boosting and an initial seed for the random-number generator. We already met <em>FilteredClassifier</em> in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0100">Section 11.3</a>: It runs a classifier on data that has been passed through a filter, which is a parameter. The filter’s own parameters are based exclusively on the training data, which is the appropriate way to apply a supervised filter to test data.</p><a id="p475"></a><p class="table_caption"><span class="tab_num">Table 11.6. </span> Metalearning Algorithms in Weka</p>
<table id="t0035" frame="box" rules="all">
<thead>
<tr><td class="tch"></td><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb">Meta</td>
<td class="tb"><em>AdaBoostM1</em></td>
<td class="tb">Boost using the AdaBoostM1 method</td></tr>
<tr><td class="tb"></td><td class="tb"><em>AdditiveRegression</em></td>
<td class="tb">Enhance the performance of a regression method by iteratively fitting the residuals</td></tr>
<tr><td class="tb"></td><td class="tb"><em>AttributeSelectedClassifier</em></td>
<td class="tb">Reduce dimensionality of data by attribute selection</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Bagging</em></td>
<td class="tb">Bag a classifier; works for regression too</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ClassificationViaClustering</em></td>
<td class="tb">Perform classification using a clusterer</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ClassificationViaRegression</em></td>
<td class="tb">Perform classification using a regression method</td></tr>
<tr><td class="tb"></td><td class="tb"><em>CostSensitiveClassifier</em></td>
<td class="tb">Make a base classifier cost sensitive</td></tr>
<tr><td class="tb"></td><td class="tb"><em>CVParameterSelection</em></td>
<td class="tb">Perform parameter selection by cross-validation</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Dagging</em></td>
<td class="tb">Similar to <em>Bagging</em>, but using disjoint training sets</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Decorate</em></td>
<td class="tb">Build ensembles of classifiers by using specially constructed artificial training examples</td></tr>
<tr><td class="tb"></td><td class="tb"><em>END</em></td>
<td class="tb">Ensembles of nested dichotomies</td></tr>
<tr><td class="tb"></td><td class="tb"><em>FilteredClassifier</em></td>
<td class="tb">Run a classifier on filtered data</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Grading</em></td>
<td class="tb">Metalearners of which the inputs are base-level predictions that have been marked as correct or incorrect</td></tr>
<tr><td class="tb"></td><td class="tb"><em>GridSearch</em></td>
<td class="tb">Perform a grid search over a pair of classifier options</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LogitBoost</em></td>
<td class="tb">Perform additive logistic regression</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MetaCost</em></td>
<td class="tb">Make a classifier cost sensitive</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MultiBoostAB</em></td>
<td class="tb">Combine boosting and bagging using the MultiBoosting method</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MultiClassClassifier</em></td>
<td class="tb">Use two-class classifier for multiclass datasets</td></tr>
<tr><td class="tb"></td><td class="tb"><em>MultiScheme</em></td>
<td class="tb">Use cross-validation to select a classifier from several candidates</td></tr>
<tr><td class="tb"></td><td class="tb"><em>OrdinalClassClassifier</em></td>
<td class="tb">Apply standard classification algorithms to problems with an ordinal class value</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RacedIncrementalLogitBoost</em></td>
<td class="tb">Batch-based incremental learning by racing logit-boosted committees</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RandomCommittee</em></td>
<td class="tb">Build an ensemble of randomizable base classifiers</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RandomSubSpace</em></td>
<td class="tb">Build an ensemble of base classifiers, each trained using a different randomly chosen attribute subset</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RegressionByDiscretization</em></td>
<td class="tb">Discretize the class attribute and employ a classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RotationForest</em></td>
<td class="tb">Build an ensemble of base classifiers, each trained using a random subspace that has been transformed using principal components analysis</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Stacking</em></td>
<td class="tb">Combine several classifiers using the stacking method</td></tr>
<tr><td class="tb"></td><td class="tb"><em>StackingC</em></td>
<td class="tb">More efficient version of stacking</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ThresholdSelector</em></td>
<td class="tb">Optimize the <em>F</em>-measure for a probabilistic classifier</td></tr>
<tr><td class="tb"></td><td class="tb"><em>Vote</em></td>
<td class="tb">Combine classifiers using average of probability estimates or numeric predictions</td></tr>
</tbody>
</table>
<div id="s0225">
<h3 id="st0225">Bagging and Randomization</h3>
<p id="p0960" class="noindent"><em>Bagging</em> bags a classifier to reduce variance (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0015">Section 8.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p352">page 352</a>). This implementation works for both classification and regression, depending on the base learner. In the case of classification, predictions are generated by averaging probability estimates, not by voting. One parameter is the size of the bags as a percentage of the training set. Another is whether to calculate the out-of-bag error, which gives the average error of the ensemble members (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib40">Breiman, 2001</a>).</p>
<p id="p0965" class="para_indented"><em>Dagging</em> is similar to <em>Bagging</em>, but as input to each member of the ensemble it uses disjoint stratified folds of the training data instead of bootstrap samples (Ting and Witten, 1997b). This can be useful when building an ensemble of classifiers that have poor time complexity in terms of the number of instances. The number of folds <a id="p476"></a>is a parameter that controls not only the size of the training set presented to each base classifier but also the number of classifiers in the ensemble.</p>
<p id="p0970" class="para_indented"><em>RandomCommittee</em> is even simpler: It builds an ensemble of base classifiers and averages their predictions. Each one is based on the same data but uses a different random-number seed (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0030">Section 8.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p356">page 356</a>). This only makes sense if the base classifier is randomized; otherwise, all classifiers would be the same.</p>
<p id="p0975" class="para_indented"><em>RandomSubSpace</em> builds an ensemble of classifiers, each trained using a randomly selected subset of the input attributes (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0030">Section 8.3</a>). Aside from the number of iterations and random seed to use, it provides a parameter to control the size of the attribute subsets. <em>RotationForest</em> implements the rotation forest ensemble learner described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0030">Section 8.3</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p357">page 357</a>). Although the classic paper on rotation forests (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib268">Rodriguez et al., 2006</a>) uses random subspaces and principal components to create an ensemble of decision trees, Weka’s implementation allows the base classifier to be any classification or regression scheme. The principal components transformation is performed by Weka’s filter of the same name. <em>RotationForest</em> can be configured to use other projections such as random projections or partial least squares. Other parameters control the size of the subspaces and the number of instances that are input to the projection filter.</p>
</div>
<div id="s0230">
<h3 id="st0230">Boosting</h3>
<p id="p0980" class="noindent"><em>AdaBoostM1</em> implements the algorithm described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0045">Section 8.4</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#f0015">Figure 8.2</a>). It can be accelerated by specifying a threshold for weight pruning. <em>AdaBoostM1</em> resamples if the base classifier cannot handle weighted instances (you can force resampling anyway). <em>MultiBoostAB</em> combines boosting with a variant of bagging to prevent overfitting (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib310">Webb, 2000</a>).</p>
<p id="p0985" class="para_indented">Whereas boosting only applies to nominal classes, <em>AdditiveRegression</em> enhances the performance of a regression learner (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0060">Section 8.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p362">page 362</a>). There are two parameters: shrinkage, which governs the learning rate, and the maximum number of models to generate. If the latter is infinite, work continues until the error stops decreasing.</p>
<p id="p0990" class="para_indented"><em>Decorate</em> builds ensembles of diverse classifiers by using specially constructed artificial training examples (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib224">Melville and Mooney, 2005</a>).<sup><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#fn0015" id="cc000110fn0015" class="totri-footnote">2</a></sup> One parameter is the number of artificial examples to use as a proportion of the training data. Another is the desired number of classifiers in the ensemble, although execution may terminate prematurely because the number of iterations can also be capped. Larger ensembles usually produce more accurate models but have greater training time and model complexity.</p>
<p id="p0995" class="para_indented"><em>LogitBoost</em> performs additive logistic regression (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0060">Section 8.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p364">page 364</a>). Like <em>AdaBoostM1</em>, it can be accelerated by specifying a threshold for weight pruning. The <a id="p477"></a>appropriate number of iterations can be determined using internal cross-validation; there is a shrinkage parameter that can be tuned to prevent overfitting; also, you can choose resampling instead of reweighting. <em>RacedIncrementalLogitBoost</em> learns by racing LogitBoosted committees, and operates incrementally by processing the data in batches, making it useful for large datasets (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib110">Frank et al., 2002</a>). Each committee member is learned from a different batch. The batch size starts at a given minimum and repeatedly doubles until it reaches a preset maximum. Resampling is used if the base classifier cannot handle weighted instances (you can force resampling anyway). Log-likelihood pruning can be used within each committee: This discards new committee members if they decrease the log-likelihood based on the validation data. It is up to you to determine how many instances to hold out for validation. The validation data is also used to determine which committee to retain when training terminates.</p>
</div>
<div id="s0235">
<h3 id="st0235">Combining Classifiers</h3>
<p id="p1000" class="noindent"><em>Vote</em> provides a baseline method for combining classifiers. The default scheme is to average their probability estimates or numeric predictions, for classification and regression, respectively. Other combination schemes are available—for example, using majority voting for classification. <em>MultiScheme</em> selects the best classifier from a set of candidates using cross-validation of percentage accuracy or mean-squared error for classification and regression, respectively. The number of folds is a parameter. Performance on training data can be used instead.</p>
<p id="p1005" class="para_indented"><em>Stacking</em> combines classifiers using stacking (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0090">Section 8.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p369">page 369</a>) for both classification and regression problems. You specify the base classifiers, the metalearner, and the number of cross-validation folds. <em>StackingC</em> implements a more efficient variant for which the metalearner must be a numeric prediction scheme (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib280">Seewald, 2002</a>). In <em>Grading</em>, the inputs to the metalearner are base-level predictions that have been marked (i.e., “graded”) as correct or incorrect. For each base classifier, a metalearner is learned that predicts when the base classifier will err. Just as stacking may be viewed as a generalization of voting, grading generalizes selection by cross-validation (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib281">Seewald and Fürnkranz, 2001</a>).</p>
</div>
<div id="s0240">
<h3 id="st0240">Cost-Sensitive Learning</h3>
<p id="p1010" class="noindent">There are two metalearners for cost-sensitive learning (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p167">page 167</a>). The cost matrix can be supplied as a parameter or loaded from a file in the directory set by the <em>onDemandDirectory</em> property, named by the relation name and with the extension <em>cost</em>. <em>CostSensitiveClassifier</em> either reweights training instances according to the total cost assigned to each class (cost-sensitive learning, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p167">page 167</a>) or predicts the class with the least expected misclassification cost rather than the most likely one (cost-sensitive classification, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p166">page 166</a>). <em>MetaCost</em> generates a single cost-sensitive classifier from the base learner (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#s0015">Section 8.2</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#p356">page 356</a>). This implementation uses <a id="p478"></a>all bagging iterations when reclassifying training data (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib83">Domingos, 1999</a>, reports a marginal improvement when using only those iterations containing each training instance to reclassify it). You can specify each bag’s size and the number of bagging iterations.</p>
</div>
<div id="s0245">
<h3 id="st0245">Optimizing Performance</h3>
<p id="p1015" class="noindent">Four metalearners use the wrapper technique to optimize the base classifier’s performance. <em>AttributeSelectedClassifier</em> selects attributes, reducing the data’s dimensionality before passing it to the classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p308">page 308</a>). You can choose the attribute evaluator and the search method as in the <em>Select attributes</em> panel described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0050">Section 11.2</a>. <em>CVParameterSelection</em> optimizes performance by using cross-validation to select parameters. For each parameter you give a string containing its lower and upper bounds and the desired number of increments. For example, to vary parameter –P from 1 to 10 in increments of 1, use</p>
<p class="figure" id="e0030"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110si5.jpg" alt="image" width="100" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110si5.jpg"></p>
<p>The number of cross-validation folds can be specified.</p>
<p id="p1020" class="para_indented"><em>GridSearch</em> is similar to <em>CVParameterSelection</em>, but is limited to optimizing two parameters by searching a two-dimensional grid. However, it offers the ability to optimize parameters of a classifier, a preprocessing filter, or one parameter from each. The user can choose to optimize accuracy, root mean-squared error, root relative-squared error, mean absolute error, relative absolute error, correlation coefficient, kappa or a linear combination of correlation coefficient, root relative-squared error, and relative absolute error (whereas <em>CVParameterSelection</em> can only optimize accuracy or root mean-squared error for classification and regression, respectively).</p>
<p id="p1025" class="para_indented">Like <em>CVParameterSelection</em>, <em>GridSearch</em> allows users to specify the lower and upper bounds for each parameter, and the desired number of increments. It also allows the value of each target parameter to be set using a mathematical expression involving the same operators and functions as for the <em>MathExpressionFilter</em>, and parameters that include arbitrary constants, the value of one of the bounds, the step size, or the current iteration number. It performs a quick initial evaluation of the grid using twofold cross-validation on the training data. Following this, the best grid point—according to the chosen evaluation metric—is explored more closely using a hill-climbing search that considers adjacent parameter pairs in the grid. For this phase, tenfold cross-validation is used. If one of the adjacent points proves to be superior, it becomes the new center and another tenfold cross-validation is performed. This process continues until no better point is found, or the best point is on the boundary of the grid. In the latter case the user has the option of allowing <em>GridSearch</em> to automatically extend the grid and continue the search. The maximum number of times the grid can be extended in this fashion is another user-definable option.</p>
<p id="p1030" class="para_indented"><a id="p479"></a>The fourth metalearner, <em>ThresholdSelector</em>, optimizes one of a number of different evaluation metrics, including <em>F</em>-measure, precision, recall, accuracy, and true positive rate (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#s0065">Section 5.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#p163">page 163</a>), by selecting a probability threshold on the classifier’s output. Performance can measured on the training data, on a holdout set, or by cross-validation. The probabilities returned by the base learner can be rescaled into the full range [0,1], which is useful if the scheme’s probabilities are restricted to a narrow subrange. The metalearner can be applied to multiclass problems by specifying the class value for which the optimization is performed as</p>
<div class="none">
<p class="hang" id="o0070">1 <a id="p1035"></a>The first class value.</p>
<p class="hang" id="o0075">2 <a id="p1040"></a>The second class value.</p>
<p class="hang" id="o0080">3 <a id="p1045"></a>Whichever value is least frequent.</p>
<p class="hang" id="o0085">4 <a id="p1050"></a>Whichever value is most frequent.</p>
<p class="hang" id="o0090">5 <a id="p1055"></a>The first class named <em>yes</em>, <em>pos(itive)</em>, or <em>1</em>.</p>
</div>
</div>
<div id="s0250">
<h3 id="st0250">Retargeting Classifiers for Different Tasks</h3>
<p id="p1060" class="noindent">Six metalearners adapt learners designed for one kind of task to another. <em>ClassificationViaRegression</em> performs classification using a regression method by binarizing the class and building a regression model for each value. <em>RegressionByDiscretization</em> is a regression scheme that discretizes the class attribute into a specified number of bins using equal-width discretization and then employs a classifier. The predictions are the weighted average of the mean class value for each discretized interval, with weights based on the predicted probabilities for the intervals. <em>ClassificationViaClustering</em> performs classification using a clustering algorithm; the majority class in each cluster is used for making predictions. <em>OrdinalClassClassifier</em> applies standard classification algorithms to ordinal-class problems (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib108">Frank and Hall, 2001</a>).</p>
<p id="p1065" class="para_indented"><em>MultiClassClassifier</em> handles multiclass problems with two-class classifiers using any of these methods:</p>
<div class="none">
<p class="hang" id="o0095">1 <a id="p1070"></a>One versus all the rest.</p>
<p class="hang" id="o0100">2 <a id="p1075"></a>Pairwise classification using voting to predict.</p>
<p class="hang" id="o0105">3 <a id="p1080"></a>Exhaustive error-correcting codes (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0135">Section 7.6</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p341">page 341</a>).</p>
<p class="hang" id="o0110">4 <a id="p1085"></a>Randomly selected error-correcting codes.</p>
</div>
<p id="p1090" class="para_indented">Random code vectors are known to have good error-correcting properties: A parameter specifies the length of the code vector (in bits). For pairwise classification, pairwise coupling of probability estimates can be turned on. <em>END</em> implements the ensembles of nested dichotomies method (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0135">Section 7.6</a>) for handling multiclass problems with two-class classifiers. Several types of nested dichotomies can be used, including two that are balanced with respect to the data or to classes in the data.</p>
</div>
</div>
<div id="s0255">
<h2 id="st0255">11.6 <a id="p480"></a>Clustering algorithms</h2>
<p id="p1095" class="noindent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0040">Table 11.7</a> lists Weka’s clustering algorithms. <em>Cobweb</em>, <em>EM</em>, <em>SimpleKMeans</em>, and <em>HierarchicalClusterer</em> were described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p272">page 272</a>). For the <em>EM</em> implementation you can specify how many clusters to generate, or the algorithm can decide using cross-validation—in which case, the number of folds is fixed at 10 (unless there are fewer than 10 training instances). You can specify the maximum number of iterations and set the minimum allowable standard deviation for the normal-density calculation. Clusters are Gaussian distributions with diagonal covariance matrices. <em>SimpleKMeans</em> clusters data using <em>k</em>-means; the number of clusters is specified by a parameter. The user can choose between the Euclidean and Manhattan distance metrics. In the latter case the algorithm is actually <em>k</em>-medians instead of <em>k</em>-means, and the centroids are based on medians rather than means in order to minimize the within-cluster distance function.</p>
<p class="table_caption"><span class="tab_num">Table 11.7. </span> Clustering Algorithms</p>
<table id="t0040" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>CLOPE</em></td>
<td class="tb">Fast clustering of transactional data</td></tr>
<tr><td class="tb"><em>Cobweb</em></td>
<td class="tb">Implements the Cobweb and Classit clustering algorithms</td></tr>
<tr><td class="tb"><em>DBScan</em></td>
<td class="tb">Nearest-neighbor-based clustering that automatically determines the number of clusters</td></tr>
<tr><td class="tb"><em>EM</em></td>
<td class="tb">Cluster using expectation maximization</td></tr>
<tr><td class="tb"><em>FarthestFirst</em></td>
<td class="tb">Cluster using the farthest first traversal algorithm</td></tr>
<tr><td class="tb"><em>FilteredClusterer</em></td>
<td class="tb">Runs a clusterer on filtered data</td></tr>
<tr><td class="tb"><em>HierarchicalClusterer</em></td>
<td class="tb">Agglomerative hierarchical clustering</td></tr>
<tr><td class="tb"><em>MakeDensityBasedCluster</em></td>
<td class="tb">Wrap a clusterer to make it return distribution and density</td></tr>
<tr><td class="tb"><em>OPTICS</em></td>
<td class="tb">Extension of DBScan to hierarchical clustering</td></tr>
<tr><td class="tb"><em>sIB</em></td>
<td class="tb">Cluster using the sequential information bottleneck algorithm</td></tr>
<tr><td class="tb"><em>SimpleKMeans</em></td>
<td class="tb">Cluster using the <em>k</em>-means method</td></tr>
<tr><td class="tb"><em>XMeans</em></td>
<td class="tb">Extension of <em>k</em>-means</td></tr>
</tbody>
</table>
<p id="p1100" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0150">Figure 11.29</a> shows the output of <em>SimpleKMeans</em> for the weather data, with default options: two clusters and Euclidean distance. The result of clustering is shown as a table with rows that are attribute names and columns that correspond to the cluster centroids; an additional cluster at the beginning shows the entire dataset. The number of instances in each cluster appears in parentheses at the top of its column. Each table entry is either the mean (numeric attribute) or mode (nominal <a id="p481"></a>attribute) of the corresponding attribute for the cluster in that column; users can choose to show standard deviations (numeric attributes) and frequency counts (nominal attributes) as well. The bottom of the output shows the result of applying the learned clustering model. In this case, it assigned each training instance to one of the clusters, showing the same result as the parenthetical numbers at the top of each column. An alternative is to use a separate test set or a percentage split of the training data, in which case the figures would be different.</p>
<p id="f0150" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-029-9780123748560.jpg" alt="image" width="514" height="506" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-029-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.29</span> Output of <em>SimpleKMeans</em> on the weather data.</p>
<p id="p1105" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0155">Figure 11.30</a> shows the output of <em>EM</em> for the same data, with the number of clusters set to two. Although there is no notion of the number of instances in each cluster, the columns are again headed by its prior probability in parentheses. The table entries show the parameters of normal distributions for numeric attributes or frequency counts for the values of nominal attributes—and here the fractional count <a id="p483"></a>values reveal the “soft” nature of the clusters produced by the EM algorithm, in that any instance can be split between several clusters. At the bottom, the log-likelihood of the model (again with respect to the training data) is shown, as well as the number of instances assigned to each cluster when the learned model is applied to the data as a classifier.</p><a id="p482"></a><p id="f0155" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-030-9780123748560.jpg" alt="image" width="514" height="647" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-030-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.30</span> Output of <em>EM</em> on the weather data.</p>
<p id="p1110" class="para_indented"><em>XMeans</em> implements an extended version of <em>k</em>-means by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib234">Moore and Pelleg (2000)</a>. It uses a Bayesian information criterion for selecting the number of clusters (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#p292">page 292</a>), and can use <em>k</em>D-trees for speed (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p132">page 132</a>). The user can specify the distance function to use, the minimum and maximum number of clusters to consider, and the maximum number of iterations to perform.</p>
<p id="p1115" class="para_indented"><em>Cobweb</em> implements both the Cobweb algorithm for nominal attributes and the Classit algorithm for numeric attributes. The ordering and priority of the merging and splitting operators differ between the original Cobweb and Classit papers (where they are somewhat ambiguous). This implementation always compares four different ways of treating a new instance and chooses the best: adding it to the best host, making it into a new leaf, merging the two best hosts and adding it to the merged node, and splitting the best host and adding it to one of the splits. <em>Acuity</em> and <em>cutoff</em> are parameters.</p>
<p id="p1120" class="para_indented"><em>HierarchicalClusterer</em> implements agglomerative (bottom-up) generation of hierarchical clusters (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p273">page 273</a>). Several different link types, which are ways of measuring the distance between clusters, are available as options.</p>
<p id="p1125" class="para_indented"><em>FilteredClusterer</em> allows the data to be passed through a filter before it reaches a clusterer. Both the filter and the base clusterer are options that the user can configure.</p>
<p id="p1130" class="para_indented"><em>FarthestFirst</em> implements the farthest-first traversal algorithm of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib161">Hochbaum and Shmoys (1985)</a>, cited by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib69">Sanjoy Dasgupta (2002)</a>; it is a fast, simple, approximate clusterer modeled on <em>k</em>-means. <em>MakeDensityBasedClusterer</em> is a metaclusterer that wraps a clustering algorithm to make it return a probability distribution and density. To each cluster and attribute it fits a discrete distribution or a symmetric normal distribution (of which the minimum standard deviation is a parameter).</p>
<p id="p1135" class="para_indented"><em>CLOPE</em> implements a fast clustering technique for market basket–type data (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib330">Yang et al., 2002</a>). It uses a cluster-quality heuristic based on histograms—that is, the number of distinct items and the count of each item computed from the item sets in a cluster. A good cluster is one where the number of items shared among the item sets in a cluster is high compared to the number of distinct items. The overall goodness of a set of clusters is the sum of the goodness of the individual clusters. The degree of intracluster similarity (i.e., the preference for common items between item sets in a cluster) can be controlled with a “repulsion” parameter.</p>
<p id="p1140" class="para_indented"><em>DBScan</em> uses the Euclidean distance metric to determine which instances belong together in a cluster (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib94">Ester et al., 1996</a>), but, unlike <em>k</em>-means, it can determine the number of clusters automatically, find arbitrarily shaped clusters, and incorporate a notion of outlier. A cluster is defined as containing at least a minimum number of points, every pair of points of which either lies within a user-specified distance (<em>ε</em>) of each other or is connected by a series of points in the cluster that each lie within <a id="p484"></a>a distance <em>ε</em> of the next point in the chain. Smaller values of <em>ε</em> yield denser clusters because instances must be closer to one another to belong to the same cluster. Depending on the value of <em>ε</em> and the minimum cluster size, it is possible that some instances will not belong to any cluster. These are considered outliers.</p>
<p id="p1145" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0160">Figure 11.31</a> shows the clusters formed by <em>DBScan</em> on the iris data (without the class attribute) using <em>ε</em> = 0.2 and a minimum cluster size of 5. Two clusters have been found—the <em>Iris setosas</em> in one and the <em>Iris viginicas</em> and <em>versicolors</em> in the other. Three instances are deemed outliers (denoted by <em>M</em> in the plot), one <em>setosa</em> (lower left) and two <em>virginicas</em> (upper right). In the two-dimensional space that is being used to visualize the results (sepal width versus petal width), they do indeed appear to lie outside both clusters. If the minimum cluster size were reduced to 2, then the two outlying <em>virginicas</em> would form a third cluster because they lie within <em>ε</em> = 0.2 of each other.</p>
<p id="f0160" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-031-9780123748560.jpg" alt="image" width="440" height="334" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-031-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.31</span> Clusters formed by <em>DBScan</em> on the iris data.</p>
<p id="p1150" class="para_indented"><em>OPTICS</em> is an extension of <em>DBScan</em> to hierarchical clustering (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib10">Ankerst et al., 1999</a>). It imposes an ordering on the instances, which, along with two-dimensional visualization, exposes the hierarchical structure of the clusters. The ordering process places instances that are closest to one another, according to the distance metric, beside one another in the list. Furthermore, it annotates each adjacent pair of instances with the “reachability distance,” which is the minimum value of <em>ε</em> that allows the pair to belong to the same cluster. The clusters become apparent when ordering is plotted against reachability distance. Because instances in a cluster have <a id="p485"></a>low reachability distance to their nearest neighbors, the clusters appear as valleys in the visualization. The deeper the valley, the denser the cluster.</p>
<p id="p1155" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0165">Figure 11.32</a> shows an <em>OPTICS</em> visualization for the iris data, using the same values for <em>ε</em> and minimum cluster size as for <em>DBScan</em>. The two clusters found by <em>DBScan</em> using these parameter settings correspond to the two main valleys between the three high peaks in the plot. Various clusterings can be obtained by setting a threshold on the reachability value—that is, by drawing a horizontal line at a given reachability value through the plot. The valley on either side of a peak intersected by the line is a cluster of its own.</p>
<p id="f0165" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-032-9780123748560.jpg" alt="image" width="513" height="345" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-032-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.32</span><em>OPTICS</em> visualization for the iris data.</p>
<p id="p1160" class="para_indented"><em>sIB</em> is an algorithm designed for document clustering that uses an information-theoretic distance metric (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib285">Slonim et al., 2002</a>). The number of clusters to find and the maximum number of iterations to perform can be specified by the user.</p>
</div>
<div id="s0260">
<h2 id="st0260">11.7 Association-rule learners</h2>
<p id="p1165" class="noindent">Weka has six association-rule learners, listed in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0045">Table 11.8</a>. <em>Apriori</em> implements the Apriori algorithm (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>). It starts with a minimum support of 100% of the data items and decreases this in steps of 5% until there are at least 10 rules with <a id="p486"></a>the required minimum confidence of 0.9 or until the support has reached a lower bound of 10%, whichever occurs first. (These default values can be changed.) There are four alternative metrics for ranking rules: <em>Confidence</em>, which is the proportion of the examples covered by the premise that are also covered by the consequent (called <em>accuracy</em> in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>); <em>Lift</em>, which is determined by dividing the confidence by the support (called <em>coverage</em> in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0085">Section 4.5</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#p116">page 116</a>); <em>Leverage</em>, which is the proportion of additional examples covered by both the premise and the consequent beyond those expected if the premise and consequent were statistically independent; and <em>Conviction</em>, a measure defined by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib42">Brin et al. (1997)</a>. You can also specify a significance level, and rules will be tested for significance at this level. <em>Apriori</em> has an option to limit the rules found to those that contain just the value of a single attribute in the consequence of the rule. Such rules are called “class” association rules—that is, classification rules.</p>
<p class="table_caption"><span class="tab_num">Table 11.8. </span> Association-Rule Learners</p>
<table id="t0045" frame="box" rules="all">
<thead>
<tr><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>Apriori</em></td>
<td class="tb">Find association rules using the Apriori algorithm</td></tr>
<tr><td class="tb"><em>FilteredAssociator</em></td>
<td class="tb">Run an associator on filtered data</td></tr>
<tr><td class="tb"><em>FPGrowth</em></td>
<td class="tb">Association rule mining using frequent pattern trees</td></tr>
<tr><td class="tb"><em>GeneralizedSequentialPatterns</em></td>
<td class="tb">Find large item sets in sequential data</td></tr>
<tr><td class="tb"><em>PredictiveApriori</em></td>
<td class="tb">Find association rules sorted by predictive accuracy</td></tr>
<tr><td class="tb"><em>Tertius</em></td>
<td class="tb">Confirmation-guided discovery of association or classification rules</td></tr>
</tbody>
</table>
<p id="p1170" class="para_indented">In order to process market basket data with <em>Apriori</em>, where we are interested in knowing (from the items present in shoppers’ baskets) which items are purchased together, it is necessary to encode the input ARFF data in a specific way. In particular, since we are not interested in co-occurrence of items not present in shopping baskets, the attributes corresponding to items should be declared as single-valued nominal ones in the ARFF file. Missing values can be used to indicate the absence of an item from a shopping basket.</p>
<p id="p1175" class="para_indented"><em>FPGrowth</em> implements the frequent-pattern tree mining algorithm described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0100">Section 6.3</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#p216">page 216</a>). Being designed for market basket data, <em>Apriori</em>’s special encoding for this type of data is not implemented here. All attributes are expected to be binary nominal ones, and the user can specify which of the two values is to be treated as positive—that is, which one indicates presence in the basket (the default is to use the second value). <em>FPGrowth</em> can operate on either standard or sparse instances. Most of its options are the same as for <em>Apriori</em>. It finds the requested number of rules in the same way—by iteratively decreasing the minimum support. Optionally, the user can have <em>FPGrowth</em> find all the rules that meet the lower bound for the minimum support <a id="p487"></a>and the minimum value set for the ranking metric (confidence, lift, leverage, or conviction).</p>
<p id="p1180" class="para_indented"><em>FilteredAssociator</em> allows the data to be passed through a filter before it reaches an associator. Both the filter and the base associator are options that the user can configure.</p>
<p id="p1185" class="para_indented"><em>GeneralizedSequentialPatterns</em> implements a version of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib288">Srikant and Agrawal’s (1996)</a> GSP algorithm for finding large item sets that occur sequentially over time. The input data must contain a special nominal attribute that groups transactions (instances) together in time. All instances with the same nominal sequence identifier are grouped together, defining a time window from which sequential patterns can be extracted—for example, the identifier might group transactions according to the day on which they occur. The instances within each group are treated as occurring sequentially, in the order in which they appear in the data. Sequential large item sets, consisting of item combinations that occur across transactions sequentially within a group, are found that meet a user-supplied minimum-support threshold. The output can be optionally filtered to show only those sequential patterns that contain specific items the user is interested in.</p>
<p id="p1190" class="para_indented"><em>PredictiveApriori</em> combines confidence and support into a single measure of <em>predictive accuracy</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib275">Scheffer, 2001</a>) and finds the best <em>n</em> association rules in order. Internally, the algorithm successively increases the support threshold because the value of predictive accuracy depends on it. <em>Tertius</em> finds rules according to a confirmation measure (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib102">Flach and Lachiche, 1999</a>), seeking rules with multiple conditions in the consequent, like Apriori—but differing in that these conditions are ORed together, not ANDed. It can be set to find rules that predict a single condition or a predetermined attribute (i.e., classification rules). One parameter determines whether negation is allowed in the antecedent, the consequent, or both; others give the number of rules sought, minimum degree of confirmation, minimum coverage, maximum proportion of counterinstances, and maximum rule size. Missing values can match any value, never match, or be significant and possibly appear in rules.</p>
</div>
<div id="s0265">
<h2 id="st0265">11.8 Attribute selection</h2>
<p id="p1195" class="noindent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0170">Figure 11.33</a> shows that part of Weka’s attribute selection panel where you specify the attribute evaluator and search method; <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0050">Tables 11.9</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0055">11.10</a> list the choices. Attribute selection is normally done by searching the space of attribute subsets, evaluating each one (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p307">page 307</a>). This is achieved by combining 1 of the 6 attribute subset evaluators in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0050">Table 11.9</a> with 1 of the 10 search methods in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0055">Table 11.10</a>. A potentially faster but less accurate approach is to evaluate the attributes individually and sort them, discarding attributes that fall below a chosen cutoff point. This is achieved by selecting one of the 11 single-attribute evaluators in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0050">Table 11.9</a> and using the ranking method in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0055">Table 11.10</a>. The Weka interface allows both possibilities by letting the user choose a selection method from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0050">Table 11.9</a> and a search method from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0055">Table 11.10</a>, producing an error message if you select an inappropriate <a id="p488"></a>combination. The status line refers you to the error log for the message (see the end of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0010">Section 11.1</a>).</p>
<p id="f0170" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000110f011-033-9780123748560.jpg" alt="image" width="440" height="94" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000110f011-033-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 11.33</span> Attribute selection: specifying an evaluator and a search method.</p><a id="p489"></a><p class="table_caption"><span class="tab_num">Table 11.9. </span> Attribute Evaluation Methods for Attribute Selection</p>
<table id="t0050" frame="box" rules="all">
<thead>
<tr><td class="tch"></td><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb">Attribute Subset Evaluator</td>
<td class="tb"><em>CfsSubsetEval</em></td>
<td class="tb">Consider predictive value of each attribute individually, along with the degree of redundancy among them</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ClassifierSubsetEval</em></td>
<td class="tb">Use a classifier to evaluate the attribute set</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ConsistencySubsetEval</em></td>
<td class="tb">Project training set onto attribute set and measure consistency in class values</td></tr>
<tr><td class="tb"></td><td class="tb"><em>CostSensitiveSubsetEval</em></td>
<td class="tb">Makes its base subset evaluator cost sensitive</td></tr>
<tr><td class="tb"></td><td class="tb"><em>FilteredSubsetEval</em></td>
<td class="tb">Apply a subset evaluator to filtered data</td></tr>
<tr><td class="tb"></td><td class="tb"><em>WrapperSubsetEval</em></td>
<td class="tb">Use a classifier plus cross-validation</td></tr>
<tr><td class="tb">Single-Attribute Evaluator</td>
<td class="tb"><em>ChiSquaredAttributeEval</em></td>
<td class="tb">Compute the chi-squared statistic of each attribute with respect to the class</td></tr>
<tr><td class="tb"></td><td class="tb"><em>CostSensitiveAttributeEval</em></td>
<td class="tb">Make its base attribute evaluator cost sensitive</td></tr>
<tr><td class="tb"></td><td class="tb"><em>FilteredAttributeEval</em></td>
<td class="tb">Apply an attribute evaluator to filtered data</td></tr>
<tr><td class="tb"></td><td class="tb"><em>GainRatioAttributeEval</em></td>
<td class="tb">Evaluate attribute based on gain ratio</td></tr>
<tr><td class="tb"></td><td class="tb"><em>InfoGainAttributeEval</em></td>
<td class="tb">Evaluate attribute based on information gain</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LatentSemanticAnalysis</em></td>
<td class="tb">Perform a latent semantic analysis and transformation</td></tr>
<tr><td class="tb"></td><td class="tb"><em>OneRAttributeEval</em></td>
<td class="tb">Use <em>OneR</em>’s methodology to evaluate attributes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>PrincipalComponents</em></td>
<td class="tb">Perform principal components analysis and transformation</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ReliefFAttributeEval</em></td>
<td class="tb">Instance-based attribute evaluator</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SVMAttributeEval</em></td>
<td class="tb">Use a linear support vector machine to determine the value of attributes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SymmetricalUncertAttributeEval</em></td>
<td class="tb">Evaluate attribute based on symmetrical uncertainty</td></tr>
</tbody>
</table>
<p class="table_caption"><span class="tab_num">Table 11.10. </span> Search Methods for Attribute Selection</p>
<table id="t0055" frame="box" rules="all">
<thead>
<tr><td class="tch"></td><td class="tch">Name</td>
<td class="tch">Function</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb">Search Method</td>
<td class="tb"><em>BestFirst</em></td>
<td class="tb">Greedy hill climbing with backtracking</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ExhaustiveSearch</em></td>
<td class="tb">Search exhaustively</td></tr>
<tr><td class="tb"></td><td class="tb"><em>GeneticSearch</em></td>
<td class="tb">Search using a simple genetic algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>GreedyStepwise</em></td>
<td class="tb">Greedy hill climbing without backtracking; optionally generate ranked list of attributes</td></tr>
<tr><td class="tb"></td><td class="tb"><em>LinearForwardSelection</em></td>
<td class="tb">Extension of <em>BestFirst</em> that considers a restricted number of the remaining attributes when expanding the current point in the search</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RaceSearch</em></td>
<td class="tb">Use race search methodology</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RandomSearch</em></td>
<td class="tb">Search randomly</td></tr>
<tr><td class="tb"></td><td class="tb"><em>RankSearch</em></td>
<td class="tb">Sort the attributes and rank promising subsets using an attribute subset evaluator</td></tr>
<tr><td class="tb"></td><td class="tb"><em>ScatterSearchV1</em></td>
<td class="tb">Search using an evolutionary scatter search algorithm</td></tr>
<tr><td class="tb"></td><td class="tb"><em>SubsetSizeForwardSelection</em></td>
<td class="tb">Extension of <em>LinearForwardSelection</em> that performs an internal cross-validation in order to determine the optimal subset size</td></tr>
<tr><td class="tb">Ranking Method</td>
<td class="tb"><em>Ranker</em></td>
<td class="tb">Rank individual attributes (not subsets) according to their evaluation</td></tr>
</tbody>
</table>
<div id="s0270">
<h3 id="st0270">Attribute Subset Evaluators</h3>
<p id="p1200" class="noindent">Subset evaluators take a subset of attributes and return a numerical measure that guides the search. They are configured like any other Weka object. <em>CfsSubsetEval</em> assesses the predictive ability of each attribute individually and the degree of redundancy among them, preferring sets of attributes that are highly correlated with the class but with low intercorrelation (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p310">page 310</a>). An option iteratively adds attributes that have the highest correlation with the class, provided that the set does not already contain an attribute whose correlation with the attribute in question is even higher. <em>Missing</em> can be treated as a separate value, or its counts can be distributed among other values in proportion to their frequency. <em>ConsistencySubsetEval</em> evaluates attribute sets by the degree of consistency in class values when the training instances are projected onto the set. The consistency of any subset of attributes can never improve on that of the full set, so this evaluator is usually used in conjunction with a random or exhaustive search that seeks the smallest subset with a consistency that is the same as that of the full attribute set.</p>
<p id="p1205" class="para_indented">Whereas the previously mentioned subset evaluators are filter methods of attribute selection (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p309">page 309</a>), <em>ClassifierSubsetEval</em> and <em>WrapperSubsetEval</em> are wrapper methods. <em>ClassifierSubsetEval</em> uses a classifier, specified in the object editor as a parameter, to evaluate sets of attributes on the training data or on a separate holdout set. <em>WrapperSubsetEval</em> also uses a classifier to evaluate attribute sets, but it employs cross-validation to estimate the accuracy of the learning scheme for each set.</p>
<p id="p1210" class="para_indented">The remaining two subset evaluators are meta-evaluators—that is, they augment a base subset evaluator with preprocessing options. <em>CostSensitiveSubsetEval</em> takes a base subset evaluator and makes it cost sensitive by weighting or resampling the training data according to a supplied cost matrix. <em>FilteredSubsetEval</em> applies a filter to the training data before attribute selection is performed. Selecting a filter that alters the number or ordering of the original attributes generates an error message.</p>
</div>
<div id="s0275">
<h3 id="st0275"><a id="p490"></a>Single-Attribute Evaluators</h3>
<p id="p1215" class="noindent">Single-attribute evaluators are used with the <em>Ranker</em> search method to generate a ranked list from which <em>Ranker</em> discards a given number (explained in the next section). They can also be used in the <em>RankSearch</em> method. <em>ReliefFAttributeEval</em> is instance-based: It samples instances randomly and checks neighboring instances of the same and different classes (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p310">page 310</a>). It operates on discrete and continuous class data. Parameters specify the number of instances to sample, the number <a id="p491"></a>of neighbors to check, whether to weight neighbors by distance, and an exponential function that governs how rapidly weights decay with distance.</p>
<p id="p1220" class="para_indented"><em>InfoGainAttributeEval</em> evaluates attributes by measuring their information gain with respect to the class. It discretizes numeric attributes first using the MDL-based discretization method (it can be set to binarize them instead). This method, along with the next three, can treat <em>missing</em> as a separate value or distribute the counts among other values in proportion to their frequency. <em>ChiSquaredAttributeEval</em> evaluates attributes by computing the chi-squared statistic with respect to the class. <em>GainRatioAttributeEval</em> evaluates attributes by measuring their gain ratio with respect to the class. <em>SymmetricalUncertAttributeEval</em> evaluates an attribute by measuring its symmetrical uncertainty with respect to the class (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p310">page 310</a>).</p>
<p id="p1225" class="para_indented"><em>OneRAttributeEval</em> uses the simple accuracy measure adopted by the <em>OneR</em> classifier. It can use the training data for evaluation, as <em>OneR</em> does, or it can apply internal cross-validation: The number of folds is a parameter. It adopts <em>OneR</em>’s simple discretization method: The minimum bucket size is a parameter.</p>
<p id="p1230" class="para_indented"><em>SVMAttributeEval</em> evaluates attributes using recursive feature elimination with a linear support vector machine (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p309">page 309</a>). Attributes are selected one by one based on the size of their coefficients, relearning after each one. To speed things up a fixed number (or proportion) of attributes can be removed at each stage. Indeed, a proportion can be used until a certain number of attributes remain, thereupon switching to the fixed-number method—rapidly eliminating many attributes and then considering each remaining one more intensively. Various parameters are passed on to the support vector machine: complexity, epsilon, tolerance, and the filtering method used.</p>
<p id="p1235" class="para_indented">Unlike other single-attribute evaluators, <em>PrincipalComponents</em> and <em>LatentSemanticAnalysis</em> transform the set of attributes. In the case of <em>PrincipalComponents</em>, the new attributes are ranked in order of their eigenvalues (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0060">Section 7.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p324">page 324</a>). Optionally, a subset is selected by choosing sufficient eigenvectors to account for a given proportion of the variance (95% by default). Finally, the reduced data can be transformed back to the original space.</p>
<p id="p1240" class="para_indented"><em>LatentSemanticAnalysis</em> applies a singular value decomposition to the training data. Singular value decomposition is related to principal components analysis—both produce directions that are linear combinations of the original attribute values—but differs in that it is computed from a matrix containing the original data values rather than the attribute correlation or covariance matrix. Selecting the <em>k</em> directions with the highest singular values gives a rank <em>k</em> approximation to the original data matrix. Latent semantic analysis is so named because of its application to text mining, where instances represent documents and attributes represent the terms that occur in them. In some sense, the directions that the technique produces can be thought of as merging terms with similar meaning. <em>LatentSemanticAnalysis</em> allows the user to specify the number of directions to extract (i.e., the rank) and whether or not the data is normalized before the analysis is performed.</p>
<p id="p1245" class="para_indented">The remaining two attribute evaluators, <em>CostSensitiveAttributeEval</em> and <em>FilteredAttributeEval</em>, are meta-evaluators: They are the single-attribute versions of their <a id="p492"></a>subset-based counterparts described earlier. The former augments a base evaluator by weighting or resampling the training data according to a cost matrix; the latter applies a filter to the training data before the base evaluator is applied.</p>
</div>
<div id="s0280">
<h3 id="st0280">Search methods</h3>
<p id="p1250" class="noindent">Search methods traverse the attribute space to find a good subset. Quality is measured by the chosen attribute subset evaluator. Each search method can be configured with Weka’s object editor. <em>BestFirst</em> performs greedy hill climbing with backtracking; you can specify how many consecutive nonimproving nodes must be encountered before the system backtracks. It can search forward from the empty set of attributes, backward from the full set, or start at an intermediate point (specified by a list of attribute indexes) and search in both directions by considering all possible single-attribute additions and deletions. Subsets that have been evaluated are cached for efficiency; the cache size is a parameter.</p>
<p id="p1255" class="para_indented"><em>GreedyStepwise</em> searches greedily through the space of attribute subsets. Like <em>BestFirst</em>, it may progress forward from the empty set or backward from the full set. Unlike <em>BestFirst</em>, it does not backtrack but terminates as soon as adding or deleting the best remaining attribute decreases the evaluation metric. In an alternative mode, it ranks attributes by traversing the space from empty to full (or vice versa) and recording the order in which attributes are selected. You can specify the number of attributes to retain or set a threshold below which attributes are discarded.</p>
<p id="p1260" class="para_indented"><em>LinearForwardSelection</em> and <em>SubsetSizeForwardSelection</em> are extensions of <em>BestFirst</em> aimed at, respectively, reducing the number of evaluations performed during the search and producing a compact final subset (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib144">Gutlein et al., 2009</a>). <em>LinearForwardSelection</em> limits the number of attribute expansions in each forward selection step. There are two modes of operation; both begin by ranking the attributes individually using a specified subset evaluator. In the first mode, called <em>fixed set</em>, a forward best-first search is performed on just the <em>k</em> top-ranked attributes. In the second mode, called <em>fixed width</em>, the search considers expanding the best subset at each step by selecting an attribute from the <em>k</em> top-ranked attributes. However, rather than shrinking the available pool of attributes after every expansion, it is held at a constant size <em>k</em> by adding further attributes from the initial ranked list (so long as any remain). The mode of operation and the value of <em>k</em> are parameters. Like <em>BestFirst</em>, you can set the degree of backtracking, the size of the lookup cache, and a list of attributes to begin the search. As well as the standard forward search, <em>LinearForwardSelection</em> offers an option to perform a floating forward search, which considers a number of consecutive single-attribute elimination steps after each forward step—so long as this results in an improvement.</p>
<p id="p1265" class="para_indented"><em>SubsetSizeForwardSelection</em> extends <em>LinearForwardSelection</em> with a process to determine the optimal subset size. This is achieved by performing an <em>m</em>-fold cross-validation on the training data. <em>LinearForwardSelection</em> is applied <em>m</em> times—once for each training set in the cross-validation. A given test fold is used to evaluate each <a id="p493"></a>size of subset explored by <em>LinearForwardSelection</em> in its corresponding training set. The performance for each size of subset is then averaged over the folds. Finally, <em>LinearForwardSelection</em> is performed on all the data to find a subset of that optimal size. As well as the options provided by <em>LinearForwardSelection</em>, the evaluator to use in determining the optimal subset size can be specified, along with the number of folds.</p>
<p id="p1270" class="para_indented">When paired with wrapper-based evaluation, both <em>LinearForwardSelection</em> and <em>SubsetSizeForwardSelection</em> have been shown to combat the overfitting that can occur when standard forward selection or best-first searches are used with wrappers. Moreover, both select smaller final subsets than standard forward selection and best-first, while maintaining comparable accuracy (provided <em>k</em> is chosen to be sufficiently large). <em>LinearForwardSelection</em> is faster than standard forward selection and best-first selection.</p>
<p id="p1275" class="para_indented"><em>RaceSearch</em>, used with <em>ClassifierSubsetEval</em>, calculates the cross-validation error of competing attribute subsets using race search (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#s0010">Section 7.1</a>). The four different searches described on <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#p313">page 313</a> are implemented: forward selection, backward elimination, schemata search, and rank racing. In the last case, a separate attribute evaluator (which can also be specified) is used to generate an initial ranking. Using forward selection, it is also possible to generate a ranked list of attributes by continuing racing until all attributes have been selected: The ranking is set to the order in which they are added. As with <em>GreedyStepwise</em>, you can specify the number of attributes to retain or set a threshold below which attributes are discarded.</p>
<p id="p1280" class="para_indented"><em>GeneticSearch</em> uses a simple genetic algorithm (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib139">Goldberg, 1989</a>). Parameters include population size, number of generations, and probabilities of crossover and mutation. You can specify a list of attribute indexes as the starting point, which becomes a member of the initial population. Progress reports can be generated every so many generations. <em>RandomSearch</em> randomly searches the space of attribute subsets. If an initial set is supplied, it searches for subsets that improve on (or equal) the starting point and have fewer (or the same number of) attributes. Otherwise, it starts from a random point and reports the best subset found. Placing all attributes in the initial set yields <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib213">Liu and Setiono’s (1996)</a> probabilistic feature-selection algorithm. You can determine the fraction of the search space to explore. <em>ExhaustiveSearch</em> searches through the space of attribute subsets, starting from the empty set, and reports the best subset found. If an initial set is supplied, it searches backward from this starting point and reports the smallest subset with a better (or equal) evaluation.</p>
<p id="p1285" class="para_indented"><em>RankSearch</em> sorts attributes using a single-attribute evaluator and then ranks promising subsets using an attribute subset evaluator. The latter was specified earlier in the top box of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#f0170">Figure 11.33</a>, as usual; the attribute evaluator is specified as a property in <em>RankSearch</em>’s object editor. It starts by sorting the attributes with the single-attribute evaluator and then evaluates subsets of increasing size using the subset evaluator—the best attribute, the best attribute plus the next best one, and so on—reporting the best subset. This procedure has low computational complexity: The number of times both evaluators are called is linear in the number of attributes. <a id="p494"></a>Using a simple single-attribute evaluator (e.g., <em>GainRatioAttributeEval</em>), the selection procedure is very fast.</p>
<p id="p1290" class="para_indented"><em>ScatterSearchV1</em> uses an evolution-based scatter search algorithm (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib198">Laguna and Marti, 2003</a>). Parameters include the population, size, random number seed, and strategy used to generate new population members from existing ones.</p>
<p id="p1295" class="para_indented">Finally, we describe <em>Ranker</em>, which as noted earlier is not a search method for attribute subsets but a ranking scheme for individual attributes. It sorts attributes by their individual evaluations and must be used in conjunction with one of the single-attribute evaluators in the lower part of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0050">Table 11.9</a>—not an attribute subset evaluator. <em>Ranker</em> not only ranks attributes but also performs attribute selection by removing the lower-ranking ones. You can set a cutoff threshold below which attributes are discarded, or specify how many attributes to retain. You can specify certain attributes that must be retained regardless of their rank.</p>
</div>
</div>
<div class="footnote">
<p class="footnote" id="fn0010"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#cc000110fn0010" class="totri-footnote"><span class="sup">1</span></a> The <em>rint</em> function rounds to the closest integer.</p>
<p class="footnote" id="fn0015"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#cc000110fn0015" class="totri-footnote"><span class="sup">2</span></a> The random forest scheme was mentioned on page 357, 455. It is really a metalearner, but Weka includes it among the decision tree methods because it is hardwired to a particular classifier, <em>RandomTree</em>.</p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0010.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 10. Introduction to Weka</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0012.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 12. The Knowledge Flow Interface</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.2055096384459585"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.4818085669620311" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=92778476-69b0-7c94-1ddb-c795160787ad&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Chapter%2011.%20The%20Explorer%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fxhtml%2Fc0011.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=945016"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.005px; left: 1081.01px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#">Reset</a>
</div>
</div><iframe src="cid:frame-27F46E3A7A2AAD30B347EE47FB09E770@mhtml.blink" style="display: none;"></iframe><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span></body></html>