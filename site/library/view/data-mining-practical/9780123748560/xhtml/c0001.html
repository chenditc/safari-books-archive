<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0001.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0001.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Chapter 1. What’s It All About? - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html"><meta name="description" content="CHAPTER 1 What’s It All About? Human in vitro fertilization involves collecting several eggs from a woman’s ovaries, which, after fertilization with partner or donor sperm, produce several ... "><meta property="og:title" content="Chapter 1. What’s It All About?"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Chapter 1. What’s It All About?"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0001.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content="CHAPTER 1 What’s It All About? Human in vitro fertilization involves collecting several eggs from a woman’s ovaries, which, after fertilization with partner or donor sperm, produce several ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/xhtml/c0001.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0001.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0001.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0001.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Chapter%201.%20What%E2%80%99s%20It%20All%20About%3F&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0001.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/library/view/data-mining-practical/9780123748560/xhtml/p1.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">PART I. Introduction to Data Mining</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0002.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 2. Input</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="c0001"></a><h1 class="chapterlabel" id="c0001tit1">CHAPTER 1</h1>
<h1 class="chaptertitle" id="c0001tit">What’s It All About?</h1>
<p id="p0010" class="noindent"><a id="p3"></a>Human in vitro fertilization involves collecting several eggs from a woman’s ovaries, which, after fertilization with partner or donor sperm, produce several embryos. Some of these are selected and transferred to the woman’s uterus. The challenge is to select the “best” embryos to use—the ones that are most likely to survive. Selection is based on around 60 recorded features of the embryos—characterizing their morphology, oocyte, and follicle, and the sperm sample. The number of features is large enough to make it difficult for an embryologist to assess them all simultaneously and correlate historical data with the crucial outcome of whether that embryo did or did not result in a live child. In a research project in England, machine learning has been investigated as a technique for making the selection, using historical records of embryos and their outcome as training data.</p>
<p id="p0015" class="para_indented">Every year, dairy farmers in New Zealand have to make a tough business decision: which cows to retain in their herd and which to sell off to an abattoir. Typically, one-fifth of the cows in a dairy herd are culled each year near the end of the milking season as feed reserves dwindle. Each cow’s breeding and milk production history influences this decision. Other factors include age (a cow nears the end of its productive life at eight years), health problems, history of difficult calving, undesirable temperament traits (kicking or jumping fences), and not being pregnant with calf for the following season. About 700 attributes for each of several million cows have been recorded over the years. Machine learning has been investigated as a way of ascertaining what factors are taken into account by successful farmers—not to automate the decision but to propagate their skills and experience to others.</p>
<p id="p0020" class="para_indented">Life and death. From Europe to the Antipodes. Family and business. Machine learning is a burgeoning new technology for mining knowledge from data, a technology that a lot of people are starting to take seriously.</p>
<div id="s0010">
<h2 id="st0010">1.1 Data mining and machine learning</h2>
<p id="p0025" class="noindent">We are overwhelmed with data. The amount of data in the world and in our lives seems ever-increasing—and there’s no end in sight. Omnipresent computers make it too easy to save things that previously we would have trashed. Inexpensive disks and online storage make it too easy to postpone decisions about what to do with all <a id="p4"></a>this stuff—we simply get more memory and keep it all. Ubiquitous electronics record our decisions, our choices in the supermarket, our financial habits, our comings and goings. We swipe our way through the world, every swipe a record in a database. The World Wide Web (WWW) overwhelms us with information; meanwhile, every choice we make is recorded. And all of these are just personal choices—they have countless counterparts in the world of commerce and industry. We could all testify to the growing gap between the <em>generation</em> of data and our <em>understanding</em> of it. As the volume of data increases, inexorably, the proportion of it that people understand decreases alarmingly. Lying hidden in all this data is information—potentially useful information—that is rarely made explicit or taken advantage of.</p>
<p id="p0030" class="para_indented">This book is about looking for patterns in data. There is nothing new about this. People have been seeking patterns in data ever since human life began. Hunters seek patterns in animal migration behavior, farmers seek patterns in crop growth, politicians seek patterns in voter opinion, and lovers seek patterns in their partners’ responses. A scientist’s job (like a baby’s) is to make sense of data, to discover the patterns that govern how the physical world works and encapsulate them in theories that can be used for predicting what will happen in new situations. The entrepreneur’s job is to identify opportunities—that is, patterns in behavior that can be turned into a profitable business—and exploit them.</p>
<p id="p0035" class="para_indented">In <em>data mining</em>, the data is stored electronically and the search is automated—or at least augmented—by computer. Even this is not particularly new. Economists, statisticians, forecasters, and communication engineers have long worked with the idea that patterns in data can be sought automatically, identified, validated, and used for prediction. What is new is the staggering increase in opportunities for finding patterns in data. The unbridled growth of databases in recent years, databases for such everyday activities as customer choices, brings data mining to the forefront of new business technologies. It has been estimated that the amount of data stored in the world’s databases doubles every 20 months, and although it would surely be difficult to justify this figure in any quantitative sense, we can all relate to the pace of growth qualitatively. As the flood of data swells and machines that can undertake the searching become commonplace, the opportunities for data mining increase. As the world grows in complexity, overwhelming us with the data it generates, data mining becomes our only hope for elucidating hidden patterns. Intelligently analyzed data is a valuable resource. It can lead to new insights, and, in commercial settings, to competitive advantages.</p>
<p id="p0040" class="para_indented">Data mining is about solving problems by analyzing data already present in databases. Suppose, to take a well-worn example, the problem is fickle customer loyalty in a highly competitive marketplace. A database of customer choices, along with customer profiles, holds the key to this problem. Patterns of behavior of former customers can be analyzed to identify distinguishing characteristics of those likely to switch products and those likely to remain loyal. Once such characteristics are found, they can be put to work to identify present customers who are likely to jump ship. This group can be targeted for special treatment, treatment too costly to apply to the customer base as a whole. More positively, the same techniques can be used <a id="p5"></a>to identify customers who might be attracted to another service the enterprise provides, one they are not presently enjoying, to target them for special offers that promote this service. In today’s highly competitive, customer-centered, service-oriented economy, data is the raw material that fuels business growth—if only it can be mined.</p>
<p id="p0045" class="para_indented">Data mining is defined as the process of discovering patterns in data. The process must be automatic or (more usually) semiautomatic. The patterns discovered must be meaningful in that they lead to some advantage, usually an economic one. The data is invariably present in substantial quantities.</p>
<p id="p0050" class="para_indented">And how are the patterns expressed? Useful patterns allow us to make nontrivial predictions on new data. There are two extremes for the expression of a pattern: as a black box whose innards are effectively incomprehensible, and as a transparent box whose construction reveals the structure of the pattern. Both, we are assuming, make good predictions. The difference is whether or not the patterns that are mined are represented in terms of a structure that can be examined, reasoned about, and used to inform future decisions. Such patterns we call <em>structural</em> because they capture the decision structure in an explicit way. In other words, they help to explain something about the data.</p>
<p id="p0055" class="para_indented">Now, again, we can say what this book is about: It is about techniques for finding and describing structural patterns in data. Most of the techniques that we cover have developed within a field known as <em>machine learning</em>. But first let us look at what structural patterns are.</p>
<div id="s0015">
<h3 id="st0015">Describing Structural Patterns</h3>
<p id="p0060" class="noindent">What is meant by <em>structural patterns</em>? How do you describe them? And what form does the input take? We will answer these questions by way of illustration rather than by attempting formal, and ultimately sterile, definitions. There will be plenty of examples later in this chapter, but let’s examine one right now to get a feeling for what we’re talking about.</p>
<p id="p0065" class="para_indented">Look at the contact lens data in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0010">Table 1.1</a>. It gives the conditions under which an optician might want to prescribe soft contact lenses, hard contact lenses, or no contact lenses at all; we will say more about what the individual features mean later. Each line of the table is one of the examples. Part of a structural description of this information might be as follows:</p>
<p class="table_caption"><span class="tab_num">Table 1.1. </span> Contact Lens Data</p>
<p id="t0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0010.jpg" alt="Image" width="448" height="430" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0010.jpg"></p>
<p id="p0070" class="noindent"><span class="monospace">If tear production rate = reduced then recommendation = none Otherwise, if age = young and astigmatic = no then recommendation = soft</span></p> 
<p id="p0080" class="para_indented">Structural descriptions need not necessarily be couched as rules such as these. Decision trees, which specify the sequences of decisions that need to be made along with the resulting recommendation, are another popular means of expression.</p>
<p id="p0085" class="para_indented">This example is a very simplistic one. For a start, all combinations of possible values are represented in the table. There are 24 rows, representing three possible <a id="p6"></a>values of age and two values each for spectacle prescription, astigmatism, and tear production rate (3 × 2 × 2 × 2 = 24). The rules do not really generalize from the data; they merely summarize it. In most learning situations, the set of examples given as input is far from complete, and part of the job is to generalize to other, new examples. You can imagine omitting some of the rows in the table for which the tear production rate is <em>reduced</em> and still coming up with the rule</p>
<p id="p0090" class="noindent"><span class="monospace">If tear production rate = reduced then recommendation = none</span></p>
<p id="p0095" class="para_indented">This would generalize to the missing rows and fill them in correctly. Second, values are specified for all the features in all the examples. Real-life datasets invariably contain examples in which the values of some features, for some reason or other, are unknown—for example, measurements were not taken or were lost. Third, the <a id="p7"></a>preceding rules classify the examples correctly, whereas often, because of errors or <em>noise</em> in the data, misclassifications occur even on the data that is used to create the classifier.</p>
</div>
<div id="s0020">
<h3 id="st0020">Machine Learning</h3>
<p id="p0100" class="noindent">Now that we have some idea of the inputs and outputs, let’s turn to machine learning. What is learning, anyway? What is <em>machine</em> learning? These are philosophical questions, and we will not be too concerned with philosophy in this book; our emphasis is firmly on the practical. However, it is worth spending a few moments at the outset on fundamental issues, just to see how tricky they are, before rolling up our sleeves and looking at machine learning in practice.</p>
<p id="p0105" class="para_indented">Our dictionary defines “to learn” as</p><a id="p0110"></a><div class="none">
<p class="hang" id="u0010">• <a id="p0115"></a>To get knowledge of something by study, experience, or being taught.</p>
<p class="hang" id="u0015">• <a id="p0120"></a>To become aware by information or from observation</p>
<p class="hang" id="u0020">• <a id="p0125"></a>To commit to memory</p>
<p class="hang" id="u0025">• <a id="p0130"></a>To be informed of or to ascertain</p>
<p class="hang" id="u0030">• <a id="p0135"></a>To receive instruction</p>
</div>
<p id="p0140" class="para_indented">These meanings have some shortcomings when it comes to talking about computers. For the first two, it is virtually impossible to test whether learning has been achieved or not. How do you know whether a machine has got knowledge of something? You probably can’t just ask it questions; even if you could, you wouldn’t be testing its ability to learn but its ability to answer questions. How do you know whether it has become aware of something? The whole question of whether computers can be aware, or conscious, is a burning philosophical issue.</p>
<p id="p9000" class="para_indented">As for the last three meanings, although we can see what they denote in human terms, merely committing to memory and receiving instruction seem to fall far short of what we might mean by machine learning. They are too passive, and we know that computers find these tasks trivial. Instead, we are interested in improvements in performance, or at least in the potential for performance, in new situations. You can commit something to memory or be informed of something by rote learning without being able to apply the new knowledge to new situations. In other words, you can receive instruction without benefiting from it at all.</p>
<p id="p0145" class="para_indented">Earlier we defined data mining operationally, as the process of discovering patterns, automatically or semiautomatically, in large quantities of data—and the patterns must be useful. An operational definition can be formulated in the same way for learning:</p><a id="p9010"></a><div class="none">
<p class="hang" id="u0035">• <a id="p9015"></a>Things learn when they change their behavior in a way that makes them perform better in the future</p>
</div>
<p id="p9005" class="noindent">This ties learning to <em>performance</em> rather than <em>knowledge</em>. You can test learning by observing present behavior and comparing it with past behavior. This is a much more objective kind of definition and appears to be far more satisfactory.</p>
<p id="p0150" class="para_indented"><a id="p8"></a>But still there’s a problem. Learning is a rather slippery concept. Lots of things change their behavior in ways that make them perform better in the future, yet we wouldn’t want to say that they have actually <em>learned</em>. A good example is a comfortable slipper. Has it learned the shape of your foot? It has certainly changed its behavior to make it perform better as a slipper! Yet we would hardly want to call this <em>learning</em>. In everyday language, we often use the word <em>training</em> to denote a mindless kind of learning. We train animals and even plants, although it would be stretching the word a bit to talk of training objects such as slippers, which are not in any sense alive. But learning is different. Learning implies thinking and purpose. Something that learns has to do so intentionally. That is why we wouldn’t say that a vine has learned to grow around a trellis in a vineyard—we’d say it has been trained. Learning without purpose is merely training. Or, more to the point, in learning the purpose is the learner’s, whereas in training it is the teacher’s.</p>
<p id="p0155" class="para_indented">Thus, on closer examination the second definition of learning, in operational, performance-oriented terms, has its own problems when it comes to talking about computers. To decide whether something has actually learned, you need to see whether it intended to, whether there was any purpose involved. That makes the concept moot when applied to machines because whether artifacts can behave purposefully is unclear. Philosophical discussions of what is really meant by <em>learning</em>, like discussions of what is really meant by <em>intention</em> or <em>purpose</em>, are fraught with difficulty. Even courts of law find intention hard to grapple with.</p>
</div>
<div id="s0025">
<h3 id="st0025">Data Mining</h3>
<p id="p0160" class="noindent">Fortunately, the kind of learning techniques explained in this book do not present these conceptual problems—they are called <em>machine learning</em> without really presupposing any particular philosophical stance about what learning actually is. Data mining is a topic that involves learning in a practical, nontheoretical sense. We are interested in techniques for finding and describing structural patterns in data, as a tool for helping to explain that data and make predictions from it. The data will take the form of a set of examples, such as customers who have switched loyalties, for instance, or situations in which certain kinds of contact lenses can be prescribed. The output takes the form of predictions about new examples—a prediction of whether a particular customer will switch or a prediction of what kind of lens will be prescribed under given circumstances. But because this book is about finding <em>and</em> describing patterns in data, the output may also include an actual description of a structure that can be used to classify unknown examples. As well as <em>performance</em>, it is helpful to supply an explicit representation of the <em>knowledge</em> that is acquired. In essence, this reflects both definitions of learning considered above: the acquisition of knowledge and the ability to use it.</p>
<p id="p0165" class="para_indented">Many learning techniques look for structural descriptions of what is learned—descriptions that can become fairly complex and are typically expressed as sets of rules, such as the ones described previously or the decision trees described later in this chapter. Because they can be understood by people, these descriptions serve to <a id="p9"></a>explain what has been learned—in other words, to explain the basis for new predictions. Experience shows that in many applications of machine learning to data mining, the explicit knowledge structures that are acquired, the structural descriptions, are at least as important as the ability to perform well on new examples. People frequently use data mining to gain knowledge, not just predictions. Gaining knowledge from data certainly sounds like a good idea if you can do it. To find out how, read on!</p>
</div>
</div>
<div id="s0030">
<h2 id="st0030">1.2 Simple examples: the weather and other problems</h2>
<p id="p0170" class="noindent">We will be using a lot of examples in this book, which seems particularly appropriate considering that the book is all about learning from examples! There are several standard datasets that we will come back to repeatedly. Different datasets tend to expose new issues and challenges, and it is interesting and instructive to have in mind a variety of problems when considering learning methods. In fact, the need to work with different datasets is so important that a corpus containing around 100 example problems has been gathered together so that different algorithms can be tested and compared on the same set of problems.</p>
<p id="p0175" class="para_indented">The set of problems in this section are all unrealistically simple. Serious application of data mining involves thousands, hundreds of thousands, or even millions of individual cases. But when explaining what algorithms do and how they work, we need simple examples that capture the essence of the problem but are small enough to be comprehensible in every detail. We will be working with the datasets in this section throughout the book, and they are intended to be “academic” in the sense that they will help us to understand what is going on. Some actual fielded applications of learning techniques are discussed in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0065">Section 1.3</a>, and many more are covered in the books mentioned in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0160">Section 1.7</a>, Further reading, at the end of the chapter.</p>
<p id="p0180" class="para_indented">Another problem with actual real-life datasets is that they are often proprietary. No one is going to share their customer and product choice database with you so that you can understand the details of their data mining application and how it works. Corporate data is a valuable asset, the value of which has increased enormously with the development of data mining techniques such as those described in this book. Yet, we are concerned here with understanding how the methods used for data mining work, and understanding the details of these methods so that we can trace their operation on actual data. That is why our illustrative datasets are simple ones. But they are not <em>simplistic</em>: They exhibit the features of real datasets.</p>
<div id="s0035">
<h3 id="st0035">The Weather Problem</h3>
<p id="p0185" class="noindent">The weather problem is a tiny dataset that we will use repeatedly to illustrate machine learning methods. Entirely fictitious, it supposedly concerns the conditions <a id="p10"></a>that are suitable for playing some unspecified game. In general, instances in a dataset are characterized by the values of features, or <em>attributes</em>, that measure different aspects of the instance. In this case there are four attributes: <em>outlook</em>, <em>temperature</em>, <em>humidity</em>, and <em>windy</em>. The outcome is whether to play or not.</p>
<p id="p0190" class="para_indented">In its simplest form, shown in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a>, all four attributes have values that are symbolic categories rather than numbers. Outlook can be <em>sunny</em>, <em>overcast</em>, or <em>rainy</em>; temperature can be <em>hot</em>, <em>mild</em>, or <em>cool</em>; humidity can be <em>high</em> or <em>normal</em>; and windy can be <em>true</em> or <em>false</em>. This creates 36 possible combinations (3 × 3 × 2 × 2 = 36), of which 14 are present in the set of input examples.</p>
<p class="table_caption"><span class="tab_num">Table 1.2. </span> Weather Data</p>
<p id="t0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0015.jpg" alt="Image" width="448" height="253" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0015.jpg"></p>
<p id="p0195" class="para_indented">A set of rules learned from this information—not necessarily a very good one—might look like this:</p>
<p id="p0200" class="para_indented"><span class="monospace">If outlook = sunny and humidity = high then play = no</span></p>
<p id="p0205" class="para_indented"><span class="monospace">If outlook = rainy and windy = true then play = no</span></p>
<p id="p0210" class="para_indented"><span class="monospace">If outlook = overcast then play = yes</span></p>
<p id="p0215" class="para_indented"><span class="monospace">If humidity = normal then play = yes</span></p>
<p id="p0220" class="para_indented"><span class="monospace">If none of the above then play = yes</span></p>
<p id="p0225" class="para_indented">These rules are meant to be interpreted in order: The first one; then, if it doesn’t apply, the second; and so on. A set of rules that are intended to be interpreted in sequence is called a <em>decision list</em>. Interpreted as a decision list, the rules correctly classify all of the examples in the table, whereas taken individually, out of context, some of the rules are incorrect. For example, the rule <span class="monospace"><em>if humidity = normal then play = yes</em></span> gets one of the examples wrong (check which one). The meaning of a set of rules depends on how it is interpreted—not surprisingly!</p>
<p id="p0230" class="para_indented">In the slightly more complex form shown in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>, two of the attributes—temperature and humidity—have numeric values. This means that any learning <a id="p11"></a>scheme must create inequalities involving these attributes rather than simple equality tests as in the former case. This is called a <em>numeric-attribute problem</em>—in this case, a <em>mixed-attribute problem</em> because not all attributes are numeric.</p>
<p class="table_caption"><span class="tab_num">Table 1.3. </span> Weather Data with Some Numeric Attributes</p>
<p id="t0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0020.jpg" alt="Image" width="448" height="255" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0020.jpg"></p>
<p id="p0235" class="para_indented">Now the first rule given earlier might take the form</p>
<p id="p0240" class="noindent"><span class="monospace">If outlook = sunny and humidity &gt; 83 then play = no</span></p>
<p id="p0245" class="para_indented">A slightly more complex process is required to come up with rules that involve numeric tests.</p>
<p id="p0250" class="para_indented">The rules we have seen so far are <em>classification rules</em>: They predict the classification of the example in terms of whether to play or not. It is equally possible to disregard the classification and just look for any rules that strongly associate different attribute values. These are called <em>association rules</em>. Many association rules can be derived from the weather data in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a>. Some good ones are</p>
<p id="p0255" class="noindent"><span class="monospace">If temperature = cool then humidity = normal</span></p>
<p id="p0260" class="noindent"><span class="monospace">If humidity = normal and windy = false then play = yes</span></p>
<p id="p0265" class="noindent"><span class="monospace">If outlook = sunny and play = no then humidity = high</span></p>
<p id="p0270" class="noindent"><span class="monospace">If windy = false and play = no then outlook = sunny and humidity = high</span></p>
<p id="p0275" class="para_indented">All these rules are 100% correct on the given data; they make no false predictions. The first two apply to four examples in the dataset, the third to three examples, and the fourth to two examples. And there are many other rules. In fact, nearly 60 association rules can be found that apply to two or more examples of the weather data and are completely correct on this data. And if you look for rules that are less than 100% correct, then you will find many more. There are so many because, unlike <a id="p12"></a>classification rules, association rules can “predict” any of the attributes, not just a specified class, and can even predict more than one thing. For example, the fourth rule predicts both that <em>outlook</em> will be <em>sunny</em> and that <em>humidity</em> will be <em>high</em>.</p>
</div>
<div id="s0040">
<h3 id="st0040">Contact Lenses: An Idealized Problem</h3>
<p id="p0280" class="noindent">The contact lens data introduced earlier tells you the kind of contact lens to prescribe, given certain information about a patient. Note that this example is intended for illustration only: It grossly oversimplifies the problem and should certainly not be used for diagnostic purposes!</p>
<p id="p0285" class="para_indented">The first column of <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0010">Table 1.1</a> gives the age of the patient. In case you’re wondering, <em>presbyopia</em> is a form of longsightedness that accompanies the onset of middle age. The second gives the spectacle prescription: <em>Myope</em> means shortsighted and <em>hypermetrope</em> means longsighted. The third shows whether the patient is astigmatic, while the fourth relates to the rate of tear production, which is important in this context because tears lubricate contact lenses. The final column shows which kind of lenses to prescribe, whether <em>hard</em>, <em>soft</em>, or <em>none</em>. All possible combinations of the attribute values are represented in the table.</p>
<p id="p0290" class="para_indented">A sample set of rules learned from this information is shown in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0010">Figure 1.1</a>. This is a rather large set of rules, but they do correctly classify all the examples. These rules are complete and deterministic: They give a unique prescription for every conceivable example. Generally this is not the case. Sometimes there are situations in which no rule applies; other times more than one rule may apply, resulting in <a id="p13"></a>conflicting recommendations. Sometimes probabilities or weights may be associated with the rules themselves to indicate that some are more important, or more reliable, than others.</p>
<p id="f0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000018f001-001-9780123748560.jpg" alt="image" width="497" height="260" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000018f001-001-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 1.1</span> Rules for the contact lens data.</p>
<p id="p0295" class="para_indented">You might be wondering whether there is a smaller rule set that performs as well. If so, would you be better off using the smaller rule set, and, if so, why? These are exactly the kinds of questions that will occupy us in this book. Because the examples form a complete set for the problem space, the rules do no more than summarize all the information that is given, expressing it in a different and more concise way. Even though it involves no generalization, this is often a very useful thing to do! People frequently use machine learning techniques to gain insight into the structure of their data rather than to make predictions for new cases. In fact, a prominent and successful line of research in machine learning began as an attempt to compress a huge database of possible chess endgames and their outcomes into a data structure of reasonable size. The data structure chosen for this enterprise was not a set of rules but a decision tree.</p>
<p id="p0300" class="para_indented"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0015">Figure 1.2</a> shows a structural description for the contact lens data in the form of a decision tree, which for many purposes is a more concise and perspicuous representation of the rules and has the advantage that it can be visualized more easily. (However, this decision tree, in contrast to the rule set given in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0010">Figure 1.1</a>, classifies two examples incorrectly.) The tree calls first for a test on the <em>tear production rate</em>, and the first two branches correspond to the two possible outcomes. If the <em>tear production rate</em> is <em>reduced</em> (the left branch), the outcome is <em>none</em>. If it is <em>normal</em> (the right branch), a second test is made, this time on <em>astigmatism</em>. Eventually, whatever the outcome of the tests, a leaf of the tree is reached that dictates the contact lens recommendation for that case. The question of what is the most natural and easily understood format for the output from a machine learning scheme is one that we will return to in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#c0003">Chapter 3</a>.</p>
<p id="f0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000018f001-002-9780123748560.jpg" alt="image" width="748" height="728" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000018f001-002-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 1.2</span> Decision tree for the contact lens data.</p>
</div>
<div id="s0045">
<h3 id="st0045">Irises: A Classic Numeric Dataset</h3>
<p id="p0305" class="noindent">The iris dataset, which dates back to seminal work by the eminent statistician R. A. Fisher in the mid-1930s and is arguably the most famous dataset used in data mining, contains 50 examples of each of three types of plant: <em>Iris setosa</em>, <em>Iris versicolor</em>, and <em>Iris virginica</em>. It is excerpted in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0025">Table 1.4</a>. There are four attributes: <em>sepal length</em>, <a id="p14"></a><em>sepal width</em>, <em>petal length</em>, and <em>petal width</em> (all measured in centimeters). Unlike previous datasets, all attributes have values that are numeric.</p>
<p class="table_caption"><span class="tab_num">Table 1.4. </span> Iris Data</p>
<p id="t0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0025.jpg" alt="Image" width="448" height="334" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0025.jpg"></p>
<p id="p0310" class="para_indented">The following set of rules might be learned from this dataset:</p>
<p id="p0315" class="noindent"><span class="monospace">If petal-length &lt; 2.45 then Iris-setosa</span></p>
<p id="p0320" class="noindent"><span class="monospace">If sepal-width &lt; 2.10 then Iris-versicolor</span></p>
<p id="p0325" class="noindent"><span class="monospace">If sepal-width &lt; 2.45 and petal-length &lt; 4.55 then Iris-versicolor</span></p>
<p id="p0330" class="noindent"><span class="monospace">If sepal-width &lt; 2.95 and petal-width &lt; 1.35 then Iris-versicolor</span></p>
<p id="p0335" class="noindent"><span class="monospace">If petal-length ≥ 2.45 and petal-length &lt; 4.45 then Iris-versicolor</span></p>
<p id="p0340" class="noindent"><span class="monospace">If sepal-length ≥ 5.85 and petal-length &lt; 4.75 then Iris-versicolor</span></p>
<p id="p0345" class="noindent"><span class="monospace">If sepal-width &lt; 2.55 and petal-length &lt; 4.95 and petal-width &lt; 1.55 then Iris-versicolor</span></p>
<p id="p0350" class="noindent"><span class="monospace">If petal-length ≥ 2.45 and petal-length &lt; 4.95 and petal-width &lt; 1.55 then Iris-versicolor</span></p>
<p id="p0355" class="noindent"><span class="monospace">If sepal-length ≥ 6.55 and petal-length &lt; 5.05 then Iris-versicolor</span></p>
<p id="p0360" class="noindent"><span class="monospace">If sepal-width &lt; 2.75 and petal-width &lt; 1.65 and sepal-length &lt; 6.05 then Iris-versicolor</span></p>
<p id="p0365" class="noindent"><span class="monospace">If sepal-length ≥ 5.85 and sepal-length &lt; 5.95 and petal-length &lt; 4.85 then Iris-versicolor</span></p>
<p id="p0370" class="noindent"><span class="monospace">If petal-length ≥ 5.15 then Iris-virginica</span></p>
<p id="p0375" class="noindent"><span class="monospace">If petal-width ≥ 1.85 then Iris-virginica</span></p>
<p id="p0380" class="noindent"><span class="monospace">If petal-width ≥ 1.75 and sepal-width &lt; 3.05 then Iris-virginica</span></p>
<p id="p0385" class="noindent"><span class="monospace">If petal-length ≥ 4.95 and petal-width &lt; 1.55 then Iris-virginica</span></p>
<p id="p0390" class="noindent"><a id="p15"></a>These rules are very cumbersome, and we will see in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#c0003">Chapter 3</a> how more compact rules can be expressed that convey the same information.</p>
</div>
<div id="s0050">
<h3 id="st0050">CPU Performance: Introducing Numeric Prediction</h3>
<p id="p0395" class="noindent">Although the iris dataset involves numeric attributes, the outcome—the type of iris—is a category, not a numeric value. <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0030">Table 1.5</a> shows some data for which both the outcome and the attributes are numeric. It concerns the relative performance of computer processing power on the basis of a number of relevant attributes; each row represents one of 209 different computer configurations.</p><a id="p16"></a><p class="table_caption"><span class="tab_num">Table 1.5. </span> CPU Performance Data</p>
<p id="t0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0030.jpg" alt="Image" width="697" height="229" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0030.jpg"></p>
<p id="p0400" class="para_indented">The classic way of dealing with continuous prediction is to write the outcome as a linear sum of the attribute values with appropriate weights, for example,</p>
<p id="f9000" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000018u001-001-9780123748560.jpg" alt="image" width="750" height="88" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000018u001-001-9780123748560.jpg"></p>
<p></p>
<p id="p0405" class="para_indented">(The abbreviated variable names are given in the second row of the table.) This is called a <em>regression equation</em>, and the process of determining the weights is called <em>regression</em>, a well-known procedure in statistics that we will review in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>. However, the basic regression method is incapable of discovering nonlinear relationships (although variants do exist—indeed, one will be described in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>), and in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#c0003">Chapter 3</a> we will examine different representations that can be used for predicting numeric quantities.</p>
<p id="p0410" class="para_indented">In the iris and central processing unit (CPU) performance data, all the attributes have numeric values. Practical situations frequently present a mixture of numeric and nonnumeric attributes.</p>
</div>
<div id="s0055">
<h3 id="st0055">Labor Negotiations: A More Realistic Example</h3>
<p id="p0415" class="noindent">The labor negotiations dataset in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0035">Table 1.6</a> summarizes the outcome of Canadian contract negotiations in 1987 and 1988. It includes all collective agreements reached in the business and personal services sector for organizations with at least 500 members (teachers, nurses, university staff, police, etc.). Each case concerns one contract, and the outcome is whether the contract is deemed <em>acceptable</em> or <em>unacceptable</em>. The acceptable contracts are ones in which agreements were accepted by both labor and management. The unacceptable ones are either known offers that fell through because one party would not accept them or acceptable contracts that had been significantly perturbed to the extent that, in the view of experts, they would not have been accepted.</p><a id="p17"></a><p class="table_caption"><span class="tab_num">Table 1.6. </span> Labor Negotiations Data</p>
<p id="t0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0035.jpg" alt="Image" width="698" height="304" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0035.jpg"></p>
<p id="p0420" class="para_indented">There are 40 examples in the dataset (plus another 17 that are normally reserved for test purposes). Unlike the other tables here, <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0035">Table 1.6</a> presents the examples as columns rather than as rows; otherwise, it would have to be stretched over several pages. Many of the values are unknown or missing, as indicated by question marks. This is a much more realistic dataset than the others we have seen. <a id="p18"></a>It contains many missing values, and it seems unlikely that an exact classification can be obtained.</p>
<p id="p0425" class="para_indented"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a> shows two decision trees that represent the dataset. <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(a)</a> is simple and approximate—it doesn’t represent the data exactly. For example, it will predict <em>bad</em> for some contracts that are actually marked <em>good</em>. However, it does make intuitive sense: A contract is bad (for the employee!) if the wage increase in the first year is too small (less than 2.5%). If the first-year wage increase is larger than this, it is good if there are lots of statutory holidays (more than 10 days). Even if there are fewer statutory holidays, it is good if the first-year wage increase is large enough (more than 4%).</p>
<p id="f0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000018f001-003-9780123748560.jpg" alt="image" width="750" height="302" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000018f001-003-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 1.3</span> Decision trees for the labor negotiations data.</p>
<p id="p0430" class="para_indented"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(b)</a> is a more complex decision tree that represents the same dataset. Take a detailed look down the left branch. At first sight it doesn’t seem to make sense intuitively that, if the working hours exceed 36, a contract is bad if there is no health-plan contribution or a full health-plan contribution, but is good if there is a half health-plan contribution. It is certainly reasonable that the health-plan contribution plays a role in the decision, but it seems anomalous that half is good and both full and none are bad. However, on reflection this could make sense after all, because “good” contracts are ones that have been accepted by <em>both</em> parties: labor and management. Perhaps this structure reflects compromises that had to be made to reach agreement. This kind of detailed reasoning about what parts of decision trees mean is a good way of getting to know your data and thinking about the underlying problem.</p>
<p id="p0435" class="para_indented">In fact, <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(b)</a> is a more accurate representation of the training dataset than <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(a)</a>. But it is not necessarily a more accurate representation of the underlying concept of good versus bad contracts. Although it is more accurate on the data that was used to train the classifier, it may perform less well on an independent set of test data. It may be “overfitted” to the training data—following it too <a id="p19"></a>slavishly. The tree in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(a)</a> is obtained from the one in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3(b)</a> by a process of pruning, which we will learn more about in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a>.</p>
</div>
<div id="s0060">
<h3 id="st0060">Soybean Classification: A Classic Machine Learning Success</h3>
<p id="p0440" class="noindent">An often quoted early success story in the application of machine learning to practical problems is the identification of rules for diagnosing soybean diseases. The data is taken from questionnaires describing plant diseases. There are about 680 examples, each representing a diseased plant. Plants were measured on 35 attributes, each one having a small set of possible values. Examples are labeled with the diagnosis of an expert in plant biology: There are 19 disease categories altogether—horrible-sounding diseases such as diaporthe stem canker, rhizoctonia root rot, and bacterial blight, to mention just a few.</p>
<p id="p0445" class="para_indented"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0040">Table 1.7</a> gives the attributes, the number of different values that each can have, and a sample record for one particular plant. The attributes are placed in different categories just to make them easier to read.</p><a id="p20"></a><p class="table_caption"><span class="tab_num">Table 1.7. </span> Soybean Data</p>
<p id="t0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000018tabt0040.jpg" alt="Image" width="449" height="639" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000018tabt0040.jpg"></p>
<p id="p0450" class="para_indented">Here are two example rules, learned from this data:</p>
<p id="p0455" class="noindent"><span class="monospace">If leaf condition = normal and</span></p>
<p id="p0460" class="para_indented"><span class="monospace"> stem condition = abnormal and</span></p>
<p id="p0465" class="para_indented"><span class="monospace"> stem cankers = below soil line and</span></p>
<p id="p0470" class="para_indented"><span class="monospace"> canker lesion color = brown</span></p>
<p id="p0475" class="noindent"><span class="monospace">then</span></p>
<p id="p0480" class="para_indented"><span class="monospace"> diagnosis is rhizoctonia root rot</span></p>
<p id="p0490" class="noindent"><span class="monospace">If leaf malformation = absent and</span></p>
<p id="p0495" class="para_indented"><span class="monospace"> stem condition = abnormal and</span></p>
<p id="p0500" class="para_indented"><span class="monospace"> stem cankers = below soil line and</span></p>
<p id="p0505" class="para_indented"><span class="monospace"> canker lesion color = brown</span></p>
<p id="p0510" class="noindent"><span class="monospace">then</span></p>
<p id="p0515" class="para_indented"><span class="monospace"> diagnosis is rhizoctonia root rot</span></p>
<p id="p0520" class="para_indented">These rules nicely illustrate the potential role of prior knowledge—often called <em>domain knowledge</em>—in machine learning, for in fact the only difference between the two descriptions is <em>leaf condition is normal</em> versus <em>leaf malformation is absent</em>. Now, in this domain, if the leaf condition is normal then leaf malformation is necessarily absent, so one of these conditions happens to be a special case of the other. Thus, if the first rule is true, the second is necessarily true as well. The only time the second rule comes into play is when leaf malformation is absent but leaf condition is <em>not</em> normal—that is, when something other than malformation is wrong with the leaf. This is certainly not apparent from a casual reading of the rules.</p>
<p id="p0525" class="para_indented">Research on this problem in the late 1970s found that these diagnostic rules could be generated by a machine learning algorithm, along with rules for every other disease category, from about 300 training examples. These training examples were carefully selected from the corpus of cases as being quite different from one another—“far apart” in the example space. At the same time, the plant pathologist who had produced the diagnoses was interviewed, and his expertise was translated <a id="p21"></a>into diagnostic rules. Surprisingly, the computer-generated rules outperformed the expert-derived rules on the remaining test examples. The correct disease was ranked at the top 97.5% of the time compared with only 72% for the expert-derived rules. Furthermore, not only did the learning algorithm find rules that outperformed those of the expert collaborator, but the same expert was so impressed that he allegedly adopted the discovered rules in place of his own!</p>
</div>
</div>
<div id="s0065">
<h2 id="st0065">1.3 Fielded applications</h2>
<p id="p0530" class="noindent">The examples that we opened with are speculative research projects, not production systems. And the previous figures are toy problems: They are deliberately chosen to be small so that we can use them to work through algorithms later in the book. Where’s the beef? Here are some applications of machine learning that have actually been put into use.</p>
<p id="p0535" class="para_indented">Being fielded applications, the examples that follow tend to stress the use of learning in performance situations, in which the emphasis is on the ability to perform well on new examples. This book also describes the use of learning systems to gain knowledge from decision structures that are inferred from the data. We believe that this is as important—probably even more important in the long run—a use of the technology as making high-performance predictions. Still, it will tend to be underrepresented in fielded applications because when learning techniques are used to gain insight, the result is not normally a system that is put to work as an application in its own right. Nevertheless, in three of the following examples, the fact that the decision structure is comprehensible is a key feature in the successful adoption of the application.</p>
<div id="s0070">
<h3 id="st0070">Web Mining</h3>
<p id="p0540" class="noindent">Mining information on the World Wide Web is an exploding growth area. Search engine companies examine the hyperlinks in web pages to come up with a measure of “prestige” for each web page and web site. Dictionaries define <em>prestige</em> as “high standing achieved through success or influence.” A metric called PageRank, introduced by Google’s founders and also used in various guises by other search engine developers, attempts to measure the standing of a web page. The more pages that link to your web site, the higher its prestige, especially if the pages that link in have high prestige themselves. The definition sounds circular, but it can be made to work. Search engines use PageRank (among other things) to sort web pages into order before displaying the results of your search.</p>
<p id="p0545" class="para_indented">Another way in which search engines tackle the problem of how to rank web pages is to use machine learning based on a training set of example queries—documents that contain the terms in the query and human judgments about how relevant the documents are to that query. Then a learning algorithm analyzes this training data and comes up with a way to predict the relevance judgment for any <a id="p22"></a>document and query. For each document, a set of feature values is calculated that depends on the query term—for example, whether it occurs in the title tag, whether it occurs in the document’s URL, how often it occurs in the document itself, and how often it appears in the anchor text of hyperlinks that point to the document. For multiterm queries, features include how often two different terms appear close together in the document, and so on. There are many possible features—typical algorithms for learning ranks use hundreds or thousands of them.</p>
<p id="p0550" class="para_indented">Search engines mine the content of the Web. They also mine the content of your queries—the terms you search for—to select advertisements that you might be interested in. They have a strong incentive to do this accurately because they get paid by advertisers only when users click on their links. Search engine companies mine your clicks because knowledge of which results you click on can be used to improve the search next time. Online booksellers mine the purchasing database to come up with recommendations such as “users who bought this book also bought these ones”; again, they have a strong incentive to present you with compelling, personalized choices. Movie sites recommend movies based on your previous choices and other people’s choices—they win if they make recommendations that keep customers coming back to their web site.</p>
<p id="p0555" class="para_indented">And then there are social networks and other personal data. We live in the age of self-revelation: People share their innermost thoughts in blogs and tweets; their photographs, their music and movie tastes, their opinions of books, software, gadgets, and hotels; their social life. They may believe they are doing this anonymously, or pseudonymously, but often they are incorrect (see <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0140">Section 1.6</a>). There is huge commercial interest in making money by mining the Web.</p>
</div>
<div id="s0075">
<h3 id="st0075">Decisions Involving Judgment</h3>
<p id="p0560" class="noindent">When you apply for a loan, you have to fill out a questionnaire asking for relevant financial and personal information. This information is used by the loan company as the basis for its decision as to whether to lend you money. Such decisions are typically made in two stages. First, statistical methods are used to determine clear “accept” and “reject” cases. The remaining borderline cases are more difficult and call for human judgment.</p>
<p id="p0565" class="para_indented">For example, one loan company uses a statistical decision procedure to calculate a numeric parameter based on the information supplied in their questionnaire. Applicants are accepted if this parameter exceeds a preset threshold and rejected if it falls below a second threshold. This accounts for 90% of cases, and the remaining 10% are referred to loan officers for a decision. On examining historical data on whether applicants did indeed repay their loans, however, it turned out that half of the borderline applicants who were granted loans actually defaulted. Although it would be tempting simply to deny credit to borderline customers, credit industry professionals point out that if only their repayment future could be reliably determined, it is precisely these customers whose business should be wooed; they tend to be active customers of a credit institution because their finances remain in a chronically <a id="p23"></a>volatile condition. A suitable compromise must be reached between the viewpoint of a company accountant, who dislikes bad debt, and that of a sales executive, who dislikes turning business away.</p>
<p id="p0570" class="para_indented">Enter machine learning. The input was 1000 training examples of borderline cases for which a loan had been made that specified whether the borrower had finally paid off or defaulted. For each training example, about 20 attributes were extracted from the questionnaire, such as age, years with current employer, years at current address, years with the bank, and other credit cards possessed. A machine learning procedure was used to produce a small set of classification rules that made correct predictions on two-thirds of the borderline cases in an independently chosen test set. Not only did these rules improve the success rate of the loan decisions, but the company also found them attractive because they could be used to explain to applicants the reasons behind the decision. Although the project was an exploratory one that took only a small development effort, the loan company was apparently so pleased with the result that the rules were put into use immediately.</p>
</div>
<div id="s0080">
<h3 id="st0080">Screening Images</h3>
<p id="p0575" class="noindent">Since the early days of satellite technology, environmental scientists have been trying to detect oil slicks from satellite images to give early warning of ecological disasters and deter illegal dumping. Radar satellites provide an opportunity for monitoring coastal waters day and night, regardless of weather conditions. Oil slicks appear as dark regions in the image, the size and shape of which evolve depending on weather and sea conditions. However, other look-alike dark regions can be caused by local weather conditions such as high wind. Detecting oil slicks is an expensive manual process requiring highly trained personnel who assess each region in the image.</p>
<p id="p0580" class="para_indented">A hazard detection system has been developed to screen images for subsequent manual processing. Intended to be marketed worldwide to a wide variety of users—government agencies and companies—with different objectives, applications, and geographical areas, this system needs to be highly customizable to individual circumstances. Machine learning allows the system to be trained on examples of spills and nonspills supplied by the user and lets the user control the tradeoff between undetected spills and false alarms. Unlike other machine learning applications, which generate a classifier that is then deployed in the field, here it is the learning scheme itself that will be deployed.</p>
<p id="p0585" class="para_indented">The input is a set of raw pixel images from a radar satellite, and the output is a much smaller set of images with putative oil slicks marked by a colored border. First, standard image-processing operations are applied to normalize the image. Then suspicious dark regions are identified. Several dozen attributes are extracted from each region, characterizing its size, shape, area, intensity, sharpness and jaggedness of the boundaries, proximity to other regions, and information about the background in the vicinity of the region. Finally, standard learning techniques are applied to the resulting attribute vectors.</p>
<p id="p0590" class="para_indented"><a id="p24"></a>Several interesting problems were encountered. One was the scarcity of training data. Oil slicks are (fortunately) very rare, and manual classification is extremely costly. Another was the unbalanced nature of the problem: Of the many dark regions in the training data, only a very small fraction were actual oil slicks. A third is that the examples grouped naturally into batches, with regions drawn from each image forming a single batch, and background characteristics varied from one batch to another. Finally, the performance task was to serve as a filter, and the user had to be provided with a convenient means of varying the false-alarm rate.</p>
</div>
<div id="s0085">
<h3 id="st0085">Load Forecasting</h3>
<p id="p0595" class="noindent">In the electricity supply industry, it is important to determine future demand for power as far in advance as possible. If accurate estimates can be made for the maximum and minimum load for each hour, day, month, season, and year, utility companies can make significant economies in areas such as setting the operating reserve, maintenance scheduling, and fuel inventory management.</p>
<p id="p0600" class="para_indented">An automated load forecasting assistant has been operating at a major utility supplier for more than a decade to generate hourly forecasts two days in advance. The first step was to use data collected over the previous 15 years to create a sophisticated load model manually. This model had three components: base load for the year, load periodicity over the year, and the effect of holidays. To normalize for the base load, the data for each previous year was standardized by subtracting the average load for that year from each hourly reading and dividing by the standard deviation over the year.</p>
<p id="p9020" class="para_indented">Electric load shows periodicity at three fundamental frequencies: diurnal, where usage has an early morning minimum and midday and afternoon maxima; weekly, where demand is lower at weekends; and seasonal, where increased demand during winter and summer for heating and cooling, respectively, creates a yearly cycle. Major holidays, such as Thanksgiving, Christmas, and New Year’s Day, show significant variation from the normal load and are each modeled separately by averaging hourly loads for that day over the past 15 years. Minor official holidays, such as Columbus Day, are lumped together as school holidays and treated as an offset to the normal diurnal pattern. All of these effects are incorporated by reconstructing a year’s load as a sequence of typical days, fitting the holidays in their correct position, and denormalizing the load to account for overall growth.</p>
<p id="p0605" class="para_indented">Thus far, the load model is a static one, constructed manually from historical data, and it implicitly assumes “normal” climatic conditions over the year. The final step was to take weather conditions into account by locating the previous day most similar to the current circumstances and using the historical information from that day as a predictor. The prediction is treated as an additive correction to the static load model. To guard against outliers, the eight most similar days are located and their additive corrections averaged. A database was constructed of temperature, humidity, wind speed, and cloud cover at three local weather centers for each hour of the 15-year historical record, along with the difference between the actual load <a id="p25"></a>and that predicted by the static model. A linear regression analysis was performed to determine the relative effects of these parameters on load, and the coefficients were used to weight the distance function used to locate the most similar days.</p>
<p id="p0610" class="para_indented">The resulting system yielded the same performance as that of trained human forecasters but was far quicker—taking seconds rather than hours to generate a daily forecast. Human operators can analyze the forecast’s sensitivity to simulated changes in weather and bring up for examination the “most similar” days that the system used for weather adjustment.</p>
</div>
<div id="s0090">
<h3 id="st0090">Diagnosis</h3>
<p id="p0615" class="noindent">Diagnosis is one of the principal application areas of expert systems. Although the handcrafted rules used in expert systems often perform well, machine learning can be useful in situations in which producing rules manually is too labor intensive.</p>
<p id="p0620" class="para_indented">Preventative maintenance of electromechanical devices such as motors and generators can forestall failures that disrupt industrial processes. Technicians regularly inspect each device, measuring vibrations at various points to determine whether the device needs servicing. Typical faults include shaft misalignment, mechanical loosening, faulty bearings, and unbalanced pumps. A particular chemical plant uses more than 1000 different devices, ranging from small pumps to very large turbo-alternators, which until recently were diagnosed by a human expert with 20 years or more of experience. Faults are identified by measuring vibrations at different places on the device’s mounting and using Fourier analysis to check the energy present in three different directions at each harmonic of the basic rotation speed. This information, which is very noisy because of limitations in the measurement and recording procedure, is studied by the expert to arrive at a diagnosis. Although handcrafted expert system rules had been developed for some situations, the elicitation process would have to be repeated several times for different types of machinery; so a learning approach was investigated.</p>
<p id="p0625" class="para_indented">Six hundred faults, each comprising a set of measurements along with the expert’s diagnosis, were available, representing 20 years of experience. About half were unsatisfactory for various reasons and had to be discarded; the remainder were used as training examples. The goal was not to determine whether or not a fault existed but to diagnose the kind of fault, given that one was there. Thus, there was no need to include fault-free cases in the training set. The measured attributes were rather low level and had to be augmented by intermediate concepts—that is, functions of basic attributes—which were defined in consultation with the expert and embodied some causal domain knowledge. The derived attributes were run through an induction algorithm to produce a set of diagnostic rules. Initially, the expert was not satisfied with the rules because he could not relate them to his own knowledge and experience. For him, mere statistical evidence was not, by itself, an adequate explanation. Further background knowledge had to be used before satisfactory rules were generated. Although the resulting rules were quite complex, the expert liked them because he could justify them in light of his mechanical knowledge. He was <a id="p26"></a>pleased that a third of the rules coincided with ones he used himself and was delighted to gain new insight from some of the others.</p>
<p id="p0630" class="para_indented">Performance tests indicated that the learned rules were slightly superior to the handcrafted ones that had previously been elicited from the expert, and this result was confirmed by subsequent use in the chemical factory. It is interesting to note, however, that the system was put into use not because of its good performance but because the domain expert approved of the rules that had been learned.</p>
</div>
<div id="s0095">
<h3 id="st0095">Marketing and Sales</h3>
<p id="p0635" class="noindent">Some of the most active applications of data mining have been in the area of marketing and sales. These are domains in which companies possess massive volumes of precisely recorded data, which, it has only recently been realized, is potentially extremely valuable. In these applications, predictions themselves are the chief interest: The structure of how decisions are made is often completely irrelevant.</p>
<p id="p0640" class="para_indented">We have already mentioned the problem of fickle customer loyalty and the challenge of detecting customers who are likely to defect so that they can be wooed back into the fold by giving them special treatment. Banks were early adopters of data mining technology because of their successes in the use of machine learning for credit assessment. Data mining is now being used to reduce customer attrition by detecting changes in individual banking patterns that may herald a change of bank, or even life changes, such as a move to another city, that can result in a different bank being chosen. It may reveal, for example, a group of customers with above-average attrition rate who do most of their banking by phone after hours when telephone response is slow. Data mining may determine groups for whom new services are appropriate, such as a cluster of profitable, reliable customers who rarely get cash advances from their credit cards except in November and December, when they are prepared to pay exorbitant interest rates to see them through the holiday season.</p>
<p id="p0645" class="para_indented">In another domain, cellular phone companies fight <em>churn</em> by detecting patterns of behavior that could benefit from new services, and then advertise such services to retain their customer base. Incentives provided specifically to retain existing customers can be expensive, and successful data mining allows them to be precisely targeted to those customers who are likely to yield maximum benefit.</p>
<p id="p0650" class="para_indented"><em>Market basket analysis</em> is the use of association techniques to find groups of items that tend to occur together in transactions, typically supermarket checkout data. For many retailers this is the only source of sales information that is available for data mining. For example, automated analysis of checkout data may uncover the fact that customers who buy beer also buy chips, a discovery that could be significant from the supermarket operator’s point of view (although rather an obvious one that probably does not need a data mining exercise to discover). Or analysis may come up with the fact that on Thursdays customers often purchase diapers and beer together, an initially surprising result that, on reflection, makes some sense as young parents <a id="p27"></a>stock up for a weekend at home. Such information could be used for many purposes: planning store layouts, limiting special discounts to just one of a set of items that tend to be purchased together, offering coupons for a matching product when one of them is sold alone, and so on.</p>
<p id="p0655" class="para_indented">There is enormous added value in being able to identify individual customer’s sales histories. Discount or “loyalty” cards let retailers identify all the purchases that each individual customer makes. This personal data is far more valuable than the cash value of the discount. Identification of individual customers not only allows historical analysis of purchasing patterns but also permits precisely targeted special offers to be mailed out to prospective customers—or perhaps personalized coupons can be printed in real time at the checkout for use during the next grocery run. Supermarkets want you to feel that although we may live in a world of inexorably rising prices, they don’t increase so much <em>for you</em> because the bargains offered by personalized coupons make it attractive for you to stock up on things that you wouldn’t normally have bought.</p>
<p id="p0660" class="para_indented">Direct marketing is another popular domain for data mining. Bulk-mail promotional offers are expensive and have a low—but highly profitable—response rate. Anything that helps focus promotions, achieving the same or nearly the same response from a smaller sample, is valuable. Commercially available databases containing demographic information that characterizes neighborhoods based on zip codes can be correlated with information on existing customers to predict what kind of people might buy which items. This model can be trialed on information gained in response to an initial mailout, where people send back a response card or call an 800 number for more information, to predict likely future customers. Unlike shopping-mall retailers, direct-mail companies have complete purchasing histories for each individual customer and can use data mining to determine those likely to respond to special offers. Targeted campaigns save money by directing offers only to those likely to want the product.</p>
</div>
<div id="s0100">
<h3 id="st0100">Other Applications</h3>
<p id="p0665" class="noindent">There are countless other applications of machine learning. We briefly mention a few more areas to illustrate the breadth of what has been done.</p>
<p id="p0670" class="para_indented">Sophisticated manufacturing processes often involve tweaking control parameters. Separating crude oil from natural gas is an essential prerequisite to oil refinement, and controlling the separation process is a tricky job. British Petroleum used machine learning to create rules for setting the parameters. This now takes just 10 minutes, whereas previously human experts took more than a day. Westinghouse faced problems in their process for manufacturing nuclear fuel pellets and used machine learning to create rules to control the process. This was reported to have saved them more than $10 million per year (in 1984). The Tennessee printing company R. R. Donnelly applied the same idea to control rotogravure printing presses to reduce artifacts caused by inappropriate parameter settings, reducing the number of artifacts from more than 500 each year to less than 30.</p>
<p id="p0675" class="para_indented"><a id="p28"></a>In the realm of customer support and service, we have already described adjudicating loans and marketing and sales applications. Another example arises when a customer reports a telephone problem and the company must decide what kind of technician to assign to the job. An expert system developed by Bell Atlantic in 1991 to make this decision was replaced in 1999 by a set of rules developed using machine learning, which saved more than $10 million per year by making fewer incorrect decisions.</p>
<p id="p0680" class="para_indented">There are many scientific applications. In biology, machine learning is used to help identify the thousands of genes within each new genome. In biomedicine, it is used to predict drug activity by analyzing not just the chemical properties of drugs but also their three-dimensional structure. This accelerates drug discovery and reduces its cost. In astronomy, machine learning has been used to develop a fully automatic cataloging system for celestial objects that are too faint to be seen by visual inspection. In chemistry, it has been used to predict the structure of certain organic compounds from magnetic resonance spectra. In all of these applications, machine learning techniques have attained levels of performance—or should we say skill?—that rival or surpass those of human experts.</p>
<p id="p0685" class="para_indented">Automation is especially welcome in situations involving continuous monitoring, a job that is time consuming and exceptionally tedious for humans. Ecological applications include the oil spill monitoring described earlier. Other applications are rather less consequential—for example, machine learning is being used to predict preferences for TV programs based on past choices and to advise viewers about available channels. Still other applications may save lives. Intensive-care patients may be monitored to detect changes in variables that cannot be explained by circadian rhythm, medication, and so on, raising an alarm when appropriate. Finally, in a world that relies on vulnerable networked computer systems and is increasingly concerned about cybersecurity, machine learning is used to detect intrusion by recognizing unusual patterns of operation.</p>
</div>
</div>
<div id="s0105">
<h2 id="st0105">1.4 Machine learning and statistics</h2>
<p id="p0690" class="noindent">What is the difference between machine learning and statistics? Cynics, looking wryly at the explosion of commercial interest (and hype) in this area, equate data mining to statistics plus marketing. In truth, you should not look for a dividing line between machine learning and statistics because there is a continuum—and a multidimensional one at that—of data analysis techniques. Some derive from the skills taught in standard statistics courses, and others are more closely associated with the kind of machine learning that has arisen out of computer science. Historically, the two sides have had rather different traditions. If forced to point to a single difference of emphasis, it might be that statistics has been more concerned with testing hypotheses, whereas machine learning has been more concerned with formulating the process of generalization as a search through possible hypotheses. But this is a gross oversimplification: Statistics is far more <a id="p29"></a>than just hypothesis testing, and many machine learning techniques do not involve any searching at all.</p>
<p id="p0695" class="para_indented">In the past, very similar schemes have developed in parallel in machine learning and statistics. One is decision tree induction. Four statisticians (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib41">Breiman et al., 1984</a>) published a book, <em>Classification and regression trees</em>, in the mid-1980s, and throughout the 1970s and early 1980s a prominent machine learning researcher, J. Ross Quinlan, was developing a system for inferring classification trees from examples. These two independent projects produced quite similar schemes for generating trees from examples, and the researchers only became aware of one another’s work much later.</p>
<p id="p0700" class="para_indented">A second area where similar methods have arisen involves the use of nearest-neighbor methods for classification. These are standard statistical techniques that have been extensively adapted by machine learning researchers, both to improve classification performance and to make the procedure more efficient computationally. We will examine both decision tree induction and nearest-neighbor methods in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>.</p>
<p id="p0705" class="para_indented">But now the two perspectives have converged. The techniques we will examine in this book incorporate a great deal of statistical thinking. Right from the beginning, when constructing and refining the initial example set, standard statistical methods apply: visualization of data, selection of attributes, discarding outliers, and so on. Most learning algorithms use statistical tests when constructing rules or trees and for correcting models that are “overfitted” in that they depend too strongly on the details of the particular examples used to produce them (we have already seen an example of this in the two decision trees in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#f0020">Figure 1.3</a> for the labor negotiations problem). Statistical tests are used to validate machine learning models and to evaluate machine learning algorithms. In our study of practical techniques for data mining, we will learn a great deal about statistics.</p>
</div>
<div id="s0110">
<h2 id="st0110">1.5 Generalization as search</h2><a id="p0710"></a><div class="boxg" id="b0010">
<p id="p0715" class="noindent">One way of visualizing the problem of learning—and one that distinguishes it from statistical approaches—is to imagine a search through a space of possible concept descriptions for one that fits the data. Although the idea of generalization as search is a powerful conceptual tool for thinking about machine learning, it is not essential for understanding the practical schemes described in this book. That is why this section is set apart (boxed), suggesting that it is <em>optional</em>.</p>
<p id="p0720" class="para_indented">Suppose, for definiteness, that <em>concept descriptions</em>—the result of learning—are expressed as rules such as the ones given for the weather problem in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0030">Section 1.2</a> (although other concept description languages would do just as well). Suppose that we list all possible sets of rules and then look for ones that satisfy a given set of examples. A big job? Yes. An <em>infinite</em> job? At first glance it seems so because there is no limit to the number of rules there might be. But actually the number of possible rule sets is finite. Note first that each rule is no greater than a fixed maximum size, with at most one term for each attribute: For the weather data of <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a> this involves four terms in all. <a id="p30"></a>Because the number of possible rules is finite, the number of possible rule <em>sets</em> is finite too, although extremely large. However, we’d hardly be interested in sets that contained a very large number of rules. In fact, we’d hardly be interested in sets that had more rules than there are examples because it is difficult to imagine needing more than one rule for each example. So if we were to restrict consideration to rule sets smaller than that, the problem would be substantially reduced, although still very large.</p>
<p id="p0725" class="para_indented">The threat of an infinite number of possible concept descriptions seems more serious for the second version of the weather problem in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a> because these rules contain numbers. If they are real numbers, you can’t enumerate them, even in principle. However, on reflection the problem again disappears because the numbers really just represent breakpoints in the numeric values that appear in the examples. For instance, consider the <em>temperature</em> attribute in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>. It involves the numbers 64, 65, 68, 69, 70, 71, 72, 75, 80, 81, 83, and 85—12 different numbers. There are 13 possible places in which we might want to put a breakpoint for a rule involving temperature. The problem isn’t infinite after all.</p>
<p id="p0730" class="para_indented">So the process of generalization can be regarded as a search through an enormous, but finite, search space. In principle, the problem can be solved by enumerating descriptions and striking out those that do not fit the examples presented. A positive example eliminates all descriptions that it does not match, and a negative one eliminates those it does match. With each example the set of remaining descriptions shrinks (or stays the same). If only one is left, it is the target description—the target concept.</p>
<p id="p0735" class="para_indented">If several descriptions are left, they may still be used to classify unknown objects. An unknown object that matches all remaining descriptions should be classified as matching the target; if it fails to match any description it should be classified as being outside the target concept. Only when it matches some descriptions but not others is there ambiguity. In this case if the classification of the unknown object were revealed, it would cause the set of remaining descriptions to shrink because rule sets that classified the object the wrong way would be rejected.</p>
<div id="s0115">Enumerating the Concept Space<p id="p0740" class="noindent">Regarding it as search is a good way of looking at the learning process. However, the search space, although finite, is extremely big, and it is generally quite impractical to enumerate all possible descriptions and then see which ones fit. In the weather problem there are 4 × 4 × 3 × 3 × 2 = 288 possibilities for each rule. There are four possibilities for the <em>outlook</em> attribute: <em>sunny</em>, <em>overcast</em>, <em>rainy</em>, or it may not participate in the rule at all. Similarly, there are four for <em>temperature</em>, three each for <em>windy</em> and <em>humidity</em> and two for the class. If we restrict the rule set to contain no more than 14 rules (because there are 14 examples in the training set), there are around 2.7 × 10<sup>34</sup> possible different rule sets. That’s a lot to enumerate, especially for such a patently trivial problem.</p>
<p id="p0745" class="para_indented">Although there are ways of making the enumeration procedure more feasible, a serious problem remains: In practice, it is rare for the process to converge on a unique acceptable description. Either many descriptions are still in the running after the examples are processed or the descriptors are all eliminated. The first case arises when the examples are not sufficiently comprehensive to eliminate all possible descriptions except for the “correct” one. In practice, people often want a single “best” description, and it is necessary to apply some other criteria to select the best one from the set of remaining descriptions. The second problem arises either because the description language is not expressive enough to capture the actual concept or because of noise in the examples. If an example comes in with the “wrong” classification because of an error in some of the attribute values or in the class that is assigned to it, this will likely eliminate the correct description from the space. The result is that the set of remaining descriptions becomes empty. This situation is very likely to happen if the examples contain any noise at all, which inevitably they do except in artificial situations.</p>
<p id="p0750" class="para_indented"><a id="p31"></a>Another way of looking at generalization as search is to imagine it not as a process of enumerating descriptions and striking out those that don’t apply but as a kind of hill climbing in description space to find the description that best matches the set of examples according to some prespecified matching criterion. This is the way that most practical machine learning methods work. However, except in the most trivial cases, it is impractical to search the whole space exhaustively; most practical algorithms involve heuristic search and cannot guarantee to find the optimal description.</p>
</div>
<div id="s0120">Bias<p id="p0755" class="noindent">Viewing generalization as a search in a space of possible concepts makes it clear that the most important decisions in a machine learning system are:</p>
<div class="none">
<p class="hang" id="u0040">• <a id="p0760"></a>The concept description language</p>
<p class="hang" id="u0045">• <a id="p0765"></a>The order in which the space is searched</p>
<p class="hang" id="u0050">• <a id="p0770"></a>The way that overfitting to the particular training data is avoided</p>
</div>
<p id="p0775" class="para_indented">These three properties are generally referred to as the <em>bias</em> of the search and are called <em>language bias</em>, <em>search bias</em>, and <em>overfitting-avoidance bias</em>. You bias the learning scheme by choosing a language in which to express concepts, by searching in a particular way for an acceptable description, and by deciding when the concept has become so complex that it needs to be simplified.</p>
<div id="s0125">Language Bias<p id="p0780" class="noindent">The most important question for language bias is whether the concept description language is universal or whether it imposes constraints on what concepts can be learned. If you consider the set of all possible examples, a concept is really just a division of that set into subsets. In the weather example, if you were to enumerate all possible weather conditions, the <em>play</em> concept is a subset of possible weather conditions. A “universal” language is one that is capable of expressing every possible subset of examples. In practice, the set of possible examples is generally huge, and in this respect our perspective is a theoretical, not a practical, one.</p>
<p id="p0785" class="para_indented">If the concept description language permits statements involving logical <em>or</em>—that is, <em>disjunctions</em>—then any subset can be represented. If the description language is rule-based, disjunction can be achieved by using separate rules. For example, one possible concept representation is just to enumerate the examples:</p>
<p id="p0790" class="para_indented"><span class="monospace">If outlook = overcast and temperature = hot and humidity = high</span></p>
<p id="p0795" class="para_indented"><span class="monospace">and windy = false then play = yes</span></p>
<p id="p0800" class="para_indented"><span class="monospace">If outlook = rainy and temperature = mild and humidity = high</span></p>
<p id="p0805" class="para_indented"><span class="monospace">and windy = false then play = yes</span></p>
<p id="p0810" class="para_indented"><span class="monospace">If outlook = rainy and temperature = cool and humidity = normal</span></p>
<p id="p0815" class="para_indented"><span class="monospace">and windy = false then play = yes</span></p>
<p id="p0820" class="para_indented"><span class="monospace">If outlook = overcast and temperature = cool and humidity = normal</span></p>
<p id="p0825" class="para_indented"><span class="monospace">and windy = true then play = yes</span></p>
<p id="p0830" class="para_indented"><span class="monospace">…</span></p>
<p id="p0835" class="para_indented"><span class="monospace">If none of the above then play = no</span></p>
<p id="p0840" class="para_indented">This is not a particularly enlightening concept description: It simply records the positive examples that have been observed and assumes that all the rest are negative. Each positive example is given its own rule, and the concept is the disjunction of the rules. Alternatively, you could imagine having individual rules for each of the negative examples, too—an equally uninteresting concept. In either case, the concept description does not perform any generalization; it simply records the original data.</p>
<p id="p0845" class="para_indented">On the other hand, if disjunction is <em>not</em> allowed, some possible concepts—sets of examples—may not be able to be represented at all. In that case, a machine learning scheme may simply be unable to achieve good performance.</p>
<p id="p0850" class="para_indented"><a id="p32"></a>Another kind of language bias is that obtained from knowledge of the particular domain being used. For example, it may be that some combinations of attribute values can never happen. This would be the case if one attribute implied another. We saw an example of this when considering the rules for the soybean problem described in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0030">Section 1.2</a>. Then it would be pointless to even consider concepts that involved redundant or impossible combinations of attribute values. Domain knowledge can be used to cut down the search space. Knowledge is power: A little goes a long way, and even a small hint can reduce the search space dramatically.</p>
</div>
<div id="s0130">Search Bias<p id="p0855" class="noindent">In realistic data mining problems, there are many alternative concept descriptions that fit the data, and the problem is to find the “best” one according to some criterion—usually simplicity. We use the term <em>fit</em> in a statistical sense; we seek the best description that fits the data reasonably well. Moreover, it is often computationally infeasible to search the whole space and guarantee that the description found really is the best. Consequently, the search procedure is heuristic, and no guarantees can be made about the optimality of the final result. This leaves plenty of room for bias: Different search heuristics bias the search in different ways.</p>
<p id="p0860" class="para_indented">For example, a learning algorithm might adopt a “greedy” search for rules by trying to find the best rule at each stage and adding it to the rule set. However, it may be that the best <em>pair</em> of rules is not just the two rules that are individually found best. Or when building a decision tree, a commitment to split early on using a particular attribute might turn out later to be ill-considered in light of how the tree develops below that node. To get around these problems, a <em>beam search</em> could be used where irrevocable commitments are not made but instead a set of several active alternatives—the number of which is the <em>beam width</em>—are pursued in parallel. This will complicate the learning algorithm quite considerably but has the potential to avoid the myopia associated with a greedy search. Of course, if the beam width is not large enough, myopia may still occur. There are more complex search strategies that help to overcome this problem.</p>
<p id="p0865" class="para_indented">A more general and higher-level kind of search bias concerns whether the search is done by starting with a general description and refining it or by starting with a specific example and generalizing it. The former is called a <em>general-to-specific</em> search bias; the latter, a <em>specific-to-general</em> one. Many learning algorithms adopt the former policy, starting with an empty decision tree, or a very general rule, and specializing it to fit the examples. However, it is perfectly possible to work in the other direction. Instance-based methods start with a particular example and see how it can be generalized to cover other nearby examples in the same class.</p>
</div>
<div id="s0135">Overfitting-Avoidance Bias<p id="p0870" class="noindent">Overfitting-avoidance bias is often just another kind of search bias. However, because it addresses a rather special problem, we treat it separately. Recall the disjunction problem described previously. The problem is that if disjunction is allowed, useless concept descriptions that merely summarize the data become possible, whereas if it is prohibited, some concepts are unlearnable. To get around this problem, it is common to search the concept space starting with the simplest concept descriptions and proceeding to more complex ones: simplest-first ordering. This biases the search in favor of simple concept descriptions.</p>
<p id="p0875" class="para_indented">Using a simplest-first search and stopping when a sufficiently complex concept description is found is a good way of avoiding overfitting. It is sometimes called <em>forward pruning</em> or <em>prepruning</em> because complex descriptions are pruned away before they are reached. The alternative, <em>backward pruning</em> or <em>postpruning</em>, is also viable. Here, we first find a description that fits the data well and then prune it back to a simpler description that also fits the data. This is not as redundant as it sounds: Often the best way to arrive <a id="p33"></a>at a simple theory is to find a complex one and then simplify it. Forward and backward pruning are both a kind of overfitting-avoidance bias.</p>
<p id="p0880" class="para_indented">In summary, although generalization as search is a nice way to think about the learning problem, bias is the only way to make it feasible in practice. Different learning algorithms correspond to different concept description spaces searched with different biases. This is what makes it interesting: Different description languages and biases serve some problems well and other problems badly. There is no universal “best” learning method—as every teacher knows!</p>
</div>
</div>
</div>
<p></p>
</div>
<div id="s0140">
<h2 id="st0140">1.6 Data mining and ethics</h2>
<p id="p0885" class="noindent">The use of data—particularly data about people—for data mining has serious ethical implications, and practitioners of data mining techniques must act responsibly by making themselves aware of the ethical issues that surround their particular application.</p>
<p id="p0890" class="para_indented">When applied to people, data mining is frequently used to discriminate—who gets the loan, who gets the special offer, and so on. Certain kinds of discrimination—racial, sexual, religious, and so on—are not only unethical but also illegal. However, the situation is complex: Everything depends on the application. Using sexual and racial information for medical diagnosis is certainly ethical, but using the same information when mining loan payment behavior is not. Even when sensitive information is discarded, there is a risk that models will be built that rely on variables that can be shown to substitute for racial or sexual characteristics. For example, people frequently live in areas that are associated with particular ethnic identities, and so using a zip code in a data mining study runs the risk of building models that are based on race—even though racial information has been explicitly excluded from the data.</p>
<div id="s0145">
<h3 id="st0145">Reidentification</h3>
<p id="p0895" class="noindent">Recent work in what are being called <em>reidentification</em> techniques has provided sobering insights into the difficulty of anonymizing data. It turns out, for example, that over 85% of Americans can be identified from publicly available records using just three pieces of information: five-digit zip code, birth date (including year), and sex. Don’t know the zip code?—over half of Americans can be identified from just city, birth date, and sex. When the Commonwealth of Massachusetts released medical records summarizing every state employee’s hospital record in the mid-1990s, the governor gave a public assurance that it had been anonymized by removing all identifying information such as name, address, and social security number. He was surprised to receive his own health records (which included diagnoses and prescriptions) in the mail.</p>
<p id="p0900" class="para_indented">Stories abound of companies releasing allegedly anonymous data in good faith, only to find that many individuals are easily identifiable. In 2006, an Internet services company released to the research community the records of 20 million user searches. <a id="p34"></a>The records were anonymized by removing all personal information—or so the company thought. But pretty soon journalists from <em>The New York Times</em> were able to identify the actual person corresponding to user number 4417749 (they sought her permission before exposing her). They did so by analyzing the search terms she used, which included queries for landscapers in her hometown and for several people with the same last name as hers, which reporters correlated with public databases.</p>
<p id="p0905" class="para_indented">Two months later, Netflix, an online movie rental service, released 100 million records of movie ratings (from 1 to 5) with their dates. To their surprise, it turned out to be quite easy to identify people in the database and thus discover all the movies they had rated. For example, if you know approximately when (give or take two weeks) a person in the database rated six movies and you know the ratings, you can identify 99% of the people in the database. By knowing only two movies with their ratings and dates, give or take three days, nearly 70% of people can be identified. From just a little information about your friends (or enemies) you can determine all the movies they have rated on Netflix.</p>
<p id="p0910" class="para_indented">The moral is that if you really do remove all possible identification information from a database, you will probably be left with nothing useful.</p>
</div>
<div id="s0150">
<h3 id="st0150">Using Personal Information</h3>
<p id="p0915" class="noindent">It is widely accepted that before people make a decision to provide personal information they need to know how it will be used and what it will be used for, what steps will be taken to protect its confidentiality and integrity, what the consequences of supplying or withholding the information are, and any rights of redress they may have. Whenever such information is collected, individuals should be told these things—not in legalistic small print but straightforwardly in plain language they can understand.</p>
<p id="p0920" class="para_indented">The potential use of data mining techniques means that the ways in which a repository of data can be used may stretch far beyond what was conceived when the data was originally collected. This creates a serious problem: It is necessary to determine the conditions under which the data was collected and for what purposes it may be used. Does the ownership of data bestow the right to use it in ways other than those purported when it was originally recorded? Clearly, in the case of explicitly collected personal data, it does not. But in general the situation is complex.</p>
<p id="p0925" class="para_indented">Surprising things emerge from data mining. For example, it has been reported that one of the leading consumer groups in France has found that people with red cars are more likely to default on their car loans. What is the status of such a “discovery”? What information is it based on? Under what conditions was that information collected? In what ways is it ethical to use it? Clearly, insurance companies are in the business of discriminating among people based on stereotypes—young males pay heavily for automobile insurance—but such stereotypes are not based solely on statistical correlations; they also draw on commonsense knowledge about the world as well. Whether the preceding finding says something about the kind of person who chooses a red car, or whether it should be discarded as an irrelevancy, is a matter <a id="p35"></a>for human judgment based on knowledge of the world rather than on purely statistical criteria.</p>
<p id="p0930" class="para_indented">When presented with data, you need to ask who is permitted to have access to it, for what purpose it was collected, and what kind of conclusions are legitimate to draw from it. The ethical dimension raises tough questions for those involved in practical data mining. It is necessary to consider the norms of the community that is used to dealing with the kind of data involved, standards that may have evolved over decades or centuries but ones that may not be known to the information specialist. For example, did you know that in the library community it is taken for granted that the privacy of readers is a right that is jealously protected? If you call your university library and ask who has such-and-such a textbook out on loan, they will not tell you. This prevents a student being subjected to pressure from an irate professor to yield access to a book that she desperately needs for her latest grant application. It also prohibits enquiry into the dubious recreational reading tastes of the university ethics committee chairperson. Those who build, say, digital libraries may not be aware of these sensitivities and might incorporate data mining systems that analyze and compare individuals’ reading habits to recommend new books—perhaps even selling the results to publishers!</p>
</div>
<div id="s0155">
<h3 id="st0155">Wider Issues</h3>
<p id="p0935" class="noindent">In addition to various community standards for the use of data, logical and scientific standards must be adhered to when drawing conclusions from it. If you do come up with conclusions (e.g., red car owners being greater credit risks), you need to attach caveats to them and back them up with arguments other than purely statistical ones. The point is that data mining is just a tool in the whole process. It is people who take the results, along with other knowledge, and decide what action to apply.</p>
<p id="p0940" class="para_indented">Data mining prompts another question, which is really a political one concerning the use to which society’s resources are being put. We mentioned earlier the application of data mining to basket analysis, where supermarket checkout records are analyzed to detect associations among items that people purchase. What use should be made of the resulting information? Should the supermarket manager place the beer and chips together, to make it easier for shoppers, or farther apart to make it less convenient for them, to maximize their time in the store and therefore their likelihood of being drawn into further purchases? Should the manager move the most expensive, most profitable diapers near the beer, increasing sales to harried fathers of a high-margin item, and add further luxury baby products nearby?</p>
<p id="p0945" class="para_indented">Of course, anyone who uses advanced technologies should consider the wisdom of what they are doing. If <em>data</em> is characterized as recorded facts, then <em>information</em> is the set of patterns, or expectations, that underlie the data. You could go on to define <em>knowledge</em> as the accumulation of your set of expectations and <em>wisdom</em> as the value attached to knowledge. Although we will not pursue it further here, this issue is worth pondering.</p>
<p id="p0950" class="para_indented"><a id="p36"></a>As we saw at the very beginning of this chapter, the techniques described in this book may be called upon to help make some of the most profound and intimate decisions that life presents. Data mining is a technology that we need to take seriously.</p>
</div>
</div>
<div id="s0160">
<h2 id="st0160">1.7 Further reading</h2>
<p id="p0955" class="noindent">To avoid breaking up the flow of the main text, all references are collected in a section at the end of each chapter. This section describes papers, books, and other resources relevant to the material covered in this chapter. The human in vitro fertilization research mentioned in the opening was undertaken by the Oxford University Computing Laboratory, and the research on cow culling was performed in the Computer Science Department at Waikato University, New Zealand.</p>
<p id="p0960" class="para_indented">The weather problem is from <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib255">Quinlan (1986)</a> and has been widely used to explain machine learning schemes. The corpus of example problems mentioned in the introduction to <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0030">Section 1.2</a> is available from <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib12">Asuncion and Newman (2007)</a>. The contact lens example is from <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib53">Cendrowska (1987)</a>, who introduced the PRISM rule-learning algorithm that we will encounter in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapter 4</a>. The iris dataset was described in a classic early paper on statistical inference (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib100">Fisher, 1936</a>). The labor negotiations data is from the <em>Collective Bargaining Review</em>, a publication of Labour Canada issued by the Industrial Relations Information Service (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib28">BLI 1988</a>), and the soybean problem was first described by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib225">Michalski and Chilausky (1980)</a>.</p>
<p id="p0965" class="para_indented">Some of the applications in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0065">Section 1.3</a> are covered in an excellent paper that gives plenty of other applications of machine learning and rule induction (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib203">Langley and Simon, 1995</a>); another source of fielded applications is a special issue of the <em>Machine Learning Journal</em> (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib189">Kohavi and Provost, 1998</a>). <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib54">Chakrabarti (2003)</a> has written an excellent and comprehensive book on techniques of web mining; another, more recent, book is <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib212">Liu’s <em>Web data mining</em> (2009)</a>. The loan company application is described in more detail by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib226">Michie (1989)</a>, the oil slick detector is from <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib195">Kubat et al. (1998)</a>, the electric load forecasting work is by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib166">Jabbour et al. (1988)</a>, and the application to preventative maintenance of electromechanical devices is from <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib272">Saitta and Neri (1998</a>). Fuller descriptions of some of the other projects mentioned in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0065">Section 1.3</a> (including the figures of dollar amounts saved and related literature references) appear at the web site of the Alberta Ingenuity Centre for Machine Learning. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib215">Luan (2002)</a> describes applications for data mining in higher education. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib70">Dasu et al. (2006)</a> have some recommendations for successful data mining. Another special issue of the <em>Machine Learning Journal</em> addresses the lessons that have been learned from data mining applications and collaborative problem solving (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib204">Lavrac et al., 2004</a>).</p>
<p id="p0970" class="para_indented">The “diapers and beer” story is legendary. According to an article in London’s <em>Financial Times</em> (February 7, 1996),</p><a id="p0975"></a><div class="boxg" id="b0015">
<p id="p0980" class="noindent"><em>The oft-quoted example of what data mining can achieve is the case of a large US supermarket chain which discovered a strong association for many customers between a brand of babies’ nappies (diapers) and a brand of beer. Most customers who bought the nappies also bought the beer. The best hypothesisers in the world would find it difficult to propose this combination but data mining showed it existed, and the retail outlet was able to exploit it by moving the products closer together on the shelves.</em></p>
</div>
<p></p>
<p id="p0985" class="para_indented"><a id="p37"></a>However, it seems that it is just a legend after all; <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib252">Power (2002)</a> traces its history.</p>
<p id="p0990" class="para_indented">The book <em>Classification and regression trees</em>, mentioned in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#s0105">Section 1.4</a>, is by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib41">Breiman et al. (1984)</a>, and Quinlan’s independently derived but similar scheme was described in a series of papers that eventually led to a book (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib257">Quinlan, 1993</a>).</p>
<p id="p0995" class="para_indented">The first book on data mining was written by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib250">Piatetsky-Shapiro and Frawley (1991)</a>—a collection of papers presented at a workshop on knowledge discovery in databases in the late 1980s. Another book from the same stable has appeared since (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib98">Fayyad et al., 1996</a>) from a 1994 workshop. There followed a rash of business-oriented books on data mining, focusing mainly on practical aspects of how it can be put into practice with only rather superficial descriptions of the technology that underlies the methods used. They are valuable sources of applications and inspiration. For example, <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib2">Adriaans and Zantige (1996)</a> from Syllogic, a European systems and database consultancy, is an early introduction to data mining. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib22">Berry and Linoff (1997)</a>, from a Pennsylvania-based firm specializing in data warehousing and data mining, give an excellent and example-studded review of data mining techniques for marketing, sales, and customer support. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib48">Cabena et al. (1998)</a>, written by people from five international IBM laboratories, contains an overview of the data mining process with many examples of real-world applications.</p>
<p id="p9025" class="para_indented"><a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib76">Dhar and Stein (1997)</a> give a business perspective on data mining and include broad-brush, popularized reviews of many of the technologies involved. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib142">Groth (1998)</a>, working for a provider of data mining software, gives a brief introduction to data mining and then a fairly extensive review of data mining software products; the book includes a CD-ROM containing a demo version of his company’s product. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib315">Weiss and Indurkhya (1998)</a> look at a wide variety of statistical techniques for making predictions from what they call “big data.” <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib151">Han and Kamber (2006)</a> cover data mining from a database perspective, focusing on the discovery of knowledge in large corporate databases; they also discuss mining complex types of data. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib152">Hand et al. (2001)</a> produced an interdisciplinary book on data mining from an international group of authors who are well respected in the field. Finally, <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib243">Nisbet et al. (2009)</a> have produced a comprehensive handbook of statistical analysis and data mining applications.</p>
<p id="p1000" class="para_indented">Books on machine learning, on the other hand, tend to be academic texts suited for use in university courses rather than as practical guides. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib228">Mitchell (1997)</a> wrote an excellent book that covers many techniques of machine learning, including some—notably genetic algorithms and reinforcement learning—that are not covered here. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib200">Langley (1996)</a> offers another good text. Although the previously mentioned book by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib257">Quinlan (1993)</a> concentrates on a particular learning algorithm, C4.5, which we will cover in detail in <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#c0004">Chapters 4</a> and 6, it is a good introduction to some of the problems and techniques of machine learning. An absolutely excellent book on <a id="p38"></a>machine learning from a statistical perspective is <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib156">Hastie et al. (2009)</a>. This is quite a theoretically oriented work, and is beautifully produced with apt and telling figures. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib270">Russell and Norvig’s <em>Artificial intelligence: A modern approach</em> (2009)</a> is the third edition of a classic text that includes a great deal of information on machine learning and data mining.</p>
<p id="p1005" class="para_indented">Pattern recognition is a topic that is closely related to machine learning, and many of the same techniques apply. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib90">Duda et al. (2001)</a> is the second edition of a classic and successful book on pattern recognition (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib89">Duda and Hart, 1973</a>). <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib266">Ripley (1996)</a> and <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib26">Bishop (1995)</a> describe the use of neural networks for pattern recognition; <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib26">Bishop has a more recent book, <em>Pattern recognition and machine learning</em> (2006)</a>. Data mining with neural networks is the subject of a 1996 book by Bigus of IBM, which features the IBM Neural Network Utility Product that he developed.</p>
<p id="p1010" class="para_indented">There is a great deal of current interest in support vector machines. <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib67">Cristianini and Shawe-Taylor (2000)</a> give a nice introduction, and a follow-up work generalizes this to cover additional algorithms, kernels, and solutions with applications to pattern discovery problems in fields such as bioinformatics, text analysis, and image analysis (<a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib284">Shawe-Taylor and Cristianini, 2004</a>). <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib278">Schölkopf and Smola (2002)</a> provide a comprehensive introduction to support vector machines and related kernel methods by two young researchers who did their Ph.D. research in this rapidly developing area.</p>
<p id="p1015" class="para_indented">The emerging area of reidentification techniques is explored, along with its implications for anonymization, by <a href="/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib245">Ohm (2009)</a>.</p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/library/view/data-mining-practical/9780123748560/xhtml/p1.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">PART I. Introduction to Data Mining</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0002.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 2. Input</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.6411935936073236"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.8086058652767119" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=a666b194-d4ae-68fd-359f-3f91196a7271&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Chapter%201.%20What%E2%80%99s%20It%20All%20About%3F%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fxhtml%2Fc0001.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=321448"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#">Reset</a>
</div>
</div></body></html>