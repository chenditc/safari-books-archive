<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0004.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/xhtml/c0004.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0004.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Chapter 4. Algorithms - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html"><meta name="description" content="CHAPTER 4 Algorithms: The Basic Methods Now that we’ve seen how the inputs and outputs can be represented, it’s time to look at the learning algorithms themselves. This ... "><meta property="og:title" content="Chapter 4. Algorithms"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Chapter 4. Algorithms"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0004.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content="CHAPTER 4 Algorithms: The Basic Methods Now that we’ve seen how the inputs and outputs can be represented, it’s time to look at the learning algorithms themselves. This ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/xhtml/c0004.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0004.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0004.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0004.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Chapter%204.%20Algorithms&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/xhtml/c0004.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 3. Output</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 5. Credibility</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="c0004"></a><h1 class="chapterlabel" id="c0004tit1">CHAPTER 4</h1>
<h1 class="chaptertitle" id="c0004tit">Algorithms: The Basic Methods</h1><p id="p0010" class="noindent"><a id="p85"></a>Now that we’ve seen how the inputs and outputs can be represented, it’s time to look at the learning algorithms themselves. This chapter explains the basic ideas behind the techniques that are used in practical data mining. We will not delve too deeply into the trickier issues—advanced versions of the algorithms, optimizations that are possible, complications that arise in practice. These topics are deferred to <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a>, where we come to grips with real implementations of machine learning schemes such as the ones included in data mining toolkits and used for real-world applications. It is important to understand these more advanced issues so that you know what is really going on when you analyze a particular dataset.</p>
<p id="p0015" class="para_indented">In this chapter we look at the basic ideas. One of the most instructive lessons is that simple ideas often work very well, and we strongly recommend the adoption of a “simplicity-first” methodology when analyzing practical datasets. There are many different kinds of simple structure that datasets can exhibit. In one dataset, there might be a single attribute that does all the work and the others are irrelevant or redundant. In another dataset, the attributes might contribute independently and equally to the final outcome. A third might have a simple logical structure, involving just a few attributes, which can be captured by a decision tree. In a fourth, there may be a few independent rules that govern the assignment of instances to different classes. A fifth might exhibit dependencies among different subsets of attributes. A sixth might involve linear dependence among numeric attributes, where what matters is a weighted sum of attribute values with appropriately chosen weights. In a seventh, classifications appropriate to particular regions of instance space might be governed by the distances between the instances themselves. And in an eighth, it might be that no class values are provided: The learning is unsupervised.</p>
<p id="p0020" class="para_indented">In the infinite variety of possible datasets there are many different kinds of structures that can occur, and a data mining tool—no matter how capable—that is looking for one class of structure may completely miss regularities of a different kind, regardless of how rudimentary those may be. The result is a baroque and opaque classification structure of one kind instead of a simple, elegant, immediately comprehensible structure of another.</p>
<p id="p0025" class="para_indented">Each of the eight examples of different kinds of datasets just sketched leads to a different machine learning scheme that is well suited to discovering the underlying concept. The sections of this chapter look at each of these structures in turn. A final <a id="p86"></a>section introduces simple ways of dealing with multi-instance problems, where each example comprises several different instances.</p>
<div id="s0010">
<h2 id="st0010">4.1 Inferring rudimentary rules</h2>
<p id="p0030" class="noindent">Here’s an easy way to find very simple classification rules from a set of instances. Called <em>1R</em> for <em>1-rule</em>, it generates a one-level decision tree expressed in the form of a set of rules that all test one particular attribute. 1R is a simple, cheap method that often comes up with quite good rules for characterizing the structure in data. It turns out that simple rules frequently achieve surprisingly high accuracy. Perhaps this is because the structure underlying many real-world datasets is quite rudimentary, and just one attribute is sufficient to determine the class of an instance quite accurately. In any event, it is always a good plan to try the simplest things first.</p>
<p id="p0035" class="para_indented">The idea is this: We make rules that test a single attribute and branch accordingly. Each branch corresponds to a different value of the attribute. It is obvious what is the best classification to give each branch: Use the class that occurs most often in the training data. Then the error rate of the rules can easily be determined. Just count the errors that occur on the training data—that is, the number of instances that do not have the majority class.</p>
<p id="p0040" class="para_indented">Each attribute generates a different set of rules, one rule for every value of the attribute. Evaluate the error rate for each attribute’s rule set and choose the best. It’s that simple! <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0010">Figure 4.1</a> shows the algorithm in the form of pseudocode.</p>
<p id="f0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-001-9780123748560.jpg" alt="image" width="449" height="114" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-001-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.1</span> Pseudocode for 1R.</p>
<p id="p0045" class="para_indented">To see the 1R method at work, consider the weather data of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a> on <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p10">page 10</a> (we will encounter it many times again when looking at how learning algorithms work). To classify on the final column, <em>play</em>, 1R considers four sets of rules, one for each attribute. These rules are shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0010">Table 4.1</a>. An asterisk indicates that a random choice has been made between two equally likely outcomes. The number of errors is given for each rule, along with the total number of errors for the rule set as a whole. 1R chooses the attribute that produces rules with the smallest number of <a id="p87"></a>errors—that is, the first and third rule sets. Arbitrarily breaking the tie between these two rule sets gives</p>
<p class="table_caption"><span class="tab_num">Table 4.1. </span> Evaluating Attributes in the Weather Data</p>
<p id="t0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0010.jpg" alt="Image" width="560" height="242" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0010.jpg"></p>
<p class="table_source">*A random choice has been made between two equally likely outcomes.</p>
<p id="p0050" class="noindent"><span class="monospace">outlook: sunny → no</span></p>
<p id="p0055" class="para_indented"><span class="monospace"> overcast → yes</span></p>
<p id="p0060" class="para_indented"><span class="monospace"> rainy → yes</span></p>
<p id="p0065" class="para_indented">We noted at the outset that the game for the weather data is unspecified. Oddly enough, it is apparently played when it is overcast or rainy but not when it is sunny. Perhaps it’s an indoor pursuit.</p>
<div id="s0015">
<h3 id="st0015">Missing Values and Numeric Attributes</h3>
<p id="p0070" class="noindent">Although a very rudimentary learning scheme, 1R does accommodate both missing values and numeric attributes. It deals with these in simple but effective ways. <em>Missing</em> is treated as just another attribute value so that, for example, if the weather data had contained missing values for the <em>outlook</em> attribute, a rule set formed on <em>outlook</em> would specify four possible class values, one for each of <em>sunny</em>, <em>overcast</em>, and <em>rainy</em>, and a fourth for <em>missing</em>.</p>
<p id="p0075" class="para_indented">We can convert numeric attributes into nominal ones using a simple discretization method. First, sort the training examples according to the values of the numeric attribute. This produces a sequence of class values. For example, sorting the numeric version of the weather data (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p11">page 11</a>) according to the values of <em>temperature</em> produces the sequence</p><a id="p0080"></a>
<p id="t0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0015.jpg" alt="Image" width="448" height="22" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0015.jpg"></p>
<p id="p0085" class="para_indented">Discretization involves partitioning this sequence by placing breakpoints in it. One possibility is to place breakpoints wherever the class changes, producing the following eight categories:</p>
<p id="p0090" class="noindent"><span class="monospace">yes | no | yes yes yes | no no | yes yes yes | no | yes yes | no</span></p>
<p id="p0095" class="noindent"><a id="p88"></a>Choosing breakpoints halfway between the examples on either side places them at 64.5, 66.5, 70.5, 72, 77.5, 80.5, and 84. However, the two instances with value 72 cause a problem because they have the same value of <em>temperature</em> but fall into different classes. The simplest fix is to move the breakpoint at 72 up one example, to 73.5, producing a mixed partition in which <em>no</em> is the majority class.</p>
<p id="p0100" class="para_indented">A more serious problem is that this procedure tends to form an excessively large number of categories. The 1R method will naturally gravitate toward choosing an attribute that splits into many categories, because this will partition the dataset into many pieces, making it more likely that instances will have the same class as the majority in their partition. In fact, the limiting case is an attribute that has a different value for each instance—that is, an <em>identification code</em> attribute that pinpoints instances uniquely—and this will yield a zero error rate on the training set because each partition contains just one instance. Of course, highly branching attributes do not usually perform well on test examples; indeed, the identification code attribute will never get any examples outside the training set correct. This phenomenon is known as <em>overfitting</em>; we have already described overfitting-avoidance bias in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#c0001">Chapter 1</a>, and we will encounter this problem repeatedly in subsequent chapters.</p>
<p id="p0105" class="para_indented">For 1R, overfitting is likely to occur whenever an attribute has a large number of possible values. Consequently, when discretizing a numeric attribute, a minimum limit is imposed on the number of examples of the majority class in each partition. Suppose that minimum is set at 3. This eliminates all but two of the preceding partitions. Instead, the partitioning process begins</p>
<p id="p0110" class="noindent"><span class="monospace">yes no yes yes | yes  …</span></p>
<p id="p0115" class="noindent">ensuring that there are three occurrences of <em>yes</em>, the majority class, in the first partition. However, because the next example is also <em>yes</em>, we lose nothing by including that in the first partition, too. This leads to a new division of</p>
<p id="p0120" class="noindent"><span class="monospace">yes no yes yes yes | no no yes yes yes | no yes yes no</span></p>
<p id="p0125" class="noindent">where each partition contains at least three instances of the majority class, except the last one, which will usually have less. Partition boundaries always fall between examples of different classes.</p>
<p id="p0130" class="para_indented">Whenever adjacent partitions have the same majority class, as do the first two partitions above, they can be merged together without affecting the meaning of the rule sets. Thus, the final discretization is</p>
<p id="p0135" class="noindent"><span class="monospace">yes no yes yes yes no no yes yes yes | no yes yes no</span></p>
<p id="p0140" class="noindent">which leads to the rule set</p>
<p id="p0145" class="noindent"><span class="monospace">temperature: ≤ 77.5 → yes</span></p>
<p id="p0150" class="para_indented"><span class="monospace"> &gt; 77.5 → no</span></p>
<p id="p0155" class="para_indented"><a id="p89"></a>The second rule involved an arbitrary choice; as it happens, <em>no</em> was chosen. If <em>yes</em> had been chosen instead, there would be no need for any breakpoint at all—and as this example illustrates, it might be better to use the adjacent categories to help break ties. In fact, this rule generates five errors on the training set and so is less effective than the preceding rule for <em>outlook</em>. However, the same procedure leads to this rule for <em>humidity</em>:</p>
<p id="p0160" class="noindent"><span class="monospace">humidity: ≤ 82.5 → yes</span></p>
<p id="p0165" class="para_indented"><span class="monospace"> &gt; 82.5 and ≤ 95.5 → no</span></p>
<p id="p0170" class="para_indented"><span class="monospace"> &gt; 95.5 → yes</span></p>
<p id="p0175" class="noindent">This generates only three errors on the training set and is the best 1-rule for the data in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>.</p>
<p id="p0180" class="para_indented">Finally, if a numeric attribute has missing values, an additional category is created for them, and the discretization procedure is applied just to the instances for which the attribute’s value is defined.</p>
</div>
<div id="s0020">
<h3 id="st0020">Discussion</h3>
<p id="p0185" class="noindent">In a seminal paper entitled “Very simple classification rules perform well on most commonly used datasets” (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib164">Holte, 1993</a>), a comprehensive study of the performance of the 1R procedure was reported on 16 datasets frequently used by machine learning researchers to evaluate their algorithms. <em>Cross-validation</em>, an evaluation technique that we will explain in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#c0005">Chapter 5</a>, was used to ensure that the results were the same as would be obtained on independent test sets. After some experimentation, the minimum number of examples in each partition of a numeric attribute was set at six, not three as used in our illustration.</p>
<p id="p0190" class="para_indented">Surprisingly, despite its simplicity 1R did well in comparison with the state-of-the-art learning schemes, and the rules it produced turned out to be just a few percentage points less accurate, on almost all of the datasets, than the decision trees produced by a state-of-the-art decision tree induction scheme. These trees were, in general, considerably larger than 1R’s rules. Rules that test a single attribute are often a viable alternative to more complex structures, and this strongly encourages a simplicity-first methodology in which the baseline performance is established using simple, rudimentary techniques before progressing to more sophisticated learning schemes, which inevitably generate output that is harder for people to interpret.</p>
<p id="p0195" class="para_indented">The 1R procedure learns a one-level decision tree whose leaves represent the various different classes. A slightly more expressive technique is to use a different rule for each class. Each rule is a conjunction of tests, one for each attribute. For numeric attributes the test checks whether the value lies within a given interval; for nominal ones it checks whether it is in a certain subset of that attribute’s values. These two types of tests—that is, intervals and subsets—are learned from the training data pertaining to each of the classes. For a numeric attribute, the end <a id="p90"></a>points of the interval are the minimum and the maximum values that occur in the training data for that class. For a nominal one, the subset contains just those values that occur for that attribute in the training data for the individual class. Rules representing different classes usually overlap, and at prediction time the one with the most matching tests is predicted. This simple technique often gives a useful first impression of a dataset. It is extremely fast and can be applied to very large quantities of data.</p>
</div>
</div>
<div id="s0025">
<h2 id="st0025">4.2 Statistical modeling</h2>
<p id="p0200" class="noindent">The 1R method uses a single attribute as the basis for its decisions and chooses the one that works best. Another simple technique is to use all attributes and allow them to make contributions to the decision that are <em>equally important</em> and <em>independent</em> of one another, given the class. This is unrealistic, of course: What makes real-life datasets interesting is that the attributes are certainly not equally important or independent. But it leads to a simple scheme that, again, works surprisingly well in practice.</p>
<p id="p0205" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0020">Table 4.2</a> shows a summary of the weather data obtained by counting how many times each attribute–value pair occurs with each value (<em>yes</em> and <em>no</em>) for <em>play</em>. For example, you can see from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p10">page 10</a>) that <em>outlook</em> is <em>sunny</em> for five examples, two of which have <em>play</em> = <em>yes</em> and three of which have <em>play</em> = <em>no</em>. The cells in the first row of the new table simply count these occurrences for all possible values of each attribute, and the <em>play</em> figure in the final column counts the total number of occurrences of <em>yes</em> and <em>no</em>. The lower part of the table contains the same information expressed as fractions, or observed probabilities. For example, of the nine days that <em>play</em> is <em>yes, outlook</em> is <em>sunny</em> for two, yielding a fraction of 2/9. For <em>play</em> the fractions are different: They are the proportion of days that <em>play</em> is <em>yes</em> and <em>no</em>, respectively.</p><a id="p91"></a><p class="table_caption"><span class="tab_num">Table 4.2. </span> Weather Data with Counts and Probabilities</p>
<p id="t0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0020.jpg" alt="Image" width="872" height="194" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0020.jpg"></p>
<p id="p0210" class="para_indented">Now suppose we encounter a new example with the values that are shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0025">Table 4.3</a>. We treat the five features in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0020">Table 4.2</a>—<em>outlook</em>, <em>temperature</em>, <em>humidity</em>, <em>windy</em>, and the overall likelihood that <em>play</em> is <em>yes</em> or <em>no</em>—as equally important, independent pieces of evidence and multiply the corresponding fractions. Looking at the outcome <em>yes</em> gives</p>
<p class="figure" id="e0010"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si1.jpg" alt="image" width="477" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si1.jpg"></p>
<a id="p92"></a><p class="table_caption"><span class="tab_num">Table 4.3. </span> A New Day</p>
<p id="t0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0025.jpg" alt="Image" width="559" height="55" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0025.jpg"></p>
<p id="p0215" class="para_indented">The fractions are taken from the <em>yes</em> entries in the table according to the values of the attributes for the new day, and the final 9/14 is the overall fraction representing the proportion of days on which <em>play</em> is <em>yes</em>. A similar calculation for the outcome <em>no</em> leads to</p>
<p class="figure" id="e0015"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si2.jpg" alt="image" width="469" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si2.jpg"></p>
<p id="p0220" class="noindent">This indicates that for the new day, <em>no</em> is more likely than <em>yes</em>—four times more likely. The numbers can be turned into probabilities by normalizing them so that they sum to 1:</p>
<p class="figure" id="e0020"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si3.jpg" alt="image" width="387" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si3.jpg"></p>
<p class="figure" id="e0025"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si4.jpg" alt="image" width="381" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si4.jpg"></p>
<p id="p0225" class="noindent">This simple and intuitive method is based on Bayes’ rule of conditional probability. Bayes’ rule says that if you have a hypothesis <em>H</em> and evidence <em>E</em> that bears on that hypothesis, then</p>
<p class="figure" id="e0030"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si5.jpg" alt="image" width="237" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si5.jpg"></p>
<p id="p0230" class="para_indented">We use the notation that Pr[<em>A</em>] denotes the probability of an event <em>A</em> and Pr[<em>A</em> | <em>B</em>] denotes the probability of <em>A</em> conditional on another event <em>B</em>. The hypothesis <em>H</em> is that <em>play</em> will be, say, <em>yes</em>, and Pr[<em>H</em> | <em>E</em>] is going to turn out to be 20.5%, just as determined previously. The evidence <em>E</em> is the particular combination of attribute values for the new day—<em>outlook</em> = <em>sunny</em>, <em>temperature</em> = <em>cool</em>, <em>humidity</em> = <em>high</em>, and <em>windy</em> = <em>true</em>. Let’s call these four pieces of evidence <em>E</em><span class="sub">1</span>, <em>E</em><span class="sub">2</span>, <em>E</em><span class="sub">3</span>, and <em>E</em><span class="sub">4</span>, respectively. Assuming that these pieces of evidence are independent (given the class), their combined probability is obtained by multiplying the probabilities:</p>
<p class="figure" id="e0035"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si6.jpg" alt="image" width="627" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si6.jpg"></p>
<p id="p0235" class="para_indented">Don’t worry about the denominator: We will ignore it and eliminate it in the final normalizing step when we make the probabilities for <em>yes</em> and <em>no</em> sum to 1, just as we did previously. The Pr[<em>yes</em>] at the end is the probability of a <em>yes</em> outcome without knowing any of the evidence <em>E</em>—that is, without knowing anything about the particular day in question—and it’s called the <em>prior probability</em> of the hypothesis <em>H</em>. In this case, it’s just 9/14, because 9 of the 14 training examples had a <em>yes</em><a id="p93"></a>value for <em>play</em>. Substituting the fractions in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0020">Table 4.2</a> for the appropriate evidence probabilities leads to</p>
<p class="figure" id="e0040"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si7.jpg" alt="image" width="342" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si7.jpg"></p>
<p class="noindent">just as we calculated previously. Again, the Pr[<em>E</em>] in the denominator will disappear when we normalize.</p>
<p id="p0240" class="para_indented">This method goes by the name of <em>Naïve Bayes</em> because it’s based on Bayes’ rule and “naïvely” assumes independence—it is only valid to multiply probabilities when the events are independent. The assumption that attributes are independent (given the class) in real life certainly is a simplistic one. But despite the disparaging name, Naïve Bayes works very effectively when tested on actual datasets, particularly when combined with some of the attribute selection procedures, which are introduced in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#c0007">Chapter 7</a>, that eliminate redundant, and hence nonindependent, attributes.</p>
<p id="p0245" class="para_indented">Things go badly awry in Naïve Bayes if a particular attribute value does not occur in the training set in conjunction with <em>every</em> class value. Suppose that in the training data the attribute value <em>outlook</em> = <em>sunny</em> was always associated with the outcome <em>no</em>. Then the probability of <em>outlook</em> = <em>sunny</em> being given a <em>yes</em>—that is, Pr[<em>outlook</em> = <em>sunny</em> | <em>yes</em>]—would be zero, and because the other probabilities are multiplied by this, the final probability of <em>yes</em> in the previous example would be zero no matter how large they were. Probabilities that are zero hold a veto over the other ones. This is not a good idea. But the bug is easily fixed by minor adjustments to the method of calculating probabilities from frequencies.</p>
<p id="p0250" class="para_indented">For example, the upper part of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0020">Table 4.2</a> shows that for <em>play = yes</em>, <em>outlook</em> is <em>sunny</em> for two examples, <em>overcast</em> for four, and <em>rainy</em> for three, and the lower part gives these events probabilities of 2/9, 4/9, and 3/9, respectively. Instead, we could add 1 to each numerator, and compensate by adding 3 to the denominator, giving probabilities of 3/12, 5/12, and 4/12, respectively. This will ensure that an attribute value that occurs zero times receives a probability which is nonzero, albeit small. The strategy of adding 1 to each count is a standard technique called the <em>Laplace estimator</em> after the great eighteenth-century French mathematician Pierre Laplace. Although it works well in practice, there is no particular reason for adding 1 to the counts: We could instead choose a small constant µ and use</p>
<p class="figure" id="e0045"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si8.jpg" alt="image" width="288" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si8.jpg"></p>
<p id="p0255" class="para_indented">The value of <em>µ</em>, which was set to 3 before, effectively provides a weight that determines how influential the a priori values of 1/3, 1/3, and 1/3 are for each of the three possible attribute values. A large <em>µ</em> says that these priors are very important compared with the new evidence coming in from the training set, whereas a small one gives them less influence. Finally, there is no particular reason for dividing <em>µ</em> into three <em>equal</em> parts in the numerators: We could use</p>
<p class="figure" id="e0050"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si9.jpg" alt="image" width="292" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si9.jpg"></p>
<p class="noindent"><a id="p94"></a>instead, where <em>p</em><span class="sub">1</span>, <em>p</em><span class="sub">2</span>, and <em>p</em><span class="sub">3</span> sum to 1. Effectively, these three numbers are a priori probabilities of the values of the <em>outlook</em> attribute being <em>sunny</em>, <em>overcast</em>, and <em>rainy</em>, respectively.</p>
<p id="p0260" class="para_indented">This is now a fully Bayesian formulation where prior probabilities have been assigned to everything in sight. It has the advantage of being completely rigorous, but the disadvantage that it is not usually clear just how these prior probabilities should be assigned. In practice, the prior probabilities make little difference provided that there are a reasonable number of training instances, and people generally just estimate frequencies using the Laplace estimator by initializing all counts to 1 instead of 0.</p>
<div id="s0030">
<h3 id="st0030">Missing Values and Numeric Attributes</h3>
<p id="p0265" class="noindent">One of the really nice things about Naïve Bayes is that missing values are no problem at all. For example, if the value of <em>outlook</em> were missing in the example of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0025">Table 4.3</a>, the calculation would simply omit this attribute, yielding</p>
<p class="figure" id="e0055"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si10.jpg" alt="image" width="431" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si10.jpg"></p>
<p class="figure" id="e0060"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si11.jpg" alt="image" width="423" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si11.jpg"></p>
<p id="p0270" class="para_indented">These two numbers are individually a lot higher than they were before because one of the fractions is missing. But that’s not a problem because a fraction is missing in both cases, and these likelihoods are subject to a further normalization process. This yields probabilities for <em>yes</em> and <em>no</em> of 41% and 59%, respectively.</p>
<p id="p0275" class="para_indented">If a value is missing in a training instance, it is simply not included in the frequency counts, and the probability ratios are based on the number of values that actually occur rather than on the total number of instances.</p>
<p id="p0280" class="para_indented">Numeric values are usually handled by assuming that they have a “normal” or “Gaussian” probability distribution. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0030">Table 4.4</a> gives a summary of the weather data with numeric features from <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0020">Table 1.3</a>. For nominal attributes, we calculate counts as before, while for numeric ones we simply list the values that occur. Then, instead of normalizing counts into probabilities as we do for nominal attributes, we calculate the mean and the standard deviation for each class and each numeric attribute. The mean value of <em>temperature</em> over the <em>yes</em> instances is 73, and its standard deviation is 6.2. The mean is simply the average of the values—that is, the sum divided by the number of values. The standard deviation is the square root of the sample variance, which we calculate as follows: Subtract the mean from each value, square the result, sum them together, and then divide by <em>one less than</em> the number of values. After we have found this “sample variance,” take its square root to yield the standard deviation. This is the standard way of calculating the mean and the standard deviation of a set of numbers. (The “one less than” has to do with the number of degrees of freedom in the sample, a statistical notion that we don’t want to get into here.)</p><a id="p95"></a><p class="table_caption"><span class="tab_num">Table 4.4. </span> Numeric Weather Data with Summary Statistics</p>
<p id="t0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0030.jpg" alt="Image" width="900" height="297" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0030.jpg"></p>
<p id="p0285" class="para_indented"><a id="p96"></a>The probability density function for a normal distribution with mean <em>µ</em> and standard deviation <em>σ</em> is given by the rather formidable expression</p>
<p class="figure" id="e0065"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si12.jpg" alt="image" width="183" height="67" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si12.jpg"></p>
<p id="p0290" class="noindent">But fear not! All this means is that if we are considering a <em>yes</em> outcome when <em>temperature</em> has a value of, say, 66, we just need to plug <em>x</em> = 66, <em>µ</em> = 73, and <em>σ</em> = 6.2 into the formula. So the value of the probability density function is</p>
<p class="figure" id="e0070"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si13.jpg" alt="image" width="477" height="67" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si13.jpg"></p>
<p id="p0295" class="noindent">And by the same token, the probability density of a <em>yes</em> outcome when <em>humidity</em> has a value of, say, 90, is calculated in the same way:</p>
<p class="figure" id="e0075"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si14.jpg" alt="image" width="267" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si14.jpg"></p>
<p id="p0300" class="para_indented">The probability density function for an event is very closely related to its probability. However, it is not quite the same thing. If temperature is a continuous scale, the probability of the temperature being <em>exactly</em> 66—or <em>exactly</em> any other value, such as 63.14159262—is zero. The real meaning of the density function <em>f</em>(<em>x</em>) is that the probability that the quantity lies within a small region around <em>x</em>, say between <em>x</em> − <em>ε</em>/2 and <em>x</em> + <em>ε</em>/2, is <em>ε</em> × <em>f(x)</em>. You might think we ought to factor in the accuracy figure <em>ε</em> when using these density values, but that’s not necessary. The same <em>ε</em> would appear in both the <em>yes</em> and <em>no</em> likelihoods that follow and cancel out when the probabilities were calculated.</p>
<p id="p0305" class="para_indented">Using these probabilities for the new day in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0035">Table 4.5</a> yields</p>
<p class="figure" id="e0080"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si15.jpg" alt="image" width="556" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si15.jpg"></p>
<p class="figure" id="e0085"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si16.jpg" alt="image" width="548" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si16.jpg"></p>
<p class="noindent">which leads to probabilities</p>
<p class="figure" id="e0090"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si17.jpg" alt="image" width="429" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si17.jpg"></p>
<p class="figure" id="e0095"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si18.jpg" alt="image" width="423" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si18.jpg"></p>
<p class="table_caption"><span class="tab_num">Table 4.5. </span> Another New Day</p>
<p id="t0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0035.jpg" alt="Image" width="560" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0035.jpg"></p>
<p id="p0310" class="noindent"><a id="p97"></a>These figures are very close to the probabilities calculated earlier for the new day in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0025">Table 4.3</a> because the <em>temperature</em> and <em>humidity</em> values of 66 and 90 yield similar probabilities to the <em>cool</em> and <em>high</em> values used before.</p>
<p id="p0315" class="para_indented">The normal-distribution assumption makes it easy to extend the Naïve Bayes classifier to deal with numeric attributes. If the values of any numeric attributes are missing, the mean and standard deviation calculations are based only on the ones that are present.</p>
</div>
<div id="s0035">
<h3 id="st0035">Naïve Bayes for Document Classification</h3>
<p id="p0320" class="noindent">An important domain for machine learning is document classification, in which each instance represents a document and the instance’s class is the document’s topic. Documents might be news items and the classes might be domestic news, overseas news, financial news, and sports. Documents are characterized by the words that appear in them, and one way to apply machine learning to document classification is to treat the presence or absence of each word as a Boolean attribute. Naïve Bayes is a popular technique for this application because it is very fast and quite accurate.</p>
<p id="p0325" class="para_indented">However, this does not take into account the number of occurrences of each word, which is potentially useful information when determining the category of a document. Instead, a document can be viewed as a <em>bag of words</em>—a set that contains all the words in the document, with multiple occurrences of a word appearing multiple times (technically, a <em>set</em> includes each of its members just once, whereas a <em>bag</em> can have repeated elements). Word frequencies can be accommodated by applying a modified form of Naïve Bayes called <em>multinominal</em> Naïve Bayes.</p><a id="p0330"></a><div class="boxg" id="b0010">
<p id="p0335" class="noindent">Suppose <em>n</em><span class="sub">1</span>, <em>n</em><span class="sub">2</span>, …, <em>n<span class="sub">k</span></em> is the number of times word <em>i</em> occurs in the document, and <em>P</em><span class="sub">1</span>, <em>P</em><span class="sub">2</span>, …, <em>P<span class="sub">k</span></em> is the probability of obtaining word <em>i</em> when sampling from all the documents in category <em>H</em>. Assume that the probability is independent of the word’s context and position in the document. These assumptions lead to a <em>multinomial distribution</em> for document probabilities. For this distribution, the probability of a document <em>E</em> given its class <em>H</em>—in other words, the formula for computing the probability Pr[<em>E</em> | <em>H</em>] in Bayes’ rule—is</p>
<p class="figure" id="e0100"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si19.jpg" alt="image" width="171" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si19.jpg"></p>
<p class="noindent">where <em>N</em> = <em>n</em><span class="sub">1</span> + <em>n</em><span class="sub">2</span> + … + <em>n<span class="sub">k</span></em> is the number of words in the document. The reason for the factorials is to account for the fact that the ordering of the occurrences of each word is immaterial according to the bag-of-words model. <em>P<span class="sub">i</span></em> is estimated by computing the relative frequency of word <em>i</em> in the text of all training documents pertaining to category <em>H</em>. In reality, there could be a further term that gives the probability that the model for category <em>H</em> generates a document whose length is the same as the length of <em>E</em>, but it is common to assume that this is the same for all classes and hence can be dropped.</p>
</div>
<p id="p0340" class="para_indented"><a id="p98"></a>For example, suppose there are only two words, <em>yellow</em> and <em>blue</em>, in the vocabulary, and a particular document class <em>H</em> has Pr[<em>yellow</em> | <em>H</em>] = 75% and Pr[<em>blue</em> | <em>H</em>] = 25% (you might call <em>H</em> the class of <em>yellowish green</em> documents). Suppose <em>E</em> is the document <em>blue yellow blue</em> with a length of <em>N</em> = 3 words. There are four possible bags of three words. One is {<em>yellow yellow yellow</em>}, and its probability according to the preceding formula is</p>
<p class="figure" id="e0105"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si20.jpg" alt="image" width="477" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si20.jpg"></p>
<p id="p0345" class="para_indented">The other three, with their probabilities, are</p>
<p class="figure" id="e0110"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si21.jpg" alt="image" width="250" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si21.jpg"></p>
<p class="figure" id="e0115"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si22.jpg" alt="image" width="288" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si22.jpg"></p>
<p class="figure" id="e0120"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si23.jpg" alt="image" width="269" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si23.jpg"></p>
<p id="p0350" class="noindent"><em>E</em> corresponds to the last case (recall that in a bag of words the order is immaterial); thus, its probability of being generated by the <em>yellowish green</em> document model is 9/64, or 14%. Suppose another class, <em>very bluish green</em> documents (call it <em>H</em>′), has Pr[<em>yellow</em> | <em>H</em>′] = 10% and Pr[<em>blue</em> | <em>H</em>′] = 90%. The probability that <em>E</em> is generated by this model is 24%.</p>
<p id="p0355" class="para_indented">If these are the only two classes, does that mean that <em>E</em> is in the <em>very bluish green</em> document class? Not necessarily. Bayes’ rule, given earlier, says that you have to take into account the prior probability of each hypothesis. If you know that in fact <em>very bluish green</em> documents are twice as rare as <em>yellowish green</em> ones, this would be just sufficient to outweigh the 14 to 24% disparity and tip the balance in favor of the <em>yellowish green</em> class.</p>
<p id="p0360" class="para_indented">The factorials in the probability formula don’t actually need to be computed because, being the same for every class, they drop out in the normalization process anyway. However, the formula still involves multiplying together many small probabilities, which soon yields extremely small numbers that cause underflow on large documents. The problem can be avoided by using logarithms of the probabilities instead of the probabilities themselves.</p>
<p id="p0365" class="para_indented">In the multinomial Naïve Bayes formulation a document’s class is determined not just by the words that occur in it but also by the number of times they occur. In general, it performs better than the ordinary Naïve Bayes model for document classification, particularly for large dictionary sizes.</p>
</div>
<div id="s0040">
<h3 id="st0040"><a id="p99"></a>Discussion</h3>
<p id="p0370" class="noindent">Naïve Bayes gives a simple approach, with clear semantics, to representing, using, and learning probabilistic knowledge. It can achieve impressive results. People often find that Naïve Bayes rivals, and indeed outperforms, more sophisticated classifiers on many datasets. The moral is, always try the simple things first. Over and over again people have eventually, after an extended struggle, managed to obtain good results using sophisticated learning schemes, only to discover later that simple methods such as 1R and Naïve Bayes do just as well—or even better.</p>
<p id="p0375" class="para_indented">There are many datasets for which Naïve Bayes does not do well, however, and it is easy to see why. Because attributes are treated as though they were independent given the class, the addition of redundant ones skews the learning process. As an extreme example, if you were to include a new attribute with the same values as <em>temperature</em> to the weather data, the effect of the <em>temperature</em> attribute would be multiplied: All of its probabilities would be squared, giving it a great deal more influence in the decision. If you were to add 10 such attributes, the decisions would effectively be made on <em>temperature</em> alone. Dependencies between attributes inevitably reduce the power of Naïve Bayes to discern what is going on. They can, however, be ameliorated by using a subset of the attributes in the decision procedure, making a careful selection of which ones to use. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#c0007">Chapter 7</a> shows how.</p>
<p id="p0380" class="para_indented">The normal-distribution assumption for numeric attributes is another restriction on Naïve Bayes as we have formulated it here. Many features simply aren’t normally distributed. However, there is nothing to prevent us from using other distributions—there is nothing magic about the normal distribution. If you know that a particular attribute is likely to follow some other distribution, standard estimation procedures for that distribution can be used instead. If you suspect it isn’t normal but don’t know the actual distribution, there are procedures for “kernel density estimation” that do not assume any particular distribution for the attribute values. Another possibility is simply to discretize the data first.</p>
</div>
</div>
<div id="s0045">
<h2 id="st0045">4.3 Divide-and-conquer: constructing decision trees</h2>
<p id="p0385" class="noindent">The problem of constructing a decision tree can be expressed recursively. First, select an attribute to place at the root node, and make one branch for each possible value. This splits up the example set into subsets, one for every value of the attribute. Now the process can be repeated recursively for each branch, using only those instances that actually reach the branch. If at any time all instances at a node have the same classification, stop developing that part of the tree.</p>
<p id="p0390" class="para_indented">The only thing left is how to determine which attribute to split on, given a set of examples with different classes. Consider (again!) the weather data. There are four possibilities for each split, and at the top level they produce the trees in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a>. <a id="p100"></a>Which is the best choice? The number of <em>yes</em> and <em>no</em> classes is shown at the leaves. Any leaf with only one class—<em>yes</em> or <em>no</em>—will not have to be split further, and the recursive process down that branch will terminate. Because we seek small trees, we would like this to happen as soon as possible. If we had a measure of the purity of each node, we could choose the attribute that produces the purest daughter nodes. Take a moment to look at <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a> and ponder which attribute you think is the best choice.</p>
<p id="f0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-002ad-9780123748560.jpg" alt="image" width="424" height="424" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-002ad-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.2</span> Tree stumps for the weather data: (a) outlook, (b) temperature, (c) humidity, and (d) windy.</p>
<p id="p0395" class="para_indented">The measure of purity that we will use is called the <em>information</em> and is measured in units called <em>bits</em>. Associated with each node of the tree, it represents the expected amount of information that would be needed to specify whether a new instance should be classified <em>yes</em> or <em>no</em>, given that the example reached that node. Unlike the bits in computer memory, the expected amount of information usually involves fractions of a bit—and is often less than 1! It is calculated based on the number of <em>yes</em> and <em>no</em> classes at the node. We will look at the details of the calculation shortly, but first let’s see how it’s used. When evaluating the first tree in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a>, the number of <em>yes</em> and <em>no</em> classes at the leaf nodes are [2, 3], [4, 0], and [3, 2], respectively, and the information values of these nodes are</p>
<p class="figure" id="e0125"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si24.jpg" alt="image" width="202" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si24.jpg"></p>
<p class="figure" id="e0130"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si25.jpg" alt="image" width="183" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si25.jpg"></p>
<p class="figure" id="e0135"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si26.jpg" alt="image" width="202" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si26.jpg"></p>
<p id="p0400" class="para_indented"><a id="p101"></a>We calculate the average information value of these, taking into account the number of instances that go down each branch—five down the first and third and four down the second:</p>
<p class="figure" id="e0140"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si27.jpg" alt="image" width="575" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si27.jpg"></p>
<p id="p0405" class="noindent">This average represents the amount of information that we expect would be necessary to specify the class of a new instance, given the tree structure in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2(a)</a>.</p>
<p id="p0410" class="para_indented">Before any of the nascent tree structures in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a> were created, the training examples at the root comprised nine <em>yes</em> and five <em>no</em> nodes, corresponding to an information value of</p>
<p class="figure" id="e0145"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si28.jpg" alt="image" width="202" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si28.jpg"></p>
<p id="p0415" class="noindent">Thus, the tree in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2(a)</a> is responsible for an information gain of</p>
<p class="figure" id="e0150"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si29.jpg" alt="image" width="583" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si29.jpg"></p>
<p class="noindent">which can be interpreted as the informational value of creating a branch on the <em>outlook</em> attribute.</p>
<p id="p0420" class="para_indented">The way forward is clear. We calculate the information gain for each attribute and split on the one that gains the most information. In the situation that is shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a>:</p><a id="p0425"></a><div class="none">
<p class="hang" id="u0010">• <a id="p0430"></a>gain(<em>outlook</em>)= 0.247 bits</p>
<p class="hang" id="u0015">• <a id="p0435"></a>gain(<em>temperature</em>) = 0.029 bits</p>
<p class="hang" id="u0020">• <a id="p0440"></a>gain(<em>humidity</em>) = 0.152 bits</p>
<p class="hang" id="u0025">• <a id="p0445"></a>gain(<em>windy</em>) = 0.048 bits</p>
</div>
<p id="p0450" class="noindent">Therefore, we select <em>outlook</em> as the splitting attribute at the root of the tree. Hopefully this accords with your intuition as the best one to select. It is the only choice for which one daughter node is completely pure, and this gives it a considerable advantage over the other attributes. <em>Humidity</em> is the next best choice because it produces a larger daughter node that is almost completely pure.</p>
<p id="p0455" class="para_indented">Then we continue, recursively. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0020">Figure 4.3</a> shows the possibilities for a further branch at the node reached when <em>outlook</em> is <em>sunny</em>. Clearly, a further split on <em>outlook</em><a id="p102"></a>will produce nothing new, so we only consider the other three attributes. The information gain for each turns out to be</p>
<p id="f0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-003ac-9780123748560.jpg" alt="image" width="450" height="454" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-003ac-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.3</span> Expanded tree stumps for the weather data: (a) temperature, (b) humidity, and (c) windy.</p><a id="p0460"></a><div class="none">
<p class="hang" id="u0030">• <a id="p0465"></a>gain(<em>temperature</em>) = 0.571 bits</p>
<p class="hang" id="u0035">• <a id="p0470"></a>gain(<em>humidity</em>) = 0.971 bits</p>
<p class="hang" id="u0040">• <a id="p0475"></a>gain(<em>windy</em>) = 0.020 bits</p>
</div>
<p id="p0480" class="noindent">Therefore, we select <em>humidity</em> as the splitting attribute at this point. There is no need to split these nodes any further, so this branch is finished.</p>
<p id="p0485" class="para_indented">Continued application of the same idea leads to the decision tree of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0025">Figure 4.4</a> for the weather data. Ideally, the process terminates when all leaf nodes are pure—that is, when they contain instances that all have the same classification. However, it might not be possible to reach this happy situation because there is nothing to stop the training set containing two examples with identical sets of attributes but different classes. Consequently, we stop when the data cannot be split any further. Alternatively, one could stop if the information gain is zero. This is slightly more conservative because <a id="p103"></a>it is possible to encounter cases where the data can be split into subsets exhibiting identical class distributions, which would make the information gain zero.</p>
<p id="f0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-004-9780123748560.jpg" alt="image" width="750" height="497" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-004-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.4</span> Decision tree for the weather data.</p>
<div id="s0050">
<h3 id="st0050">Calculating Information</h3>
<p id="p0490" class="noindent">Now it is time to explain how to calculate the information measure that is used as the basis for evaluating different splits. We describe the basic idea in this section, then in the next we examine a correction that is usually made to counter a bias toward selecting splits on attributes with large numbers of possible values.</p>
<p id="p0495" class="para_indented">Before examining the detailed formula for calculating the amount of information required to specify the class of an example given that it reaches a tree node with a certain number of <em>yes</em>’s and <em>no</em>’s, consider first the kind of properties we would expect this quantity to have</p><a id="p0500"></a><div class="none">
<p class="hang" id="o0045">1. <a id="p0505"></a>When the number of either <em>yes</em>’s or <em>no</em>’s is zero, the information is zero.</p>
<p class="hang" id="o0050">2. <a id="p0510"></a>When the number of <em>yes</em>’s and <em>no</em>’s is equal, the information reaches a maximum.</p>
</div>
<p id="p0515" class="noindent">Moreover, the measure should be applicable to multiclass situations, not just to two-class ones.</p>
<p id="p0520" class="para_indented">The information measure relates to the amount of information obtained by making a decision, and a more subtle property of information can be derived by considering the nature of decisions. Decisions can be made in a single stage, or they can be made in several stages, and the amount of information involved is the same in both cases. For example, the decision involved in</p>
<p class="figure" id="e0155"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si30.jpg" alt="image" width="119" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si30.jpg"></p>
<p class="noindent">can be made in two stages. First decide whether it’s the first case or one of the other two cases:</p>
<p class="figure" id="e0160"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si31.jpg" alt="image" width="100" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si31.jpg"></p>
<p class="noindent"><a id="p104"></a>and then decide which of the other two cases it is:</p>
<p class="figure" id="e0165"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si32.jpg" alt="image" width="98" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si32.jpg"></p>
<p id="p0525" class="noindent">In some cases the second decision will not need to be made, namely, when the decision turns out to be the first one. Taking this into account leads to the equation</p>
<p class="figure" id="e0170"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si33.jpg" alt="image" width="402" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si33.jpg"></p>
<p id="p0530" class="para_indented">Of course, there is nothing special about these particular numbers, and a similar relationship should hold regardless of the actual values. Thus, we could add a further criterion to the list above:</p><a id="p0535"></a><div class="none">
<p class="hang" id="o0055">3. <a id="p0540"></a>The information should obey the multistage property that we have illustrated.</p>
</div>
<p id="p0545" class="para_indented">Remarkably, it turns out that there is only one function that satisfies all these properties, and it is known as the <em>information value</em> or <em>entropy</em>:</p>
<p class="figure" id="e0175"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si34.jpg" alt="image" width="498" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si34.jpg"></p>
<p id="p0550" class="para_indented">The reason for the minus signs is that logarithms of the fractions <em>p</em><span class="sub">1</span>, <em>p</em><span class="sub">2</span>, … , <em>p<span class="sub">n</span></em> are negative, so the entropy is actually positive. Usually the logarithms are expressed in base 2, and then the entropy is in units called <em>bits</em>—just the usual kind of bits used with computers.</p>
<p id="p0555" class="para_indented">The arguments <em>p</em><span class="sub">1</span>, <em>p</em><span class="sub">2</span>, … of the entropy formula are expressed as fractions that add up to 1, so that, for example,</p>
<p class="figure" id="e0180"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si35.jpg" alt="image" width="323" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si35.jpg"></p>
<p id="p0560" class="para_indented">Thus, the multistage decision property can be written in general as</p>
<p class="figure" id="e0185"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si36.jpg" alt="image" width="565" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si36.jpg"></p>
<p class="noindent">where <em>p</em> + <em>q</em> + <em>r</em> = 1.</p>
<p id="p0565" class="para_indented">Because of the way the log function works, you can calculate the information measure without having to work out the individual fractions:</p>
<p class="figure" id="e0190"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si37.jpg" alt="image" width="506" height="63" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si37.jpg"></p>
<p id="p0570" class="para_indented">This is the way that the information measure is usually calculated in practice. So the information value for the first node of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2(a)</a> is</p>
<p class="figure" id="e0195"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si38.jpg" alt="image" width="462" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si38.jpg"></p>
</div>
<div id="s0055">
<h3 id="st0055"><a id="p105"></a>Highly Branching Attributes</h3>
<p id="p0575" class="noindent">When some attributes have a large number of possible values, giving rise to a multiway branch with many child nodes, a problem arises with the information gain calculation. The problem can best be appreciated in the extreme case when an attribute has a different value for each instance in the dataset—as, for example, an identification code attribute might.</p>
<p id="p0580" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0040">Table 4.6</a> gives the weather data with this extra attribute. Branching on <em>ID code</em> produces the tree stump in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0030">Figure 4.5</a>. The information required to specify the class given the value of this attribute is</p>
<p class="figure" id="e0200"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si39.jpg" alt="image" width="563" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si39.jpg"></p>
<p class="noindent">which is 0 because each of the 14 terms is 0. This is not surprising: The <em>ID code</em> attribute identifies the instance, which determines the class without any ambiguity—just as <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0040">Table 4.6</a> shows. Consequently, the information gain of this attribute is just the information at the root, info([9,5]) = 0.940 bits. This is greater than the information gain of any other attribute, and so <em>ID code</em> will inevitably be chosen as the splitting attribute. But branching on the identification code is no good for predicting the class of unknown instances and tells nothing about the structure of the decision, which after all are the twin goals of machine learning.</p><a id="p106"></a><p class="table_caption"><span class="tab_num">Table 4.6. </span> Weather Data with Identification Codes</p>
<p id="t0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0040.jpg" alt="Image" width="872" height="319" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0040.jpg"></p>
<p id="f0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-005-9780123748560.jpg" alt="image" width="800" height="283" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-005-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.5</span> Tree stump for the <em>ID code</em> attribute.</p>
<p id="p0585" class="para_indented">The overall effect is that the information gain measure tends to prefer attributes with large numbers of possible values. To compensate for this, a modification of the measure called the <em>gain ratio</em> is widely used. The gain ratio is derived by taking into account the number and size of daughter nodes into which an attribute splits the dataset, disregarding any information about the class. In the situation shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0030">Figure 4.5</a>, all counts have a value of 1, so the information value of the split is</p>
<p class="figure" id="e0205"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si40.jpg" alt="image" width="329" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si40.jpg"></p>
<p class="noindent">because the same fraction, 1/14, appears 14 times. This amounts to log 14, or 3.807 bits, which is a very high value. This is because the information value of a split is <a id="p107"></a>the number of bits needed to determine to which branch each instance is assigned, and the more branches there are, the greater this value. The gain ratio is calculated by dividing the original information gain, 0.940 in this case, by the information value of the attribute, 3.807—yielding a gain ratio value of 0.247 for the <em>ID code</em> attribute.</p>
<p id="p0590" class="para_indented">Returning to the tree stumps for the weather data in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a>, <em>outlook</em> splits the dataset into three subsets of size 5, 4, and 5, and thus has an intrinsic information value of</p>
<p class="figure" id="e0210"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si41.jpg" alt="image" width="183" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si41.jpg"></p>
<p class="noindent">without paying any attention to the classes involved in the subsets. As we have seen, this intrinsic information value is greater for a more highly branching attribute such as the hypothesized <em>ID code</em>. Again, we can correct the information gain by dividing by the intrinsic information value to get the gain ratio.</p>
<p id="p0595" class="para_indented">The results of these calculations for the tree stumps of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a> are summarized in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0045">Table 4.7</a>. <em>Outlook</em> still comes out on top, but <em>humidity</em> is now a much closer contender because it splits the data into two subsets instead of three. In this particular example, the hypothetical <em>ID code</em> attribute, with a gain ratio of 0.247, would still be preferred to any of these four. However, its advantage is greatly reduced. In practical implementations, we can use an ad hoc test to guard against splitting on such a useless attribute.</p>
<p class="table_caption"><span class="tab_num">Table 4.7. </span> Gain Ratio Calculations for <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0015">Figure 4.2</a> Tree Stumps</p>
<p id="t0045" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0045.jpg" alt="Image" width="641" height="112" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0045.jpg"></p>
<p id="p0600" class="para_indented">Unfortunately, in some situations the gain ratio modification overcompensates and can lead to preferring an attribute just because its intrinsic information is much lower than for the other attributes. A standard fix is to choose the attribute that maximizes the gain ratio, provided that the information gain for that attribute is at least as great as the average information gain for all the attributes examined.</p>
</div>
<div id="s0060">
<h3 id="st0060">Discussion</h3>
<p id="p0605" class="noindent">The divide-and-conquer approach to decision tree induction, sometimes called <em>top-down induction of decision trees</em>, was developed and refined over many years by J. Ross Quinlan at the University of Sydney in Australia. Although others have <a id="p108"></a>worked on similar methods, Quinlan’s research has always been at the very forefront of decision tree induction. The scheme that has been described using the information gain criterion is essentially the same as one known as ID3. The use of the gain ratio was one of many improvements that were made to ID3 over several years; Quinlan described it as robust under a wide variety of circumstances. Although a practical solution, it sacrifices some of the elegance and clean theoretical motivation of the information gain criterion.</p>
<p id="p0610" class="para_indented">A series of improvements to ID3 culminated in a practical and influential system for decision tree induction called C4.5. These improvements include methods for dealing with numeric attributes, missing values, noisy data, and generating rules from trees, and they are described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0010">Section 6.1</a>.</p>
</div>
</div>
<div id="s0065">
<h2 id="st0065">4.4 Covering algorithms: constructing rules</h2>
<p id="p0615" class="noindent">As we have seen, decision tree algorithms are based on a divide-and-conquer approach to the classification problem. They work top-down, seeking at each stage an attribute to split on that best separates the classes, and then recursively processing the subproblems that result from the split. This strategy generates a decision tree, which can if necessary be converted into a set of classification rules—although if it is to produce effective rules, the conversion is not trivial.</p>
<p id="p0620" class="para_indented">An alternative approach is to take each class in turn and seek a way of covering all instances in it, at the same time excluding instances not in the class. This is called a <em>covering</em> approach because at each stage you identify a rule that “covers” some of the instances. By its very nature, this covering approach leads to a set of rules rather than to a decision tree.</p>
<p id="p0625" class="para_indented">The covering method can readily be visualized in a two-dimensional space of instances as shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0035">Figure 4.6(a)</a>. We first make a rule covering the <em>a</em>’s. For the first test in the rule, split the space vertically as shown in the center picture. This gives the beginnings of a rule:</p>
<p id="f0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-006-9780123748560.jpg" alt="image" width="504" height="360" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-006-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.6</span> Covering algorithm: (a) covering the instances, and (b) decision tree for the same problem.</p>
<p id="p0630" class="noindent"><span class="monospace">If x &gt; 1.2 then class = a</span></p>
<p id="p0635" class="noindent">However, the rule covers many <em>b</em>’s as well as <em>a</em>’s, so a new test is added to it by further splitting the space horizontally as shown in the third diagram:</p>
<p id="p0640" class="noindent"><span class="monospace">If x &gt; 1.2 and y &gt; 2.6 then class = a</span></p>
<p id="p0645" class="noindent">This gives a rule covering all but one of the <em>a</em>’s. It’s probably appropriate to leave it at that, but if it were felt necessary to cover the final <em>a</em>, another rule would be needed, perhaps</p>
<p id="p0650" class="noindent"><span class="monospace">If x &gt; 1.4 and y &lt; 2.4 then class = a</span></p>
<p id="p0655" class="noindent">The same procedure leads to two rules covering the <em>b</em>’s:</p>
<p id="p0660" class="noindent"><span class="monospace">If x ≤ 1.2 then class = b</span></p>
<p id="p0665" class="noindent"><span class="monospace">If x &gt; 1.2 and y ≤ 2.6 then class = b</span></p>
<p id="p0670" class="noindent"><a id="p109"></a>Again, one <em>a</em> is erroneously covered by these rules. If it were necessary to exclude it, more tests would have to be added to the second rule, and additional rules would be needed to cover the <em>b</em>’s that these new tests exclude.</p>
<div id="s0070">
<h3 id="st0070">Rules versus Trees</h3>
<p id="p0675" class="noindent">A top-down divide-and-conquer algorithm operates on the same data in a manner that is, at least superficially, quite similar to a covering algorithm. It might first split the dataset using the <em>x</em> attribute, and would probably end up splitting it at the same place, <em>x</em> = 1.2. However, whereas the covering algorithm is concerned only with covering a single class, the division would take both classes into account because divide-and-conquer algorithms create a single concept description that applies to all classes. The second split might also be at the same place, <em>y</em> = 2.6, leading to the decision tree in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0035">Figure 4.6(b)</a>. This tree corresponds exactly to the set of rules, and in this case there is no difference in effect between the covering and the divide-and-conquer algorithms.</p>
<p id="p0680" class="para_indented">But in many situations there <em>is</em> a difference between rules and trees in terms of the perspicuity of the representation. For example, when we described the replicated subtree problem in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, we noted that rules can be symmetric whereas trees must select one attribute to split on first, and this can lead to trees that are much <a id="p110"></a>larger than an equivalent set of rules. Another difference is that, in the multiclass case, a decision tree split takes all classes into account in trying to maximize the purity of the split, whereas the rule-generating method concentrates on one class at a time, disregarding what happens to the other classes.</p>
</div>
<div id="s0075">
<h3 id="st0075">A Simple Covering Algorithm</h3>
<p id="p0685" class="noindent">Covering algorithms operate by adding tests to the rule that is under construction, always striving to create a rule with maximum accuracy. In contrast, divide-and-conquer algorithms operate by adding tests to the tree that is under construction, always striving to maximize the separation between the classes. Each of these involves finding an attribute to split on. But the criterion for the best attribute is different in each case. Whereas divide-and-conquer algorithms such as ID3 choose an attribute to maximize the information gain, the covering algorithm we will describe chooses an attribute–value pair to maximize the probability of the desired classification.</p>
<p id="p0690" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0040">Figure 4.7</a> gives a picture of the situation, showing the space containing all the instances, a partially constructed rule, and the same rule after a new term has been added. The new term restricts the coverage of the rule: The idea is to include as many instances of the desired class as possible and exclude as many instances of other classes as possible. Suppose the new rule will cover a total of <em>t</em> instances, of which <em>p</em> are positive examples of the class and <em>t</em> – <em>p</em> are in other classes—that is, they are errors made by the rule. Then choose the new term to maximize the ratio <em>p</em>/<em>t</em>.</p>
<p id="f0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-007-9780123748560.jpg" alt="image" width="627" height="433" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-007-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.7</span> The instance space during operation of a covering algorithm.</p>
<p id="p0695" class="para_indented">An example will help. For a change, we use the contact lens problem of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0010">Table 1.1</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p6">page 6</a>). We will form rules that cover each of the three classes—<em>hard</em>, <em>soft</em>, and <em>none—</em>in turn. To begin, we seek a rule:</p>
<p id="p0700" class="noindent"><span class="monospace">If ? then recommendation = hard</span></p>
<p id="p0705" class="noindent">For the unknown term <span class="monospace">?</span>, we have nine choices:</p>
<p id="p0710" class="noindent"><span class="monospace">age = young   2/8</span></p>
<p id="p0715" class="noindent"><span class="monospace">age = pre-presbyopic   1/8</span></p>
<p id="p0720" class="noindent"><span class="monospace">age = presbyopic   1/8</span></p>
<p id="p0725" class="noindent"><span class="monospace">spectacle prescription = myope   3/12</span></p>
<p id="p0730" class="noindent"><span class="monospace">spectacle prescription = hypermetrope  1/12</span></p>
<p id="p0735" class="noindent"><span class="monospace">astigmatism = no   0/12</span></p>
<p id="p0740" class="noindent"><span class="monospace">astigmatism = yes   4/12</span></p>
<p id="p0745" class="noindent"><span class="monospace">tear production rate = reduced   0/12</span></p>
<p id="p0750" class="noindent"><span class="monospace">tear production rate = normal   4/12</span></p>
<p id="p0755" class="noindent"><a id="p111"></a>The numbers on the right show the fraction of “correct” instances in the set singled out by that choice. In this case, “correct” means that the recommendation is <em>hard</em>. For instance, <em>age = young</em> selects 8 instances, 2 of which recommend hard contact lenses, so the first fraction is 2/8. (To follow this, you will need to look back at the contact lens data in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0010">Table 1.1</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p6">page 6</a>) and count up the entries in the table.)</p>
<p id="p0760" class="para_indented">We select the largest fraction, 4/12, arbitrarily choosing between the seventh and the last choice in the list, and create the rule:</p>
<p id="p0765" class="noindent"><span class="monospace">If astigmatism = yes then recommendation = hard</span></p>
<p id="p0770" class="noindent">This rule is quite inaccurate, getting only 4 instances correct out of the 12 that it covers, shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0050">Table 4.8</a>. So we refine it further:</p><a id="p112"></a><p class="table_caption"><span class="tab_num">Table 4.8. </span> Part of Contact Lens Data for Which <em>Astigmatism = yes</em></p>
<p id="t0050" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0050.jpg" alt="Image" width="772" height="294" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0050.jpg"></p>
<p id="p0775" class="noindent"><span class="monospace">If astigmatism = yes and ? then recommendation = hard</span></p>
<p id="p0780" class="para_indented">Considering the possibilities for the unknown term, <span class="monospace">?</span> yields the following seven choices:</p>
<p id="p0785" class="noindent"><span class="monospace">age = young   2/4</span></p>
<p id="p0790" class="noindent"><span class="monospace">age = pre-presbyopic   1/4</span></p>
<p id="p0795" class="noindent"><span class="monospace">age = presbyopic   1/4</span></p>
<p id="p0800" class="noindent"><span class="monospace">spectacle prescription = myope   3/6</span></p>
<p id="p0805" class="noindent"><span class="monospace">spectacle prescription = hypermetrope  1/6</span></p>
<p id="p0810" class="noindent"><span class="monospace">tear production rate = reduced   0/6</span></p>
<p id="p0815" class="noindent"><span class="monospace">tear production rate = normal   4/6</span></p>
<p id="p0820" class="noindent">(Again, count the entries in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0050">Table 4.8</a>.) The last is a clear winner, getting 4 instances correct out of the 6 that it covers, and it corresponds to the rule</p>
<p id="p0825" class="noindent"><span class="monospace">If astigmatism = yes and tear production rate = normal</span></p>
<p id="p0830" class="para_indented"><span class="monospace"> then recommendation = hard</span></p>
<p id="p0835" class="para_indented">Should we stop here? Perhaps. But let’s say we are going for exact rules, no matter how complex they become. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0055">Table 4.9</a> shows the cases that are covered by the rule so far. The possibilities for the next term are now</p><a id="p113"></a><p class="table_caption"><span class="tab_num">Table 4.9. </span> Part of Contact Lens Data for Which <em>Astigmatism = yes</em> and <em>Tear Production rate = normal</em></p>
<p id="t0055" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0055.jpg" alt="Image" width="872" height="173" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0055.jpg"></p>
<p id="p0840" class="noindent"><span class="monospace">age = young   2/2</span></p>
<p id="p0845" class="noindent"><span class="monospace">age = pre-presbyopic   1/2</span></p>
<p id="p0850" class="noindent"><span class="monospace">age = presbyopic   1/2</span></p>
<p id="p0855" class="noindent"><span class="monospace">spectacle prescription = myope   3/3</span></p>
<p id="p0860" class="noindent"><span class="monospace">spectacle prescription = hypermetrope  1/3</span></p>
<p id="p0865" class="noindent">It is necessary for us to choose between the first and fourth. So far we have treated the fractions numerically, but although these two are equal (both evaluate to 1), they have different coverage: One selects just two correct instances and the other selects <a id="p114"></a>three. In the event of a tie, we choose the rule with the greater coverage, giving the final rule:</p>
<p id="p0870" class="noindent"><span class="monospace">If astigmatism = yes and tear production rate = normal</span></p>
<p id="p0875" class="para_indented"><span class="monospace"> and spectacle prescription = myope then recommendation = hard</span></p>
<p id="p0880" class="para_indented">This is indeed one of the rules given for the contact lens problem. But it only covers three out of the four <em>hard</em> recommendations. So we delete these three from the set of instances and start again, looking for another rule of the form:</p>
<p id="p0885" class="noindent"><span class="monospace">If ? then recommendation = hard</span></p>
<p id="p0890" class="para_indented">Following the same process, we will eventually find that <em>age = young</em> is the best choice for the first term. Its coverage is one out of 7 the reason for the 7 is that 3 instances have been removed from the original set, leaving 21 instances altogether. The best choice for the second term is <em>astigmatism = yes</em>, selecting 1/3 (actually, this is a tie); <em>tear production rate = normal</em> is the best for the third, selecting 1/1.</p>
<p id="p0895" class="noindent"><span class="monospace">If age = young and astigmatism = yes</span></p>
<p id="p0900" class="para_indented"><span class="monospace"> and tear production rate = normal</span></p>
<p id="p0905" class="para_indented"><span class="monospace"> then recommendation = hard</span></p>
<p id="p0910" class="noindent">This rule actually covers two of the original set of instances, one of which is covered by the previous rule—but that’s all right because the recommendation is the same for each rule.</p>
<p id="p0915" class="para_indented">Now that all the hard-lens cases are covered, the next step is to proceed with the soft-lens ones in just the same way. Finally, rules are generated for the <em>none</em> case—unless we are seeking a rule set with a default rule, in which case explicit rules for the final outcome are unnecessary.</p>
<p id="p0920" class="para_indented">What we have just described is the PRISM method for constructing rules. It generates only correct or “perfect” rules. It measures the success of a rule by the accuracy formula <em>p</em>/<em>t</em>. Any rule with accuracy less than 100% is “incorrect” in that <a id="p115"></a>it assigns cases to the class in question that actually do not have that class. PRISM continues adding clauses to each rule until it is perfect—its accuracy is 100%. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0045">Figure 4.8</a> gives a summary of the algorithm. The outer loop iterates over the classes, generating rules for each class in turn. Note that we reinitialize to the full set of examples each time around. Then we create rules for that class and remove the examples from the set until there are none of that class left. Whenever we create a rule, we start with an empty rule (which covers all the examples), and then restrict it by adding tests until it covers only examples of the desired class. At each stage we choose the most promising test—that is, the one that maximizes the accuracy of the rule. Finally, we break ties by selecting the test with greatest coverage.</p>
<p id="f0045" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-008-9780123748560.jpg" alt="image" width="514" height="188" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-008-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.8</span> Pseudocode for a basic rule learner.</p>
</div>
<div id="s0080">
<h3 id="st0080">Rules versus Decision Lists</h3>
<p id="p0925" class="noindent">Consider the rules produced for a particular class—that is, the algorithm in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0045">Figure 4.8</a> with the outer loop removed. It seems clear from the way that these rules are produced that they are intended to be interpreted in order—that is, as a decision list—testing the rules in turn until one applies and then using that. This is because the instances covered by a new rule are removed from the instance set as soon as the rule is completed (in the last line of the code in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0045">Figure 4.8</a>): Thus, subsequent rules are designed for instances that are <em>not</em> covered by the rule. However, although it appears that we are supposed to check the rules in turn, we do not have to do so. Consider that any subsequent rules generated for this class will have the same effect—they all predict the same class. This means that it does not matter what order they are executed in: Either a rule will be found that covers this instance, in which case the class in question is predicted, or no such rule is found, in which case the class is not predicted.</p>
<p id="p0930" class="para_indented">Now return to the overall algorithm. Each class is considered in turn, and rules are generated that distinguish instances in that class from the others. No ordering is implied between the rules for one class and those for another. Consequently, the rules that are produced can be executed in any order.</p>
<p id="p0935" class="para_indented">As described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, order-independent rules seem to provide more modularity by acting as independent nuggets of “knowledge,” but they suffer from the disadvantage that it is not clear what to do when conflicting rules apply. With rules generated in this way, a test example may receive multiple classifications—that is, it may satisfy rules that apply to different classes. Other test examples may receive no classification at all. A simple strategy to force a decision in ambiguous cases is to choose, from the classifications that are predicted, the one with the most training examples or, if no classification is predicted, to choose the category with the most training examples overall. These difficulties do not occur with decision lists because they are meant to be interpreted in order and execution stops as soon as one rule applies: The addition of a default rule at the end ensures that any test instance receives a classification. It is possible to generate good decision lists for the multiclass case using a slightly different method, as we will see in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0060">Section 6.2</a>.</p>
<p id="p0940" class="para_indented">Methods, such as PRISM, can be described as <em>separate-and-conquer</em> algorithms: You identify a rule that covers many instances in the class (and excludes ones not in the class), separate out the covered instances because they are already taken care of <a id="p116"></a>by the rule, and continue with the process on those that remain. This contrasts with the divide-and-conquer approach of decision trees. The “separate” step results in an efficient method because the instance set continually shrinks as the operation proceeds.</p>
</div>
</div>
<div id="s0085">
<h2 id="st0085">4.5 Mining association rules</h2>
<p id="p0945" class="noindent">Association rules are like classification rules. You could find them in the same way, by executing a divide-and-conquer rule-induction procedure for each possible expression that could occur on the right side of the rule. However, not only might any attribute occur on the right side with any possible value, but a single association rule often predicts the value of more than one attribute. To find such rules, you would have to execute the rule-induction procedure once for every possible <em>combination</em> of attributes, with every possible combination of values, on the right side. That would result in an enormous number of association rules, which would then have to be pruned down on the basis of their <em>coverage</em> (the number of instances that they predict correctly) and their <em>accuracy</em> (the same number expressed as a proportion of the number of instances to which the rule applies). This approach is quite infeasible. (Note that, as we mentioned in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a>, what we are calling <em>coverage</em> is often called <em>support</em> and what we are calling <em>accuracy</em> is often called <em>confidence</em>.)</p>
<p id="p0950" class="para_indented">Instead, we capitalize on the fact that we are only interested in association rules with high coverage. We ignore, for the moment, the distinction between the left and right sides of a rule and seek combinations of attribute–value pairs that have a prespecified minimum coverage. These are called <em>item sets</em>: An attribute–value pair is an <em>item</em>. The terminology derives from market basket analysis, in which the items are articles in your shopping cart and the supermarket manager is looking for associations among these purchases.</p>
<div id="s0090">
<h3 id="st0090">Item Sets</h3>
<p id="p0955" class="noindent">The first column of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0060">Table 4.10</a> shows the individual items for the weather data in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#t0015">Table 1.2</a> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0001.html#p10">page 10</a>), with the number of times each item appears in the dataset given at the right. These are the one-item sets. The next step is to generate the two-item sets by making pairs of the one-item sets. Of course, there is no point in generating a set containing two different values of the same attribute (such as <em>outlook</em> = <em>sunny</em> and <em>outlook</em> = <em>overcast</em>) because that cannot occur in any actual instance.</p><a id="p117"></a><a id="p118"></a><p class="table_caption"><span class="tab_num">Table 4.10. </span> Item Sets for Weather Data with Coverage 2 or Greater</p>
<p id="t0060" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0060.jpg" alt="Image" width="864" height="565" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0060.jpg"></p>
<p id="t0060a" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0060a.jpg" alt="Image" width="863" height="497" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0060a.jpg"></p>
<p id="p0960" class="para_indented">Assume that we seek association rules with minimum coverage 2; thus, we discard any item sets that cover fewer than two instances. This leaves 47 two-item sets, some of which are shown in the second column along with the number of times they appear. The next step is to generate the three-item sets, of which 39 have a coverage of 2 or greater. There are six four-item sets, and no five-item sets—for this data, a five-item set with coverage 2 or greater could only correspond to a repeated instance. The first rows of the table, for example, show that there are five days when <em>outlook</em> = <em>sunny</em>, two of which have <em>temperature</em> = <em>hot</em>, and, in fact, on both of those days <em>humidity</em> = <em>high</em> and <em>play</em> = <em>no</em> as well.</p>
</div>
<div id="s0095">
<h3 id="st0095"><a id="p119"></a>Association Rules</h3>
<p id="p0965" class="noindent">Shortly we will explain how to generate these item sets efficiently. But first let us finish the story. Once all item sets with the required coverage have been generated, the next step is to turn each into a rule, or a set of rules, with at least the specified minimum accuracy. Some item sets will produce more than one rule; others will produce none. For example, there is one three-item set with a coverage of 4 (row 38 of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0060">Table 4.10</a>):</p>
<p id="p0970" class="noindent"><span class="monospace">humidity = normal, windy = false, play = yes</span></p>
<p id="p0975" class="para_indented">This set leads to seven potential rules:</p>
<p id="p0980" class="noindent"><span class="monospace">If humidity = normal and windy = false then play = yes   4/4</span></p>
<p id="p0985" class="noindent"><span class="monospace">If humidity = normal and play = yes then windy = false   4/6</span></p>
<p id="p0990" class="noindent"><span class="monospace">If windy = false and play = yes then humidity = normal   4/6</span></p>
<p id="p0995" class="noindent"><span class="monospace">If humidity = normal then windy = false and play = yes   4/7</span></p>
<p id="p1000" class="noindent"><span class="monospace">If windy = false then humidity = normal and play = yes   4/8</span></p>
<p id="p1005" class="noindent"><span class="monospace">If play = yes then humidity = normal and windy = false   4/9</span></p>
<p id="p1010" class="noindent"><span class="monospace">If – then humidity = normal and windy = false and play = yes  4/14</span></p>
<p id="p1015" class="noindent">The figures at the right in this list show the number of instances for which all three conditions are true—that is, the coverage—divided by the number of instances for which the conditions in the antecedent are true. Interpreted as a fraction, they represent the proportion of instances on which the rule is correct—that is, its accuracy. Assuming that the minimum specified accuracy is 100%, only the first of these rules will make it into the final rule set. The denominators of the fractions are readily obtained by looking up the antecedent expression in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0060">Table 4.10</a> (although some are not shown in the table). The final rule above has no conditions in the antecedent, and its denominator is the total number of instances in the dataset.</p>
<p id="p1020" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0065">Table 4.11</a> shows the final rule set for the weather data, with minimum coverage 2 and minimum accuracy 100%, sorted by coverage. There are 58 rules, 3 with coverage 4, 5 with coverage 3, and 50 with coverage 2. Only 7 have two conditions in the consequent, and none has more than two. The first rule comes from the item set described previously. Sometimes several rules arise from the same item set. For example, rules 9, 10, and 11 all arise from the four-item set in row 6 of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0060">Table 4.10</a>:</p><a id="p120"></a><a id="p121"></a><p class="table_caption"><span class="tab_num">Table 4.11. </span> Association Rules for Weather Data</p>
<p id="t0065" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0065.jpg" alt="Image" width="561" height="850" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0065.jpg"></p>
<p id="t0065a" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000043tabt0065a.jpg" alt="Image" width="562" height="795" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000043tabt0065a.jpg"></p>
<p id="p1025" class="noindent"><span class="monospace">temperature = cool, humidity = normal, windy = false, play = yes</span></p>
<p id="p1030" class="noindent">which has coverage 2. Three subsets of this item set also have coverage 2:</p>
<p id="p1035" class="noindent"><span class="monospace">temperature = cool, windy = false</span></p>
<p id="p1040" class="noindent"><span class="monospace">temperature = cool, humidity = normal, windy = false</span></p>
<p id="p1045" class="noindent"><span class="monospace">temperature = cool, windy = false, play = yes</span></p>
<p id="p1050" class="noindent">and these lead to rules 9, 10, and 11, all of which are 100% accurate (on the training data).</p>
</div>
<div id="s0100">
<h3 id="st0100"><a id="p122"></a>Generating Rules Efficiently</h3>
<p id="p1055" class="noindent">We now consider in more detail an algorithm for producing association rules with specified minimum coverage and accuracy. There are two stages: generating item sets with the specified minimum coverage, and from each item set determining the rules that have the specified minimum accuracy.</p>
<p id="p1060" class="para_indented">The first stage proceeds by generating all one-item sets with the given minimum coverage (the first column of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#t0060">Table 4.10</a>) and then using this to generate the two-item sets (second column), three-item sets (third column), and so on. Each operation involves a pass through the dataset to count the items in each set, and after the pass the surviving item sets are stored in a hash table—a standard data structure that allows elements stored in it to be found very quickly. From the one-item sets, candidate two-item sets are generated, and then a pass is made through the dataset, counting the coverage of each two-item set; at the end the candidate sets with less than minimum coverage are removed from the table. The candidate two-item sets are simply all of the one-item sets taken in pairs, because a two-item set cannot have the minimum coverage unless both its constituent one-item sets have the minimum coverage, too. This applies in general: A three-item set can only have the minimum coverage if all three of its two-item subsets have minimum coverage as well, and similarly for four-item sets.</p>
<p id="p1065" class="para_indented">An example will help to explain how candidate item sets are generated. Suppose there are five three-item sets—(A B C), (A B D), (A C D), (A C E), and (B C D)—where, for example, A is a feature such as <em>outlook</em> = <em>sunny</em>. The union of the first two, (A B C D), is a candidate four-item set because its other three-item subsets (A C D) and (B C D) have greater than minimum coverage. If the three-item sets are sorted into lexical order, as they are in this list, then we need only consider pairs with the same first two members. For example, we do not consider (A C D) and (B C D) because (A B C D) can also be generated from (A B C) and (A B D), and if these two are not candidate three-item sets, then (A B C D) cannot be a candidate four-item set. This leaves the pairs (A B C) and (A B D), which we have already explained, and (A C D) and (A C E). This second pair leads to the set (A C D E) whose three-item subsets do not all have the minimum coverage, so it is discarded. The hash table assists with this check: We simply remove each item from the set in turn and check that the remaining three-item set is indeed present in the hash table. Thus, in this example there is only one candidate four-item set, (A B C D). Whether or not it actually has minimum coverage can only be determined by checking the instances in the dataset.</p>
<p id="p1070" class="para_indented">The second stage of the procedure takes each item set and generates rules from it, checking that they have the specified minimum accuracy. If only rules with a single test on the right side were sought, it would be simply a matter of considering each condition in turn as the consequent of the rule, deleting it from the item set, and dividing the coverage of the entire item set by the coverage of the resulting subset—obtained from the hash table—to yield the accuracy of the corresponding rule. Given that we are also interested in association rules with multiple tests in the <a id="p123"></a>consequent, it looks like we have to evaluate the effect of placing each <em>subset</em> of the item set on the right side, leaving the remainder of the set as the antecedent.</p>
<p id="p1075" class="para_indented">This brute-force method will be excessively computation intensive unless item sets are small, because the number of possible subsets grows exponentially with the size of the item set. However, there is a better way. We observed when describing association rules in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0025">Section 3.4</a> that if the double-consequent rule</p>
<p id="p1080" class="noindent"><span class="monospace">If windy = false and play = no</span></p>
<p id="p1085" class="para_indented"><span class="monospace"> then outlook = sunny and humidity = high</span></p>
<p id="p1090" class="para_indented">holds with a given minimum coverage and accuracy, then both single-consequent rules formed from the same item set must also hold:</p>
<p id="p1095" class="noindent"><span class="monospace">If humidity = high and windy = false and play = no</span></p>
<p id="p1100" class="para_indented"><span class="monospace"> then outlook = sunny</span></p>
<p id="p1105" class="noindent"><span class="monospace">If outlook = sunny and windy = false and play = no</span></p>
<p id="p1110" class="para_indented"><span class="monospace"> then humidity = high</span></p>
<p id="p1115" class="para_indented">Conversely, if one or other of the single-consequent rules does not hold, there is no point in considering the double-consequent one. This gives a way of building up from single-consequent rules to candidate double-consequent ones, from double-consequent rules to candidate triple-consequent ones, and so on. Of course, each candidate rule must be checked against the hash table to see if it really does have more than the specified minimum accuracy. But this generally involves checking far fewer rules than the brute-force method. It is interesting that this way of building up candidate (<em>n</em> + 1)-consequent rules from actual <em>n</em>-consequent ones is really just the same as building up candidate (<em>n</em> + 1)-item sets from actual <em>n</em>-item sets, described earlier.</p>
</div>
<div id="s0105">
<h3 id="st0105">Discussion</h3>
<p id="p1120" class="noindent">Association rules are often sought for very large datasets, and efficient algorithms are highly valued. The method we have described makes one pass through the dataset for each different size of item set. Sometimes the dataset is too large to read in to main memory and must be kept on disk; then it may be worth reducing the number of passes by checking item sets of two consecutive sizes at the same time. For example, once sets with two items have been generated, all sets of three items could be generated from them before going through the instance set to count the actual number of items in the sets. More three-item sets than necessary would be considered, but the number of passes through the entire dataset would be reduced.</p>
<p id="p1125" class="para_indented">In practice, the amount of computation needed to generate association rules depends critically on the minimum coverage specified. The accuracy has less influence because it does not affect the number of passes that must be made through the dataset. In many situations we would like to obtain a certain number of rules—say 50—with the greatest possible coverage at a prespecified minimum accuracy level. One way to do this is to begin by specifying the coverage to be rather high and to <a id="p124"></a>then successively reduce it, reexecuting the entire rule-finding algorithm for each of the coverage values and repeating until the desired number of rules has been generated.</p>
<p id="p1130" class="para_indented">The tabular input format that we use throughout this book, and in particular the standard ARFF format based on it, is very inefficient for many association-rule problems. Association rules are often used in situations where attributes are binary—either present or absent—and most of the attribute values associated with a given instance are absent. This is a case for the sparse data representation described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#s0035">Section 2.4</a>; the same algorithm for finding association rules applies.</p>
</div>
</div>
<div id="s0110">
<h2 id="st0110">4.6 Linear models</h2>
<p id="p1135" class="noindent">The methods we have been looking at for decision trees and rules work most naturally with nominal attributes. They can be extended to numeric attributes either by incorporating numeric-value tests directly into the decision tree or rule-induction scheme, or by prediscretizing numeric attributes into nominal ones. We will see how in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapters 6</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0007.html#c0007" class="totri-footnote">7</a>, respectively. However, there are methods that work most naturally with numeric attributes, namely the linear models introduced in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0015">Section 3.2</a>; we examine them in more detail here. They can form components of more complex learning methods, which we will investigate later.</p>
<div id="s0115">
<h3 id="st0115">Numeric Prediction: Linear Regression</h3>
<p id="p1140" class="noindent">When the outcome, or class, is numeric, and all the attributes are numeric, linear regression is a natural technique to consider. This is a staple method in statistics. The idea is to express the class as a linear combination of the attributes, with predetermined weights:</p>
<p class="figure" id="e0215"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si42.jpg" alt="image" width="271" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si42.jpg"></p>
<p class="noindent">where <em>x</em> is the class; <em>a</em> <span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>, …, <em>a<span class="sub">k</span>
</em> are the attribute values; and <em>w</em>
<span class="sub">0</span>, <em>w</em>
<span class="sub">1</span>, …, <em>w<span class="sub">k</span>
</em> are weights.</p><a id="p1145"></a><div class="boxg" id="b0015">
<p id="p1150" class="noindent">The weights are calculated from the training data. Here the notation gets a little heavy, because we need a way of expressing the attribute values for each training instance. The first instance will have a class, say <em>x</em><sup>(1)</sup>, and attribute values <em>a</em>
<span class="sub">1</span><sup>(1)</sup>, <em>a</em>
<span class="sub">2</span><sup>(1)</sup>, … , <em>a<span class="sub">k</span></em><sup>(1)</sup>, where the superscript denotes that it is the first example. Moreover, it is notationally convenient to assume an extra attribute <em>a</em>
<span class="sub">0</span>, with a value that is always 1.</p>
<p id="p1155" class="para_indented">The predicted value for the first instance’s class can be written as</p>
<p class="figure" id="e0220"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si43.jpg" alt="image" width="348" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si43.jpg"></p>
<p id="p1160" class="para_indented">This is the predicted, not the actual, value for the class. Of interest is the difference between the predicted and actual values. The method of linear regression is to choose the <a id="p125"></a>coefficients <em>w<span class="sub">j</span></em>—there are <em>k</em> + 1 of them—to minimize the sum of the squares of these differences over all the training instances. Suppose there are <em>n</em> training instances; denote the <em>i</em>th one with a superscript (<em>i</em>). Then the sum of the squares of the differences is</p>
<p class="figure" id="e0225"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si44.jpg" alt="image" width="152" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si44.jpg"></p>
<p class="noindent">where the expression inside the parentheses is the difference between the <em>i</em>th instance’s actual class and its predicted class. This sum of squares is what we have to minimize by choosing the coefficients appropriately.</p>
<p id="p1165" class="para_indented">This is all starting to look rather formidable. However, the minimization technique is straightforward if you have the appropriate math background. Suffice it to say that given enough examples—roughly speaking, more examples than attributes—choosing weights to minimize the sum of the squared differences is really not difficult. It does involve a matrix inversion operation, but this is readily available as prepackaged software.</p>
</div>
<p id="p1170" class="para_indented">Once the math has been accomplished, the result is a set of numeric weights, based on the training data, which can be used to predict the class of new instances. We saw an example of this when looking at the CPU performance data, and the actual numeric weights are given in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#f0025">Figure 3.4</a>(a) (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#p68">page 68</a>). This formula can be used to predict the CPU performance of new test instances.</p>
<p id="p1175" class="para_indented">Linear regression is an excellent, simple method for numeric prediction, and it has been widely used in statistical applications for decades. Of course, linear models suffer from the disadvantage of, well, linearity. If the data exhibits a nonlinear dependency, the best-fitting straight line will be found, where “best” is interpreted as the least mean-squared difference. This line may not fit very well. However, linear models serve well as building blocks for more complex learning methods.</p>
</div>
<div id="s0120">
<h3 id="st0120">Linear Classification: Logistic Regression</h3>
<p id="p1180" class="noindent">Linear regression can easily be used for classification in domains with numeric attributes. Indeed, we can use <em>any</em> regression technique, whether linear or nonlinear, for classification. The trick is to perform a regression for each class, setting the output equal to 1 for training instances that belong to the class and 0 for those that do not. The result is a linear expression for the class. Then, given a test example of unknown class, calculate the value of each linear expression and choose the one that is largest. This scheme is sometimes called <em>multiresponse linear regression</em>.</p>
<p id="p1185" class="para_indented">One way of looking at multiresponse linear regression is to imagine that it approximates a numeric <em>membership function</em> for each class. The membership function is 1 for instances that belong to that class and 0 for other instances. Given a new instance, we calculate its membership for each class and select the biggest.</p>
<p id="p1190" class="para_indented">Multiresponse linear regression often yields good results in practice. However, it has two drawbacks. First, the membership values it produces are not proper probabilities because they can fall outside the range 0 to 1. Second, least-squares regression assumes that the errors are not only statistically independent but are also <a id="p126"></a>normally distributed with the same standard deviation, an assumption that is blatently violated when the method is applied to classification problems because the observations only ever take on the values 0 and 1.</p>
<p id="p1195" class="para_indented">A related statistical technique called <em>logistic regression</em> does not suffer from these problems. Instead of approximating the 0 and 1 values directly, thereby risking illegitimate probability values when the target is overshot, logistic regression builds a linear model based on a transformed target variable.</p><a id="p1200"></a><div class="boxg" id="b0020">
<p id="p1205" class="noindent">Suppose first that there are only two classes. Logistic regression replaces the original target variable</p>
<p class="figure" id="e0230"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si45.jpg" alt="image" width="138" height="27" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si45.jpg"></p>
<p class="noindent">which cannot be approximated accurately using a linear function, by</p>
<p class="figure" id="e0235"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si46.jpg" alt="image" width="342" height="27" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si46.jpg"></p>
<p id="p1210" class="para_indented">The resulting values are no longer constrained to the interval from 0 to 1 but can lie anywhere between negative infinity and positive infinity. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0050">Figure 4.9(a)</a> plots the transformation function, which is often called the <em>logit transformation</em>.</p>
<p id="f0050" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-009ab-9780123748560.jpg" alt="image" width="513" height="197" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-009ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.9</span> Logistic regression: (a) the logit transformation and (b) example logistic regression function.</p>
<p id="p1215" class="para_indented">The transformed variable is approximated using a linear function just like the ones generated by linear regression. The resulting model is</p>
<p class="figure" id="e0240"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si47.jpg" alt="image" width="394" height="27" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si47.jpg"></p>
<p class="noindent">with weights <em>w</em>. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0050">Figure 4.9(b)</a> shows an example of this function in one dimension, with two weights <em>w</em><span class="sub">0</span> = –1.25 and <em>w</em><span class="sub">1</span> = 0.5.</p>
<p id="p1220" class="para_indented">Just as in linear regression, weights must be found that fit the training data well. Linear regression measures goodness of fit using the squared error. In logistic regression the <em>log-likelihood</em> of the model is used instead. This is given by</p>
<p class="figure" id="e0245"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si48.jpg" alt="image" width="521" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si48.jpg"></p>
<p class="noindent">where the <em>x</em><sup>(</sup><em><sup>i</sup></em><sup>)</sup> are either 0 or 1.</p>
<p id="p1225" class="para_indented">The weights <em>w<span class="sub">i</span>
</em> need to be chosen to maximize the log-likelihood. There are several methods for solving this maximization problem. A simple one is to iteratively solve a sequence of weighted least-squares regression problems until the log-likelihood converges to a maximum, which usually happens in a few iterations.</p>
<p id="p1230" class="para_indented">To generalize logistic regression to several classes, one possibility is to proceed in the way described above for multiresponse linear regression by performing logistic regression independently for each class. Unfortunately, the resulting probability estimates will not sum to 1. To obtain proper probabilities it is necessary to couple the individual models for each class. This yields a joint optimization problem, and there are efficient solution methods for this.</p>
</div>
<p id="p1235" class="para_indented">The use of linear functions for classification can easily be visualized in instance space. The decision boundary for two-class logistic regression lies where the prediction probability is 0.5—that is:</p>
<p class="figure" id="e0250"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si49.jpg" alt="image" width="517" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si49.jpg"></p>
<p id="p1240" class="para_indented"><a id="p127"></a>This occurs when</p>
<p class="figure" id="e0255"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si50.jpg" alt="image" width="227" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si50.jpg"></p>
<p id="p1245" class="para_indented">Because this is a linear equality in the attribute values, the boundary is a plane, or <em>hyperplane</em>, in instance space. It is easy to visualize sets of points that cannot be separated by a single hyperplane, and these cannot be discriminated correctly by logistic regression.</p>
<p id="p1250" class="para_indented">Multiresponse linear regression suffers from the same problem. Each class receives a weight vector calculated from the training data. Focus for the moment on a particular pair of classes. Suppose the weight vector for class 1 is</p>
<p class="figure" id="e0260"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si51.jpg" alt="image" width="298" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si51.jpg"></p>
<p class="noindent">and the same for class 2 with appropriate superscripts. Then an instance will be assigned to class 1 rather than class 2 if</p>
<p class="figure" id="e0265"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si52.jpg" alt="image" width="473" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si52.jpg"></p>
<p id="p1255" class="para_indented">In other words, it will be assigned to class 1 if</p>
<p class="figure" id="e0270"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si53.jpg" alt="image" width="473" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si53.jpg"></p>
<p id="p1260" class="para_indented">This is a linear inequality in the attribute values, so the boundary between each pair of classes is a hyperplane.</p>
</div>
<div id="s0125">
<h3 id="st0125">Linear Classification Using the Perceptron</h3>
<p id="p1265" class="noindent">Logistic regression attempts to produce accurate probability estimates by maximizing the probability of the training data. Of course, accurate probability estimates <a id="p128"></a>lead to accurate classifications. However, it is not necessary to perform probability estimation if the sole purpose of the model is to predict class labels. A different approach is to learn a hyperplane that separates the instances pertaining to the different classes—let’s assume that there are only two of them. If the data can be separated perfectly into two groups using a hyperplane, it is said to be <em>linearly separable</em>. It turns out that if the data is linearly separable, there is a very simple algorithm for finding a separating hyperplane.</p>
<p id="p1270" class="para_indented">The algorithm is called the <em>perceptron learning rule</em>. Before looking at it in detail, let’s examine the equation for a hyperplane again:</p>
<p class="figure" id="e0275"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si54.jpg" alt="image" width="292" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si54.jpg"></p>
<p id="p1275" class="para_indented">Here, <em>a</em>
<span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>, … , <em>a<span class="sub">k</span>
</em> are the attribute values, and <em>w</em>
<span class="sub">0</span>, <em>w</em>
<span class="sub">1</span>, … , <em>w<span class="sub">k</span>
</em> are the weights that define the hyperplane. We will assume that each training instance <em>a</em>
<span class="sub">1</span>, <em>a</em>
<span class="sub">2</span>, … is extended by an additional attribute <em>a</em>
<span class="sub">0</span> that always has the value 1 (as we did in the case of linear regression). This extension, which is called the <em>bias</em>, just means that we don’t have to include an additional constant element in the sum. If the sum is greater than 0, we will predict the first class; otherwise, we will predict the second class. We want to find values for the weights so that the training data is correctly classified by the hyperplane.</p>
<p id="p1280" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0055">Figure 4.10(a)</a> gives the perceptron learning rule for finding a separating hyperplane. The algorithm iterates until a perfect solution has been found, but it will only work properly if a separating hyperplane exists—that is, if the data is linearly separable. Each iteration goes through all the training instances. If a misclassified instance is encountered, the parameters of the hyperplane are changed so that the misclassified instance moves closer to the hyperplane or maybe even across the hyperplane onto the correct side. If the instance belongs to the first class, this is done by adding its attribute values to the weight vector; otherwise, they are subtracted from it.</p>
<p id="f0055" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-010ab-9780123748560.jpg" alt="image" width="514" height="377" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-010ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.10</span> The perceptron: (a) learning rule, and (b) representation as a neural network.</p>
<p id="p1285" class="para_indented">To see why this works, consider the situation after an instance <em>a</em> pertaining to the first class has been added:</p>
<p class="figure" id="e0280"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si55.jpg" alt="image" width="471" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si55.jpg"></p>
<p id="p1290" class="para_indented">This means that the output for <em>a</em> has increased by</p>
<p class="figure" id="e0285"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si56.jpg" alt="image" width="325" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si56.jpg"></p>
<p id="p1295" class="para_indented">This number is always positive. Thus, the hyperplane has moved in the correct direction for classifying instance <em>a</em> as positive. Conversely, if an instance belonging to the second class is misclassified, the output for that instance decreases after the modification, again moving the hyperplane in the correct direction.</p>
<p id="p1300" class="para_indented">These corrections are incremental, and can interfere with earlier updates. However, it can be shown that the algorithm converges in a finite number of iterations if the data is linearly separable. Of course, if the data is not linearly separable, the algorithm will not terminate, so an upper bound needs to be imposed on the number of iterations when this method is applied in practice.</p>
<p id="p1305" class="para_indented"><a id="p129"></a>The resulting hyperplane is called a <em>perceptron</em>, and it’s the grandfather of neural networks (we return to neural networks in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>). <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0055">Figure 4.10(b)</a> represents the perceptron as a graph with nodes and weighted edges, imaginatively termed a “network” of “neurons.” There are two layers of nodes: input and output. The input layer has one node for every attribute, plus an extra node that is always set to 1. The output layer consists of just one node. Every node in the input layer is connected to the output layer. The connections are weighted, and the weights are those numbers found by the perceptron learning rule.</p>
<p id="p1310" class="para_indented">When an instance is presented to the perceptron, its attribute values serve to “activate” the input layer. They are multiplied by the weights and summed up at the output node. If the weighted sum is greater than 0 the output signal is 1, representing the first class; otherwise, it is –1, representing the second.</p>
</div>
<div id="s0130">
<h3 id="st0130">Linear Classification Using Winnow</h3>
<p id="p1315" class="noindent">The perceptron algorithm is not the only method that is guaranteed to find a separating hyperplane for a linearly separable problem. For datasets with binary attributes there is an alternative known as <em>Winnow</em>, which is illustrated in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0060">Figure 4.11(a)</a>. <a id="p130"></a>The structure of the two algorithms is very similar. Like the perceptron, Winnow only updates the weight vector when a misclassified instance is encountered—it is <em>mistake driven</em>.</p>
<p id="f0060" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-011ab-9780123748560.jpg" alt="image" width="514" height="435" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-011ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.11</span> The Winnow algorithm: (a) unbalanced version and (b) balanced version.</p>
<p id="p1320" class="para_indented">The two methods differ in how the weights are updated. The perceptron rule employs an additive mechanism that alters the weight vector by adding (or subtracting) the instance’s attribute vector. Winnow employs multiplicative updates and alters weights individually by multiplying them by a user-specified parameter <em>α</em> (or its inverse). The attribute values <em>a<span class="sub">i</span>
</em> are either 0 or 1 because we are working with binary data. Weights are unchanged if the attribute value is 0, because then they do not participate in the decision. Otherwise, the multiplier is <em>α</em> if that attribute helps to make a correct decision and 1/<em>α</em> if it does not.</p>
<p id="p1325" class="para_indented">Another difference is that the threshold in the linear function is also a user-specified parameter. We call this threshold <em>θ</em> and classify an instance as belonging to class 1 if and only if</p>
<p class="figure" id="e0290"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si57.jpg" alt="image" width="292" height="29" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si57.jpg"></p>
<p id="p1330" class="para_indented"><a id="p131"></a>The multiplier <em>α</em> needs to be greater than 1, and the <em>w<span class="sub">i</span>
</em> are set to a constant at the start.</p>
<p id="p1335" class="para_indented">The algorithm we have described doesn’t allow for negative weights, which—depending on the domain—can be a drawback. However, there is a version, called <em>Balanced Winnow</em>, which does allow them. This version maintains two weight vectors, one for each class. An instance is classified as belonging to class 1 if</p>
<p class="figure" id="e0295"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si58.jpg" alt="image" width="448" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si58.jpg"></p>
<p id="p1340" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0060">Figure 4.11(b)</a> shows the balanced algorithm.</p>
<p id="p1345" class="para_indented">Winnow is very effective in homing in on the relevant features in a dataset; therefore, it is called an <em>attribute-efficient</em> learner. That means that it may be a good candidate algorithm if a dataset has many (binary) features and most of them are irrelevant. Both Winnow and the perceptron algorithm can be used in an online setting in which new instances arrive continuously, because they can incrementally update their hypotheses as new instances arrive.</p>
</div>
</div>
<div id="s0135">
<h2 id="st0135">4.7 Instance-based learning</h2>
<p id="p1350" class="noindent">In instance-based learning the training examples are stored verbatim, and a distance function is used to determine which member of the training set is closest to an unknown test instance. Once the nearest training instance has been located, its class is predicted for the test instance. The only remaining problem is defining the distance function, and that is not very difficult to do, particularly if the attributes are numeric.</p>
<div id="s0140">
<h3 id="st0140">Distance Function</h3>
<p id="p1355" class="noindent">Although there are other possible choices, most instance-based learners use Euclidean distance. The distance between an instance with attribute values <em>a</em>
<span class="sub">1</span><sup>(1)</sup>, <em>a</em>
<span class="sub">2</span><sup>(1)</sup>, … , <em>a<span class="sub">k</span>
</em><sup>(1)</sup> (where <em>k</em> is the number of attributes) and one with values <em>a</em>
<span class="sub">1</span><sup>(2)</sup>, <em>a</em>
<span class="sub">2</span><sup>(2)</sup>, … , <em>a<span class="sub">k</span>
</em><sup>(2)</sup> is defined as</p>
<p class="figure" id="e0300"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si59.jpg" alt="image" width="429" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si59.jpg"></p>
<p id="p1360" class="para_indented">When comparing distances it is not necessary to perform the square root operation—the sums of squares can be compared directly. One alternative to the Euclidean distance is the Manhattan, or city-block, metric, where the difference between attribute values is not squared but just added up (after taking the absolute value). Others are obtained by taking powers higher than the square. Higher powers increase the influence of large differences at the expense of small differences. Generally, the Euclidean distance represents a good compromise. Other distance metrics may be more appropriate in special circumstances. The key is to think of actual instances and what it means for them to be separated by a certain distance—what would twice that distance mean, for example?</p>
<p id="p1365" class="para_indented"><a id="p132"></a>Different attributes are often measured on different scales, so if the Euclidean distance formula were used directly, the effect of some attributes might be completely dwarfed by others that had larger scales of measurement. Consequently, it is usual to normalize all attribute values to lie between 0 and 1 by calculating</p>
<p class="figure" id="e0305"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043si60.jpg" alt="image" width="171" height="60" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043si60.jpg"></p>
<p class="noindent">where <em>v<span class="sub">i</span> </em> is the actual value of attribute <em>i</em>, and the maximum and minimum are taken over all instances in the training set.</p>
<p id="p1370" class="para_indented">These formulae implicitly assume numeric attributes. Here the difference between two values is just the numerical difference between them, and it is this difference that is squared and added to yield the distance function. For nominal attributes that take on values that are symbolic rather than numeric, the difference between two values that are not the same is often taken to be 1, whereas if the values are the same the difference is 0. No scaling is required in this case because only the values 0 and 1 are used.</p>
<p id="p1375" class="para_indented">A common policy for handling missing values is as follows. For nominal attributes, assume that a missing feature is maximally different from any other feature value. Thus, if either or both values are missing, or if the values are different, the difference between them is taken as 1; the difference is 0 only if they are not missing and both are the same. For numeric attributes, the difference between two missing values is also taken as 1. However, if just one value is missing, the difference is often taken as either the (normalized) size of the other value or 1 minus that size, whichever is larger. This means that if values are missing, the difference is as large as it can possibly be.</p>
</div>
<div id="s0145">
<h3 id="st0145">Finding Nearest Neighbors Efficiently</h3>
<p id="p1380" class="noindent">Although instance-based learning is simple and effective, it is often slow. The obvious way to find which member of the training set is closest to an unknown test instance is to calculate the distance from every member of the training set and select the smallest. This procedure is linear in the number of training instances. In other words, the time it takes to make a single prediction is proportional to the number of training instances. Processing an entire test set takes time proportional to the product of the number of instances in the training and test sets.</p>
<p id="p1385" class="para_indented">Nearest neighbors can be found more efficiently by representing the training set as a tree, although it is not quite obvious how. One suitable structure is a <em>kD-tree</em>. This is a binary tree that divides the input space with a hyperplane and then splits each partition again, recursively. All splits are made parallel to one of the axes, either vertically or horizontally, in the two-dimensional case. The data structure is called a <em>kD-tree</em> because it stores a set of points in <em>k</em>-dimensional space, with <em>k</em> being the number of attributes.</p>
<p id="p1390" class="para_indented"><a id="p133"></a><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0065">Figure 4.12(a)</a> gives a small example with <em>k</em> = 2, and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0065">Figure 4.12(b)</a> shows the four training instances it represents, along with the hyperplanes that constitute the tree. Note that these hyperplanes are <em>not</em> decision boundaries: Decisions are made on a nearest-neighbor basis as explained later. The first split is horizontal (<em>h</em>), through the point (7,4)—this is the tree’s root. The left branch is not split further: It contains the single point (2,2), which is a leaf of the tree. The right branch is split vertically (<em>v</em>) at the point (6,7). Its right child is empty, and its left child contains the point (3,8). As this example illustrates, each region contains just one point—or, perhaps, no points. Sibling branches of the tree—for example, the two daughters of the root in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0065">Figure 4.12(a)</a>—are not necessarily developed to the same depth. Every point in the training set corresponds to a single node, and up to half are leaf nodes.</p>
<p id="f0065" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-012ab-9780123748560.jpg" alt="image" width="509" height="282" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-012ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.12</span> A <em>kD</em>-tree for four training instances: (a) the tree and (b) instances and splits.</p>
<p id="p1395" class="para_indented">How do you build a <em>kD</em>-tree from a dataset? Can it be updated efficiently as new training examples are added? And how does it speed up nearest-neighbor calculations? We tackle the last question first.</p>
<p id="p1400" class="para_indented">To locate the nearest neighbor of a given target point, follow the tree down from its root to locate the region containing the target. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0070">Figure 4.13</a> shows a space like that of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0065">Figure 4.12(b)</a> but with a few more instances and an extra boundary. The target, which is not one of the instances in the tree, is marked by a star. The leaf node of the region containing the target is colored black. This is not necessarily the target’s closest neighbor, as this example illustrates, but it is a good first approximation. In particular, any nearer neighbor must lie closer—within the dashed circle in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0070">Figure 4.13</a>. To determine whether one exists, first check whether it is possible for a closer neighbor to lie within the node’s sibling. The black node’s sibling is shaded in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0070">Figure 4.13</a>, and the circle does not intersect it, so the sibling cannot contain a closer <a id="p134"></a>neighbor. Then back up to the parent node and check <em>its</em> sibling, which here covers everything above the horizontal line. In this case it <em>must</em> be explored because the area it covers intersects with the best circle so far. To explore it, find its daughters (the original point’s two aunts); check whether they intersect the circle (the left one does not, but the right one does); and descend to see if it contains a closer point (it does).</p>
<p id="f0070" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-013-9780123748560.jpg" alt="image" width="367" height="367" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-013-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.13</span> Using a <em>kD</em>-tree to find the nearest neighbor of the star.</p>
<p id="p1405" class="para_indented">In a typical case, this algorithm is far faster than examining all points to find the nearest neighbor. The work involved in finding the initial approximate nearest neighbor—the black point in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0070">Figure 4.13</a>—depends on the depth of the tree, given by the logarithm of the number of nodes, log<span class="sub">2</span><em>n</em> if the tree is well balanced. The amount of work involved in backtracking to check whether this really is the nearest neighbor depends a bit on the tree, and on how good the initial approximation is. But for a well-constructed tree with nodes that are approximately square rather than long skinny rectangles, it can also be shown to be logarithmic in the number of nodes (if the number of attributes in the dataset is not too large).</p>
<p id="p1410" class="para_indented">How do you build a good tree for a set of training examples? The problem boils down to selecting the first training instance to split at and the direction of the split. Once you can do that, apply the same method recursively to each child of the initial split to construct the entire tree.</p>
<p id="p1415" class="para_indented"><a id="p135"></a>To find a good direction for the split, calculate the variance of the data points along each axis individually, select the axis with the greatest variance, and create a splitting hyperplane perpendicular to it. To find a good place for the hyperplane, locate the median value along that axis and select the corresponding point. This makes the split perpendicular to the direction of greatest spread, with half the points lying on either side. This produces a well-balanced tree. To avoid long skinny regions it is best for successive splits to be along different axes, which is likely because the dimension of greatest variance is chosen at each stage. However, if the distribution of points is badly skewed, choosing the median value may generate several successive splits in the same direction, yielding long, skinny hyperrectangles. A better strategy is to calculate the mean rather than the median and use the point closest to that. The tree will not be perfectly balanced, but its regions will tend to be squarish because there is a greater chance that different directions will be chosen for successive splits.</p>
<p id="p1420" class="para_indented">An advantage of instance-based learning over most other machine learning methods is that new examples can be added to the training set at any time. To retain this advantage when using a <em>kD</em>-tree, we need to be able to update it incrementally with new data points. To do this, determine which leaf node contains the new point and find its hyperrectangle. If it is empty, simply place the new point there. Otherwise, split the hyperrectangle along its longest dimension to preserve squareness. This simple heuristic does not guarantee that adding a series of points will preserve the tree’s balance, nor that the hyperrectangles will be well shaped for a nearest-neighbor search. It is a good idea to rebuild the tree from scratch occasionally—for example, when its depth grows to twice the best possible depth.</p>
<p id="p1425" class="para_indented">As we have seen, <em>kD</em>-trees are good data structures for finding nearest neighbors efficiently. However, they are not perfect. Skewed datasets present a basic conflict between the desire for the tree to be perfectly balanced and the desire for regions to be squarish. More important, rectangles—even squares—are not the best shape to use anyway, because of their corners. If the dashed circle in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0070">Figure 4.13</a> were any bigger, which it would be if the black instance were a little further from the target, it would intersect the lower right corner of the rectangle at the top left and then that rectangle would have to be investigated, too—despite the fact that the training instances that define it are a long way from the corner in question. The corners of rectangular regions are awkward.</p>
<p id="p1430" class="para_indented">The solution? Use hyperspheres, not hyperrectangles. Neighboring spheres may overlap, whereas rectangles can abut, but this is not a problem because the nearest-neighbor algorithm for <em>kD</em>-trees does not depend on the regions being disjoint. A data structure called a <em>ball tree</em> defines <em>k</em>-dimensional hyperspheres (“balls”) that cover the data points, and arranges them into a tree.</p>
<p id="p1435" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0075">Figure 4.14(a)</a> shows 16 training instances in two-dimensional space, overlaid by a pattern of overlapping circles, and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0075">Figure 4.14(b)</a> shows a tree formed from these circles. Circles at different levels of the tree are indicated by different styles of dash, and the smaller circles are drawn in shades of gray. Each node of the tree represents a ball, and the node is dashed or shaded according to the same convention <a id="p136"></a>so that you can identify which level the balls are at. To help you understand the tree, numbers are placed on the nodes to show how many data points are deemed to be inside that ball. But be careful: This is not necessarily the same as the number of points falling within the spatial region that the ball represents. The regions at each level sometimes overlap, but points that fall into the overlap area are assigned to only one of the overlapping balls (the diagram does not show which one). Instead of the occupancy counts in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0075">Figure 4.14(b)</a>, the nodes of actual ball trees store the center and radius of their ball; leaf nodes record the points they contain as well.</p>
<p id="f0075" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-014ab-9780123748560.jpg" alt="image" width="514" height="267" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-014ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.14</span> Ball tree for 16 training instances: (a) instances and balls and (b) the tree.</p>
<p id="p1440" class="para_indented">To use a ball tree to find the nearest neighbor to a given target, start by traversing the tree from the top down to locate the leaf that contains the target and find the closest point to the target in that ball. This gives an upper bound for the target’s distance from its nearest neighbor. Then, just as for the <em>kD</em>-tree, examine the sibling node. If the distance from the target to the sibling’s center exceeds its radius plus the current upper bound, it cannot possibly contain a closer point; otherwise, the sibling must be examined by descending the tree further.</p>
<p id="p9000" class="para_indented">In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0080">Figure 4.15</a> the target is marked with a star and the black dot is its closest currently known neighbor. The entire contents of the gray ball can be ruled out: It cannot contain a closer point because its center is too far away. Proceed recursively back up the tree to its root, examining any ball that may possibly contain a point nearer than the current upper bound.</p>
<p id="f0080" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-015-9780123748560.jpg" alt="image" width="699" height="660" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-015-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.15</span> Ruling out an entire ball (the gray one) based on a target point (star) and its current nearest neighbor.</p>
<p id="p1445" class="para_indented">Ball trees are built from the top down, and as with <em>kD</em>-trees the basic problem is to find a good way of splitting a ball containing a set of data points into two. In practice, you do not have to continue until the leaf balls contain just two points: You can stop earlier, once a predetermined minimum number is reached—and the same goes for <em>kD</em>-trees. Here is one possible splitting method. Choose the point in the ball that <a id="p137"></a>is farthest from its center, and then a second point that is farthest from the first one. Assign all data points in the ball to the closest one of these two cluster centers; then compute the centroid of each cluster and the minimum radius required for it to enclose all the data points it represents. This method has the merit that the cost of splitting a ball containing <em>n</em> points is only linear in <em>n</em>. There are more elaborate algorithms that produce tighter balls, but they require more computation. We will not describe sophisticated algorithms for constructing ball trees or updating them incrementally as new training instances are encountered.</p>
</div>
<div id="s0150">
<h3 id="st0150">Discussion</h3>
<p id="p1450" class="noindent">Nearest-neighbor instance-based learning is simple and often works very well. In the scheme we have described, each attribute has exactly the same influence on the decision, just as it does in the Naïve Bayes method. Another problem is that the database can easily become corrupted by noisy exemplars. One solution is to adopt the <em>k</em>-nearest-neighbor strategy, where some fixed, small number <em>k</em> of nearest neighbors—say five—are located and used together to determine the class of the test instance through a simple majority vote. (Note that earlier we used <em>k</em> to denote the number of attributes; this is a different, independent usage.) Another way of proofing the database against noise is to choose the exemplars that are added to it selectively and judiciously. Improved procedures, which are described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a>, address these shortcomings.</p>
<p id="p1455" class="para_indented">The nearest-neighbor method originated many decades ago, and statisticians analyzed <em>k</em>-nearest-neighbor schemes in the early 1950s. If the number of training instances is large, it makes intuitive sense to use more than one nearest neighbor, but clearly this is dangerous if there are few instances. It can be shown that when <em>k</em> and the number <em>n</em> of instances both become infinite in such a way that <em>k</em>/<em>n</em> → 0, the probability of error approaches the theoretical minimum for the dataset. The nearest-neighbor method was adopted as a classification scheme in the early 1960s and has been widely used in the field of pattern recognition for almost half a century.</p>
<p id="p1460" class="para_indented">Nearest-neighbor classification was notoriously slow until <em>kD</em>-trees began to be applied in the early 1990s, although the data structure itself was developed much earlier. In practice, these trees become inefficient when the dimension of the space <a id="p138"></a>increases and they are only worthwhile when the number of attributes is small—up to 10. Ball trees were developed much more recently and are an instance of a more general structure called a <em>metric tree</em>. Sophisticated algorithms can create metric trees that deal successfully with thousands of dimensions.</p>
<p id="p1465" class="para_indented">Instead of storing all training instances, you can compress them into regions. A very simple technique, mentioned at the end of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0010">Section 4.1</a>, is to just record the range of values observed in the training data for each attribute and category. Given a test instance, you work out which ranges the attribute values fall into and choose the category with the greatest number of correct ranges for that instance. A slightly more elaborate technique is to construct intervals for each attribute and use the training set to count the number of times each class occurs for each interval on each attribute. Numeric attributes can be discretized into intervals, and “intervals” consisting of a single point can be used for nominal ones. Then, given a test instance, you can determine which intervals the instance resides in and classify it by voting, a method called <em>voting feature intervals</em>. These methods are very approximate, but very fast, and can be useful for initial analysis of large datasets.</p>
</div>
</div>
<div id="s0155">
<h2 id="st0155">4.8 Clustering</h2>
<p id="p1470" class="noindent">Clustering techniques apply when there is no class to be predicted but the instances are to be divided into natural groups. These clusters presumably reflect some mechanism that is at work in the domain from which instances are drawn, a mechanism that causes some instances to bear a stronger resemblance to each other than they do to the remaining instances. Clustering naturally requires different techniques to the classification and association learning methods that we have considered so far.</p>
<p id="p1475" class="para_indented">As we saw in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html#s0055">Section 3.6</a>, there are different ways in which the result of clustering can be expressed. The groups that are identified may be exclusive: Any instance belongs in only one group. Or they may be overlapping: An instance may fall into several groups. Or they may be probabilistic: An instance belongs to each group with a certain probability. Or they may be hierarchical: A rough division of instances into groups at the top level and each group refined further—perhaps all the way down to individual instances. Really, the choice among these possibilities should be dictated by the nature of the mechanisms that are thought to underlie the particular clustering phenomenon. However, because these mechanisms are rarely known—the very existence of clusters is, after all, something that we’re trying to discover—and for pragmatic reasons too, the choice is usually dictated by the clustering tools that are available.</p>
<p id="p1480" class="para_indented">We will examine an algorithm that works in numeric domains, partitioning instances into disjoint clusters. Like the basic nearest-neighbor method of instance-based learning, it is a simple and straightforward technique that has been used for several decades. In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a> we examine newer clustering methods that perform incremental and probabilistic clustering.</p>
<div id="s0160">
<h3 id="st0160"><a id="p139"></a>Iterative Distance-Based Clustering</h3>
<p id="p1485" class="noindent">The classic clustering technique is called <em>k-means</em>. First, you specify in advance how many clusters are being sought: This is the parameter <em>k</em>. Then <em>k</em> points are chosen at random as cluster centers. All instances are assigned to their closest cluster center according to the ordinary Euclidean distance metric. Next the <em>centroid</em>, or mean, of the instances in each cluster is calculated—this is the “means” part. These centroids are taken to be new center values for their respective clusters. Finally, the whole process is repeated with the new cluster centers. Iteration continues until the same points are assigned to each cluster in consecutive rounds, at which stage the cluster centers have stabilized and will remain the same forever.</p>
<p id="p1490" class="para_indented">This clustering method is simple and effective. It is easy to prove that choosing the cluster center to be the centroid minimizes the total squared distance from each of the cluster’s points to its center. Once the iteration has stabilized, each point is assigned to its nearest cluster center, so the overall effect is to minimize the total squared distance from all points to their cluster centers. But the minimum is a local one; there is no guarantee that it is the global minimum. The final clusters are quite sensitive to the initial cluster centers. Completely different arrangements can arise from small changes in the initial random choice. In fact, this is true of all practical clustering techniques: It is almost always infeasible to find globally optimal clusters. To increase the chance of finding a global minimum people often run the algorithm several times with different initial choices and choose the best final result—the one with the smallest total squared distance.</p>
<p id="p1495" class="para_indented">It is easy to imagine situations in which <em>k</em>-means fails to find a good clustering. Consider four instances arranged at the vertices of a rectangle in two-dimensional space. There are two natural clusters, formed by grouping together the two vertices at either end of a short side. But suppose the two initial cluster centers happen to fall at the midpoints of the <em>long</em> sides. This forms a stable configuration. The two clusters each contain the two instances at either end of a long side—no matter how great the difference between the long and the short sides.</p>
<p id="p1500" class="para_indented"><em>k</em>-means clustering can be dramatically improved by careful choice of the initial cluster centers, often called <em>seeds</em>. Instead of beginning with an arbitrary set of seeds, here is a better procedure. Choose the initial seed at random from the entire space, with a uniform probability distribution. Then choose the second seed with a probability that is proportional to the square of the distance from the first. Proceed, at each stage choosing the next seed with a probability proportional to the square of the distance from the closest seed that has already been chosen. This procedure, called <em>k</em>-<em>means</em>++, improves both speed and accuracy over the original algorithm with random seeds.</p>
</div>
<div id="s0165">
<h3 id="st0165">Faster Distance Calculations</h3>
<p id="p1505" class="noindent">The <em>k</em>-means clustering algorithm usually requires several iterations, each involving finding the distance of the <em>k</em> cluster centers from every instance to determine <a id="p140"></a>its cluster. There are simple approximations that speed this up considerably. For example, you can project the dataset and make cuts along selected axes, instead of using the arbitrary hyperplane divisions that are implied by choosing the nearest cluster center. But this inevitably compromises the quality of the resulting clusters.</p>
<p id="p1510" class="para_indented">Here’s a better way of speeding things up. Finding the closest cluster center is not so different from finding nearest neighbors in instance-based learning. Can the same efficient solutions—<em>kD</em>-trees and ball trees—be used? Yes! Indeed, they can be applied in an even more efficient way, because in each iteration of <em>k</em>-means all the data points are processed together whereas, in instance-based learning, test instances are processed individually.</p>
<p id="p1515" class="para_indented">First, construct a <em>kD</em>-tree or ball tree for all the data points, which will remain static throughout the clustering procedure. Each iteration of <em>k</em>-means produces a set of cluster centers, and all data points must be examined and assigned to the nearest center. One way of processing the points is to descend the tree from the root until reaching a leaf and check each individual point in the leaf to find its closest cluster center. But it may be that the region represented by a higher interior node falls entirely within the domain of a single cluster center. In that case, all the data points under that node can be processed in one blow!</p>
<p id="p1520" class="para_indented">The aim of the exercise, after all, is to find new positions for the cluster centers by calculating the centroid of the points they contain. The centroid can be calculated by keeping a running vector sum of the points in the cluster, and a count of how many there are so far. At the end, just divide one by the other to find the centroid. Suppose that with each node of the tree we store the vector sum of the points within that node and a count of the number of points. If the whole node falls within the ambit of a single cluster, the running totals for that cluster can be updated immediately. If not, look inside the node by proceeding recursively down the tree.</p>
<p id="p1525" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0085">Figure 4.16</a> shows the same instances and ball tree as in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0075">Figure 4.14</a>, but with two cluster centers marked as black stars. Because all instances are assigned to the closest center, the space is divided in two by the thick line shown in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0085">Figure 4.16(a)</a>. Begin at the root of the tree in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0085">Figure 4.16(b)</a>, with initial values for the vector sum and counts for each cluster; all initial values are 0. Proceed recursively down the tree. When node A is reached, all points within it lie in cluster 1, so cluster 1’s sum and count can be updated with the sum and count for node A, and we need not descend any further. Recursing back to node B, its ball straddles the boundary between the clusters, so its points must be examined individually. When node C is reached, it falls entirely within cluster 2; again, we can update cluster 2 immediately and we need not descend any further. The tree is only examined down to the frontier marked by the dashed line in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#f0085">Figure 4.16(b)</a>, and the advantage is that the nodes below need not be opened—at least not on this particular iteration of <em>k</em>-means. Next time, the cluster centers will have changed and things may be different.</p>
<p id="f0085" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000043f004-016ab-9780123748560.jpg" alt="image" width="514" height="274" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000043f004-016ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 4.16</span> A ball tree: (a) two cluster centers and their dividing line and (b) the corresponding tree.</p>
</div>
<div id="s0170">
<h3 id="st0170"><a id="p141"></a>Discussion</h3>
<p id="p1530" class="noindent">Many variants of the basic <em>k</em>-means procedure have been developed. Some produce a hierarchical clustering by applying the algorithm with <em>k</em> = 2 to the overall dataset and then repeating, recursively, within each cluster.</p>
<p id="p1535" class="para_indented">How do you choose <em>k</em>? Often nothing is known about the likely number of clusters, and the whole point of clustering is to find out. One way is to try different values and choose the best. To do this you need to learn how to evaluate the success of machine learning, which is what <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#c0005">Chapter 5</a> is about. We return to clustering in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a>.</p>
</div>
</div>
<div id="s0175">
<h2 id="st0175">4.9 Multi-instance learning</h2>
<p id="p1540" class="noindent">In <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0002.html#c0002">Chapter 2</a> we introduced <em>multi-instance</em> learning, where each example in the data comprises several different instances. We call these examples <em>bags</em> (we noted the difference between bags and sets in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0025">Section 4.2</a>). In supervised multi-instance learning, a class label is associated with each bag, and the goal of learning is to determine how the class can be inferred from the instances that make up the bag. While advanced algorithms have been devised to tackle such problems, it turns out that the simplicity-first methodology can be applied here with surprisingly good results. A simple but effective approach is to manipulate the input data to transform it into a single-instance learning problem and then apply standard learning methods, <a id="p142"></a>such as the ones described in this chapter. Two such approaches are described in the following sections.</p>
<div id="s0180">
<h3 id="st0180">Aggregating the Input</h3>
<p id="p1545" class="noindent">You can convert a multiple-instance problem to a single-instance one by calculating values such as mean, mode, minimum, and maximum that summarize the instances in the bag and adding these as new attributes. Each “summary” instance retains the class label of the bag it was derived from. To classify a new bag the same process is used: A single aggregated instance is created with attributes that summarize the instances in the bag. Surprisingly, for the original drug activity dataset that spurred the development of multi-instance learning, results comparable with special-purpose multi-instance learners can be obtained using just the minimum and maximum values of each attribute for each bag, combined with a support vector machine classifier (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a>). One potential drawback of this approach is that the best summary statistics to compute depend on the problem at hand. However, the additional computational cost associated with exploring combinations of different summary statistics is offset by the fact that the summarizing process means that fewer instances are processed by the learning algorithm.</p>
</div>
<div id="s0185">
<h3 id="st0185">Aggregating the Output</h3>
<p id="p1550" class="noindent">Instead of aggregating the instances in each bag, another approach is to learn a classifier directly from the original instances that comprise the bag. To achieve this, the instances in a given bag are all assigned the bag’s class label. At classification time, a prediction is produced for each instance in the bag to be predicted, and the predictions are aggregated in some fashion to form a prediction for the bag as a whole. One approach is to treat the predictions as votes for the various class labels. If the classifier is capable of assigning probabilities to the class labels, these could be averaged to yield an overall probability distribution for the bag’s class label. This method treats the instances independently and gives them equal influence on the predicted class label.</p>
<p id="p1555" class="para_indented">One problem is that the bags in the training data can contain different numbers of instances. Ideally, each bag should have the same influence on the final model that is learned. If the learning algorithm can accept instance-level weights, this can be achieved by assigning each instance in a given bag a weight inversely proportional to the bag’s size. If a bag contains <em>n</em> instances, giving each one a weight of 1/<em>n</em> ensures that the instances contribute equally to the bag’s class label and each bag receives a total weight of 1.</p>
</div>
<div id="s0190">
<h3 id="st0190">Discussion</h3>
<p id="p1560" class="noindent">Both methods described previously for tackling multi-instance problems disregard the original multi-instance assumption that a bag is positive if and only if at least one <a id="p143"></a>of its instances is positive. Instead, making each instance in a bag contribute equally to its label is the key element that allows standard learning algorithms to be applied. Otherwise, it is necessary to try to identify the “special” instances that are the key to determining the bag’s label.</p>
</div>
</div>
<div id="s0195">
<h2 id="st0195">4.10 Further reading</h2>
<p id="p1565" class="noindent">The 1R scheme was proposed and thoroughly investigated by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib164">Holte (1993)</a>. It was never really intended as a machine learning “method.” The point was more to demonstrate that very simple structures underlie most of the practical datasets being used to evaluate machine learning schemes at the time and that putting high-powered inductive inference schemes to work on simple datasets was like using a sledgehammer to crack a nut. Why grapple with a complex decision tree when a simple rule will do? The scheme that generates one simple rule per class is due to Lucio de Souza Coelho of Brazil and Len Trigg of New Zealand, and it has been dubbed <em>hyperpipes</em>. A very simple algorithm, it has the advantage of being extremely fast and is quite feasible even with an enormous number of attributes.</p>
<p id="p1570" class="para_indented">Bayes was an eighteenth-century English philosopher who set out his theory of probability in an “Essay towards solving a problem in the doctrine of chances,” published in the <em>Philosophical Transactions of the Royal Society of London</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib19">Bayes, 1763</a>). The rule that bears his name has been a cornerstone of probability theory ever since. The difficulty with the application of Bayes’ rule in practice is the assignment of prior probabilities.</p>
<p id="p9005" class="para_indented">Some statisticians, dubbed Bayesians, take the rule as gospel and insist that people make serious attempts to estimate prior probabilities accurately—although such estimates are often subjective. Others, non-Bayesians, prefer the kind of prior-free analysis that typically generates statistical confidence intervals, which we will see in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html#c0005">Chapter 5</a>. With a particular dataset, prior probabilities for Naïve Bayes are usually reasonably easy to estimate, which encourages a Bayesian approach to learning. The independence assumption made by the Naïve Bayes method is a great stumbling block, however, and efforts are being made to apply Bayesian analysis without assuming independence. The resulting models are called <em>Bayesian networks</em> (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib157">Heckerman et al., 1995</a>), and we describe them in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0270">Section 6.7</a>.</p>
<p id="p1575" class="para_indented">Bayesian techniques had been used in the field of pattern recognition (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib89">Duda and Hart, 1973</a>) for 20 years before they were adopted by machine learning researchers (e.g., <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib201">Langley et al., 1992</a>) and made to work on datasets with redundant attributes (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib202">Langley and Sage 1994</a>) and numeric attributes (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib171">John and Langley, 1995</a>). The label <em>Naïve Bayes</em> is unfortunate because it is hard to use this method without feeling simpleminded. However, there is nothing naïve about its use in appropriate circumstances. The multinomial Naïve Bayes model, which is particularly useful for text classification, was investigated by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib221">McCallum and Nigam (1998)</a>.</p>
<p id="p1580" class="para_indented"><a id="p144"></a>The classic paper on decision tree induction is <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib255">Quinlan (1986)</a>, who describes the basic ID3 procedure developed in this chapter. A comprehensive description of the method, including the improvements that are embodied in C4.5, appears in a classic book by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib257">Quinlan (1993)</a>, which gives a listing of the complete C4.5 system, written in the C programming language. PRISM was developed by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib53">Cendrowska (1987)</a>, who also introduced the contact lens dataset.</p>
<p id="p1585" class="para_indented">Association rules are introduced and described in the database literature rather than in the machine learning literature. Here the emphasis is very much on dealing with huge amounts of data rather than on sensitive ways of testing and evaluating algorithms on limited datasets. The algorithm introduced in this chapter is the Apriori method developed by Agrawal and his associates (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib4">Agrawal et al., 1993a</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib5">1993b</a>; <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib3">Agrawal and Srikant, 1994</a>). A survey of association-rule mining appears in an article by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib57">Chen et al. (1996)</a>.</p>
<p id="p1590" class="para_indented">Linear regression is described in most standard statistical texts, and a particularly comprehensive treatment can be found in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib205">Lawson and Hanson (1995)</a>. The use of linear models for classification enjoyed a great deal of popularity in the 1960s; <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib242">Nilsson (1965)</a> is an excellent reference. He defines a <em>linear threshold unit</em> as a binary test of whether a linear function is greater or less than zero and a <em>linear machine</em> as a set of linear functions, one for each class, whose value for an unknown example is compared and the largest chosen as its predicted class. In the distant past, perceptrons fell out of favor on publication of an influential book that showed that they had fundamental limitations (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib227">Minsky and Papert, 1969</a>); however, more complex systems of linear functions have enjoyed a resurgence in recent years in the form of neural networks, described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#s0120">Section 6.4</a>. The Winnow algorithms were introduced by Nick Littlestone in his Ph.D. thesis (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib209">Littlestone, 1988</a>, <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib210">1989</a>). Multiresponse linear classifiers have found application in an operation called <em>stacking</em> that combines the output of other learning algorithms, described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0008.html#c0008">Chapter 8</a> (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib322">Wolpert, 1992</a>).</p>
<p id="p1595" class="para_indented"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib101">Fix and Hodges (1951)</a> performed the first analysis of the nearest-neighbor method, and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib172">Johns (1961)</a> pioneered its use in classification problems. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib66">Cover and Hart (1967)</a> obtained the classic theoretical result that, for large enough datasets, its probability of error never exceeds twice the theoretical minimum. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib75">Devroye et al. (1996)</a> showed that <em>k</em>-nearest neighbor is asymptotically optimal for large <em>k</em> and <em>n</em> with <em>k</em>/<em>n</em> → 0. Nearest-neighbor methods gained popularity in machine learning through the work of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib6">Aha (1992)</a>, who showed that instance-based learning can be combined with noisy exemplar pruning and attribute weighting and that the resulting methods perform well in comparison with other learning methods. We take this up again in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006.html#c0006">Chapter 6</a>.</p>
<p id="p1600" class="para_indented">The <em>kD</em>-tree data structure was developed by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib122">Friedman et al. (1977)</a>. Our description closely follows an explanation given by Andrew Moore in his Ph.D. thesis (<a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib230">Moore, 1991</a>). Moore, who, along with <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib246">Omohundro (1987)</a>, pioneered its use in machine learning. <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib231">Moore (2000)</a> describes sophisticated ways of constructing ball trees that perform well even with thousands of attributes. We took our ball tree example from lecture notes by Alexander Gray of Carnegie-Mellon University. The <a id="p145"></a>voting feature interval method mentioned in the Discussion section at the end of <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#s0135">Section 4.7</a> is described by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib73">Demiroz and Guvenir (1997)</a>.</p>
<p id="p1605" class="para_indented">The <em>k</em>-means algorithm is a classic technique, and many descriptions and variations are available (e.g., <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib154">Hartigan, 1975</a>). The <em>k</em>-means++ variant, which yields a significant improvement by choosing the initial seeds more carefully, was introduced as recently as 2007 by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib11">Arthur and Vassilvitskii (2007)</a>. The clever use of <em>kD</em>-trees to speed up <em>k</em>-means clustering, which we have chosen to illustrate using ball trees instead, was pioneered by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib234">Moore and Pelleg (2000)</a> in their <em>X</em>-means clustering algorithm. That algorithm contains some other innovations, described in <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0006a.html#s0300">Section 6.8</a>.</p>
<p id="p1610" class="para_indented">The method of dealing with multi-instance learning problems by applying standard single-instance learners to summarized bag-level data was applied in conjunction with support vector machines by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib132">Gärtner et al. (2002)</a>. The alternative approach of aggregating the output is explained by <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/bib00023.html#bib106">Frank and Xu (2003)</a>.</p>
</div>
<div id="s0200">
<h2 id="st0200">4.11 Weka implementations</h2>
<p id="p1615" class="noindent">For classifiers, see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0175">Section 11.4</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0030">Table 11.5</a>.</p><a id="p1620"></a><div class="none">
<p class="hang" id="u0060">• <a id="p1625"></a>Inferring rudimentary rules: <em>OneR, HyperPipes</em> (learns one rule per class)</p>
<p class="hang" id="u0065">• <a id="p1630"></a>Statistical modeling:</p>
<div class="none">
<p class="hang1" id="u0070">• <a id="p1635"></a><em>NaïveBayes</em> and many variants, including <em>NaiveBayesMultinomial</em></p>
</div>
<p class="hang" id="u0075">• <a id="p1640"></a>Decision trees: <em>Id3</em></p>
<p class="hang" id="u0080">• <a id="p1645"></a>Decision rules: <em>Prism</em></p>
<p class="hang" id="u0085">• <a id="p1650"></a>Association rules (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0260">Section 11.7</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0045">Table 11.8</a>): <em>a priori</em></p>
<p class="hang" id="u0090">• <a id="p1655"></a>Linear models:</p>
<div class="none">
<p class="hang1" id="u0095">• <a id="p1660"></a>SimpleLinearRegression, LinearRegression, Logistic (regression)</p>
<p class="hang1" id="u0100">• <a id="p1665"></a>VotedPerceptron, Winnow</p>
</div>
<p class="hang" id="u0105">• <a id="p1670"></a>Instance-based learning: <em>IB1</em>, <em>VFI</em> (voting feature intervals)</p>
<p class="hang" id="u0110">• <a id="p1675"></a>Clustering (see <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#s0255">Section 11.6</a> and <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0011.html#t0040">Table 11.7</a>): <em>SimpleKMeans</em></p>
<p class="hang" id="u0115">• <a id="p1680"></a>Multi-instance learning: <em>SimpleMI</em>, <em>MIWrapper</em></p>
</div>
<p class="normal"><a id="p146"></a></p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0003.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 3. Output</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0005.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 5. Credibility</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.03466336422451732"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.8538608579678231" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=795adb9e-3c3e-61d0-8469-77e347fe32e5&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Chapter%204.%20Algorithms%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fxhtml%2Fc0004.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=27009"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/safari-books-archive/site/library/view/data-mining-practical/9780123748560/xhtml/c0004.html#">Reset</a>
</div>
</div></body></html>