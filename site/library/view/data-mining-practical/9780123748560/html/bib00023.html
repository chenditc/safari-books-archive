<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/html/bib00023.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="bib00023.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/html/bib00023.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="bib00023.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Data Mining - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html"><meta name="description" content=" References Abe, N., Zadrozny, B., &amp; Langford, J. (2006). Outlier detection by active learning. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp ... "><meta property="og:title" content="Data Mining"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Data Mining"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/bib00023.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content=" References Abe, N., Zadrozny, B., &amp; Langford, J. (2006). Outlier detection by active learning. In Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (pp ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/html/bib00023.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/bib00023.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/bib00023.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/bib00023.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Data%20Mining&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/bib00023.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0017.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 17. Tutorial Exercises for the Weka Explorer</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/index00024.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Index</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="bib00023"></a>
<div id="fr0010">
<h2 class="bibliography_title" id="st0010">References</h2>
<p class="ref_entry" id="bib1">Abe, N., Zadrozny, B., &amp; Langford, J. (2006). Outlier detection by active learning. In <em>Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 767â€“772). New York: ACM Press.</p>
<p class="ref_entry" id="bib2">Adriaans, P., &amp; Zantige, D. (1996). <em>Data mining</em>. Harlow, England: Addison-Wesley.</p>
<p class="ref_entry" id="bib3">Agrawal, R., &amp; Srikant, R. (1994). Fast algorithms for mining association rules in large databases. In J. Bocca, M. Jarke, &amp; C. Zaniolo (Eds.), <em>Proceedings of the International Conference on Very Large Data Bases</em> (pp. 478â€“499). Santiago, Chile. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib4">Agrawal, R., Imielinski, T., &amp; Swami, A. (1993a). Database mining: A performance perspective. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 5(6), 914â€“925.</p>
<p class="ref_entry" id="bib5">______. (1993b). Mining association rules between sets of items in large databases. In P. Buneman, &amp; S. Jajodia (Eds.), <em>Proceedings of the ACM SIGMOD International Conference on Management of Data</em> (pp. 207â€“216). Washington, DC. New York: ACM Press.</p>
<p class="ref_entry" id="bib6">Aha, D. (1992). Tolerating noisy, irrelevant, and novel attributes in instance-based learning algorithms. <em>International Journal of Man-Machine Studies</em>, 36(2), 267â€“287.</p>
<p class="ref_entry" id="bib7">Almuallin, H., &amp; Dietterich, T. G. (1991). Learning with many irrelevant features. In <em>Proceedings of the Ninth National Conference on Artificial Intelligence</em> (pp. 547â€“552). Anaheim, CA. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib8">______. (1992). Efficient algorithms for identifying relevant features. In <em>Proceedings of the Ninth Canadian Conference on Artificial Intelligence</em> (pp. 38â€“45). Vancouver, BC. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib9">Andrews, S., Tsochantaridis, I., &amp; Hofmann, T. (2003). Support vector machines for multiple-instance learning. In <em>Proceedings of the Conference on Neural Information Processing Systems</em> (pp. 561â€“568). Vancouver, BC. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib10">Ankerst, M., Breunig, M. M., Kriegel, H.-P., &amp; Sander, J. (1999). OPTICS: Ordering points to identify the clustering structure. In <em>Proceedings of the ACM SIGMOD International Conference on Management of Data</em> (pp. 49â€“60). New York: ACM Press.</p>
<p class="ref_entry" id="bib9000">Appelt, D. (1999). An introduction to information extraction. <em>Artificial Intelligence Communications</em>, 12(3), 161â€“172.</p>
<p class="ref_entry" id="bib11">Arthur, D., &amp; Vassilvitskii, S. (2007). K-means++: The advantages of careful seeding. In <em>Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms</em> (pp. 1027â€“1035). New Orleans. Philadelphia: Society for Industrial and Applied Mathematics.</p>
<p class="ref_entry" id="bib12">Asuncion, A., &amp; Newman, D. J. (2007). <em>UCI Machine Learning Repository</em> [<em>http:// www.ics.uci.edu/~mlearn/MLRepository.html </em>]. Irvine: University of California, School of Information and Computer Science.</p>
<p class="ref_entry" id="bib13">Asmis, E. (1984). <em>Epicurusâ€™ scientific method</em>. Ithaca, NY: Cornell University Press.</p>
<p class="ref_entry" id="bib14">Atkeson, C. G., Schaal, S. A., &amp; Moore, A. W. (1997). Locally weighted learning. <em>AI Review</em>, 11, 11â€“71.</p>
<p class="ref_entry" id="bib15">Auer, P., &amp; Ortner, R. (2004). A boosting approach to multiple instance learning. In <em>Proceedings of the European Conference on Machine Learning</em> (pp. 63â€“74). Pisa, Italy. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib16">Barnett, V., &amp; Lewis, T. (1994). <em>Outliers in Statistical Data</em>. West Sussex, England: John Wiley, &amp; Sons.</p>
<p class="ref_entry" id="bib17">Bay, S. D. (1999). Nearest neighbor classification from multiple feature subsets. <em>Intelligent Data Analysis</em>, 3(3), 191â€“209.</p>
<p class="ref_entry" id="bib18">Bay, S. D., &amp; Schwabacher, M. (2003). Near linear time detection of distance-based outliers and applications to security. In <em>Proceedings of the Workshop on Data Mining for Counter Terrorism and Security</em>. San Francisco. Philadelphia: Society for Industrial and Applied Mathematics.</p>
<p class="ref_entry" id="bib19">Bayes, T. (1763). An essay towards solving a problem in the doctrine of chances. <em>Philosophical Transactions of the Royal Society of London</em>, 53, 370â€“418.</p>
<p class="ref_entry" id="bib20">Beck, J. R., &amp; Schultz, E. K. (1986). The use of ROC curves in test performance evaluation. <em>Archives of Pathology and Laboratory Medicine</em>, 110, 13â€“20.</p>
<p class="ref_entry" id="bib21">Bergadano, F., &amp; Gunetti, D. (1996). <em>Inductive logic programming: From machine learning to software engineering</em>. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib22">Berry, M. J. A., &amp; Linoff, G. (1997). <em>Data mining techniques for marketing, sales, and customer support</em>. New York: John Wiley.</p>
<p class="ref_entry" id="bib23">Beygelzimer, A., Kakade, S., &amp; Langford, J. (2006). Cover trees for nearest neighbor. In <em>Proceedings of the 23rd International Conference on Machine Learning</em> (pp. 97â€“104). New York: ACM Press.</p>
<p class="ref_entry" id="bib24">Bifet, A., Holmes, G., Kirkby, R., &amp; Pfahringer, B. (2010). MOA: Massive online analysis. <em>Journal of Machine Learning Research</em>, 9, 1601â€“1604.</p>
<p class="ref_entry" id="bib25">Bigus, J. P. (1996). <em>Data mining with neural networks</em>. New York: McGraw-Hill.</p>
<p class="ref_entry" id="bib26">Bishop, C. M. (1995). <em>Neural networks for pattern recognition</em>. New York: Oxford University Press.</p>
<p class="ref_entry" id="bib27">______. (2006). <em>Pattern recognition and machine learning</em>. Springer-Verlag.</p>
<p class="ref_entry" id="bib28">BLI (Bureau of Labour Information) (1988). <em>Collective Bargaining Review (November)</em>. Ottawa: Labour Canada, Bureau of Labour Information.</p>
<p class="ref_entry" id="bib29">Blockeel, H., Page, D., &amp; Srinivasan, A. (2005). Multi-instance tree learning. In <em>Proceedings of the 22nd International Conference on Machine Learning</em> (pp. 57â€“64). Bonn. New York: ACM Press.</p>
<p class="ref_entry" id="bib30">Blum, A., &amp; Mitchell, T. (1998). Combining labeled and unlabeled data with co-training. In <em>Proceedings of the Eleventh Annual Conference on Computational Learning Theory </em>(pp. 92â€“100). Madison, WI. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib31">Bouckaert, R. R. (1995). <em>Bayesian belief networks: From construction to inference</em>. Ph.D. Dissertation, Computer Science Department, University of Utrecht, The Netherlands.</p>
<p class="ref_entry" id="bib32">______. (2004). Bayesian network classifiers in Weka. Working Paper 14/2004, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib33">______. (2010). DensiTree: Making sense of sets of phylogenetic trees. <em>Bioinformatics</em>, 26(10), 1372â€“1373.</p>
<p class="ref_entry" id="bib34">Brachman, R. J., &amp; Levesque, H. J. (Eds.) (1985). <em>Readings in knowledge representation</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib35">Brefeld, U., &amp; Scheffer, T. (2004). Co-EM support vector learning. In R. Greiner, &amp; D. Schuurmans (Eds.), <em>Proceedings of the Twenty-First International Conference on Machine Learning</em> (pp. 121â€“128). Banff, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib36">Breiman, L. (1996a). Stacked regression. <em>Machine Learning</em>, 24(1), 49â€“64.</p>
<p class="ref_entry" id="bib37">______. (1996b). Bagging predictors. <em>Machine Learning</em>, 24(2), 123â€“140.</p>
<p class="ref_entry" id="bib38">______. (1996c). [Bias, variance, and] arcing classifiers. Technical Report 460. Department of Statistics, University of California, Berkeley.</p>
<p class="ref_entry" id="bib39">______. (1999). Pasting small votes for classification in large databases and online. <em>Machine Learning</em>, 36(1â€“2), 85â€“103.</p>
<p class="ref_entry" id="bib40">______. (2001). Random forests. <em>Machine Learning</em>, 45(1), 5â€“32.</p>
<p class="ref_entry" id="bib41">Breiman, L., Friedman, J. H., Olshen, R. A., &amp; Stone, C. J. (1984). <em>Classification and regression trees</em>. Monterey, CA: Wadsworth.</p>
<p class="ref_entry" id="bib42">Brin, S., Motwani, R., Ullman, J. D., &amp; Tsur, S. (1997). Dynamic itemset counting and implication rules for market basket data. <em>ACM SIGMOD Record</em>, 26(2), 255â€“264.</p>
<p class="ref_entry" id="bib43">Brin, S., &amp; Page, L. (1998). The anatomy of a large-scale hypertext search engine. <em>Computer Networks and ISDN Systems</em>, 33, 107â€“117.</p>
<p class="ref_entry" id="bib44">Brodley, C. E., &amp; Friedl, M. A. (1996). Identifying and eliminating mislabeled training instances. In <em>Proceedings of the Thirteenth National Conference on Artificial Intelligence </em>(pp. 799â€“805). Portland, OR. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib45">Brownstown, L., Farrell, R., Kant, E., &amp; Martin, N. (1985). <em>Programming expert systems in OPS5</em>. Reading, MA: Addison-Wesley.</p>
<p class="ref_entry" id="bib46">Buntine, W. (1992). Learning classification trees. <em>Statistics and Computing</em>, 2(2), 63â€“73.</p>
<p class="ref_entry" id="bib47">Burges, C. J. C. (1998). A tutorial on support vector machines for pattern recognition. <em>Data Mining and Knowledge Discovery</em>, 2(2), 121â€“167.</p>
<p class="ref_entry" id="bib48">Cabena, P., Hadjinian, P., Stadler, R., Verhees, J., &amp; Zanasi, A. (1998). <em>Discovering data mining: From concept to implementation</em>. Upper Saddle River, NJ: Prentice-Hall.</p>
<p class="ref_entry" id="bib49">Cardie, C. (1993). Using decision trees to improve case-based learning. In P. Utgoff (Ed.), <em>Proceedings of the Tenth International Conference on Machine Learning</em> (pp. 25â€“32). Amherst, MA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib50">Califf, M. E., &amp; Mooney, R. J. (1999). Relational learning of pattern-match rules for information extraction. In <em>Proceedings of the Sixteenth National Conference on Artificial Intelligence</em> (pp. 328â€“334). Orlando. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib51">Cavnar, W. B., &amp; Trenkle, J. M. (1994). N-Gram-based text categorization. In <em>Proceedings of the Third Symposium on Document Analysis and Information Retrieval</em> (pp. 161â€“175). Las Vegas: UNLV Publications/Reprographics.</p>
<p class="ref_entry" id="bib52">Ceglar, A., &amp; Roddick, J. F. (2006). Association mining. <em>ACM Computing Surveys</em>, 38(2). New York: ACM Press.</p>
<p class="ref_entry" id="bib53">Cendrowska, J. (1987). PRISM: An algorithm for inducing modular rules. <em>International Journal of Man-Machine Studies</em>, 27(4), 349â€“370.</p>
<p class="ref_entry" id="bib54">Chakrabarti, S. (2003). <em>Mining the web: discovering knowledge from hypertext data</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib55">Chang, C.-C., &amp; Lin, C.-J. (2001). LIBSVM: A library for support vector machines. Software available at <em><a href="http://www.csie.ntu.edu.tw/~cjlin/libsvm">http://www.csie.ntu.edu.tw/~cjlin/libsvm</a></em>.</p>
<p class="ref_entry" id="bib9001">Chawla, N. V., Bowyer, K. W., Hall, L. O., &amp; Kegelmeyer, W. P. (2002). SMOTE: Synthetic minority over-sampling technique. <em>Journal of Artificial Intelligence</em>, 16, 321â€“357.</p>
<p class="ref_entry" id="bib56">Cheeseman, P., &amp; Stutz, J. (1995). Bayesian classification (AutoClass): Theory and results. In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, &amp; R. Uthurusamy (Eds.), <em>Advances in Knowledge Discovery and Data Mining</em> (pp. 153â€“180). Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib57">Chen, M. S., Jan, J., &amp; Yu, P. S. (1996). Data mining: An overview from a database perspective. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 8(6), 866â€“883.</p>
<p class="ref_entry" id="bib58">Chen, Y., Bi, J., &amp; Wang, J. Z. (2006). MILES: Multiple-instance learning via embedded instance selection. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 28(12), 1931â€“1947.</p>
<p class="ref_entry" id="bib59">Cherkauer, K. J., &amp; Shavlik, J. W. (1996). Growing simpler decision trees to facilitate knowledge discovery. In E. Simoudis, J. W. Han, &amp; U. Fayyad (Eds.), <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em> (pp. 315â€“318). Portland, OR. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib60">Chevaleyre, Y., &amp; Zucker, J.-D. (2001). Solving multiple-instance and multiple-part learning problems with decision trees and rule sets: Application to the mutagenesis problem. In <em>Proceedings of the Biennial Conference of the Canadian Society for Computational Studies of Intelligence</em> (pp. 204â€“214). Ottawa. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib61">Cleary, J. G., &amp; Trigg, L. E. (1995). K*: An instance-based learner using an entropic distance measure. In A. Prieditis, &amp; S. Russell (Eds.), <em>Proceedings of the Twelfth International Conference on Machine Learning</em> (pp. 108â€“114). Tahoe City, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib62">Cohen, J. (1960). A coefficient of agreement for nominal scales. <em>Educational and Psychological Measurement</em>, 20, 37â€“46.</p>
<p class="ref_entry" id="bib63">Cohen, W. W. (1995). Fast effective rule induction. In A. Prieditis, &amp; S. Russell (Eds.), <em>Proceedings of the Twelfth International Conference on Machine Learning</em> (pp. 115â€“123). Tahoe City, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib64">Cooper, G. F., &amp; Herskovits, E. (1992). A Bayesian method for the induction of probabilistic networks from data. <em>Machine Learning</em>, 9(4), 309â€“347.</p>
<p class="ref_entry" id="bib65">Cortes, C., &amp; Vapnik, V. (1995). Support vector networks. <em>Machine Learning</em>, 20(3), 273â€“297.</p>
<p class="ref_entry" id="bib66">Cover, T. M., &amp; Hart, P. E. (1967). Nearest neighbor pattern classification. <em>IEEE Transactions on Information Theory</em>, IT-13, 21â€“27.</p>
<p class="ref_entry" id="bib67">Cristianini, N., &amp; Shawe-Taylor, J. (2000). <em>An introduction to support vector machines and other kernel-based learning methods</em>. Cambridge, UK: Cambridge University Press. Cypher, A. (Ed.), (1993). <em>Watch what I do: Programming by demonstration</em>. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib68">Dasgupta, S. (2002). Performance guarantees for hierarchical clustering. In J. Kivinen, &amp; R. H. Sloan (Eds.), <em>Proceedings of the Fifteenth Annual Conference on Computational Learning Theory</em> (pp. 351â€“363). Sydney. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib69">Dasu, T., Koutsofios, E., &amp; Wright, J. (2006). Zen and the art of data mining. In <em>Proceedings of the KDD Workshop on Data Mining for Business Applications</em> (pp. 37â€“43). Philadelphia. Proceedings at: <em>http://labs.accenture.com/kdd2006_workshop/dmba_proceedings.pdf </em></p>
<p class="ref_entry" id="bib70">Datta, S., Kargupta, H., &amp; Sivakumar, K. (2003). Homeland defense, privacy-sensitive data mining, and random value distortion. In <em>Proceedings of the Workshop on Data Mining for Counter Terrorism and Security</em>. San Francisco. Philadelphia: Society for International and Applied Mathematics.</p>
<p class="ref_entry" id="bib71">Day, W. H. E., &amp; EdelsbrÃ¼nner, H. (1984). Efficient algorithms for agglomerative hierarchical clustering methods. <em>Journal of Classification</em>, 1(1), 7â€“24.</p>
<p class="ref_entry" id="bib72">Demiroz, G., &amp; Guvenir, A. (1997). Classification by voting feature intervals. In M. van Someren, &amp; G. Widmer (Eds.), <em>Proceedings of the Ninth European Conference on Machine Learning</em> (pp. 85â€“92). Prague. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib73">de Raedt, L. (2008). <em>Logical and relational learning</em>. New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib74">Devroye, L., GyÃ¶rfi, L., &amp; Lugosi, G. (1996). <em>A probabilistic theory of pattern recognition</em>. New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib75">Dhar, V., &amp; Stein, R. (1997). <em>Seven methods for transforming corporate data into business intelligence</em>. Upper Saddle River, NJ: Prentice-Hall.</p>
<p class="ref_entry" id="bib76">Diederich, J., Kindermann, J., Leopold, E., &amp; Paass, G. (2003). Authorship attribution with support vector machines. <em>Applied Intelligence</em>, 19(1), 109â€“123.</p>
<p class="ref_entry" id="bib77">Dietterich, T. G. (2000). An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization. <em>Machine Learning</em>, 40(2), 139â€“158.</p>
<p class="ref_entry" id="bib78">Dietterich, T. G., &amp; Bakiri, G. (1995). Solving multiclass learning problems via error-correcting output codes. <em>Journal of Artificial Intelligence Research</em>, 2, 263â€“286.</p>
<p class="ref_entry" id="bib79">Dietterich, T. G., &amp; Kong, E. B. (1995). Error-correcting output coding corrects bias and variance. In <em>Proceedings of the Twelfth International Conference on Machine Learning </em>(pp. 313-321). Tahoe City, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib80">Dietterich, T. G., Lathrop, R. H., &amp; Lozano-Perez, T. (1997). Solving the multiple-instance problem with axis-parallel rectangles. <em>Artificial Intelligence Journal</em>, 89(1-2), 31â€“71.</p>
<p class="ref_entry" id="bib81">Domingos, P. (1997). Knowledge acquisition from examples via multiple models. In D. H. Fisher (Ed.), <em>Proceedings of the Fourteenth International Conference on Machine Learning</em> (pp. 98â€“106). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib82">______. (1999). MetaCost: A general method for making classifiers cost-sensitive. In U. M. Fayyad, S. Chaudhuri, &amp; D. Madigan (Eds.), <em>Proceedings of the Fifth International Conference on Knowledge Discovery and Data Mining</em> (pp. 155â€“164). San Diego. New York: ACM Press.</p>
<p class="ref_entry" id="bib83">Domingos, P., &amp; Hulten, G. (2000). Mining high-speed data streams. In <em>International Conference on Knowledge Discovery and Data Mining</em> (pp. 71â€“80). Boston. New York: ACM Press.</p>
<p class="ref_entry" id="bib84">Dong, L., Frank, E., &amp; Kramer, S. (2005). Ensembles of balanced nested dichotomies for multi-class problems. In <em>Proceedings of the Ninth European Conference on Principles and Practice of Knowledge Discovery in Databases</em> (pp. 84-95). Porto, Portugal. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib85">Dougherty, J., Kohavi, R., &amp; Sahami, M. (1995). Supervised and unsupervised discretization of continuous features. In A. Prieditis, &amp; S. Russell (Eds.), <em>Proceedings of the Twelfth International Conference on Machine Learning</em> (pp. 194â€“202). Tahoe City, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib86">Drucker, H. (1997). Improving regressors using boosting techniques. In D. H. Fisher (Ed.), <em>Proceedings of the Fourteenth International Conference on Machine Learning</em> (pp. 107â€“ 115). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib87">Drummond, C., &amp; Holte, R. C. (2000). Explicitly representing expected cost: An alternative to ROC representation. In R. Ramakrishnan, S. Stolfo, R. Bayardo, &amp; I. Parsa (Eds.), <em>Proceedings of the Sixth International Conference on Knowledge Discovery and Data Mining</em> (pp. 198â€“207). Boston. New York: ACM Press.</p>
<p class="ref_entry" id="bib88">Duda, R. O., &amp; Hart, P. E. (1973). <em>Pattern classification and scene analysis</em>. New York: John Wiley.</p>
<p class="ref_entry" id="bib89">Duda, R. O., Hart, P. E., &amp; Stork, D. G. (2001). <em>Pattern Classification</em> (2nd ed.). New York: John Wiley.</p>
<p class="ref_entry" id="bib90">Dumais, S. T., Platt, J., Heckerman, D., &amp; Sahami, M. (1998). Inductive learning algorithms and representations for text categorization. In <em>Proceedings of the ACM Seventh International Conference on Information and Knowledge Management</em> (pp. 148â€“155). Bethesda, MD. New York: ACM Press.</p>
<p class="ref_entry" id="bib91">Efron, B., &amp; Tibshirani, R. (1993). <em>An introduction to the bootstrap</em>. London: Chapman and Hall.</p>
<p class="ref_entry" id="bib92">Egan, J. P. (1975). <em>Signal detection theory and ROC analysis. Series in Cognition and Perception</em>. New York: Academic Press.</p>
<p class="ref_entry" id="bib93">Ester, M., Kriegel, H.-P., Sander, J., &amp; Xu, X. (1996). A density-based algorithm for discovering clusters in large spatial databases with noise. In <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining (KDD-96)</em> (pp. 226â€“231). Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib94">Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., &amp; Lin, C.-J. (2008). LIBLINEAR: A library for large linear classification. <em>Journal of Machine Learning Research</em>, 9, 1871â€“1874.</p>
<p class="ref_entry" id="bib95">Fayyad, U. M., &amp; Irani, K. B. (1993). Multi-interval discretization of continuous-valued attributes for classification learning. In <em>Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence</em> (pp. 1022â€“1027). Chambery, France. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib96">Fayyad, U. M., &amp; Smyth, P. (1995). From massive datasets to science catalogs: Applications and challenges. In <em>Proceedings of the Workshop on Massive Datasets</em> (pp. 129â€“141). Washington, DC: NRC, Committee on Applied and Theoretical Statistics.</p>
<p class="ref_entry" id="bib97">Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., &amp; Uthurusamy, R. (Eds.), (1996). <em>Advances in knowledge discovery and data mining</em>. Menlo Park, CA: AAAI Press/MIT Press.</p>
<p class="ref_entry" id="bib98">Fisher, D. (1987). Knowledge acquisition via incremental conceptual clustering. <em>Machine Learning</em>, 2(2), 139â€“172.</p>
<p class="ref_entry" id="bib99">Fisher, R. A. (1936). The use of multiple measurements in taxonomic problems. <em>Annual Eugenics</em>, 7 (part II), 179â€“188. Reprinted in <em>Contributions to Mathematical Statistics</em>, 1950. New York: John Wiley.</p>
<p class="ref_entry" id="bib100">Fix, E., &amp; Hodges, J. L. Jr. (1951). Discriminatory analysis; non-parametric discrimination: Consistency properties. Technical Report 21-49-004(4), USAF School of Aviation Medicine, Randolph Field, TX.</p>
<p class="ref_entry" id="bib101">Flach, P. A., &amp; Lachiche, N. (1999). Confirmation-guided discovery of first-order rules with Tertius. <em>Machine Learning</em>, 42, 61â€“95.</p>
<p class="ref_entry" id="bib102">Fletcher, R. (1987). <em>Practical methods of optimization</em> (2nd ed.). New York: John Wiley.</p>
<p class="ref_entry" id="bib103">Foulds, J., &amp; Frank, E. (2008). Revisiting multiple-instance learning via embedded instance selection. In <em>Proceedings of the Australasian Joint Conference on Artificial Intelligence </em>(pp. 300â€“310). Auckland. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib104">______. (2010). A review of multi-instance learning assumptions. <em>Knowledge Engineering Review</em>, 25(1), 1â€“25.</p>
<p class="ref_entry" id="bib105">Fradkin, D., &amp; Madigan, D. (2003). Experiments with random projections for machine learning. In L. Getoor, T. E. Senator, P. Domingos, &amp; C. Faloutsos (Eds.), <em>Proceedings of the Ninth International Conference on Knowledge Discovery and Data Mining</em> (pp. 517â€“522). Washington, DC. New York: ACM Press.</p>
<p class="ref_entry" id="bib106">Frank E. (2000). <em>Pruning decision trees and lists</em>. Ph.D. Dissertation, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib107">Frank, E., &amp; Hall, M. (2001). A simple approach to ordinal classification. In L. de Raedt, &amp; P. A. Flach (Eds.), <em>Proceedings of the Twelfth European Conference on Machine Learning </em>(pp. 145â€“156). Freiburg, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib108">Frank, E., Hall, M., &amp; Pfahringer, B. (2003). Locally weighted NaÃ¯ve Bayes. In U. KjÃ¦rulff, &amp; C. Meek (Eds.), <em>Proceedings of the Nineteenth Conference on Uncertainty in Artificial Intelligence</em> (pp. 249â€“256). Acapulco. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib109">Frank, E., Holmes, G., Kirkby, R., &amp; Hall, M. (2002). Racing Committees for Large Datasets. In S. Lange, K. Satoh, &amp; C. H. Smith (Eds.), <em>Proceedings of the Fifth International Conference on Discovery Science</em> (pp. 153â€“164). LÃ¼beck, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib110">Frank, E., &amp; Kramer, S. (2004). Ensembles of nested dichotomies for multi-class problems. In <em>Proceedings of the Twenty-First International Conference on Machine Learning </em>(pp. 305â€“312). Banff, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib115">Frank, E., Paynter, G. W., Witten, I. H., Gutwin, C., &amp; Nevill-Manning, C. G. (1999). Domain-specific key phrase extraction. In <em>Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</em> (pp. 668â€“673). Stockholm. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib111">Frank, E., Wang, Y., Inglis, S., Holmes, G., &amp; Witten, I. H. (1998). Using model trees for classification. <em>Machine Learning</em>, 32(1), 63â€“76.</p>
<p class="ref_entry" id="bib112">Frank, E., &amp; Witten, I. H. (1998). Generating accurate rule sets without global optimization. In J. Shavlik (Ed.), <em>Proceedings of the Fifteenth International Conference on Machine Learning</em> (pp. 144â€“151). Madison, WI. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib113">______. (1999). Making better use of global discretization. In I. Bratko, &amp; S. Dzeroski (Eds.), <em>Proceedings of the Sixteenth International Conference on Machine Learning</em> (pp. 115â€“ 123). Bled, Slovenia. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib114">Frank, E., &amp; Xu, X. (2003). Applying propositional learning algorithms to multi-instance data. Technical Report 06/03, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib9002">Freitag, D. (2002). Machine learning for information extraction in informal domains. <em>Machine Learning</em>, 39(2/3), 169â€“202.</p>
<p class="ref_entry" id="bib116">Freund, Y., &amp; Mason, L. (1999). The alternating decision tree learning algorithm. In I. Bratko, &amp; S. Dzeroski (Eds.), <em>Proceedings of the Sixteenth International Conference on Machine Learning</em> (pp. 124-133). Bled, Slovenia. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib117">Freund, Y., &amp; Schapire, R. E. (1996). Experiments with a new boosting algorithm. In L. Saitta (Ed.), <em>Proceedings of the Thirteenth International Conference on Machine Learning </em>(pp. 148â€“156). Bari, Italy. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib118">______. (1999). Large margin classification using the perceptron algorithm. <em>Machine Learning</em>, 37(3), 277â€“296.</p>
<p class="ref_entry" id="bib119">Friedman, J. H. (1996). Another approach to polychotomous classification. Technical report, Department of Statistics, Stanford University, Stanford, CA.</p>
<p class="ref_entry" id="bib120">______. (2001). Greedy function approximation: A gradient boosting machine. <em>Annals of Statistics</em>, 29(5), 1189â€“1232.</p>
<p class="ref_entry" id="bib121">Friedman, J. H., Bentley, J. L., &amp; Finkel, R. A. (1977). An algorithm for finding best matches in logarithmic expected time. <em>ACM Transactions on Mathematical Software</em>, 3(3), 209â€“266.</p>
<p class="ref_entry" id="bib122">Friedman, J. H., Hastie, T., &amp; Tibshirani, R. (2000). Additive logistic regression: A statistical view of boosting. <em>Annals of Statistics</em>, 28(2), 337â€“374.</p>
<p class="ref_entry" id="bib123">Friedman, N., Geiger, D., &amp; Goldszmidt, M. (1997). Bayesian Network Classifiers. <em>Machine Learning</em>, 29(2), 131â€“163.</p>
<p class="ref_entry" id="bib124">Fulton, T., Kasif, S., &amp; Salzberg, S. (1995). Efficient algorithms for finding multiway splits for decision trees. In A. Prieditis, &amp; S. Russell (Eds.), <em>Proceedings of the Twelfth International Conference on Machine Learning</em> (pp. 244â€“251). Tahoe City, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib125">FÃ¼rnkranz, J. (2002). Round robin classification. <em>Journal of Machine Learning Research</em>, 2, 721â€“747.</p>
<p class="ref_entry" id="bib126">______. (2003). Round robin ensembles. <em>Intelligent Data Analysis</em>, 7(5), 385-403.</p>
<p class="ref_entry" id="bib127">FÃ¼rnkranz, J., &amp; Flach, P. A. (2005). ROC â€˜nâ€™ rule learning: Towards a better understanding of covering algorithms. <em>Machine Learning</em>, 58(1), 39â€“77.</p>
<p class="ref_entry" id="bib128">FÃ¼rnkranz, J., &amp; Widmer, G. (1994). Incremental reduced-error pruning. In H. Hirsh, &amp; W. Cohen (Eds.), <em>Proceedings of the Eleventh International Conference on Machine Learning </em>(pp. 70â€“77). New Brunswick, NJ. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib129">Gaines, B. R., &amp; Compton, P. (1995). Induction of ripple-down rules applied to modeling large data bases. <em>Journal of Intelligent Information Systems</em>, 5(3), 211â€“228.</p>
<p class="ref_entry" id="bib130">Gama, J. (2004). Functional trees. <em>Machine Learning</em>, 55(3), 219â€“250.</p>
<p class="ref_entry" id="bib131">GÃ¤rtner, T., Flach, P. A., Kowalczyk, A., &amp; Smola, A. J. (2002). Multi-instance kernels. In <em>Proceedings of the International Conference on Machine Learning</em> (pp. 179â€“186). Sydney. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib132">Genkin, A., Lewis, D. D., &amp; Madigan, D. (2007). Large-scale Bayesian logistic regression for text categorization. <em>Technometrics</em>, 49(3), 291â€“304.</p>
<p class="ref_entry" id="bib133">Gennari, J. H., Langley, P., &amp; Fisher, D. (1990). Models of incremental concept formation. <em>Artificial Intelligence</em>, 40, 11â€“61.</p>
<p class="ref_entry" id="bib134">Ghani, R. (2002). Combining labeled and unlabeled data for multiclass text categorization. In C. Sammut, &amp; A. Hoffmann (Eds.), <em>Proceedings of the Nineteenth International Conference on Machine Learning</em> (pp. 187â€“194). Sydney. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib135">Gilad-Bachrach, R., Navot, A., &amp; Tishby, N. (2004). Margin based feature selection: Theory and algorithms. In R. Greiner, &amp; D. Schuurmans (Eds.), <em>Proceedings of the Twenty-First International Conference on Machine Learning</em> (pp. 337â€“344). Banff, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib136">Giraud-Carrier, C. (1996). FLARE: Induction with prior knowledge. In J. Nealon, &amp; J. Hunt (Eds.), <em>Research and Development in Expert Systems XIII</em> (pp. 11â€“24). Cambridge, England: SGES Publications.</p>
<p class="ref_entry" id="bib137">Gluck, M., &amp; Corter, J. (1985). Information, uncertainty and the utility of categories. In <em>Proceedings of the Annual Conference of the Cognitive Science Society</em> (pp. 283â€“287). Irvine, CA. Hillsdale, NJ: Lawrence Erlbaum.</p>
<p class="ref_entry" id="bib138">Goldberg, D. E. (1989). <em>Genetic algorithms in search, optimization and machine learning</em>. Reading, MA: Addison-Wesley.</p>
<p class="ref_entry" id="bib139">Good P. (1994). <em>Permutation tests: A practical guide to resampling methods for testing hypotheses</em>. New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib140">Grossman, D., &amp; Domingos, P. (2004). Learning Bayesian network classifiers by maximizing conditional likelihood. In R. Greiner, &amp; D. Schuurmans (Eds.), <em>Proceedings of the Twenty-First International Conference on Machine Learning</em> (pp. 361â€“368). Banff, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib141">Groth, R. (1998). <em>Data mining: A hands-on approach for business professionals</em>. Upper Saddle River, NJ: Prentice-Hall.</p>
<p class="ref_entry" id="bib142">Guo, Y., &amp; Greiner, R. (2004). <em>Discriminative model selection for belief net structures</em>. Canada: Department of Computing Science, TR04-22, University of Alberta.</p>
<p class="ref_entry" id="bib143">GÃ¼tlein, M., Frank, E., Hall, M., &amp; Karwath, A. (2009). Large-scale attribute selection using wrappers. In <em>Proceedings of the IEEE Symposium on Computational Intelligence and Data Mining</em> (pp. 332â€“339). Nashville. Washington, DC: IEEE Computer Society.</p>
<p class="ref_entry" id="bib144">Guyon, I., Weston, J., Barnhill, S., &amp; Vapnik, V. (2002). Gene selection for cancer classification using support vector machines. <em>Machine Learning</em>, 46(1â€“3), 389â€“422.</p>
<p class="ref_entry" id="bib145">Hall, M. (2000). Correlation-based feature selection for discrete and numeric class machine learning. In P. Langley (Ed.), <em>Proceedings of the Seventeenth International Conference on Machine Learning</em> (pp. 359â€“366). Stanford, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib146">Hall, M., Holmes, G., &amp; Frank, E. (1999). Generating rule sets from model trees. In N. Y. Foo (Ed.), <em>Proceedings of the Twelfth Australian Joint Conference on Artificial Intelligence</em> (pp. 1â€“12). Sydney. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib147">Hall, M., &amp; Frank, E. (2008). Combining NaÃ¯ve Bayes and decision tables. In <em>Proceedings of the 21st Florida Artificial Intelligence Research Society Conference</em> (pp. 318â€“319). Miami. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib148">Han, J., Pei, J., &amp; Yin, Y. (2000). Mining frequent patterns without candidate generation. In <em>Proceedings of the ACM-SIGMOD International Conference on Management of Data</em> (pp. 1â€“12). Dallas. New York: ACM Press.</p>
<p class="ref_entry" id="bib149">Han, J., Pei, J., Yin, Y., &amp; Mao, R. (2004). Mining frequent patterns without candidate generation: A frequent-pattern tree approach. <em>Data Mining and Knowledge Discovery</em>, 8(1), 53â€“87.</p>
<p class="ref_entry" id="bib150">Han, J., &amp; Kamber, M. (2006). <em>Data mining: Concepts and techniques</em> (2nd ed.). San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib151">Hand, D. J. (2006). Classifier Technology and the Illusion of Progress. <em>Statistical Science</em>, 21(1), 1â€“14.</p>
<p class="ref_entry" id="bib152">Hand, D. J., Manilla, H., &amp; Smyth, P. (2001). <em>Principles of Data Mining</em>. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib153">Hartigan, J. A. (1975). <em>Clustering algorithms</em>. New York: John Wiley.</p>
<p class="ref_entry" id="bib154">Hastie, T., &amp; Tibshirani, R. (1998). Classification by pairwise coupling. <em>Annals of Statistics</em>, 26(2), 451â€“471.</p>
<p class="ref_entry" id="bib155">Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements of statistical learning</em> (2nd ed.). New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib156">Heckerman, D., Geiger, D., &amp; Chickering, D. M. (1995). Learning Bayesian networks: The combination of knowledge and statistical data. <em>Machine Learning</em>, 20(3), 197â€“243.</p>
<p class="ref_entry" id="bib157">Hempstalk, K., Frank, E., &amp; Witten, I. H. (2008). One-class classification by combining density and class probability estimation. In <em>Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases </em>(pp. 505â€“519). Antwerp. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib158">Hempstalk, K., &amp; Frank, E. (2008). Discriminating against new classes: One-class versus multi-class classification. In <em>Proceedings of the Twenty-first Australasian Joint Conference on Artificial Intelligence</em>. Auckland (pp. 225â€“236). New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib159">Ho, T. K. (1998). The random subspace method for constructing decision forests. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 20(8), 832â€“844.</p>
<p class="ref_entry" id="bib160">Hochbaum, D. S., &amp; Shmoys, D. B. (1985). A best possible heuristic for the <em>k</em>-center problem. <em>Mathematics of Operations Research</em>, 10(2), 180â€“184.</p>
<p class="ref_entry" id="bib161">Hoerl, A. E., &amp; Kennard, R. W. (1970). Ridge regression: applications to nonorthogonal problems. <em>Technometrics</em>, 12(1), 69â€“82.</p>
<p class="ref_entry" id="bib9003">Holmes, G., &amp; Nevill-Manning, C. G. (1995). Feature selection via the discovery of simple classification rules. In G. E. Lasker, &amp; X. Liu (Eds.), <em>Proceedings of the International Symposium on Intelligent Data Analysis</em> (pp. 75â€“79). Baden-Baden, Germany: International Institute for Advanced Studies in Systems Research and Cybernetics. Baden-Baden. Windsor, Ont: International Institute for Advanced Studies in Systems Research and Cybernetics.</p>
<p class="ref_entry" id="bib162">Holmes, G., Pfahringer, B., Kirkby, R., Frank, E., &amp; Hall, M. (2002). Multiclass alternating decision trees. In T. Elomaa, H. Mannila, &amp; H. Toivonen (Eds.), <em>Proceedings of the Thirteenth European Conference on Machine Learning</em> (pp. 161â€“172). Helsinki. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib163">Holte, R. C. (1993). Very simple classification rules perform well on most commonly used datasets. <em>Machine Learning</em>, 11, 63â€“91.</p>
<p class="ref_entry" id="bib164">Huffman, S. B. (1996). Learning information extraction patterns from examples. In S. Wertmer, E. Riloff, &amp; G. Scheler (Eds.), <em>Connectionist, statistical, and symbolic approaches to learning for natural language processing</em> (pp. 246â€“260). Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib165">Jabbour, K., Riveros, J. F. V., Landsbergen, D., &amp; Meyer, W. (1988). ALFA: Automated load forecasting assistant. <em>IEEE Transactions on Power Systems</em>, 3(3), 908â€“914.</p>
<p class="ref_entry" id="bib166">Jiang, L., &amp; Zhang, H. (2006). Weightily averaged one-dependence estimators. In <em>Proceedings of the 9th Biennial Pacific Rim International Conference on Artificial Intelligence</em> (pp. 970â€“974). Guilin, China. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib167">John, G. H. (1995). Robust decision trees: Removing outliers from databases. In U. M. Fayyad, &amp; R. Uthurusamy (Eds.), <em>Proceedings of the First International Conference on Knowledge Discovery and Data Mining</em> (pp. 174â€“179). Montreal. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib168">______. (1997). <em>Enhancements to the data mining process</em>. Ph.D. Dissertation, Computer Science Department, Stanford University, Stanford, CA.</p>
<p class="ref_entry" id="bib169">John, G. H., Kohavi, R., &amp; Pï¬‚eger, P. (1994). Irrelevant features and the subset selection problem. In H. Hirsh, &amp; W. Cohen (Eds.), <em>Proceedings of the Eleventh International Conference on Machine Learning</em> (pp. 121â€“129). New Brunswick, NJ. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib170">John, G. H., &amp; Langley, P. (1995). Estimating continuous distributions in Bayesian classifiers. In P. Besnard, &amp; S. Hanks (Eds.), <em>Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence</em> (pp. 338â€“345). Montreal. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib171">Johns, M. V. (1961). An empirical Bayes approach to nonparametric two-way classification. In H. Solomon (Ed.), <em>Studies in item analysis and prediction</em>. Palo Alto, CA: Stanford University Press.</p>
<p class="ref_entry" id="bib172">Kass, R., &amp; Wasserman, L. (1995). A reference Bayesian test for nested hypotheses and its relationship to the Schwarz criterion. <em>Journal of the American Statistical Association</em>, 90, 928â€“934.</p>
<p class="ref_entry" id="bib173">Keerthi, S. S., Shevade, S. K., Bhattacharyya, C., &amp; Murthy, K. R. K. (2001). Improvements to Plattâ€™s SMO algorithm for SVM classifier design. <em>Neural Computation</em>, 13(3), 637â€“649.</p>
<p class="ref_entry" id="bib174">Kerber, R. (1992). Chimerge: Discretization of numeric attributes. In W. Swartout (Ed.), <em>Proceedings of the Tenth National Conference on Artificial Intelligence</em> (pp. 123â€“128). San Jose, CA. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib175">Kibler, D., &amp; Aha, D. W. (1987). Learning representative exemplars of concepts: An initial case study. In P. Langley (Ed.), <em>Proceedings of the Fourth Machine Learning Workshop </em>(pp. 24â€“30). Irvine, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib176">Kimball, R., &amp; Ross, M. (2002). <em>The data warehouse toolkit</em> (2nd ed.). New York: John Wiley.</p>
<p class="ref_entry" id="bib177">Kira, K., &amp; Rendell, L. (1992). A practical approach to feature selection. In D. Sleeman, &amp; P. Edwards (Eds.), <em>Proceedings of the Ninth International Workshop on Machine Learning </em>(pp. 249â€“258). Aberdeen, Scotland. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib178">Kirkby, R. (2007). <em>Improving Hoeffding trees</em>. Ph.D. Dissertation, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib179">Kittler, J. (1978). Feature set search algorithms. In C. H. Chen (Ed.), <em>Pattern recognition and signal processing</em> (pp. 41â€“60). Amsterdam: Sijthoff an Noordhoff.</p>
<p class="ref_entry" id="bib180">Kivinen, J., Smola, A. J., &amp; Williamson, R. C. (2002). Online learning with kernels. <em>IEEE Transactions on Signal Processing</em>, 52, 2165â€“2176.</p>
<p class="ref_entry" id="bib181">Kleinberg, J. (1998). Authoritative sources in a hyperlinked environment. In <em>Proceedings of the ACM-SIAM Symposium on Discrete Algorithms</em> (pp. 604â€“632). Extended version published in <em>Journal of the ACM</em> 46 (1999).</p>
<p class="ref_entry" id="bib182">Koestler, A. (1964). <em>The act of creation</em>. London: Hutchinson.</p>
<p class="ref_entry" id="bib183">Kohavi, R. (1995a). A study of cross-validation and bootstrap for accuracy estimation and model selection. In <em>Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence</em> (pp. 1137â€“1143). Montreal. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib184">______. (1995b). The power of decision tables. In N. Lavrac, &amp; S. Wrobel (Eds.), <em>Proceedings of the Eighth European Conference on Machine Learning</em> (pp. 174â€“189). IrÃ¡klion, Crete. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib185">______. (1996). Scaling up the accuracy of NaÃ¯ve Bayes classifiers: A decision-tree hybrid. In E. Simoudis, J. W. Han, &amp; U. Fayyad (Eds.), <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em> (pp. 202â€“207). Portland, OR. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib186">Kohavi, R., &amp; John, G. H. (1997). Wrappers for feature subset selection. <em>Artificial Intelligence</em>, 97(1â€“2), 273â€“324.</p>
<p class="ref_entry" id="bib187">Kohavi, R., &amp; Kunz, C. (1997). Option decision trees with majority votes. In D. Fisher (Ed.), <em>Proceedings of the Fourteenth International Conference on Machine Learning </em>(pp. 161â€“191). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib188">Kohavi, R., &amp; Provost, F. (Eds.), (1998). Machine learning: Special issue on applications of machine learning and the knowledge discovery process. <em>Machine Learning</em>, 30(2/3), 127â€“274.</p>
<p class="ref_entry" id="bib189">Kohavi, R., &amp; Sahami, M. (1996). Error-based and entropy-based discretization of continuous features. In E. Simoudis, J. W. Han, &amp; U. Fayyad (Eds.), <em>Proceedings of the Second International Conference on Knowledge Discovery and Data Mining</em> (pp. 114â€“119). Portland, OR. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib190">Komarek, P., &amp; Moore, A. (2000). A dynamic adaptation of AD-trees for efficient machine learning on large data sets. In P. Langley (Ed.), <em>Proceedings of the Seventeenth International Conference on Machine Learning</em> (pp. 495â€“502). Stanford, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib191">Kononenko, I. (1995). On biases in estimating multi-valued attributes. In <em>Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence</em> (pp. 1034â€“1040). Montreal. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib192">Koppel, M., &amp; Schler, J. (2004). Authorship verification as a one-class classification problem. In R. Greiner, &amp; D. Schuurmans (Eds.), <em>Proceedings of the Twenty-First International Conference on Machine Learning</em> (pp. 489â€“495). Banff, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib193">Krogel, M.-A., &amp; Wrobel, S. (2002). Feature selection for propositionalization. In <em>Proceedings of the International Conference on Discovery Science</em> (pp. 430â€“434). LÃ¼beck, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib194">Kubat, M., Holte, R. C., &amp; Matwin, S. (1998). Machine learning for the detection of oil spills in satellite radar images. <em>Machine Learning</em>, 30, 195â€“215.</p>
<p class="ref_entry" id="bib195">Kuncheva, L. I., &amp; Rodriguez, J. J. (2007). An experimental study on rotation forest ensembles. In <em>Proceedings of the Seventh International Workshop on Multiple Classifier Systems </em>(pp. 459â€“468). Prague. Berlin/Heidelberg: Springer-Verlag.</p>
<p class="ref_entry" id="bib196">Kushmerick, N., Weld, D. S., &amp; Doorenbos, R. (1997). Wrapper induction for information extraction. In <em>Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence</em> (pp. 729â€“735). Nagoya, Japan. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib197">Laguna, M., &amp; Marti, R. (2003). <em>Scatter search: Methodology and implementations in C.</em> Dordrecht, The Netherlands: Kluwer Academic Press.</p>
<p class="ref_entry" id="bib198">Landwehr, N., Hall, M., &amp; Frank, E. (2005). Logistic model trees. <em>Machine Learning</em>, 59(1â€“2), 161â€“205.</p>
<p class="ref_entry" id="bib199">Langley, P. (1996). <em>Elements of machine learning</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib200">Langley, P., Iba, W., &amp; Thompson, K. (1992). An analysis of Bayesian classifiers. In</p>
<p class="ref_entry" id="bib201">W. Swartout (Ed.), <em>Proceedings of the Tenth National Conference on Artificial Intelligence </em>(pp. 223â€“228). San Jose, CA. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib202">Langley, P., &amp; Sage, S. (1994). Induction of selective Bayesian classifiers. In R. L. de Mantaras, &amp; D. Poole (Eds.), <em>Proceedings of the Tenth Conference on Uncertainty in Artificial Intelligence</em> (pp. 399â€“406). Seattle. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib203">Langley, P., &amp; Simon, H. A. (1995). Applications of machine learning and rule induction. <em>Communications of the ACM</em>, 38(11), 55â€“64.</p>
<p class="ref_entry" id="bib204">Lavrac, N., Motoda, H., Fawcett, T., Holte, R., Langley, P., &amp; Adriaans, P. (Eds.), (2004). Special issue on lessons learned from data mining applications and collaborative problem solving. <em>Machine Learning</em>, 57(1/2).</p>
<p class="ref_entry" id="bib205">Lawson, C. L., &amp; Hanson, R. J. (1995). <em>Solving least squares problems</em>. Philadelphia: SIAM Publications.</p>
<p class="ref_entry" id="bib206">le Cessie, S., &amp; van Houwelingen, J. C. (1992). Ridge estimators in logistic regression. <em>Applied Statistics</em>, 41(1), 191â€“201.</p>
<p class="ref_entry" id="bib207">Li, M., &amp; Vitanyi, P. M. B. (1992). Inductive reasoning and Kolmogorov complexity. <em>Journal of Computer and System Sciences</em>, 44, 343â€“384.</p>
<p class="ref_entry" id="bib208">Lieberman, H. (Ed.), (2001). <em>Your wish is my command: Programming by example</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib209">Littlestone, N. (1988). Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. <em>Machine Learning</em>, 2(4), 285â€“318.</p>
<p class="ref_entry" id="bib210">______. (1989). <em>Mistake bounds and logarithmic linear-threshold learning algorithms</em>. Ph, D. Dissertation, University of California, Santa Cruz.</p>
<p class="ref_entry" id="bib212">Liu, B. (2009) <em>Web data mining: Exploring hyperlinks, contents, and usage data</em>. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib211">Liu, B., Hsu, W., &amp; Ma, Y. M. (1998). Integrating classification and association rule mining. In <em>Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining (KDD-98)</em> (pp. 80â€“86). New York. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib213">Liu, H., &amp; Setiono, R. (1996). A probabilistic approach to feature selection: A filter solution. In L. Saitta (Ed.), <em>Proceedings of the Thirteenth International Conference on Machine Learning</em> (pp. 319â€“327), Bari, Italy. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib214">______. (1997). Feature selection via discretization. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 9(4), 642â€“645.</p>
<p class="ref_entry" id="bib215">Luan, J. (2002). Data mining and its applications in higher education. <em>New directions for institutional research</em>, 2002(113), 17â€“36.</p>
<p class="ref_entry" id="bib216">Mann, T. (1993). <em>Library research models: A guide to classification, cataloging, and computers</em>. New York: Oxford University Press.</p>
<p class="ref_entry" id="bib217">Marill, T., &amp; Green, D. M. (1963). On the effectiveness of receptors in recognition systems. <em>IEEE Transactions on Information Theory</em>, 9(11), 11â€“17.</p>
<p class="ref_entry" id="bib218">Maron, O. (1998). <em>Learning from ambiguity</em>. Ph.D. Thesis, Massachusetts Institute of Technology, Cambridge, MA.</p>
<p class="ref_entry" id="bib219">Maron, O., &amp; Lozano-PerÃ©z, T. (1997). A framework for multiple-instance learning. In <em>Proceedings of the Conference on Neural Information Processing Systems</em> (pp. 570â€“576). Denver. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib220">Martin, B. (1995). <em>Instance-based learning: Nearest neighbour with generalisation</em>. M.Sc. Thesis, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib221">McCallum A., &amp; Nigam, K. (1998). A comparison of event models for NaÃ¯ve Bayes text classification. In <em>Proceedings of the AAAI-98 Workshop on Learning for Text Categorization</em> (pp. 41â€“48). Madison, WI. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib222">Medelyan, O., &amp; Witten, I. H. (2008). Domain independent automatic keyphrase indexing with small training sets. <em>Journal of the American Society for Information Science and Technology</em>, 59, 1026â€“1040.</p>
<p class="ref_entry" id="bib223">Mehta, M., Agrawal, R., &amp; Rissanen, J. (1996). SLIQ: A fast scalable classifier for data mining. In P. Apers, M. Bouzeghoub, &amp; G. Gardarin (Eds.), <em>Proceedings of the Fifth International Conference on Extending Database Technology</em> (pp. 18â€“32). Avignon, France. New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib224">Melville, P., &amp; Mooney, R. J. (2005). Creating diversity in ensembles using artificial data. <em>Information Fusion</em>, 6(1), 99â€“111.</p>
<p class="ref_entry" id="bib225">Michalski, R. S., &amp; Chilausky, R. L. (1980). Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. <em>International Journal of Policy Analysis and Information Systems</em>, 4(2).</p>
<p class="ref_entry" id="bib226">Michie, D. (1989). Problems of computer-aided concept formation. In J. R. Quinlan (Ed.), <em>Applications of expert systems (Vol. 2) </em>(pp. 310â€“333). Wokingham, UK: Addison-Wesley.</p>
<p class="ref_entry" id="bib227">Minsky, M., &amp; Papert, S. (1969). <em>Perceptrons</em>. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib228">Mitchell, T. M. (1997). <em>Machine Learning</em>. New York: McGraw-Hill.</p>
<p class="ref_entry" id="bib229">Mitchell T. M., Caruana, R., Freitag, D., McDermott, J., &amp; Zabowski, D. (1994). Experience with a learning personal assistant. <em>Communications of the ACM</em>, 37 (7), 81â€“91.</p>
<p class="ref_entry" id="bib230">Moore, A. W. (1991). <em>Efficient memory-based learning for robot control</em>. Ph.D. Dissertation, Computer Laboratory, University of Cambridge, UK.</p>
<p class="ref_entry" id="bib231">______. (2000). The anchors hierarchy: Using the triangle inequality to survive high-dimensional data. In C. Boutilier, &amp; M. Goldszmidt (Eds.), <em>Proceedings of the Sixteenth Conference on Uncertainty in Artificial Intelligence</em> (pp. 397â€“405). Stanford, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib232">Moore, A. W., &amp; Lee, M. S. (1994). Efficient algorithms for minimizing cross validation error. In W. W. Cohen, &amp; H. Hirsh (Eds.), <em>Proceedings of the Eleventh International Conference on Machine Learning</em> (pp. 190â€“198). New Brunswick, NJ. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib233">______. (1998). Cached sufficient statistics for efficient machine learning with large datasets. <em>Journal Artificial Intelligence Research</em>, 8, 67â€“91.</p>
<p class="ref_entry" id="bib234">Moore, A. W., &amp; Pelleg, D. (2000). X-means: Extending <em>k</em>-means with efficient estimation of the number of clusters. In P. Langley (Ed.), <em>Proceedings of the Seventeenth International Conference on Machine Learning</em> (pp. 727â€“734). Stanford, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib235">Mutter, S., Hall, M., &amp; Frank, E. (2004). Using classification to evaluate the output of confidence-based association rule mining. In <em>Proceedings of the Seventeenth Australian Joint Conference on Artificial Intelligence</em> (pp. 538â€“549). Cairns, Australia. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib236">Nadeau, C., &amp; Bengio, Y. (2003). Inference for the generalization error. <em>Machine Learning</em>, 52(3), 239â€“281.</p>
<p class="ref_entry" id="bib237">Nahm, U. Y., &amp; Mooney, R. J. (2000). Using information extraction to aid the discovery of prediction rules from texts. In <em>Proceedings of the Workshop on Text Mining at the Sixth International Conference on Knowledge Discovery and Data Mining</em> (pp. 51â€“58). Boston. Workshop proceedings at: <em>http://www.cs.cmu.edu/~dunja/WshKDD2000.html </em></p>
<p class="ref_entry" id="bib238">Niculescu-Mizil, A., &amp; Caruana, R. (2005). Predicting good probabilities with supervised learning. In <em>Proceedings of the 22nd International Conference on Machine Learning </em>(pp. 625â€“632). Bonn. New York: ACM Press.</p>
<p class="ref_entry" id="bib239">Nie, N. H., Hull, C., Jenkins, H., Steinbrenner, J. G. K., &amp; Bent, D. H. (1970). <em>Statistical package for the social sciences</em>. New York: McGraw-Hill.</p>
<p class="ref_entry" id="bib240">Nigam, K., &amp; Ghani, R. (2000). Analyzing the effectiveness and applicability of co-training. In <em>Proceedings of the Ninth International Conference on Information and Knowledge Management</em> (pp. 86â€“93). McLean, VA. New York: ACM Press.</p>
<p class="ref_entry" id="bib241">Nigam, K., McCallum, A. K., Thrun, S., &amp; Mitchell, T. M. (2000). Text classification from labeled and unlabeled documents using EM. <em>Machine Learning</em>, 39(2/3), 103â€“134.</p>
<p class="ref_entry" id="bib242">Nilsson, N. J. (1965). <em>Learning machines</em>. New York: McGraw-Hill.</p>
<p class="ref_entry" id="bib243">Nisbet, R., Elder, J., &amp; Miner, G. (2009). <em>Handbook of statistical analysis and data mining applications</em>. New York: Academic Press.</p>
<p class="ref_entry" id="bib244">Oates, T., &amp; Jensen, D. (1997). The effects of training set size on decision tree complexity. In <em>Proceedings of the Fourteenth International Conference on Machine Learning </em>(pp. 254â€“262). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib245">Ohm, P. (2009). Broken promises of privacy: Responding to the surprising failure of anonymization. University of Colorado Law Legal Studies Research Paper No. 09-12, August.</p>
<p class="ref_entry" id="bib246">Omohundro, S. M. (1987). Efficient algorithms with neural network behavior. <em>Journal of Complex Systems</em>, 1(2), 273â€“347.</p>
<p class="ref_entry" id="bib247">Paynter G. W. (2000). <em>Automating iterative tasks with programming by demonstration</em>. Ph.D. Dissertation, Department of Computer Science, University of Waikato, New Zealand.</p>
<p class="ref_entry" id="bib248">Pearson, R. (2005). Mining imperfect data. <em>Society for Industrial and Applied Mechanics</em>, Philadelphia.</p>
<p class="ref_entry" id="bib249">Pei, J., Han, J., Mortazavi-Asi, B., Wang, J., Pinto, H., Chen, Q.,  et al. (2004). Mining sequential patterns by pattern-growth: The PrefixSpan approach. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 16(11), 1424â€“1440.</p>
<p class="ref_entry" id="bib250">Piatetsky-Shapiro, G., &amp; Frawley, W. J. (Eds.) (1991). <em>Knowledge discovery in databases</em>. Menlo Park, CA: AAAI Press/MIT Press.</p>
<p class="ref_entry" id="bib251">Platt, J. (1998). Fast training of support vector machines using sequential minimal optimization. In B. SchÃ¶lkopf, C. Burges, &amp; A. Smola (Eds.), <em>Advances in kernel methods: Support vector learning</em> (pp. 185â€“209). Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib252">Power, D. J. (2002). What is the true story about data mining, beer and diapers? <em>DSS News</em>, 3(23); see <em>http: //www.dssresources.com/newsletters/66.php</em>.</p>
<p class="ref_entry" id="bib253">Provost, F., &amp; Fawcett, T. (1997). Analysis and visualization of classifier performance: Comparison under imprecise class and cost distributions. In D. Heckerman, H. Mannila, D. Pregibon, &amp; R. Uthurusamy (Eds.), <em>Proceedings of the Third International Conference on Knowledge Discovery and Data Mining</em> (pp. 43â€“48). Huntington Beach, CA. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib254">Pyle, D. (1999). <em>Data preparation for data mining</em>. San Francisco, CA: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib255">Quinlan, J. R. (1986). Induction of decision trees. <em>Machine Learning</em>, 1(1), 81â€“106.</p>
<p class="ref_entry" id="bib256">______. (1992). Learning with continuous classes. In N. Adams, &amp; L. Sterling (Eds.), <em>Proceedings of the Fifth Australian Joint Conference on Artificial Intelligence</em> (pp. 343â€“348). Hobart, Tasmania. Singapore: World Scientific.</p>
<p class="ref_entry" id="bib257">______. (1993). <em>C4.5: Programs for machine learning</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib258">______. (1996). Improved use of continuous attributes in C4.5. <em>Journal of Artificial Intelligence Research</em>, 4, 77â€“90.</p>
<p class="ref_entry" id="bib259">Ramon, J., &amp; de Raedt, L. (2000). Multi instance neural networks. In <em>Proceedings of the ICML Workshop on Attribute-Value and Relational Learning</em> (pp. 53â€“60). Stanford, CA.</p>
<p class="ref_entry" id="bib260">Ray, S., &amp; Craven, M. (2005). Supervised learning versus multiple instance learning: An empirical comparison. In <em>Proceedings of the International Conference on Machine Learning</em> (pp. 697â€“704). Bonn. New York: ACM Press.</p>
<p class="ref_entry" id="bib261">Read, J., Pfahringer, B., Holmes, G., &amp; Frank, E. (2009). Classifier chains for multi-label classification. In <em>Proceedings of the 13th European Conference on Principles and Practice of Knowledge Discovery in Databases and 20th European Conference on Machine Learning</em> (pp. 254â€“269). Bled, Slovenia. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib262">Rennie, J. D. M., Shih, L., Teevan, J., &amp; Karger, D. R. (2003). Tackling the poor assumptions of NaÃ¯ve Bayes text classifiers. In T. Fawcett, &amp; N. Mishra (Eds.), <em>Proceedings of the Twentieth International Conference on Machine Learning</em> (pp. 616â€“623). Washington, DC. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib263">Ricci, F., &amp; Aha, D. W. (1998). Error-correcting output codes for local learners. In C. Nedellec, &amp; C. Rouveird (Eds.), <em>Proceedings of the European Conference on Machine Learning </em>(pp. 280â€“291). Chemnitz, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib264">Richards, D., &amp; Compton, P. (1998). Taking up the situated cognition challenge with ripple-down rules. <em>International Journal of Human-Computer Studies</em>, 49(6), 895â€“926.</p>
<p class="ref_entry" id="bib265">Rifkin, R., &amp; Klautau, A. (2004). In defense of one-vs.-all classification. <em>Journal of Machine Learning Research</em>, 5, 101â€“141.</p>
<p class="ref_entry" id="bib266">Ripley, B. D. (1996). <em>Pattern recognition and neural networks</em>. Cambridge, UK: Cambridge University Press.</p>
<p class="ref_entry" id="bib267">Rissanen, J. (1985). The minimum description length principle. In S. Kotz, &amp; N. L. Johnson (Eds.), <em>Encylopedia of Statistical Sciences (Vol. 5)</em> (pp. 523â€“527). New York: John Wiley.</p>
<p class="ref_entry" id="bib268">Rodriguez, J. J., Kuncheva, L. I., &amp; Alonso, C. J. (2006). Rotation forest: A new classifier ensemble method. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 28(10), 1619â€“1630.</p>
<p class="ref_entry" id="bib269">Rousseeuw, P. J., &amp; Leroy, A. M. (1987). <em>Robust regression and outlier detection</em>. New York: John Wiley.</p>
<p class="ref_entry" id="bib270">Russell, S., &amp; Norvig, P. (2009). <em>Artificial intelligence: A modern approach</em> (3rd ed.). Upper Saddle River, NJ: Prentice-Hall.</p>
<p class="ref_entry" id="bib271">Sahami, M., Dumais, S., Heckerman, D., &amp; Horvitz, E. (1998). A Bayesian approach to filtering junk e-mail. In <em>Proceedings of the AAAI-98 Workshop on Learning for Text Categorization</em> (pp. 55â€“62). Madison, WI. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib272">Saitta, L., &amp; Neri, F. (1998). Learning in the â€œreal world.â€ <em>Machine Learning</em>, 30(2/3), 133â€“163.</p>
<p class="ref_entry" id="bib273">Salzberg, S. (1991). A nearest hyperrectangle learning method. <em>Machine Learning</em>, 6(3), 251â€“276.</p>
<p class="ref_entry" id="bib274">Schapire, R. E., Freund, Y., Bartlett, P., &amp; Lee, W. S. (1997). Boosting the margin: A new explanation for the effectiveness of voting methods. In D. H. Fisher (Ed.), <em>Proceedings of the Fourteenth International Conference on Machine Learning </em>(pp. 322â€“330). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib275">Scheffer, T. (2001). Finding association rules that trade support optimally against confidence. In L. de Raedt, &amp; A. Siebes (Eds.), <em>Proceedings of the Fifth European Conference on Principles of Data Mining and Knowledge Discovery</em> (pp. 424â€“435). Freiburg, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib276">SchÃ¶lkopf, B., Bartlett, P., Smola, A. J., &amp; Williamson, R. (1999). Shrinking the tube: A new support vector regression algorithm. <em>Advances in Neural Information Processing Systems</em>, 11, 330â€“336. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib277">SchÃ¶lkopf, B., Williamson, R., Smola, A., Shawe-Taylor, J., &amp; Platt, J. (2000). Support vector method for novelty detection. <em>Advances in Neural Information Processing Systems</em>, 12, 582â€“588. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib278">SchÃ¶lkopf, B., &amp; Smola, A. J. (2002). <em>Learning with kernels: Support vector machines, regularization, optimization, and beyond</em>. Cambridge, MA: MIT Press.</p>
<p class="ref_entry" id="bib279">Sebastiani, F. (2002). Machine learning in automated text categorization. <em>ACM Computing Surveys</em>, 34(1), 1â€“47.</p>
<p class="ref_entry" id="bib280">Seewald A. K. (2002). How to make stacking better and faster while also taking care of an unknown weakness. In <em>Proceedings of the Nineteenth International Conference on Machine Learning</em> (pp. 54â€“561). Sydney. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib281">Seewald, A. K., &amp; FÃ¼rnkranz, J. (2001). An evaluation of grading classifiers. In F. Hoffmann, D. J. Hand, N. M. Adams, D. H. Fisher, &amp; G. GuimarÃ£es (Eds.), <em>Proceedings of the Fourth International Conference on Advances in Intelligent Data Analysis</em> (pp. 115â€“124). Cascais, Portugal. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib282">Shafer, R., Agrawal, R., &amp; Metha, M. (1996). SPRINT: A scalable parallel classifier for data mining. In T. M. Vijayaraman, A. P. Buchmann, C. Mohan, &amp; N. L. Sarda (Eds.), <em>Proceedings of the Second International Conference on Very Large Databases</em> (pp. 544â€“555). Mumbai (Bombay). San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib283">Shalev-Shwartz, S., Singer, Y., &amp; Srebro, N. (2007). Pegasos: Primal estimated sub-gradient solver for SVM. In <em>Proceedings of the 24th international conference on Machine Learning </em>(pp. 807â€“814). New York: ACM Press.</p>
<p class="ref_entry" id="bib284">Shawe-Taylor, J., &amp; Cristianini, N. (2004). <em>Kernel methods for pattern analysis</em>. Cambridge, UK: Cambridge University Press.</p>
<p class="ref_entry" id="bib285">Slonim, N., Friedman, N., &amp; Tishby, N. (2002). Unsupervised document classification using sequential information maximization. In <em>Proceedings of the 25th International ACM SIGIR Conference on Research and Development in Information Retrieval</em> (pp. 120â€“136). New York: ACM Press.</p>
<p class="ref_entry" id="bib286">Smola, A. J., &amp; B. SchÃ¶lkopf. (2004). A tutorial on support vector regression. <em>Statistics and Computing</em>, 14(3), 199â€“222.</p>
<p class="ref_entry" id="bib287">Soderland, S., Fisher, D., Aseltine, J., &amp; Lehnert, W. (1995). Crystal: Inducing a conceptual dictionary. In <em>Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence</em> (pp. 1314â€“1319). Montreal. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib288">Srikant, R., &amp; Agrawal, R. (1996). Mining sequential patterns: Generalizations and performance improvements. In P. M. Apers, M. Bouzeghoub, &amp; G. Gardarin (Eds.), <em>Proceedings of the Fifth International Conference on Extending Database Technology. </em>Avignon, France. Lecture Notes in Computer Science. Vol. 1057 (pp. 3â€“17). London: Springer-Verlag.</p>
<p class="ref_entry" id="bib289">Stevens, S. S. (1946). On the theory of scales of measurement. <em>Science</em>, 103, 677â€“680.</p>
<p class="ref_entry" id="bib290">Stone, P., &amp; Veloso, M. (2000). Multiagent systems: A survey from a machine learning perspective. <em>Autonomous Robots</em>, 8(3), 345â€“383.</p>
<p class="ref_entry" id="bib291">Stout, Q. F. (2008). Unimodal regression via prefix isotonic regression. <em>Computational Statistics and Data Analysis</em>, 53, 289â€“297.</p>
<p class="ref_entry" id="bib292">Su, J., Zhang, H., Ling, C. X., &amp; Matwin, S. (2008). Discriminative parameter learning for Bayesian networks. In <em>Proceedings of the 25th International Conference on Machine Learning</em> (pp. 1016â€“1023). Helsinki<em>.</em> New York: ACM Press.</p>
<p class="ref_entry" id="bib293">Swets, J. (1988). Measuring the accuracy of diagnostic systems. <em>Science</em>, 240, 1285â€“1293.</p>
<p class="ref_entry" id="bib294">Ting, K. M. (2002). An instance-weighting method to induce cost-sensitive trees. <em>IEEE Transactions on Knowledge and Data Engineering</em>, 14(3), 659â€“665.</p>
<p class="ref_entry" id="bib295">Ting, K. M., &amp; Witten, I. H. (1997a). Stacked generalization: When does it work? In <em>Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence </em>(pp. 866â€“871). Nagoya, Japan. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib296">______. (1997b). Stacking bagged and dagged models. In D. H. Fisher (Ed.), <em>Proceedings of the Fourteenth International Conference on Machine Learning</em> (pp. 367â€“375). Nashville. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib297">Turney, P. D. (1999). <em>Learning to extract key phrases from text</em>. Technical Report ERB-1057, Institute for Information Technology, National Research Council of Canada, Ottawa.</p>
<p class="ref_entry" id="bib298">U.S. House of Representatives Subcommittee on Aviation (2002). Hearing on aviation security with a focus on passenger profiling, February 27, 2002; see <em>http: //www.house.gov/ transportation/aviation/02-27-02/02-27-02memo.html</em>.</p>
<p class="ref_entry" id="bib299">Utgoff, P. E. (1989). Incremental induction of decision trees. <em>Machine Learning</em>, 4(2), 161â€“186.</p>
<p class="ref_entry" id="bib300">Utgoff, P. E., Berkman, N. C., &amp; Clouse, J. A. (1997). Decision tree induction based on efficient tree restructuring. <em>Machine Learning</em>, 29(1), 5â€“44.</p>
<p class="ref_entry" id="bib301">Vafaie, H., &amp; DeJong, K. (1992). Genetic algorithms as a tool for feature selection in machine learning. In <em>Proceedings of the International Conference on Tools with Artificial Intelligence</em> (pp. 200â€“203). Arlington, VA: IEEE Computer Society Press.</p>
<p class="ref_entry" id="bib302">van Rijsbergen, C. A. (1979). <em>Information retrieval</em>. London: Butterworths.</p>
<p class="ref_entry" id="bib303">Vapnik, V. (1999). <em>The nature of statistical learning theory</em> (2nd ed.). New York: Springer-Verlag.</p>
<p class="ref_entry" id="bib304">Vitter, J. S. (1985). Random sampling with a reservoir. <em>ACM Transactions on Mathematical Software</em>, 1(11), 37â€“57.</p>
<p class="ref_entry" id="bib305">Wang, J., &amp; Zucker, J.-D. (2000). Solving the multiple-instance problem: A lazy learning approach. In <em>Proceedings of the International Conference on Machine Learning </em>(pp. 1119â€“1125). Stanford, CA. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib306">Wang, J., Han, J., &amp; Pei, J. (2003). CLOSET+: Searching for the best strategies for mining frequent closed itemsets. In <em>Proceedings of the International Conference on Knowledge Discovery and Data Mining</em> (pp. 236â€“245). Washington, DC. New York: ACM Press.</p>
<p class="ref_entry" id="bib307">Wang, Y., &amp; Witten, I. H. (1997). Induction of model trees for predicting continuous classes. In M. van Someren, &amp; G. Widmer (Eds.), <em>Proceedings of the of the Poster Papers of the European Conference on Machine Learning</em> (pp. 128â€“137). University of Economics, Faculty of Informatics and Statistics, Prague. Berlin: Springer.</p>
<p class="ref_entry" id="bib308">______. (2002). Modeling for optimal probability prediction. In C. Sammut, &amp; A. Hoffmann (Eds.), <em>Proceedings of the Nineteenth International Conference on Machine Learning </em>(pp. 650â€“657). Sydney. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib309">Webb, G. I. (1999). Decision tree grafting from the all-tests-but-one partition. In <em>Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</em> (pp. 702â€“707). San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib310">______. (2000). MultiBoosting: A technique for combining boosting and wagging. <em>Machine Learning</em>, 40(2), 159â€“196.</p>
<p class="ref_entry" id="bib311">Webb, G. I., Boughton, J., &amp; Wang, Z. (2005). Not so naÃ¯ve Bayes: Aggregating one-dependence estimators. <em>Machine Learning</em>, 58(1), 5â€“24.</p>
<p class="ref_entry" id="bib312">Weidmann, N., Frank, E., &amp; Pfahringer, B. (2003). A two-level learning method for generalized multi-instance problems. In <em>Proceedings of the European Conference on Machine Learning</em> (pp. 468â€“479). Cavtat, Croatia. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib314">Weiser, M., &amp; Brown, J. S. (1997). The coming age of calm technology. In P. J. Denning, &amp; R. M. Metcalfe (Eds.), <em>Beyond calculation: The next fifty years</em> (pp. 75â€“86). New York: Copernicus.</p>
<p class="ref_entry" id="bib315">Weiss, S. M., &amp; Indurkhya, N. (1998). <em>Predictive data mining: A practical guide</em>. San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib316">Wettschereck, D., &amp; Dietterich, T. G. (1995). An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. <em>Machine Learning</em>, 19(1), 5â€“28.</p>
<p class="ref_entry" id="bib317">Wild, C. J., &amp; Seber, G. A. F. (1995). <em>Introduction to probability and statistics</em>. Department of Statistics, University of Auckland, New Zealand.</p>
<p class="ref_entry" id="bib318">Winston, P. H. (1992). <em>Artificial intelligence</em>. Reading, MA: Addison-Wesley.</p>
<p class="ref_entry" id="bib319">Witten, I. H. (2004). Text mining. In M. P. Singh (Ed.), <em>Practical handbook of Internet computing</em> (pp. 14-1â€“14-22). Boca Raton, FL: CRC Press.</p>
<p class="ref_entry" id="bib320">Witten, I. H., Bray, Z., Mahoui, M., &amp; Teahan, W. (1999a). Text mining: A new frontier for lossless compression. In J. A. Storer, &amp; M. Cohn (Eds.), <em>Proceedings of the Data Compression Conference</em> (pp. 198â€“207). Snowbird, UT. Los Alamitos, CA: IEEE Press.</p>
<p class="ref_entry" id="bib321">Witten, I. H., Moffat, A., &amp; Bell, T. C. (1999b). <em>Managing gigabytes: Compressing and indexing documents and images</em> (2nd ed.). San Francisco: Morgan Kaufmann.</p>
<p class="ref_entry" id="bib322">Wolpert, D. H. (1992). Stacked generalization. <em>Neural Networks</em>, 5, 241â€“259.</p>
<p class="ref_entry" id="bib323">Wu, X. V., Kumar, J. R., Quinlan, J., Ghosh, Q., Yang, H., Motoda, G. J., et al. (2008). Top 10 algorithms in data mining. <em>Knowledge and Information Systems</em>, 14(1), 1â€“37.</p>
<p class="ref_entry" id="bib324">Wu, X., &amp; Kumar, V. (Eds.), (2009). <em>The top ten algorithms in data mining</em>. New York: Chapman and Hall.</p>
<p class="ref_entry" id="bib325">Xu, X., &amp; Frank, E. (2004). Logistic regression and boosting for labeled bags of instances. In <em>Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining</em> (pp. 272â€“281). Sydney. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib326">Yan, X., &amp; Han, J. (2002). gSpan: Graph-based substructure pattern mining. In <em>Proceedings of the IEEE International Conference on Data Mining</em> (pp. 721â€“724). Maebashi City, Japan. Washington, DC: IEEE Computer Society.</p>
<p class="ref_entry" id="bib327">Yan, X., &amp; Han, J. (2003). CloseGraph: Mining closed frequent graph patterns. In <em>Proceedings of the Ninth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 286â€“295). Washington, DC. New York: ACM Press.</p>
<p class="ref_entry" id="bib328">Yan, X., Han, J., &amp; Afshar, R. (2003). CloSpan: Mining closed sequential patterns in large datasets. In <em>Proceedings of the SIAM International Conference on Data Mining </em>(pp. 166â€“177). San Francisco. Philadelphia: Society for Industrial and Applied Mathematics.</p>
<p class="ref_entry" id="bib329">Yang, Y., &amp; Webb, G. I. (2001). Proportional <em>k</em>-interval discretization for NaÃ¯ve Bayes classifiers. In L. de Raedt, &amp; P. Flach (Eds.), <em>Proceedings of the Twelfth European Conference on Machine Learning</em> (pp. 564â€“575). Freiburg, Germany. Berlin: Springer-Verlag.</p>
<p class="ref_entry" id="bib330">Yang, Y., Guan, X., &amp; You, J. (2002). CLOPE: A fast and effective clustering algorithm for transactional data. In <em>Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (pp. 682â€“687). Edmonton, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib331">Yurcik, W., Barlow, J., Zhou, Y., Raje, H., Li, Y., Yin, X., et al. (2003). Scalable data management alternatives to support data mining heterogeneous logs for computer network security. In <em>Proceedings of the Workshop on Data Mining for Counter Terrorism and Security</em>. San Francisco. Philadelphia: Society for International and Applied Mathematics.</p>
<p class="ref_entry" id="bib332">Zadrozny, B., &amp; Elkan, C. (2002). Transforming classifier scores into accurate multiclass probability estimates. In <em>Proceedings of the Eighth ACM International Conference on Knowledge Discovery and Data Mining</em> (pp. 694â€“699). Edmonton, AB. New York: ACM Press.</p>
<p class="ref_entry" id="bib333">Zaki, M. J., Parthasarathy, S., Ogihara, M., &amp; Li, W. (1997). New algorithms for fast discovery of association rules. In <em>Proceedings Knowledge Discovery in Databases </em>(pp. 283â€“286). Newport Beach, CA. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib334">Zhang, H., Jiang, L., &amp; Su, J. (2005). Hidden NaÃ¯ve Bayes. In <em>Proceedings of the 20th National Conference on Artificial Intelligence</em> (pp. 919â€“924). Pittsburgh. Menlo Park, CA: AAAI Press.</p>
<p class="ref_entry" id="bib335">Zhang, T., Ramakrishnan, R., &amp; Livny, M. (1996). BIRCH: An efficient data clustering method for very large databases. In <em>Proceedings of the ACM SIGMOD International Conference on Management of Data</em> (pp. 103â€“114). Montreal. New York: ACM Press.</p>
<p class="ref_entry" id="bib336">Zhang, T. (2004). Solving large scale linear prediction problems using stochastic gradient descent algorithms. In <em>Proceedings of the 21st International Conference on Machine Learning</em> (pp. 919â€“926). Banff, AB. Madison, WI: Omnipress.</p>
<p class="ref_entry" id="bib337">Zheng, F., &amp; Webb, G. (2006). Efficient lazy elimination for averaged one-dependence estimators. In <em>Proceedings of the 23rd International Conference on Machine Learning </em>(pp. 1113â€“1120). New York: ACM Press.</p>
<p class="ref_entry" id="bib338">Zheng, Z., &amp; Webb, G. (2000). Lazy learning of Bayesian rules. <em>Machine Learning</em>, 41(1), 53â€“84.</p>
<p class="ref_entry" id="bib339">Zhou, Z.-H., &amp; Zhang, M.-L. (2007). Solving multi-instance problems with classifier ensemble based on constructive clustering. <em>Knowledge and Information Systems</em>, 11(2), 155â€“170.</p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0017.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 17. Tutorial Exercises for the Weka Explorer</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/index00024.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Index</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">Â© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    
    
  



<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.014px; left: 1081.01px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#">Reset</a>
</div>
</div><div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5270079786384323"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.833101869972934" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=d80acf44-ae75-fb28-450b-d982cd25b91d&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Data%20Mining%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fhtml%2Fbib00023.html&amp;r=&amp;lt=3823&amp;evt=pageLoad&amp;msclkid=N&amp;rn=33053"></div>


</body></html>