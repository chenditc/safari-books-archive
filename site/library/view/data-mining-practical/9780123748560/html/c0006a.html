<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/html/c0006a.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0006a.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/html/c0006a.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0006a.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>6.8. Clustering - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html"><meta name="description" content=" 6.8 Clustering In Section 4.8 we examined the k-means clustering algorithm in which k initial points are chosen to represent initial cluster centers, all data points are ... "><meta property="og:title" content="6.8. Clustering"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="6.8. Clustering"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0006a.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content=" 6.8 Clustering In Section 4.8 we examined the k-means clustering algorithm in which k initial points are chosen to represent initial cluster centers, all data points are ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style><style type="text/css" id="kampyleStyle">.noOutline{outline: none !important;}.wcagOutline:focus{outline: 1px dashed #595959 !important;outline-offset: 2px !important;transition: none !important;}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/html/c0006a.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0006a.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0006a.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0006a.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%206.8.%20Clustering&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0006a.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 6. Implementations</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 7. Data Transformations</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div id="s0300"><a id="c0006a"></a>
<h2 id="st0295">6.8 Clustering</h2>
<p id="p1770" class="noindent">In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0155">Section 4.8</a> we examined the <em>k</em>-means clustering algorithm in which <em>k</em> initial points are chosen to represent initial cluster centers, all data points are assigned to the nearest one, the mean value of the points in each cluster is computed to form its new cluster center, and iteration continues until there are no changes in the clusters. This procedure only works when the number of clusters is known in advance, and this section begins by describing what you can do if it is not.</p>
<p id="p1775" class="para_indented">Next we take a look at techniques for creating a hierarchical clustering structure by “agglomeration”—that is, starting with individual instances and successively joining them up into clusters. Then we look at a method that works incrementally; that is, process each new instance as it appears. This method was developed in the late 1980s and embodied in a pair of systems called Cobweb (for nominal attributes) and Classit (for numeric attributes). Both come up with a hierarchical grouping of instances and use a measure of cluster “quality” called <em>category utility</em>. Finally, we examine a statistical clustering method based on a mixture model of different probability distributions, one for each cluster. It does not partition instances into disjoint clusters as <em>k</em>-means does but instead assigns instances to classes probabilistically, not deterministically. We explain the basic technique and sketch the working of a comprehensive clustering scheme called AutoClass.</p>
<div id="s0305">
<h3 id="st0300"><a id="p274"></a>Choosing the Number of Clusters</h3>
<p id="p1780" class="noindent">Suppose you are using <em>k</em>-means but do not know the number of clusters in advance. One solution is to try out different possibilities and see which is best. A simple strategy is to start from a given minimum, perhaps <em>k</em> = 1, and work up to a small fixed maximum. Note that on the training data the “best” clustering according to the total squared distance criterion will always be to choose as many clusters as there are data points! To penalize solutions with many clusters you will have to apply something like the MDL criterion of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0110">Section 5.9</a>.</p>
<p id="p1785" class="para_indented">Another possibility is to begin by finding a few clusters and determining whether it is worth splitting them. You could choose <em>k</em> = 2, perform <em>k</em>-means clustering until it terminates, and then consider splitting each cluster. Computation time will be reduced considerably if the initial two-way clustering is considered irrevocable and splitting is investigated for each component independently. One way to split a cluster is to make a new seed one standard deviation away from the cluster’s center in the direction of its greatest variation, and to make a second seed the same distance in the opposite direction. (Alternatively, if this is too slow, choose a distance proportional to the cluster’s bounding box and a random direction.) Then apply <em>k</em>-means to the points in the cluster with these two new seeds.</p>
<p id="p1790" class="para_indented">Having tentatively split a cluster, is it worthwhile retaining the split or is the original cluster equally plausible by itself? It’s no good looking at the total squared distance of all points to their cluster center—this is bound to be smaller for two subclusters. A penalty should be incurred for inventing an extra cluster, and this is a job for the MDL criterion. That principle can be applied to see whether the information required to specify the two new cluster centers, along with the information required to specify each point with respect to them, exceeds the information required to specify the original center and all the points with respect to it. If so, the new clustering is unproductive and should be abandoned. If the split is retained, try splitting each new cluster further. Continue the process until no worthwhile splits remain.</p>
<p id="p1795" class="para_indented">Additional implementation efficiency can be achieved by combining this iterative clustering process with the <em>k</em>D-tree or ball tree data structure advocated in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0155">Section 4.8</a>. Then the data points are reached by working down the tree from the root. When considering splitting a cluster, there is no need to consider the whole tree; just look at those parts of it that are needed to cover the cluster. For example, when deciding whether to split the lower left cluster in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#f0085">Figure 4.16(a)</a> (below the thick line), it is only necessary to consider nodes A and B of the tree in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#f0085">Figure 4.16(b)</a> because node C is irrelevant to that cluster.</p>
</div>
<div id="s0310">
<h3 id="st0305">Hierarchical Clustering</h3>
<p id="p1800" class="noindent">Forming an initial pair of clusters and then recursively considering whether it is worth splitting each one further produces a hierarchy that can be represented as a binary tree called a <em>dendrogram</em>. In fact, we illustrated a dendrogram in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0003.html#f0060">Figure 3.11(d)</a><a id="p275"></a> (there some of the branches were three-way). The same information could be represented as a Venn diagram of sets and subsets: The constraint that the structure is hierarchical corresponds to the fact that, although subsets can include one another, they cannot intersect. In some cases there exists a measure of the degree of dissimilarity between the clusters in each set; then the height of each node in the dendrogram can be made proportional to the dissimilarity between its children. This provides an easily interpretable diagram of a hierarchical clustering.</p>
<p id="p1805" class="para_indented">An alternative to the top-down method for forming a hierarchical structure of clusters is to use a bottom-up approach, which is called <em>agglomerative</em> clustering. This idea was proposed many years ago and has recently enjoyed a resurgence in popularity. The basic algorithm is simple. All you need is a measure of distance (or a similarity measure) between any two clusters. (If you have a similarity measure instead, it is easy to convert that into a distance.) You begin by regarding each instance as a cluster in its own right; then find the two closest clusters, merge them, and keep on doing this until only one cluster is left. The record of mergings forms a hierarchical clustering structure—a binary dendrogram.</p>
<p id="p1810" class="para_indented">There are numerous possibilities for the distance measure. One is the minimum distance between the clusters—the distance between their two closest members. This yields what is called the <em>single-linkage</em> clustering algorithm. Since this measure takes into account only the two closest members of a pair of clusters, the procedure is sensitive to outliers: The addition of just a single new instance can radically alter the entire clustering structure. Also, if we define the diameter of a cluster to be the greatest distance between its members, single-linkage clustering can produce clusters with very large diameters. Another measure is the maximum distance between the clusters, instead of the minimum. Two clusters are considered close only if all instances in their union are relatively similar—sometimes called the <em>complete-linkage</em> method. This measure, which is also sensitive to outliers, seeks compact clusters with small diameters. However, some instances may end up much closer to other clusters than they are to the rest of their own cluster.</p>
<p id="p1815" class="para_indented">There are other measures that represent a compromise between the extremes of minimum and maximum distance between cluster members. One is to represent clusters by the centroid of their members, as the <em>k</em>-means algorithm does, and use the distance between centroids—the <em>centroid-linkage</em> method. This works well when the instances are positioned in multidimensional Euclidean space and the notion of centroid is clear, but not when all we have is a pairwise similarity measure between instances, because centroids are not instances and the similarity between them may be impossible to define.</p>
<p id="p1820" class="para_indented">Another measure, which avoids this problem, is to calculate the average distance between each pair of members of the two clusters—the <em>average-linkage</em> method. Although this seems like a lot of work, you would have to calculate all pairwise distances in order to find the maximum or minimum anyway, and averaging them isn’t much additional burden. Both these measures have a technical deficiency: Their results depend on the numerical scale on which distances are measured. The minimum and maximum distance measures produce a result that depends only on the <em>ordering</em><a id="p276"></a>between the distances involved. In contrast, the result of both centroid-based and average-distance clustering can be altered by a monotonic transformation of all distances, even though it preserves their relative ordering.</p>
<p id="p1825" class="para_indented">Another method, called <em>group-average</em> clustering, uses the average distance between all members of the merged cluster. This differs from the “average” method just described because it includes in the average pairs from the same original cluster. Finally, <em>Ward’s</em> clustering method calculates the increase in the sum of squares of the distances of the instances from the centroid before and after fusing two clusters. The idea is to minimize the increase in this squared distance at each clustering step.</p>
<p id="p1830" class="para_indented">All these measures will produce the same hierarchical clustering result if the clusters are compact and well separated. However, in other cases they can yield quite different structures.</p>
</div>
<div id="s0315">
<h3 id="st0310">Example of Hierarchical Clustering</h3>
<p id="p1835" class="noindent"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figure 6.22</a> shows the result of agglomerative hierarchical clustering. (These visualizations have been generated using the FigTree program.<sup><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#fn0010" id="cc000067fn0010" class="totri-footnote">1</a></sup>) In this case the dataset contained 50 examples of different kinds of creatures, from dolphin to mongoose, from giraffe to lobster. There was 1 numeric attribute (number of legs, ranging from 0 to 6, but scaled to the range [0, 1]) and 15 Boolean attributes such as <em>has feathers</em>, <em>lays eggs</em>, and <em>venomous</em>, which are treated as binary attributes with values 0 and 1 in the distance calculation.</p><a id="p277"></a><a id="p278"></a><p id="f0115" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-022ab-9780123748560.jpg" alt="image" width="513" height="844" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-022ab-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-022cd-9780123748560.jpg" alt="image" width="513" height="843" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-022cd-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.22</span> Hierarchical clustering displays.</p>
<p id="p1840" class="para_indented">Two kinds of display are shown: a standard dendrogram and a polar plot. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(a) and (b)</a> show the output from an agglomerative clusterer plotted in two different ways, and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(c) and (d)</a> show the result of a different agglomerative clusterer plotted in the same two ways. The difference is that the pair in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(a) and (b)</a> was produced using the complete-linkage measure and the pair in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(c) and (d)</a> was produced using the single-linkage measure. You can see that the complete-linkage method tends to produce compact clusters while the single-linkage method produces clusters with large diameters at fairly low levels of the tree.</p>
<p id="p1845" class="para_indented">In all four visualizations the height of each node in the dendrogram is proportional to the dissimilarity between its children, measured as the Euclidean distance between instances. A numeric scale is provided beneath <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(a) and (c)</a>. The total dissimilarity from root to leaf is far greater for the complete-linkage method in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(a) and (b)</a> than for the single-linkage method in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figures 6.22(c) and (d)</a> since the former involves the maximum distance and the latter the minimum distance between instances in each cluster. In the first case the total dissimilarity is a little less than 3.75, which is almost the maximum possible distance between instances—the distance between two instances that differ in 14 of the 15 attributes is <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-002-9780123748560.jpg" alt="image" width="32" height="21" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-002-9780123748560.jpg"> ≈ 3.74. In the second it is a little greater than 2 (that is, <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-003-9780123748560.jpg" alt="image" width="26" height="21" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-003-9780123748560.jpg">), which is what a difference in four Boolean attributes would produce.</p>
<p id="p1850" class="para_indented"><a id="p279"></a>For the complete-linkage method (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figure 6.22(a)</a>), many elements join together at a dissimilarity of 1, which corresponds to a difference in a single Boolean attribute. Only one pair has a smaller dissimilarity: <em>crab</em> and <em>crayfish</em>, which differ only in the number of legs (4/6 and 6/6, respectively, after scaling). Other popular dissimilarities are <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-004-9780123748560.jpg" alt="image" width="24" height="21" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-004-9780123748560.jpg">, <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-005-9780123748560.jpg" alt="image" width="25" height="21" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-005-9780123748560.jpg">, <img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067if006-006-9780123748560.jpg" alt="image" width="26" height="21" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067if006-006-9780123748560.jpg">, and so on, corresponding to differences in two, three, and four Boolean attributes. For the single-linkage method (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0115">Figure 6.22(c)</a>) that uses the minimum distance between clusters, even more elements join together at a dissimilarity of 1.</p>
<p id="p1855" class="para_indented">Which of the two display methods—the standard dendogram and the polar plot—is more useful is a matter of taste. Although more unfamiliar at first, the polar plot spreads the visualization more evenly over the space available.</p>
</div>
<div id="s0320">
<h3 id="st0315">Incremental Clustering</h3>
<p id="p1860" class="noindent">Whereas the <em>k</em>-means algorithm iterates over the whole dataset until convergence is reached and the hierarchical method examines all the clusters present so far at each stage of merging, the clustering methods we examine next work incrementally, instance by instance. At any stage the clustering forms a tree with instances at the leaves and a root node that represents the entire dataset. In the beginning the tree consists of the root alone. Instances are added one by one, and the tree is updated appropriately at each stage. Updating may be merely a case of finding the right place to put a leaf representing the new instance, or it may involve a radical restructuring of the part of the tree that is affected by the new instance. The key to deciding how and where to update is a quantity called <em>category utility</em> that measures the overall quality of a partition of instances into clusters. We defer detailed consideration of how this is defined until the next section and look first at how the clustering algorithm works.</p>
<p id="p1865" class="para_indented">The procedure is best illustrated by an example. We will use the familiar weather data again, but without the <em>play</em> attribute. To track progress, the 14 instances are labeled <em>a</em>, <em>b</em>, <em>c</em>, …, <em>n</em> (as in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#t0040">Table 4.6</a>), and for interest we include the classes <em>yes</em> or <em>no</em> in the label—although it should be emphasized that for this artificial dataset there is little reason to suppose that the two classes of instance should fall into separate categories. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0120">Figure 6.23</a> shows the situation at salient points throughout the clustering procedure.</p>
<p id="f0120" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-023af-9780123748560.jpg" alt="image" width="569" height="512" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-023af-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.23</span> Clustering the weather data.</p>
<p id="p1870" class="para_indented">At the beginning, when new instances are absorbed into the structure, they each form their own subcluster under the overall top-level cluster. Each new instance is processed by tentatively placing it in each of the existing leaves and evaluating the category utility of the resulting set of the top-level node’s children to see if the leaf is a good “host” for the new instance. For each of the first five instances, there is no such host: It is better, in terms of category utility, to form a new leaf for each instance. With the sixth it finally becomes beneficial to form a cluster, joining the new instance <em>f</em> with the old one—the host—<em>e</em>. If you look back at <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#t0040">Table 4.6</a> you will see that the fifth and sixth instances are indeed very similar, differing only in the <em>windy</em> attribute (and <em>play</em>, which is being ignored here). The next example, <em>g</em>, is <a id="p280"></a>placed in the same cluster (it differs from <em>f</em> only in <em>outlook</em>). This involves another call to the clustering procedure. First, <em>g</em> is evaluated to see which of the five children of the root makes the best host; it turns out to be the rightmost, the one that is already a cluster. Then the clustering algorithm is invoked with this as the root, and its two children are evaluated to see which would make the better host. In this case it proves best, according to the category utility measure, to add the new instance as a subcluster in its own right.</p>
<p id="p1875" class="para_indented">If we were to continue in this vein, there would be no possibility of any radical restructuring of the tree, and the final clustering would be excessively dependent on the ordering of examples. To avoid this, there is provision for restructuring, and you can see it come into play when instance <em>h</em> is added in the next step shown in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0120">Figure 6.23(e)</a>. In this case two existing nodes are <em>merged</em> into a single cluster: Nodes <em>a</em> and <a id="p281"></a><em>d</em> are merged before the new instance <em>h</em> is added. One way of accomplishing this would be to consider all pairs of nodes for merging and evaluate the category utility of each pair. However, that would be computationally expensive, and would involve a lot of repeated work if it were undertaken whenever a new instance was added.</p>
<p id="p1880" class="para_indented">Instead, whenever the nodes at a particular level are scanned for a suitable host, both the best-matching node—the one that produces the greatest category utility for the split at that level—and the runner-up are noted. The best one will form the host for the new instance (unless that new instance is better off in a cluster of its own). However, before setting to work on putting the new instance in with the host, consideration is given to merging the host and the runner-up. In this case, <em>a</em> is the preferred host and <em>d</em> is the runner-up. When a merge of <em>a</em> and <em>d</em> is evaluated, it turns out that it would improve the category utility measure. Consequently, these two nodes are merged, yielding a version of the fifth hierarchy before <em>h</em> is added. Then consideration is given to the placement of <em>h</em> in the new, merged node; it turns out to be best to make it a subcluster in its own right, as shown.</p>
<p id="p1885" class="para_indented">An operation converse to merging is also implemented, called <em>splitting</em>. Whenever the best host is identified, and merging has not proved beneficial, consideration is given to splitting the host node. Splitting has exactly the opposite effect of merging, taking a node and replacing it with its children. For example, splitting the rightmost node in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0120">Figure 6.23(d)</a> would raise the <em>e</em>, <em>f</em>, and <em>g</em> leaves up a level, making them siblings of <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em>. Merging and splitting provide an incremental way of restructuring the tree to compensate for incorrect choices caused by infelicitous ordering of examples.</p>
<p id="p1890" class="para_indented">The final hierarchy for all 14 examples is shown in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0120">Figure 6.23(f)</a>. There are three major clusters, each of which subdivides further into its own subclusters. If the <em>play</em>/<em>don’t play</em> distinction really represented an inherent feature of the data, a single cluster would be expected for each outcome. No such clean structure is observed, although a (very) generous eye might discern a slight tendency at lower levels for <em>yes</em> instances to group together, and likewise for <em>no</em> instances.</p>
<p id="p1895" class="para_indented">Exactly the same scheme works for numeric attributes. Category utility is defined for these as well, based on an estimate of the mean and standard deviation of the value of that attribute. Details are deferred to the next subsection. However, there is just one problem that we must attend to here: When estimating the standard deviation of an attribute for a particular node, the result will be zero if the node contains only one instance, as it does more often than not. Unfortunately, zero variances produce infinite values in the category utility formula. A simple heuristic solution is to impose a minimum variance on each attribute. It can be argued that because no measurement is completely precise, it is reasonable to impose such a minimum: It represents the measurement error in a single sample. This parameter is called <em>acuity</em>.</p>
<p id="p1900" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0125">Figure 6.24(a)</a> shows a hierarchical clustering produced by the incremental algorithm for part of the iris dataset (30 instances, 10 from each class). At the top level there are two clusters (i.e., subclusters of the single node representing the whole dataset). The first contains both <em>Iris virginicas</em> and <em>Iris versicolors</em>, and the second contains only <em>Iris setosas</em>. The <em>Iris setosas</em> themselves split into two subclusters, one <a id="p283"></a>with four cultivars and the other with six. The other top-level cluster splits into three subclusters, each with a fairly complex structure. Both the first and second contain only <em>Iris versicolors</em>, with one exception, a stray <em>Iris virginica</em>, in each case; the third contains only <em>Iris virginicas</em>. This represents a fairly satisfactory clustering of the iris data: It shows that the three genera are not artificial at all but reflect genuine differences in the data. This is, however, a slightly overoptimistic conclusion because quite a bit of experimentation with the acuity parameter was necessary to obtain such a nice division.</p><a id="p282"></a><p id="f0125" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-024a-9780123748560.jpg" alt="image" width="750" height="316" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-024a-9780123748560.jpg">
<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-024b-9780123748560.jpg" alt="image" width="551" height="333" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-024b-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.24</span> Hierarchical clusterings of the iris data.</p>
<p id="p1905" class="para_indented">The clusterings produced by this scheme contain one leaf for every instance. This produces an overwhelmingly large hierarchy for datasets of any reasonable size, corresponding, in a sense, to overfitting the particular dataset. Consequently, a second numerical parameter called <em>cutoff</em> is used to suppress growth. Some instances are deemed to be sufficiently similar to others not to warrant formation of their own child node, and this parameter governs the similarity threshold. Cutoff is specified in terms of category utility: When the increase in category utility from adding a new node is sufficiently small, that node is cut off.</p>
<p id="p1910" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0125">Figure 6.24(b)</a> shows the same iris data, clustered with cutoff in effect. Many leaf nodes contain several instances: These are children of the parent node that have been cut off. The division into the three types of iris is a little easier to see from this hierarchy because some of the detail is suppressed. Again, however, some experimentation with the cutoff parameter was necessary to get this result, and in fact a sharper cutoff leads to much less satisfactory clusters.</p>
<p id="p1915" class="para_indented"><a id="p284"></a>Similar clusterings are obtained if the full iris dataset of 150 instances is used. However, the results depend on the ordering of examples: <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0125">Figure 6.24</a> was obtained by alternating the three varieties of iris in the input file. If all <em>Iris setosas</em> are presented first, followed by all <em>Iris versicolors</em> and then all <em>Iris virginicas</em>, the resulting clusters are quite unsatisfactory.</p><a id="p1920"></a><div class="boxg" id="b0050">
<div id="s0325">Category Utility<p id="p1925" class="noindent">Now we look at how the category utility, which measures the overall quality of a partition of instances into clusters, is calculated. In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0110">Section 5.9</a> we learned how the MDL measure could, in principle, be used to evaluate the quality of clustering. Category utility is not MDL-based but rather resembles a kind of quadratic loss function defined on conditional probabilities.</p>
<p id="p1930" class="para_indented">The definition of category utility is rather formidable:</p>
<p class="figure" id="e0230"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si45.jpg" alt="image" width="492" height="54" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si45.jpg"></p>
<p>where <em>C</em>
<span class="sub">1</span>, <em>C</em>
<span class="sub">2</span>, …, <em>C<span class="sub">k</span>
</em> are the <em>k</em> clusters; the outer summation is over these clusters; the next inner one sums over the attributes; <em>a<span class="sub">i</span>
</em> is the <em>i</em>th attribute, and it takes on values <em>v<span class="sub">i</span>
</em>
<span class="sub">1</span>, <em>v<span class="sub">i</span>
</em>
<span class="sub">2</span>, …, which are dealt with by the sum over <em>j</em>. Note that the probabilities themselves are obtained by summing over all instances; thus, there is a further implied level of summation.</p>
<p id="p1935" class="para_indented">This expression makes a great deal of sense if you take the time to examine it. The point of having a cluster is that it will give some advantage in predicting the values of attributes of instances in that cluster—that is, Pr[<em>a<span class="sub">i</span>
</em> = <em>v<span class="sub">ij</span>
</em> | <em>C</em><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg" alt="ent" width="13" height="19" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg">] is a better estimate of the probability that attribute <em>a<span class="sub">i</span>
</em> has value <em>v<span class="sub">ij</span>
</em>, for an instance in cluster <em>C</em><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg" alt="ent" width="13" height="19" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg">, than Pr[<em>a<span class="sub">i</span>
</em> = <em>v<span class="sub">ij</span>
</em>] because it takes account of the cluster the instance is in. If that information doesn’t help, the clusters aren’t doing much good! So what the measure calculates, inside the multiple summation, is the amount by which that information <em>does</em> help in terms of the differences between squares of probabilities. This is not quite the standard squared-difference metric because that sums the squares of the differences (which produces a symmetric result) and the present measure sums the difference of the squares (which, appropriately, does not produce a symmetric result). The differences between squares of probabilities are summed over all attributes, and all their possible values, in the inner double summation. Then it is summed over all clusters, weighted by their probabilities, in the outer summation.</p>
<p id="p1940" class="para_indented">The overall division by <em>k</em> is a little hard to justify because the squared differences have already been summed over the categories. It essentially provides a “per cluster” figure for the category utility that discourages overfitting. Otherwise, because the probabilities are derived by summing over the appropriate instances, the very best category utility would be obtained by placing each instance in its own cluster. Then Pr[<em>a<span class="sub">i</span>
</em> = <em>v<span class="sub">ij</span>
</em> | <em>C</em><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg" alt="ent" width="13" height="19" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg">] would be 1 for the value that attribute <em>a<span class="sub">i</span>
</em> actually has for the single instance in category <em>C</em><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg" alt="ent" width="13" height="19" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/ell_8467.jpg"> and 0 for all other values; the numerator of the category utility formula will end up as</p>
<p class="figure" id="e0235"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si46.jpg" alt="image" width="173" height="35" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si46.jpg"></p>
<p></p>
<p id="p1945" class="para_indented">where <em>m</em> is the total number of attributes. This is the greatest value that the numerator can have; thus, if it were not for the additional division by <em>k</em> in the category utility formula, there would never be any incentive to form clusters containing more than one member. This extra factor is best viewed as a rudimentary overfitting-avoidance heuristic.</p>
<p id="p1950" class="para_indented">This category utility formula applies only to nominal attributes. However, it can be easily extended to numeric attributes by assuming that their distribution is normal with a given (observed) mean <em>µ</em> and standard deviation <em>σ</em>. The probability density function for an attribute <em>a</em> is</p>
<p class="figure" id="e0240"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si47.jpg" alt="image" width="215" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si47.jpg"></p>
<p></p>
<p id="p1955" class="para_indented">The analog of summing the squares of attribute–value probabilities is</p>
<p class="figure" id="e0245"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si48.jpg" alt="image" width="279" height="52" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si48.jpg"></p>
<p>where <em>σ<span class="sub">i</span>
</em> is the standard deviation of the attribute <em>a<span class="sub">i</span>
</em>. Thus, for a numeric attribute we estimate the standard deviation from the data, both within the cluster (<em>σ<span class="sub">il</span>
</em>) and for the data over all clusters (<em>σ<span class="sub">i</span>
</em>), and use these in the category utility formula:</p>
<p class="figure" id="e0250"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si49.jpg" alt="image" width="365" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si49.jpg"></p>
<p></p>
<p id="p1960" class="para_indented">Now the problem mentioned that occurs when the standard deviation estimate is zero becomes apparent: A zero standard deviation produces an infinite value of the category utility formula. Imposing a prespecified minimum variance on each attribute, the acuity, is a rough-and-ready solution to the problem.</p>
</div>
</div>
<p></p>
</div>
<div id="s0330">
<h3 id="st0325"><a id="p285"></a>Probability-Based Clustering</h3>
<p id="p1965" class="noindent">Some of the shortcomings of the heuristic clustering method have already become apparent: the arbitrary division by <em>k</em> in the category utility formula that is necessary to prevent overfitting, the need to supply an artificial minimum value for the standard deviation of clusters, and the ad hoc cutoff value to prevent every single instance from becoming a cluster in its own right. On top of this is the uncertainty inherent in incremental algorithms. To what extent is the result dependent on the order of examples? Are the local restructuring operations of merging and splitting really enough to reverse the effect of bad initial decisions caused by unlucky ordering? Does the final result represent even a <em>local</em> maximum of category utility? Add to this the problem that one never knows how far the final configuration is to a <em>global</em> maximum—and, of course, the standard trick of repeating the clustering procedure several times and choosing the best will destroy the incremental nature of the algorithm. Finally, doesn’t the hierarchical nature of the result really beg the question of which are the <em>best</em> clusters? There are so many clusters in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0125">Figure 6.24</a> that it is difficult to separate the wheat from the chaff.</p>
<p id="p1970" class="para_indented">A more principled statistical approach to the clustering problem can overcome some of these shortcomings. From a probabilistic perspective, the goal of clustering is to find the most likely set of clusters available given the data (and, inevitably, prior expectations). Because no finite amount of evidence is enough to make a completely firm decision on the matter, instances—even training instances—should not be placed categorically in one cluster or the other: Instead, they have a certain <a id="p286"></a>probability of belonging to each cluster. This helps to eliminate the brittleness that is often associated with schemes that make hard and fast judgments.</p>
<p id="p1975" class="para_indented">The foundation for statistical clustering is a statistical model called <em>finite mixtures</em>. A <em>mixture</em> is a set of <em>k</em> probability distributions, representing <em>k</em> clusters, that govern the attribute values for members of that cluster. In other words, each distribution gives the probability that a particular instance would have a certain set of attribute values if it were <em>known</em> to be a member of that cluster. Each cluster has a different distribution. Any particular instance “really” belongs to one and only one of the clusters, but it is not known which one. Finally, the clusters are not equally likely: There is some probability distribution that reflects their relative populations.</p>
<p id="p1980" class="para_indented">The simplest finite-mixture situation is when there is only one numeric attribute, which has a Gaussian or normal distribution for each cluster—but with different means and variances. The clustering problem is to take a set of instances—in this case each instance is just a number—and a prespecified number of clusters, and work out each cluster’s mean and variance and the population distribution between the clusters. The mixture model combines several normal distributions, and its probability density function looks like a mountain range with a peak for each component.</p>
<p id="p1985" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0130">Figure 6.25</a> shows a simple example. There are two clusters, A and B, and each has a normal distribution with means and standard deviations <em>µ</em>
<span class="sub">A</span> and <em>σ</em>
<span class="sub">A</span> for cluster A and <em>µ</em>
<span class="sub">B</span> and <em>σ</em>
<span class="sub">B</span> for cluster B. Samples are taken from these distributions, using cluster A with probability <em>p</em>
<span class="sub">A</span> and cluster B with probability <em>p</em>
<span class="sub">B</span> (where <em>p</em>
<span class="sub">A</span> + <em>p</em>
<span class="sub">B</span> = 1), resulting in a dataset like that shown. Now, imagine being given the dataset without the classes—just the numbers—and being asked to determine the five parameters that characterize the model: <em>µ</em>
<span class="sub">A</span>, <em>σ</em>
<span class="sub">A</span>, <em>µ</em>
<span class="sub">B</span>, <em>σ</em>
<span class="sub">B</span>, and <em>p</em>
<span class="sub">A</span> (the parameter <em>p</em>
<span class="sub">B</span> can be calculated directly from <em>p</em>
<span class="sub">A</span>). That is the finite-mixture problem.</p>
<p id="f0130" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-025-9780123748560.jpg" alt="image" width="439" height="216" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-025-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.25</span> A two-class mixture model.</p><a id="p1990"></a><div class="boxg" id="b0055">
<p id="p1995" class="noindent">If you knew which of the two distributions each instance came from, finding the five parameters would be easy—just estimate the mean and standard deviation for the cluster A samples and the cluster B samples separately, using the formulas</p>
<p class="figure" id="e0255"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si50.jpg" alt="image" width="154" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si50.jpg"></p>
<p></p>
<p id="p2000" class="para_indented">and</p>
<p class="figure" id="e0260"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si51.jpg" alt="image" width="298" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si51.jpg"></p>
<p>(The use of <em>n</em> – 1 rather than <em>n</em> as the denominator in the second formula is a technicality of sampling: It makes little difference in practice if <em>n</em> is used instead.) Here, <em>x</em>
<span class="sub">1</span>, <em>x</em>
<span class="sub">2</span>, …, <em>x<span class="sub">n</span>
</em> are the samples from the distribution A or B. To estimate the fifth parameter <em>p</em>
<span class="sub">A</span>, just take the proportion of the instances that are in the A cluster.</p>
<p id="p2005" class="para_indented">If you knew the five parameters, finding the probabilities that a given instance comes from each distribution would be easy. Given an instance <em>x</em>, the probability that it belongs to cluster A is</p>
<p class="figure" id="e0265"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si52.jpg" alt="image" width="319" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si52.jpg"></p>
<p>where <em>f</em>(<em>x</em>; <em>µ</em>
<span class="sub">A</span><em>, σ</em>
<span class="sub">A</span>) is the normal distribution function for cluster A—that is,</p>
<p class="figure" id="e0270"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si53.jpg" alt="image" width="190" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si53.jpg"></p>
<p></p>
<p id="p2010" class="para_indented">The denominator Pr[<em>x</em>] will disappear: We calculate the numerators for both Pr[A | <em>x</em>] and Pr[B | <em>x</em>] and normalize them by dividing by their sum. This whole procedure is just the same as the way numeric attributes are treated in the Naïve Bayes learning scheme of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0025">Section 4.2</a>. And the caveat explained there applies here too: Strictly speaking, <em>f</em>(<em>x</em>; <em>µ</em>
<span class="sub">A</span><em>, σ</em>
<span class="sub">A</span>) is not the probability Pr[<em>x</em> | A] because the probability of <em>x</em> being any particular real number is zero, but the normalization process makes the final result correct. Note that the final outcome is not a particular cluster but rather the <em>probabilities</em> with which <em>x</em> belongs to cluster A and cluster B.</p>
</div>
<p></p>
</div>
<div id="s0335">
<h3 id="st0330"><a id="p287"></a>The EM Algorithm</h3>
<p id="p2015" class="noindent">The problem is that we know neither of these things: not the distribution that each training instance came from nor the five mixture model parameters. So we adopt the procedure used for the <em>k</em>-means clustering algorithm and iterate. Start with initial guesses for the five parameters, use them to calculate the cluster probabilities for each instance, use these probabilities to reestimate the parameters, and repeat. (If you prefer, you can start with guesses for the classes of the instances instead.) This is called the <em>EM algorithm</em>, for <em>expectation maximization</em>. The first step—calculation of the cluster probabilities, which are the “expected” class values—is “expectation”; the second, calculation of the distribution parameters, is “maximization” of the likelihood of the distributions given the data available.</p><a id="p2020"></a><a id="p288"></a><div class="boxg" id="b0060">
<p id="p2025" class="noindent">A slight adjustment must be made to the parameter estimation equations to account for the fact that it is only cluster probabilities, not the clusters themselves, that are known for each instance. These probabilities just act like weights. If <em>w<span class="sub">i</span>
</em> is the probability that instance <em>i</em> belongs to cluster A, the mean and standard deviation for cluster A are</p>
<p class="figure" id="e0275"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si54.jpg" alt="image" width="213" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si54.jpg"></p>
<p>and</p>
<p class="figure" id="e0280"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si55.jpg" alt="image" width="356" height="50" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si55.jpg"></p>
<p>where now the <em>x<span class="sub">i</span>
</em> are <em>all</em> the instances, not just those belonging to cluster A. (This differs in a small detail from the estimate for the standard deviation given later: If all weights are equal, the denominator is <em>n</em> rather than <em>n</em> – 1. Technically speaking, this is a “maximum-likelihood” estimator for the variance whereas the previous formula is for an “unbiased” estimator. The difference is not important in practice.)</p>
<p id="p2030" class="para_indented">Now consider how to terminate the iteration. The <em>k</em>-means algorithm stops when the classes of the instances don’t change from one iteration to the next—a “fixed point” has been reached. In the EM algorithm things are not quite so easy: The algorithm converges toward a fixed point but never actually gets there. We can see how close it is getting by calculating the overall likelihood that the data came from this dataset, given the values for the five parameters. This overall likelihood is obtained by multiplying the probabilities of the individual instances <em>i</em>:</p>
<p class="figure" id="e0285"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067si56.jpg" alt="image" width="221" height="42" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067si56.jpg"></p>
<p>where the probabilities given the clusters A and B are determined from the normal distribution function <em>f</em>(<em>x</em>; <em>µ, σ</em>). This overall likelihood is a measure of the “goodness” of the clustering and increases at each iteration of the EM algorithm.</p>
<p id="p2035" class="para_indented">Again, there is a technical difficulty with equating the probability of a particular value of <em>x</em> with <em>f</em>(<em>x</em>; <em>µ, σ</em>), and in this case the effect does not disappear because no probability normalization operation is applied. The upshot is that the likelihood expression is not a probability and does not necessarily lie between 0 and 1; nevertheless, its magnitude still reflects the quality of the clustering. In practical implementations its logarithm is calculated instead: This is done by summing the logarithms of the individual components, avoiding multiplications. But the overall conclusion still holds: You should iterate until the increase in log-likelihood becomes negligible. For example, a practical implementation might iterate until the difference between successive values of log-likelihood is less than 10<sup>–10</sup> for 10 successive iterations. Typically, the log-likelihood will increase very sharply over the first few iterations and then converge rather quickly to a point that is virtually stationary.</p>
<p id="p2040" class="para_indented">Although the EM algorithm is guaranteed to converge to a maximum, this is a <em>local</em> maximum and may not necessarily be the same as the global maximum. For a better chance of obtaining the global maximum, the whole procedure should be repeated several times, with different initial guesses for the parameter values. The overall log-likelihood figure can be used to compare the different final configurations obtained: Just choose the largest of the local maxima.</p>
</div>
<p></p>
</div>
<div id="s0340">
<h3 id="st0335"><a id="p289"></a>Extending the Mixture Model</h3>
<p id="p2045" class="noindent">Now that we have seen the Gaussian mixture model for two distributions, let’s consider how to extend it to more realistic situations. The basic method is just the same, but because the mathematical notation becomes formidable we will not develop it in full detail.</p>
<p id="p2050" class="para_indented">Changing the algorithm from two-class problems to multiclass problems is completely straightforward as long as the number <em>k</em> of normal distributions is given in advance. The model can easily be extended from a single numeric attribute per instance to multiple attributes as long as independence between attributes is assumed. The probabilities for each attribute are multiplied together to obtain the joint probability for the instance, just as in the Naïve Bayes method.</p>
<p id="p2055" class="para_indented">When the dataset is known in advance to contain correlated attributes, the independence assumption no longer holds. Instead, two attributes can be modeled jointly by a bivariate normal distribution, in which each has its own mean value but the two standard deviations are replaced by a “covariance matrix” with four numeric parameters. There are standard statistical techniques for estimating the class probabilities of instances and for estimating the means and covariance matrix given the instances and their class probabilities. Several correlated attributes can be handled using a multivariate distribution. The number of parameters increases with the square of the number of jointly varying attributes. With <em>n</em> independent attributes, there are 2<em>n</em> parameters, a mean and a standard deviation for each. With <em>n</em> covariant attributes, there are <em>n</em> + <em>n</em>(<em>n</em> + 1)/2 parameters, a mean for each, and an <em>n</em> × <em>n</em> covariance matrix that is symmetric and therefore involves <em>n</em>(<em>n</em> + 1)/2 different quantities. This escalation in the number of parameters has serious consequences for overfitting, as we will explain later.</p>
<p id="p2060" class="para_indented">To cater for nominal attributes, the normal distribution must be abandoned. Instead, a nominal attribute with <em>v</em> possible values is characterized by <em>v</em> numbers representing the probability of each one. A different set of numbers is needed for every class; <em>kv</em> parameters in all. The situation is very similar to the Naïve Bayes method. The two steps of expectation and maximization correspond exactly to operations we have studied before. Expectation—estimating the cluster to which each instance belongs given the distribution parameters—is just like determining the class of an unknown instance. Maximization—estimating the parameters from the classified instances—is just like determining the attribute–value probabilities from the training instances, with the small difference that in the EM algorithm instances are assigned to classes probabilistically rather than categorically. In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0025">Section 4.2</a> we encountered the problem that probability estimates can turn out to be zero, and the same problem occurs here too. Fortunately, the solution is just as simple—use the Laplace estimator.</p>
<p id="p2065" class="para_indented">Naïve Bayes assumes that attributes are independent—that is the reason why it is called “naïve.” A pair of correlated nominal attributes with <em>v</em>
<span class="sub">1</span> and <em>v</em>
<span class="sub">2</span> possible values, respectively, can be replaced by a single covariant attribute with <em>v</em>
<span class="sub">1</span><em>v</em>
<span class="sub">2</span> possible values. Again, the number of parameters escalates as the number of dependent <a id="p290"></a>attributes increases, and this has implications for probability estimates and overfitting.</p>
<p id="p2070" class="para_indented">The presence of both numeric and nominal attributes in the data to be clustered presents no particular problem. Covariant numeric and nominal attributes are more difficult to handle, and we will not describe them here.</p>
<p id="p2075" class="para_indented">Missing values can be accommodated in various different ways. In principle, they should be treated as unknown and the EM process adapted to estimate them as well as the cluster means and variances. A simple way is to replace them by means or modes in a preprocessing step.</p>
<p id="p2080" class="para_indented">With all these enhancements, probabilistic clustering becomes quite sophisticated. The EM algorithm is used throughout to do the basic work. The user must specify the number of clusters to be sought, the type of each attribute (numeric or nominal), which attributes are to be modeled as covarying, and what to do about missing values. Moreover, different distributions can be used. Although the normal distribution is usually a good choice for numeric attributes, it is not suitable for attributes (such as weight) that have a predetermined minimum (0 in the case of weight) but no upper bound; in this case a “log-normal” distribution is more appropriate. Numeric attributes that are bounded above and below can be modeled by a “log-odds” distribution. Attributes that are integer counts rather than real values are best modeled by the “Poisson” distribution. A comprehensive system might allow these distributions to be specified individually for each attribute. In each case, the distribution involves numeric parameters—probabilities of all possible values for discrete attributes and mean and standard deviation for continuous ones.</p>
<p id="p2085" class="para_indented">In this section we have been talking about clustering. But you may be thinking that these enhancements could be applied just as well to the Naïve Bayes algorithm too—and you’d be right. A comprehensive probabilistic modeler could accommodate both clustering and classification learning, nominal and numeric attributes with a variety of distributions, various possibilities of covariation, and different ways of dealing with missing values. The user would specify, as part of the domain knowledge, which distributions to use for which attributes.</p>
</div>
<div id="s0345">
<h3 id="st0340">Bayesian Clustering</h3>
<p id="p2090" class="noindent">However, there is a snag: overfitting. You might say that if we are not sure which attributes are dependent on each other, why not be on the safe side and specify that <em>all</em> the attributes are covariant? The answer is that the more parameters there are, the greater the chance that the resulting structure is overfitted to the training data—and covariance increases the number of parameters dramatically. The problem of overfitting occurs throughout machine learning, and probabilistic clustering is no exception. There are two ways that it can occur: through specifying too large a number of clusters and through specifying distributions with too many parameters.</p>
<p id="p2095" class="para_indented">The extreme case of too many clusters occurs when there is one for every data point: Clearly, that will be overfitted to the training data. In fact, in the mixture model, problems will occur whenever any of the normal distributions becomes so <a id="p291"></a>narrow that the cluster is centered on just one data point. Consequently, implementations generally insist that clusters contain at least two different data values.</p>
<p id="p2100" class="para_indented">Whenever there are a large number of parameters, the problem of overfitting arises. If you were unsure of which attributes were covariant, you might try out different possibilities and choose the one that maximized the overall probability of the data given the clustering that was found. Unfortunately, the more parameters there are, the larger the overall data probability will tend to be—not necessarily because of better clustering but because of overfitting. The more parameters there are to play with, the easier it is to find a clustering that seems good.</p>
<p id="p2105" class="para_indented">It would be nice if somehow you could penalize the model for introducing new parameters. One principled way of doing this is to adopt a Bayesian approach in which every parameter has a prior probability distribution. Then, whenever a new parameter is introduced, its prior probability must be incorporated into the overall likelihood figure. Because this will involve multiplying the overall likelihood by a number less than 1—the prior probability—it will automatically penalize the addition of new parameters. To improve the overall likelihood, the new parameters will have to yield a benefit that outweighs the penalty.</p>
<p id="p2110" class="para_indented">In a sense, the Laplace estimator that was introduced in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0025">Section 4.2</a>, and whose use we advocated earlier to counter the problem of zero probability estimates for nominal values, is just such a device. Whenever observed probabilities are small, the Laplace estimator exacts a penalty because it makes probabilities that are zero, or close to zero, greater, and this will decrease the overall likelihood of the data. Making two nominal attributes covariant will exacerbate the problem of small probabilities. Instead of <em>v</em>
<span class="sub">1</span> + <em>v</em>
<span class="sub">2</span> parameters, where <em>v</em>
<span class="sub">1</span> and <em>v</em>
<span class="sub">2</span> are the number of possible values, there are now <em>v</em>
<span class="sub">1</span><em>v</em>
<span class="sub">2</span>, greatly increasing the chance of a large number of small estimated probabilities. In fact, the Laplace estimator is tantamount to using a particular prior distribution for the introduction of new parameters.</p>
<p id="p2115" class="para_indented">The same technique can be used to penalize the introduction of large numbers of clusters, just by using a prespecified prior distribution that decays sharply as the number of clusters increases. AutoClass is a comprehensive Bayesian clustering scheme that uses the finite-mixture model with prior distributions on all the parameters. It allows both numeric and nominal attributes and uses the EM algorithm to estimate the parameters of the probability distributions to best fit the data. Because there is no guarantee that the EM algorithm converges to the global optimum, the procedure is repeated for several different sets of initial values. But that is not all. AutoClass considers different numbers of clusters and can consider different amounts of covariance and different underlying probability distribution types for the numeric attributes. This involves an additional, outer level of search. For example, it initially evaluates the log-likelihood for 2, 3, 5, 7, 10, 15, and 25 clusters: After that, it fits a log-normal distribution to the resulting data and randomly selects from it more values to try. As you might imagine, the overall algorithm is extremely computation intensive. In fact, the actual implementation starts with a prespecified time bound and continues to iterate as long as time allows. Give it longer and the results may be better!</p>
<p id="p2120" class="para_indented"><a id="p292"></a>Rather than showing just the most likely clustering to the user, it may be best to present all of them, weighted by probability. Recently, fully Bayesian techniques for <em>hierarchical</em> clustering have been developed that produce as output a probability distribution over possible hierarchical structures representing a dataset. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#f0135">Figure 6.26</a> is a visualization, known as a <em>DensiTree</em>, that shows the set of all trees for a particular dataset in a triangular shape. The tree is best described in terms of its “clades,” a biological term from the Greek <em>klados</em> meaning <em>branch</em>, for a group of the same species that includes all ancestors. Here, there are five clearly distinguishable clades. The first and fourth correspond to a single leaf, while the fifth has two leaves that are so distinct they might be considered clades in their own right. The second and third clades each have five leaves, and there is large uncertainty in their topology. Such visualizations make it easy for people to grasp the possible hierarchical clusterings of their data, at least in terms of the big picture.</p>
<p id="f0135" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000067f006-026-9780123748560.jpg" alt="image" width="441" height="309" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000067f006-026-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 6.26</span> DensiTree showing possible hierarchical clusterings of a given dataset.</p>
</div>
<div id="s0350">
<h3 id="st0345">Discussion</h3>
<p id="p2125" class="noindent">The clustering methods that have been described produce different kinds of output. All are capable of taking new data in the form of a test set and classifying it according to clusters that were discovered by analyzing a training set. However, the hierarchical and incremental clustering methods are the only ones that generate an explicit knowledge structure that describes the clustering in a way that can be visualized and reasoned about. The other algorithms produce clusters that could be visualized in instance space if the dimensionality were not too high.</p>
<p id="p2130" class="para_indented"><a id="p293"></a>If a clustering method were used to label the instances of the training set with cluster numbers, that labeled set could then be used to train a rule or decision tree learner. The resulting rules or tree would form an explicit description of the classes. A probabilistic clustering scheme could be used for the same purpose, except that each instance would have multiple weighted labels and the rule or decision tree learner would have to be able to cope with weighted instances—as many can.</p>
<p id="p2135" class="para_indented">Another application of clustering is to fill in any values of the attributes that may be missing. For example, it is possible to make a statistical estimate of the value of unknown attributes of a particular instance, based on the class distribution for the instance itself and the values of the unknown attributes for other examples.</p>
<p id="p2140" class="para_indented">All the clustering methods we have examined make, at some level, a basic assumption of independence among the attributes. AutoClass does allow the user to specify in advance that two or more attributes are dependent and should be modeled with a joint probability distribution. (There are restrictions, however: Nominal attributes may vary jointly, as may numeric attributes, but not both together. Moreover, missing values for jointly varying attributes are not catered for.) It may be advantageous to preprocess a dataset to make the attributes more independent, using statistical techniques such as the principal components transform described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#s0060">Section 7.3</a>. Note that joint variation that is specific to particular classes will not be removed by such techniques; they only remove overall joint variation that runs across all classes.</p>
<p id="p2145" class="para_indented">Our description of how to modify <em>k</em>-means to find a good value of <em>k</em> by repeatedly splitting clusters and seeing whether the split is worthwhile follows the <em>X</em>-means algorithm of <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib234">Moore and Pelleg (2000)</a>. However, instead of the MDL principle, they use a probabilistic scheme called the Bayes Information Criterion (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib173">Kass and Wasserman, 1995</a>). Efficient agglomerative methods for hierarchical clustering were developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib72">Day and Edelsbrünner (1984)</a>, and the ideas are described in recent books (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib90">Duda et al., 2001</a>; <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib156">Hastie et al., 2009</a>). The incremental clustering procedure, based on the merging and splitting operations, was introduced in systems called Cobweb for nominal attributes (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib99">Fisher, 1987</a>) and Classit for numeric attributes (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib134">Gennari et al., 1990</a>). Both are based on a measure of category utility that had been defined previously (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib138">Gluck and Corter, 1985</a>). The AutoClass program is described by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib56">Cheeseman and Stutz (1995)</a>. Two implementations have been produced: the original research implementation, written in LISP, and a follow-up public implementation in C that is 10 or 20 times faster but somewhat more restricted—for example, only the normal-distribution model is implemented for numeric attributes. DensiTrees were developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib33">Bouckaert (2010)</a>.</p>
<p id="p2150" class="para_indented">A hierarchical clustering method called BIRCH (balanced iterative reducing and clustering using hierarchies) has been developed specifically for large multidimensional datasets, where it is necessary for efficient operation to minimize input–output costs (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib335">Zhang et al., 1996</a>). It incrementally and dynamically clusters multidimensional metric data points, seeking the best clustering within given memory and time constraints. It typically finds a good clustering with a single scan of the data, which can then be improved by further scans.</p>
</div>
</div>
<div id="s0355">
<h2 id="st0350">6.9 <a id="p294"></a>Semisupervised learning</h2>
<p id="p2155" class="noindent">When introducing the machine learning process in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#c0002">Chapter 2</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#p40">page 40</a>), we drew a sharp distinction between supervised and unsupervised learning—classification and clustering. In this chapter we have studied a lot of techniques for both. Recently, researchers have begun to explore territory between the two, sometimes called <em>semisupervised learning</em>, in which the goal is classification but the input contains both unlabeled and labeled data. You can’t do classification without labeled data, of course, because only the labels tell what the classes are. But it is sometimes attractive to augment a small amount of labeled data with a large pool of unlabeled data. It turns out that the unlabeled data can help you learn the classes. How can this be?</p>
<p id="p2160" class="para_indented">First, why would you want it? Many situations present huge volumes of raw data, but assigning classes is expensive because it requires human insight. Text mining provides some classic examples. Suppose you want to classify web pages into predefined groups. In an academic setting you might be interested in faculty pages, graduate student pages, course information pages, research group pages, and department pages. You can easily download thousands, or millions, of relevant pages from university web sites. But labeling the training data is a laborious manual process. Or suppose your job is to use machine learning to spot names in text, differentiating between personal names, company names, and place names. You can easily download megabytes, or gigabytes, of text, but making this into training data by picking out the names and categorizing them can only be done manually. Cataloging news articles, sorting electronic mail, learning users’ reading interests—the applications are legion. Leaving text aside, suppose you want to learn to recognize certain famous people in television broadcast news. You can easily record hundreds or thousands of hours of newscasts, but again labeling is manual. In any of these scenarios it would be enormously attractive to be able to leverage a large pool of unlabeled data to obtain excellent performance from just a few labeled examples, particularly if you were the graduate student who had to do the labeling!</p>
<div id="s0360">
<h3 id="st0355">Clustering for Classification</h3>
<p id="p2165" class="noindent">How can unlabeled data be used to improve classification? Here’s a simple idea. Use Naïve Bayes to learn classes from a small labeled dataset and then extend it to a large unlabeled dataset using the EM iterative clustering algorithm from the previous section. The procedure is this. First, train a classifier using the labeled data. Second, apply it to the unlabeled data to label it with class probabilities (the “expectation” step). Third, train a new classifier using the labels for all the data (the “maximization” step). Fourth, iterate until convergence. You could think of this as iterative clustering, where starting points and cluster labels are gleaned from the labeled data. The EM procedure guarantees finding model parameters that have equal or greater likelihood at each iteration. The key question, which can only be answered empirically, is whether these higher likelihood parameter estimates will improve classification accuracy.</p>
<p id="p2170" class="para_indented"><a id="p295"></a>Intuitively, this might work well. Consider document classification. Certain phrases are indicative of the classes. Some of them occur in labeled documents, whereas others occur only in unlabeled ones. There are, however, probably some documents that contain both, and the EM procedure uses these to generalize the learned model to use phrases that do not appear in the labeled dataset. For example, both <em>supervisor</em> and <em>Ph.D. topic</em> might indicate a graduate student’s home page. Suppose only the former phrase occurs in the labeled documents. EM iteratively generalizes the model to correctly classify documents that contain just the latter.</p>
<p id="p2175" class="para_indented">This might work with any classifier and any iterative clustering algorithm. But it is basically a bootstrapping procedure, and you must take care to ensure that the feedback loop is a positive one. Using probabilities rather than hard decisions seems beneficial because it allows the procedure to converge slowly instead of jumping to conclusions that may be wrong. Naïve Bayes, together with the basic probabilistic EM procedure, is a particularly apt choice because the two share the same fundamental assumption: independence between attributes or, more precisely, conditional independence between attributes given the class.</p>
<p id="p2180" class="para_indented">Of course, the independence assumption is universally violated. Even our little example used the two-word phrase <em>Ph.D. topic</em>, whereas actual implementations would likely use individual words as attributes—and the example would have been far less compelling if we had substituted either of the single terms <em>Ph.D.</em> or <em>topic</em>. The phrase <em>Ph.D. students</em> is probably more indicative of faculty rather than graduate student home pages; the phrase <em>research topic</em> is probably less discriminating. It is the very fact that <em>Ph.D.</em> and <em>topic</em> are <em>not</em> conditionally independent given the class that makes the example work: It is their combination that characterizes graduate student pages.</p>
<p id="p2185" class="para_indented">Nevertheless, coupling Naïve Bayes and EM in this manner works well in the domain of document classification. In a particular classification task it attained the performance of a traditional learner using fewer than one-third of the labeled training instances, as well as five times as many unlabeled ones. This is a good tradeoff when labeled instances are expensive but unlabeled ones are virtually free. With a small number of labeled documents, classification accuracy can be improved dramatically by incorporating many unlabeled ones.</p>
<p id="p2190" class="para_indented">Two refinements to the procedure have been shown to improve performance. The first is motivated by experimental evidence showing that when there are many labeled documents the incorporation of unlabeled data may reduce rather than increase accuracy. Hand-labeled data is (or should be) inherently less noisy than automatically labeled data. The solution is to introduce a weighting parameter that reduces the contribution of the unlabeled data. This can be incorporated into the maximization step of EM by maximizing the weighted likelihood of the labeled and unlabeled instances. When the parameter is close to 0, unlabeled documents have little influence on the shape of EM’s hill-climbing surface; when it is close to 1, the algorithm reverts to the original version in which the surface is equally affected by both kinds of document.</p>
<p id="p2195" class="para_indented"><a id="p296"></a>The second refinement is to allow each class to have several clusters. As explained in the previous section, the EM clustering algorithm assumes that the data is generated randomly from a mixture of different probability distributions, one per cluster. Until now, a one-to-one correspondence between mixture components and classes has been assumed. In many circumstances, including document classification, this is unrealistic because most documents address multiple topics. With several clusters per class, each labeled document is initially assigned randomly to each of its components in a probabilistic fashion. The maximization step of the EM algorithm remains as before, but the expectation step is modified not only to probabilistically label each example with the classes but to probabilistically assign it to the components within the class. The number of clusters per class is a parameter that depends on the domain and can be set by cross-validation.</p>
</div>
<div id="s0365">
<h3 id="st0360">Co-training</h3>
<p id="p2200" class="noindent">Another situation in which unlabeled data can improve classification performance is when there are two different and independent perspectives on the classification task. The classic example again involves documents, this time web documents, where the two perspectives are the <em>content</em> of a web page and the <em>links</em> to it from other pages. These two perspectives are well known to be both useful and different: Successful web search engines capitalize on them both using secret recipes. The text that labels a link to another web page gives a revealing clue as to what that page is about—perhaps even more revealing than the page’s own content, particularly if the link is an independent one. Intuitively, a link labeled <em>my advisor</em> is strong evidence that the target page is a faculty member’s home page.</p>
<p id="p2205" class="para_indented">The idea, called <em>co-training</em>, is this. Given a few labeled examples, first learn a different model for each perspective—in this case a content-based and a hyperlink-based model. Then use each one separately to label the unlabeled examples. For each model, select the example that it most confidently labels as positive and the one it most confidently labels as negative, and add these to the pool of labeled examples. Better yet, maintain the ratio of positive and negative examples in the labeled pool by choosing more of one kind than the other. In either case, repeat the whole procedure, training both models on the augmented pool of labeled examples, until the unlabeled pool is exhausted.</p>
<p id="p2210" class="para_indented">There is some experimental evidence, using Naïve Bayes throughout as the learner, that this bootstrapping procedure outperforms one that employs all the features from both perspectives to learn a single model from the labeled data. It relies on having two different views of an instance that are redundant but not completely correlated. Various domains have been proposed, from spotting celebrities in televised newscasts using video and audio separately to mobile robots with vision, sonar, and range sensors. The independence of the views reduces the likelihood of both hypotheses agreeing on an erroneous label.</p>
</div>
<div id="s0370">
<h3 id="st0365"><a id="p297"></a>EM and Co-training</h3>
<p id="p2215" class="noindent">On datasets with two feature sets that are truly independent, experiments have shown that co-training gives better results than using EM as described previously. Even better performance, however, can be achieved by combining the two into a modified version of co-training called <em>co-EM</em>. Co-training trains two classifiers representing different perspectives, A and B, and uses both to add new examples to the training pool by choosing whichever unlabeled examples they classify most positively or negatively. The new examples are few in number and deterministically labeled. Co-EM, on the other hand, trains classifier A on the labeled data and uses it to <em>probabilistically</em> label <em>all</em> the unlabeled data. Next it trains classifier B on both the labeled data and the unlabeled data with classifier A’s tentative labels, and then it probabilistically relabels all the data for use by classifier A. The process iterates until the classifiers converge. This procedure seems to perform consistently better than co-training because it does not commit to the class labels that are generated by classifiers A and B but rather reestimates their probabilities at each iteration.</p>
<p id="p2220" class="para_indented">The range of applicability of co-EM, like co-training, is still limited by the requirement for multiple independent perspectives. But there is some experimental evidence to suggest that even when there is no natural split of features into independent perspectives, benefits can be achieved by manufacturing such a split and using co-training—or, better yet, co-EM—on the split data. This seems to work even when the split is made randomly; performance could surely be improved by engineering the split so that the feature sets are maximally independent. Why does this work? Researchers have hypothesized that these algorithms succeed in part because the split makes them more robust to the assumptions that their underlying classifiers make.</p>
<p id="p2225" class="para_indented">There is no particular reason to restrict the base classifier to Naïve Bayes. Support vector machines probably represent the most successful technology for text categorization today. However, for the EM iteration to work it is necessary that the classifier labels the data probabilistically; it must also be able to use probabilistically weighted examples for training. Support vector machines can easily be adapted to do both. We explained how to adapt learning algorithms to deal with weighted instances in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0220">Section 6.6</a>, under Locally Weighted Linear Regression (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#p258">page 258</a>). One way of obtaining probability estimates from support vector machines is to fit a one-dimensional logistic model to the output, effectively performing logistic regression as described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0110">Section 4.6</a> on the output. Excellent results have been reported for text classification using co-EM with the support vector machine (SVM) classifier. It outperforms other variants of SVM and seems quite robust to varying proportions of labeled and unlabeled data.</p>
</div>
<div id="s0375">
<h3 id="st0370">Discussion</h3>
<p id="p2230" class="noindent"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib241">Nigam et al. (2000)</a> thoroughly explored the idea of clustering for classification, showing how the EM clustering algorithm can use unlabeled data to improve an initial classifier built by Naïve Bayes. The idea of co-training is older: <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib30">Blum and Mitchell (1998)</a><a id="p298"></a>pioneered it and developed a theoretical model for the use of labeled and unlabeled data from different independent perspectives. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib240">Nigam and Ghani (2000)</a> analyzed the effectiveness and applicability of co-training, relating it to the traditional use of standard expectation maximization to fill in missing values; they also introduced the co-EM algorithm. Up to this point, co-training and co-EM have been applied mainly to small two-class problems. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib135">Ghani (2002)</a> used error-correcting output codes to address multiclass situations with many classes. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib35">Brefeld and Scheffer (2004)</a> extended co-EM to use a support vector machine rather than Naïve Bayes.</p>
</div>
</div>
<div id="s0380">
<h2 id="st0375">6.10 Multi-instance learning</h2>
<p id="p2235" class="noindent">All the techniques described in this chapter so far are for the standard machine learning scenario where each example consists of a single instance. Before moving on to methods for transforming the input data in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#c0007">Chapter 7</a>, we revisit the more complex setting of multi-instance learning, in which each example consists of a bag of instances instead. We describe approaches that are more advanced than the simple techniques discussed in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0175">Section 4.9</a>. First, we consider how to convert multi-instance learning to single-instance learning by transforming the data. Then we discuss how to upgrade single-instance learning algorithms to the multi-instance case. Finally, we take a look at some methods that have no direct equivalent in single-instance learning.</p>
<div id="s0385">
<h3 id="st0380">Converting to Single-Instance Learning</h3>
<p id="p2240" class="noindent"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0175">Section 4.9</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#p142">page 142</a>) presented some ways of applying standard single-instance learning algorithms to multi-instance data by aggregating the input or the output. Despite their simplicity, these techniques often work surprisingly well in practice. Nevertheless, there are clearly situations in which they will fail. Consider the method of aggregating the input by computing the minimum and maximum values of numeric attributes present in the bag and treating the result as a single instance. This will yield a huge loss of information because attributes are condensed to summary statistics individually and independently. Can a bag be converted to a single instance without discarding quite so much information?</p>
<p id="p2245" class="para_indented">The answer is yes, although the number of attributes that are present in the so-called “condensed” representation may increase substantially. The basic idea is to partition the instance space into regions and create one attribute per region in the single-instance representation. In the simplest case, attributes can be Boolean: If a bag has at least one instance in the region corresponding to a particular attribute the value of the attribute is set to true; otherwise, it is set to false. However, to preserve more information the condensed representation could instead contain numeric attributes, the values of which are counts that indicate how many instances of the bag lie in the corresponding region.</p>
<p id="p2250" class="para_indented"><a id="p299"></a>Regardless of the exact types of attributes that are generated, the main problem is to come up with a partitioning of the input space. A simple approach is to partition it into hypercubes of equal size. Unfortunately, this only works when the space has very few dimensions (i.e., attributes): The number of cubes required to achieve a given granularity grows exponentially with the dimension of the space. One way to make this approach more practical is to use unsupervised learning. Simply take all the instances from all the bags in the training data, discard their class labels, and form a big single-instance dataset; then process it with a clustering technique such as <em>k</em>-means. This will create regions corresponding to the different clusters (<em>k</em> regions, in the case of <em>k</em>-means). Then, for each bag, create one attribute per region in the condensed representation and use it as described previously.</p>
<p id="p2255" class="para_indented">Clustering is a rather heavy-handed way to infer a set of regions from the training data because it ignores information about class membership. An alternative approach that often yields better results is to partition the instance space using decision tree learning. Each leaf of a tree corresponds to one region of instance space. But how can a decision tree be learned when the class labels apply to entire bags of instances rather than to individual instances? The approach described under Aggregating the Output in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0175">Section 4.9</a> can be used: Take the bag’s class label and attach it to each of its instances. This yields a single-instance dataset, ready for decision tree learning. Many of the class labels will be incorrect—the whole point of multi-instance learning is that it is not clear how bag-level labels relate to instance-level ones. However, these class labels are only being used to obtain a partition of instance space. The next step is to transform the multi-instance dataset into a single-instance one that represents how instances from each bag are distributed throughout the space. Then another single-instance learning method is applied—perhaps, again, decision tree learning—that determines the importance of individual attributes in the condensed representation which correspond to regions in the original space.</p>
<p id="p2260" class="para_indented">Using decision trees and clustering yields “hard” partition boundaries, where an instance either does or does not belong to a region. Such partitions can also be obtained using a distance function, combined with some reference points, by assigning instances to their closest reference point. This implicitly divides the space into regions, each corresponding to one reference point. (In fact, this is exactly what happens in <em>k</em>-means clustering: The cluster centers are the reference points.) But there is no fundamental reason to restrict attention to hard boundaries: We can make the region membership function “soft” by using distance—transformed into a similarity score—to compute attribute values in the condensed representation of a bag. All that is needed is some way of aggregating the similarity scores between each bag and reference point into a single value—for example, by taking the maximum similarity between each instance in that bag and the reference point.</p>
<p id="p2265" class="para_indented">In the simplest case, each instance in the training data can be used as a reference point. That creates a large number of attributes in the condensed representation, but it preserves much of the information from a bag of instances in its corresponding single-instance representation. This method has been successfully applied to multi-instance problems.</p>
<p id="p2270" class="para_indented"><a id="p300"></a>Regardless of how the approach is implemented, the basic idea is to convert a bag of instances into a single one by describing the distribution of instances from this bag in instance space. Alternatively, ordinary learning methods can be applied to multi-instance data by aggregating the output rather than the input. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0175">Section 4.9</a> described a simple way: Join instances of bags in the training data into a single dataset by attaching bag-level class labels to them, perhaps weighting instances to give each bag the same total weight. A single-instance classification model can then be built. At classification time, predictions for individual instances are combined—for example, by averaging predicted class probabilities.</p>
<p id="p2275" class="para_indented">Although this approach often works well in practice, attaching bag-level class labels to instances is simplistic. Generally, the assumption in multi-instance learning is that only some of the instances—perhaps just one—are responsible for the class label of the associated bag. How can the class labels be corrected to yield a more accurate representation of the true underlying situation? This is obviously a difficult problem; if it were solved, it would make little sense to investigate other approaches to multi-instance learning. One method that has been applied is iterative: Start by assigning each instance its bag’s class label and learn a single-instance classification model; then replace the instances’ class labels by the predicted labels of this single-instance classification model for these instances. Repeat the whole procedure until the class labels remain unchanged from one iteration to the next.</p>
<p id="p2280" class="para_indented">Some care is needed to obtain sensible results. For example, suppose every instance in a bag were to receive a class label that differs from the bag’s label. Such a situation should be prevented by forcing the bag’s label on at least one instance—for example, the one with the largest predicted probability for this class.</p>
<p id="p2285" class="para_indented">This iterative approach has been investigated for the original multi-instance scenario with two class values, where a bag is positive if and only if one of its instances is positive. In that case it makes sense to assume that all instances from negative bags are truly negative and modify only the class labels of instances from positive bags. At prediction time, bags are classified as positive if one of their instances is classified as positive.</p>
</div>
<div id="s0390">
<h3 id="st0385">Upgrading Learning Algorithms</h3>
<p id="p2290" class="noindent">Tackling multi-instance learning by modifying the input or output so that single-instance schemes can be applied is appealing because there is a large battery of such techniques that can then be used directly, without any modification. However, it may not be the most efficient approach. An alternative is to adapt the internals of a single-instance algorithm to the multi-instance setting. This can be done in a particularly elegant fashion if the algorithm in question only considers the data through application of a distance (or similarity) function, as with nearest-neighbor classifiers or support vector machines. These can be adapted by providing a distance (or similarity) function for multi-instance data that computes a score between two bags of instances.</p>
<p id="p2295" class="para_indented"><a id="p301"></a>In the case of kernel-based methods such as support vector machines, the similarity must be a proper kernel function that satisfies certain mathematical properties. One that has been used for multi-instance data is the so-called <em>set kernel</em>. Given a kernel function for pairs of instances that support vector machines can apply to single-instance data—for example, one of the kernel functions considered in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0120">Section 6.4</a>—the set kernel sums it over all pairs of instances from the two bags being compared. This idea is generic and can be applied with any single-instance kernel function.</p>
<p id="p2300" class="para_indented">Nearest-neighbor learning has been adapted to multi-instance data by applying variants of the Hausdorff distance, which is defined for sets of points. Given two bags and a distance function between pairs of instances—for example, the Euclidean distance—the Hausdorff distance between the bags is the largest distance from any instance in one bag to its closest instance in the other bag. It can be made more robust to outliers by using the <em>n</em>th-largest distance rather than the maximum.</p>
<p id="p2305" class="para_indented">For learning algorithms that are not based on similarity scores, more work is required to upgrade them to multi-instance data. There are multi-instance algorithms for rule learning and for decision tree learning, but we will not describe them here. Adapting algorithms to the multi-instance case is more straightforward if the algorithm concerned is essentially a numerical optimization strategy that is applied to the parameters of some function by minimizing a loss function on the training data. Logistic regression (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0110">Section 4.6</a>) and multilayer perceptrons (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0120">Section 6.4</a>) fall into this category; both have been adapted to multi-instance learning by augmenting them with a function that aggregates instance-level predictions. The so-called “soft maximum” is a differentiable function that is suitable for this purpose: It aggregates instance-level predictions by taking their (soft) maximum as the bag-level prediction.</p>
</div>
<div id="s0395">
<h3 id="st0390">Dedicated Multi-Instance Methods</h3>
<p id="p2310" class="noindent">Some multi-instance learning schemes are not based directly on single-instance algorithms. Here is an early technique that was specifically developed for the drug activity prediction problem mentioned in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#s0015">Section 2.2</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#p49">page 49</a>), in which instances are conformations of a molecule and a molecule (i.e., a bag) is considered positive if and only if it has at least one active conformation. The basic idea is to learn a single hyperrectangle that contains at least one instance from each positive bag in the training data and no instances from any negative bags. Such a rectangle encloses an area of instance space where all positive bags overlap, but it contains no negative instances—an area that is common to all active molecules but not represented in any inactive ones. The particular drug activity data originally considered was high-dimensional, with 166 attributes describing each instance. In such a case it is computationally difficult to find a suitable hyperrectangle. Consequently, a heuristic approach was developed that is tuned to this particular problem.</p>
<p id="p2315" class="para_indented">Other geometric shapes can be used instead of hyperrectangles. Indeed, the same basic idea has been applied using hyperspheres (balls). Training instances are treated <a id="p302"></a>as potential ball centers. For each one, a radius is found that yields the smallest number of errors for the bags in the training data. The original multi-instance assumption is used to make predictions: A bag is classified as positive if and only if it has at least one instance inside the ball. A single ball is generally not powerful enough to yield good classification performance. However, this method is not intended as a standalone algorithm. Rather, it is advocated as a “weak” learner to be used in conjunction with boosting algorithms (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#s0095">Section 8.4</a>) to obtain a powerful ensemble classifier—an ensemble of balls.</p>
<p id="p2320" class="para_indented">The dedicated multi-instance methods discussed so far have hard decision boundaries: An instance either falls inside or outside a ball or hyperrectangle. Other multi-instance algorithms use soft concept descriptions couched in terms of probability theory. The so-called <em>diverse-density</em> method is a classic example, again designed with the original multi-instance assumption in mind. Its basic and most commonly used form learns a single reference point in instance space. The probability that an instance is positive is computed from its distance to this point: It is 1 if the instance coincides with the reference point and decreases with increasing distance from this point, usually based on a bell-shaped function.</p>
<p id="p2325" class="para_indented">The probability that a bag is positive is obtained by combining the individual probabilities of the instances it contains, generally using the “noisy-OR” function. This is a probabilistic version of the logical OR. If all instance-level probabilities are 0, the noisy-OR value—and thus the bag-level probability—is 0; if at least one instance-level probability is 1, the value is 1; otherwise, the value falls somewhere in between.</p>
<p id="p2330" class="para_indented">The diverse density is defined as the probability of the class labels of the bags in the training data, computed based on this probabilistic model. It is maximized when the reference point is located in an area where positive bags overlap and no negative bags are present, just as for the two geometric methods discussed previously. A numerical optimization routine such as gradient ascent can be used to find the reference point that maximizes the diverse-density measure. In addition to the location of the reference point, implementations of diverse density also optimize the scale of the distance function in each dimension because generally not all attributes are equally important. This can improve predictive performance significantly.</p>
</div>
<div id="s0400">
<h3 id="st0395">Discussion</h3>
<p id="p2335" class="noindent">Condensing the input data by aggregating information into simple summary statistics is a well-known technique in multirelational learning, used in the RELAGGS system by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib194">Krogel and Wrobel (2002)</a>; multi-instance learning can be viewed as a special case of this more general setting (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib74">de Raedt, 2008</a>). The idea of replacing simple summary statistics by region-based attributes, derived from partitioning the instance space, was explored by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib312">Weidmann et al. (2003)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib339">Zhou and Zhang (2007)</a>. Using reference points to condense bags was investigated by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib58">Chen et al. (2006)</a> and evaluated in a broader context by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib104">Foulds and Frank (2008)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib9">Andrews et al. (2003)</a> proposed manipulating the class labels of individual instances using an iterative learning <a id="p303"></a>process for learning support vector machine classifiers based on the original multi-instance assumption.</p>
<p id="p2340" class="para_indented">Nearest-neighbor learning based on variants of the Hausdorff distance was investigated by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib305">Wang and Zucker (2000)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib132">Gärtner et al. (2002)</a> experimented with the set kernel to learn support vector machine classifiers for multi-instance data. Multi-instance algorithms for rule and decision tree learning, which are not covered here, have been described by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib60">Chevaleyre and Zucker (2001)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib29">Blockeel et al. (2005)</a>. Logistic regression has been adapted for multi-instance learning by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib325">Xu and Frank (2004)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib260">Ray and Craven (2005)</a>; multilayer perceptrons have been adapted by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib259">Ramon and de Raedt (2000)</a>.</p>
<p id="p2345" class="para_indented">Hyperrectangles and spheres were considered as concept descriptions for multi-instance learning by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib81">Dietterich et al. (1997)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib15">Auer and Ortner (2004)</a>, respectively. The diverse-density method is the subject of <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib218">Maron’s (1998)</a> Ph.D. thesis, and is also described in <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib219">Maron and Lozano-Peréz (1997)</a>.</p>
<p id="p2350" class="para_indented">The multi-instance literature makes many different assumptions regarding the type of concept to be learned, defining, for example, how the bag-level and instance-level class labels are connected, starting with the original assumption that a bag is labeled positive if and only if one of its instances is positive. A review of assumptions in multi-instance learning can be found in <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib105">Foulds and Frank (2010)</a>.</p>
</div>
</div>
<div id="s0405">
<h2 id="st0400">6.11 Weka implementations</h2>
<p id="p2355" class="noindent">For classifiers, see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0175">Section 11.4</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0030">Table 11.5</a>. For clustering methods, see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0255">Section 11.6</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0040">Table 11.7</a>.</p><a id="p2360"></a><div class="none">
<p class="hang" id="u0010">• <a id="p2365"></a>Decision trees:</p>
<div class="none">
<p class="hang1" id="u0015">• <a id="p2370"></a><em>J48</em> (implementation of C4.5)</p>
<p class="hang1" id="u0020">• <a id="p2375"></a><em>SimpleCart</em> (minimum cost-complexity pruning à la CART)</p>
<p class="hang1" id="u0025">• <a id="p2380"></a><em>REPTree</em> (reduced-error pruning)</p>
</div>
<p class="hang" id="u0030">• <a id="p2385"></a>Classification rules:</p>
<div class="none">
<p class="hang1" id="u0035">• <a id="p2390"></a><em>JRip</em> (RIPPER rule learner)</p>
<p class="hang1" id="u0040">• <a id="p2395"></a><em>Part</em> (rules from partial decision trees)</p>
<p class="hang1" id="u0045">• <a id="p2400"></a><em>Ridor</em> (ripple-down rule learner)</p>
</div>
<p class="hang" id="u0050">• <a id="p2405"></a>Association rules (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0260">Section 11.7</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0045">Table 11.8</a>):</p>
<div class="none">
<p class="hang1" id="u0055">• <a id="p2410"></a><em>FPGrowth</em> (frequent-pattern trees)</p>
<p class="hang1" id="u0060">• <a id="p2415"></a><em>GeneralizedSequentialPatterns</em> (find large item trees in sequential data)</p>
</div>
<p class="hang" id="u0065">• <a id="p2420"></a>Linear models and extensions:</p>
<div class="none">
<p class="hang1" id="u0070">• <a id="p2425"></a><em>SMO</em> and variants for learning support vector machines</p>
<p class="hang1" id="u0075">• <a id="p2430"></a><em>LibSVM</em> (uses third-party <em>libsvm</em> library)</p>
<p class="hang1" id="u0080">• <a id="p2435"></a><em>MultilayerPerceptron</em></p>
<p class="hang1" id="u0085">• <a id="p2440"></a><em>RBFNetwork</em> (radial-basis function network)</p>
<p class="hang1" id="u0090">• <a id="p2445"></a><em>SPegasos</em> (SVM using stochastic gradient descent)</p>
</div>
<p class="hang" id="u0095">• <a id="p2450"></a><a id="p304"></a>Instance-based learning:</p>
<div class="none">
<p class="hang1" id="u0100">• <a id="p2455"></a><em>IBk</em> (k-nearest neighbour classifier)</p>
<p class="hang1" id="u0105">• <a id="p9015"></a><em>KStar</em> (generalized distance functions)</p>
<p class="hang1" id="u0110">• <a id="p9020"></a><em>NNge</em> (rectangular generalizations)</p>
</div>
<p class="hang" id="u0115">• <a id="p2460"></a>Numeric prediction:</p>
<div class="none">
<p class="hang1" id="u0120">• <a id="p2465"></a><em>M5P</em> (model trees)</p>
<p class="hang1" id="u0125">• <a id="p2470"></a><em>M5Rules</em> (rules from model trees)</p>
<p class="hang1" id="u0130">• <a id="p2475"></a><em>LWL</em> (locally weighted learning)</p>
</div>
<p class="hang" id="u0135">• <a id="p2480"></a>Bayesian networks:</p>
<div class="none">
<p class="hang1" id="u0140">• <a id="p2485"></a><em>BayesNet</em></p>
<p class="hang1" id="u0145">• <a id="p2490"></a><em>AODE</em>, <em>WAODE</em> (averaged one-dependence estimator)</p>
</div>
<p class="hang" id="u0150">• <a id="p2495"></a>Clustering:</p>
<div class="none">
<p class="hang1" id="u0155">• <a id="p2500"></a><em>XMeans</em></p>
<p class="hang1" id="u0160">• <a id="p2505"></a><em>Cobweb</em> (includes Classit)</p>
<p class="hang1" id="u0165">• <a id="p2510"></a><em>EM</em></p>
</div>
<p class="hang" id="u0170">• <a id="p2515"></a>Multi-instance learning:</p>
<div class="none">
<p class="hang1" id="u0175">• <a id="p2520"></a><em>MISVM</em> (iterative method for learning SVM by relabeling instances)</p>
<p class="hang1" id="u0180">• <a id="p2525"></a><em>MISMO</em> (SVM with multi-instance kernel)</p>
<p class="hang1" id="u0185">• <a id="p2530"></a><em>CitationKNN</em> (nearest-neighbor method with Hausdorff distance)</p>
<p class="hang1" id="u0190">• <a id="p2535"></a><em>MILR</em> (logistic regression for multi-instance data)</p>
<p class="hang1" id="u0195">• <a id="p2540"></a><em>MIOptimalBall</em> (learning balls for multi-instance classification)</p>
<p class="hang1" id="u0200">• <a id="p2545"></a><em>MIDD</em> (the diverse-density method using the noisy-OR function)</p>
</div>
</div>
<div class="footnote">
<p class="footnote" id="fn0010"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#cc000067fn0010" class="totri-footnote"><span class="sup">1</span></a> See <a href="http://tree.bio.ed.ac.uk/software/figtree/">http://tree.bio.ed.ac.uk/software/figtree/</a> for more information.</p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">Chapter 6. Implementations</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 7. Data Transformations</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.565363425701843"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5197989885929162" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=684403fa-ce31-5518-cea7-a7f1b4fa1186&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=6.8.%20Clustering%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fhtml%2Fc0006a.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=400281"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 201.006px; left: 1081.01px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html#">Reset</a>
</div>
</div><iframe src="cid:frame-85BD6D4D7B616A5EABE949568B6DC4DA@mhtml.blink" style="display: none;"></iframe><span><div id="KampyleAnimationContainer" style="z-index: 2147483000; border: 0px; position: fixed; display: block; width: 0px; height: 0px;"></div></span></body></html>