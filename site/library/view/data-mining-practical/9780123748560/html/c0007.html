<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/data-mining-practical/9780123748560/html/c0007.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3283993"
  data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5"
  data-username="safaribooksonline113"
  data-account-type="Trial"
  
  data-activated-trial-date="08/25/2018"


  data-archive="9780123748560"
  data-publishers="Morgan Kaufmann"



  data-htmlfile-name="c0007.html"
  data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/data-mining-practical/9780123748560/html/c0007.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3283993" data-user-uuid="ff49cd60-92d4-4bee-94ce-69a00f89e9c5" data-username="safaribooksonline113" data-account-type="Trial" data-activated-trial-date="08/25/2018" data-archive="9780123748560" data-publishers="Morgan Kaufmann" data-htmlfile-name="c0007.html" data-epub-title="Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9780123748560"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>Chapter 7. Data Transformations - Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    #sbo-rt-content div{margin-left:2em;margin-right:2em;font-family:"Charis"}#sbo-rt-content div.itr{padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content .fm_title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;color:#241a26;background-color:#a6abee}#sbo-rt-content div.ded{text-align:center;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em;margin-top:4em}#sbo-rt-content .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ded .para_indented{font-size:100%;text-align:center;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr{text-align:left;font-weight:bold;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.idx{text-align:left;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.ctr .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.ctr .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.ctr .author{font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#39293b;font-weight:bold}#sbo-rt-content div.ctr .affiliation{text-align:left;font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content div.pre{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content div.pre .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content div.pre .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.chp{line-height:1.5em;margin-top:2em}#sbo-rt-content .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #0000A0;border-right:solid 8px #0000A0;margin-bottom:.25em;background-color:#a6abee;color:#0000A0}#sbo-rt-content .subtitle_document{font-weight:bold;font-size:large;text-align:center;margin-bottom:2em;margin-top:.5em;padding-top:.5em}#sbo-rt-content .subtitle{font-weight:bold;font-size:large;text-align:center;margin-bottom:1em}#sbo-rt-content a{color:#3152a9;text-decoration:none}#sbo-rt-content .affiliation{font-size:100%;font-style:italic;color:#5e4462}#sbo-rt-content .extract_fl{text-align:justify;font-size:100%;margin-bottom:1em;margin-top:2em;margin-left:1.5em;margin-right:1.5em}#sbo-rt-content .poem_title{text-align:center;font-size:110%;margin-top:1em;font-weight:bold}#sbo-rt-content .stanza{text-align:center;font-size:100%}#sbo-rt-content .poem_source{text-align:center;font-size:100%;font-style:italic}#sbo-rt-content .list_para{font-size:100%;margin-top:0;margin-bottom:.25em}#sbo-rt-content .objectset{font-size:90%;margin-bottom:1.5em;margin-top:1.5em;border-top:solid 3px #e88f1c;border-bottom:solid 1.5px #e88f1c;background-color:#f8d9b5;color:#4f2d02}#sbo-rt-content .objectset_title{margin-top:0;padding-top:5px;padding-left:5px;padding-right:5px;padding-bottom:5px;background-color:#efab5b;font-weight:bold;font-size:110%;color:#88520b;border-bottom:solid 1.5px #e88f1c}#sbo-rt-content .nomenclature{font-size:90%;margin-bottom:1.5em;padding-bottom:15px;border-top:solid 3px #644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .nomenclature_head{margin-top:0;padding-top:5px;padding-left:5px;padding-bottom:5px;background-color:#b29bca;font-weight:bold;font-size:110%;color:#644484;border-bottom:solid 1.5px #644484}#sbo-rt-content .list_def_entry{font-size:100%;margin-top:.5em;margin-bottom:.5em}#sbo-rt-content div.chp .para_fl{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0}#sbo-rt-content .scx{font-size:100%;font-weight:bold;text-align:left;margin-bottom:0;margin-top:0;padding-bottom:5px;padding-top:5px;padding-left:5px;padding-right:5px}#sbo-rt-content p{font-size:100%;text-align:justify;margin-bottom:0;margin-top:0}#sbo-rt-content div.chp .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .head1{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .head12{page-break-before:always;font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .sec_num{color:#0000A0}#sbo-rt-content .head2{font-size:115%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em;color:#ae3f58;border-bottom:solid 3px #dfd8cb}#sbo-rt-content .head3{font-size:110%;margin-bottom:.5em;margin-top:1em;text-align:left;font-weight:bold;color:#6f2c90}#sbo-rt-content .head4{font-weight:bold;font-size:105%;text-align:left;margin-bottom:.5em;margin-top:1em;color:#53a6df}#sbo-rt-content .bibliography_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .tch{text-align:center;border-bottom:solid 1px black;border-top:solid 1px black;border-right:solid 1px black;border-left:solid 1px black;margin-top:1.5em;margin-bottom:1.5em;font-weight:bold}#sbo-rt-content table{display:table;margin-bottom:1em}#sbo-rt-content tr{display:table-row}#sbo-rt-content td ul{display:table-cell}#sbo-rt-content .tb{margin-top:1.5em;margin-bottom:1.5em;vertical-align:top;padding-left:1em}#sbo-rt-content .table_source{font-size:75%;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content .figure{margin-top:1.5em;margin-bottom:1.5em;padding-right:5px;padding-left:5px;padding-bottom:5px;padding-top:5px;text-align:center}#sbo-rt-content .figure_legend{font-size:90%;margin-top:0;margin-bottom:1em;vertical-align:top;border-top:solid 1px #0000A0;line-height:1.5em}#sbo-rt-content .figure_source{font-size:75%;margin-top:.5em;margin-bottom:1em;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content .fig_num{font-size:110%;font-weight:bold;color:white;padding-right:10px;background-color:#0000A0}#sbo-rt-content .table_caption{font-size:90%;margin-top:2em;margin-bottom:.5em;vertical-align:top}#sbo-rt-content .tab_num{font-size:90%;font-weight:bold;color:#00f;padding-right:4px}#sbo-rt-content .tablecdt{font-size:75%;margin-bottom:1em;font-style:italic;color:#00f;text-align:left;padding-bottom:5px;padding-top:5px;border-bottom:solid 2px black;border-top:solid 1px black}#sbo-rt-content table.numbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content table.unnumbered{font-size:85%;border-top:solid 2px black;border-bottom:solid 2px black;margin-top:1.5em;margin-bottom:1em;background-color:#a6abee}#sbo-rt-content .ack{font-size:90%;margin-top:1.5em;margin-bottom:1.5em}#sbo-rt-content .boxg{font-size:90%;padding-left:.5em;padding-right:.5em;margin-bottom:1em;margin-top:1em;border-top:solid 1px red;border-bottom:solid 1px red;border-left:solid 1px red;border-right:solid 1px red;background-color:#ffda6b}#sbo-rt-content .box_title{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}#sbo-rt-content .box_source{padding-top:.5px;padding-left:.5px;padding-bottom:5px;padding-right:.5px;font-size:100%;color:#67582b;margin-top:.25em;margin-left:2.5em;margin-right:2.5em;font-style:italic}#sbo-rt-content .box_no{margin-top:0;padding-left:5px;padding-right:5px;font-weight:bold;font-size:100%;color:#ffda6b}#sbo-rt-content .headx{margin-top:.5em;padding-bottom:.25em;font-weight:bold;font-size:110%}#sbo-rt-content .heady{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:#00f}#sbo-rt-content .headz{margin-top:1.5em;padding-bottom:.25em;font-weight:bold;font-size:110%;color:red}#sbo-rt-content div.subdoc{font-weight:bold;font-size:large;text-align:center;color:#00f}#sbo-rt-content div.subdoc .document_number{display:block;font-size:100%;text-align:right;padding-bottom:10px;padding-top:10px;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;border-right:solid 8px #00f;margin-bottom:.25em;background-color:#2b73b0;color:#00f}#sbo-rt-content ul.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content ul.bull{list-style:disc;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.squf{list-style:square;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ul.circ{list-style:circle;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_a{list-style:lower-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_a{list-style:upper-alpha;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.upper_i{list-style:upper-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content ol.lower_i{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em}#sbo-rt-content div.chp .list_para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content .book_title_page{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;padding-left:0;padding-top:10px;background-color:#a6abee}#sbo-rt-content p.pagebreak{page-break-before:always}#sbo-rt-content .book_series_editor{margin-top:.5em;margin-left:.5em;margin-right:.5em;border-top:solid 6px #55390e;border-left:solid 6px #55390e;border-right:solid 6px #55390e;border-bottom:solid 6px #55390e;padding-left:5px;padding-right:5px;padding-top:10px;background-color:#a6abee}#sbo-rt-content .book_title{text-align:center;font-weight:bold;font-size:250%;color:#55390e;margin-top:70px}#sbo-rt-content .book_subtitle{text-align:center;margin-top:.25em;font-weight:bold;font-size:150%;color:#00f;border-top:solid 1px #55390e;border-bottom:solid 1px #55390e;padding-bottom:7px}#sbo-rt-content .edition{text-align:right;margin-top:1.5em;margin-right:5em;font-weight:bold;font-size:90%;color:red}#sbo-rt-content .author_group{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .editors{text-align:left;font-weight:bold;font-size:100%;color:#241a26}#sbo-rt-content .title_author{text-align:center;font-weight:bold;font-size:80%;color:#241a26}#sbo-rt-content .title_affiliation{text-align:center;font-size:80%;color:#241a26;margin-bottom:.5em;font-style:italic}#sbo-rt-content .publisher{text-align:center;font-size:100%;margin-bottom:.5em;padding-left:10px;padding-right:10px;padding-top:10px;padding-bottom:10px;background-color:#55390e;color:#a6abee}#sbo-rt-content div.qa h1.head1{font-size:110%;font-weight:bold;margin-bottom:1em;margin-top:1em;color:#6883b5;border-bottom:solid 8px #fee7ca}#sbo-rt-content div.outline{border-left:2px solid #007ec6;border-right:2px solid #007ec6;border-bottom:2px solid #007ec6;border-top:26px solid #007ec6;padding:3px;margin-bottom:1em}#sbo-rt-content div.outline .list_head{background-color:#007ec6;color:white;padding:.2em 1em .2em;margin:-.4em -.3em -.4em -.3em;margin-bottom:.5em;font-size:medium;font-weight:bold;margin-top:-1.5em}#sbo-rt-content div.fm .author{text-align:center;margin-bottom:1.5em;color:#39293b}#sbo-rt-content td p{text-align:left}#sbo-rt-content div.htu .para_indented{font-size:100%;text-align:justify;margin-bottom:.5em;margin-top:0;text-indent:1em}#sbo-rt-content div.htu .para_fl{font-size:100%;margin-bottom:.5em;margin-top:0;text-align:justify}#sbo-rt-content .headx{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content div.book_section{text-align:center;margin-top:8em}#sbo-rt-content div.book_part{text-align:center;margin-top:6em}#sbo-rt-content p.section_label{display:block;font-size:200%;text-align:center;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.section_title{display:block;font-size:200%;text-align:center;padding-right:10px;margin-bottom:2em;border-top:solid 2px #00f;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_label{display:block;font-size:250%;text-align:center;margin-top:6em;padding-right:10px;font-weight:bold;border-top:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content p.part_title{display:block;font-size:250%;text-align:center;padding-right:10px;margin-bottom:2em;font-weight:bold;border-bottom:solid 2px #00f;background-color:#a6abee;color:#00f}#sbo-rt-content div.idx li{margin-top:-.3em}#sbo-rt-content p.ueqn{text-align:center}#sbo-rt-content p.eqn{text-align:center}#sbo-rt-content p.extract_indented{margin-left:3em;margin-right:3em;margin-bottom:.5em;text-indent:1em}#sbo-rt-content td p.para_fl{font-size:90%;margin-bottom:.5em;margin-top:0;text-align:left}#sbo-rt-content .small{font-size:small}#sbo-rt-content div.abs{font-size:90%;margin-bottom:2em;margin-top:2em;margin-left:1em;margin-right:1em}#sbo-rt-content p.abstract_title{font-size:110%;margin-bottom:1em;font-weight:bold}#sbo-rt-content sup{vertical-align:4px}#sbo-rt-content sub{vertical-align:-2px}#sbo-rt-content img{max-width:100%;max-height:100%}#sbo-rt-content p.toc1{margin-left:1em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc2{margin-left:2em;margin-bottom:.5em;font-weight:bold;text-align:left}#sbo-rt-content p.toc3{margin-left:3em;margin-bottom:.5em;text-align:left}#sbo-rt-content .head5{font-weight:bold;font-size:100%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content img.inline{vertical-align:middle}#sbo-rt-content .head6{font-weight:bold;font-size:90%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .underline{text-decoration:underline}#sbo-rt-content .center{text-align:center}#sbo-rt-content span.big{font-size:2em}#sbo-rt-content p.para_indented{text-indent:2em}#sbo-rt-content p.para_indented1{text-indent:0;page-break-before:always}#sbo-rt-content p.right{text-align:right}#sbo-rt-content p.endnote{margin-left:1em;margin-right:1em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content div.block{margin-left:3em;margin-bottom:.5em;text-indent:-1em}#sbo-rt-content p.bl_para{font-size:100%;text-indent:0;text-align:justify}#sbo-rt-content div.poem{text-align:center;font-size:100%}#sbo-rt-content .acknowledge_head{font-size:150%;margin-bottom:.25em;font-weight:bold}#sbo-rt-content .intro{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content .exam{font-size:90%;margin-top:1em;margin-bottom:1em}#sbo-rt-content div.exam_head{font-size:130%;margin-top:1.5em;margin-bottom:1em;font-weight:bold}#sbo-rt-content p.table_footnotes{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content div.table_foot{font-size:80%;margin-top:.5em;margin-bottom:1em;text-indent:.01em}#sbo-rt-content p.table_legend{font-size:80%;margin-top:.5em;margin-bottom:.5em;vertical-align:top;text-indent:.01em}#sbo-rt-content .bib_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .ref_entry{font-size:90%;text-align:left;margin-left:20px;margin-bottom:.25em;margin-top:0;text-indent:-20px}#sbo-rt-content div.ind1{margin-left:.1em;margin-top:.5em}#sbo-rt-content div.ind2{margin-left:1em}#sbo-rt-content div.ind3{margin-left:1.5em}#sbo-rt-content div.ind4{margin-left:2em}#sbo-rt-content div.ind5{margin-left:2.5em}#sbo-rt-content div.ind6{margin-left:3em}#sbo-rt-content .title_document{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .sc1{margin-left:0}#sbo-rt-content .sc2{margin-left:0}#sbo-rt-content .sc3{margin-left:0}#sbo-rt-content .sc4{margin-left:0}#sbo-rt-content .sc5{margin-left:0}#sbo-rt-content .sc6{margin-left:0}#sbo-rt-content .sc7{margin-left:0}#sbo-rt-content .sc8{margin-left:0}#sbo-rt-content .sc9{margin-left:0}#sbo-rt-content .sc10{margin-left:0}#sbo-rt-content .sc11{margin-left:0}#sbo-rt-content .sc12{margin-left:0}#sbo-rt-content .sc13{margin-left:0}#sbo-rt-content .sc14{margin-left:0}#sbo-rt-content .sc15{margin-left:0}#sbo-rt-content .sc16{margin-left:0}#sbo-rt-content .sc17{margin-left:0}#sbo-rt-content .sc18{margin-left:0}#sbo-rt-content .sc19{margin-left:0}#sbo-rt-content .sc20{margin-left:0}#sbo-rt-content .sc0{margin-left:0}#sbo-rt-content .source{font-size:11px;margin-top:.5em;margin-bottom:0;font-style:italic;color:#0000A0;text-align:left}#sbo-rt-content div.footnote{font-size:small;border-style:solid;border-width:1px 0 0 0;margin-top:2em;margin-bottom:1em}#sbo-rt-content table.numbered{font-size:small}#sbo-rt-content div.hang{margin-left:.3em;margin-top:1em;text-align:left}#sbo-rt-content div.p_hang{margin-left:1.5em;text-align:left}#sbo-rt-content div.p_hang1{margin-left:2em;text-align:left}#sbo-rt-content div.p_hang2{margin-left:2.5em;text-align:left}#sbo-rt-content div.p_hang3{margin-left:3em;text-align:left}#sbo-rt-content .bibliography{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .biblio_sec{font-size:90%;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .gls{text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .glossary_sec{font-size:90%;font-style:italic;text-align:left;margin-bottom:1em;margin-top:1em}#sbo-rt-content .head7{font-weight:bold;font-size:86%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .head8{font-weight:bold;font-style:italic;font-size:81%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .box_subtitle{padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;font-weight:bold;font-size:105%;margin-top:.25em}#sbo-rt-content div.for{text-align:left;font-size:medium;padding-top:.2em;padding-bottom:.25em;padding-left:.2em;padding-right:.2em}#sbo-rt-content span.strike{text-decoration:line-through}#sbo-rt-content .head9{font-style:italic;font-size:80%;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .bib_note{font-size:85%;text-align:left;margin-left:25px;margin-bottom:.25em;margin-top:0;text-indent:-30px}#sbo-rt-content .glossary_title{font-size:110%;font-weight:bold;text-align:left;margin-bottom:1em;margin-top:1em;color:#005aaa;border-bottom:solid 3px #7f9fd3}#sbo-rt-content .collaboration{padding-left:10px;padding-right:10px;padding-top:2px;padding-bottom:2px;background-color:#ffac29;margin-left:70px;margin-right:70px;margin-top:20px;margin-bottom:20px}#sbo-rt-content .title_collab{text-align:center;font-weight:bold;font-size:85%;color:#241a26}#sbo-rt-content .head0{font-size:105%;font-weight:bold;text-align:left;margin-bottom:.5em;margin-top:1em}#sbo-rt-content .copyright{font-size:80%;text-align:left;margin-bottom:0;margin-top:0;text-indent:0}#sbo-rt-content .copyright-top{font-size:80%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0}#sbo-rt-content .idxtitle{font-size:x-large;text-align:center;font-weight:bold;line-height:1.5em;margin-top:2em;margin-bottom:2em;color:#0000A0}#sbo-rt-content .idxletter{font-size:100%;text-align:left;margin-bottom:0;margin-top:1em;text-indent:0;font-weight:bold}#sbo-rt-content h1.chaptertitle{margin-top:.5em;margin-bottom:1em;font-size:200%;font-weight:bold;text-indent:0;line-height:1em}#sbo-rt-content h1.chapterlabel{margin-top:0;margin-bottom:0;font-size:150%;font-weight:bold;text-indent:0}#sbo-rt-content p.noindent{text-align:justify;text-indent:0;margin-top:10px;margin-bottom:2px}#sbo-rt-content span.monospace{font-family:consolas,courier,monospace;margin-top:0;margin-bottom:0;white-space:pre;font-size:85%}#sbo-rt-content p.hang{text-indent:-1.1em;margin-left:1em}#sbo-rt-content p.hang1{text-indent:-1.1em;margin-left:2em}#sbo-rt-content p.hang2{text-indent:-1.6em;margin-left:2em}#sbo-rt-content p.hang3{text-indent:-1.1em;margin-left:3em}#sbo-rt-content p.hang4{text-indent:-1.6em;margin-left:4.3em}#sbo-rt-content p.hang5{text-indent:-1.1em;margin-left:5em}#sbo-rt-content p.none{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:3.2em;margin-top:2px;margin-bottom:2px}#sbo-rt-content p.blockquote{font-size:90%;margin-left:2.5em;margin-right:2em;margin-top:1em;margin-bottom:1em;line-height:1.3em}#sbo-rt-content p.none1{text-align:justify;display:list-item;list-style-type:none;text-indent:-3.2em;margin-left:2.8em;margin-top:2px;margin-bottom:2px}#sbo-rt-content .listparasub1{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:3.2em}#sbo-rt-content .listparasub2{list-style:lower-roman;margin-top:.25em;margin-bottom:.25em;margin-left:5.5em;text-indent:-3.2em}#sbo-rt-content div.none{list-style:none;margin-top:.25em;margin-bottom:1em}#sbo-rt-content div.bm{line-height:1.5em;margin-top:2em}#sbo-rt-content div.fm{line-height:1.5em;margin-top:2em}#sbo-rt-content span.sup{vertical-align:4px;font-size:75%}#sbo-rt-content span.sub{font-size:75%;vertical-align:-2px}#sbo-rt-content .box_titleC{text-align:center;padding-top:5px;padding-left:5px;padding-bottom:5px;padding-right:5px;background-color:#67582b;font-weight:bold;font-size:110%;color:white;margin-top:.25em}
    </style><link rel="canonical" href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html"><meta name="description" content=" CHAPTER 7 Data Transformations In Chapter 6 we examined a vast array of machine learning methods: decision trees, classification and association rules, linear models, instance-based schemes, numeric prediction techniques, Bayesian ... "><meta property="og:title" content="Chapter 7. Data Transformations"><meta itemprop="isPartOf" content="/library/view/data-mining-practical/9780123748560/"><meta itemprop="name" content="Chapter 7. Data Transformations"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0007.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9780123748560/"><meta property="og:description" itemprop="description" content=" CHAPTER 7 Data Transformations In Chapter 6 we examined a vast array of machine learning methods: decision trees, classification and association rules, linear models, instance-based schemes, numeric prediction techniques, Bayesian ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="Morgan Kaufmann"><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9780080890364"><meta property="og:book:author" itemprop="author" content="Mark A. Hall"><meta property="og:book:author" itemprop="author" content="Eibe Frank"><meta property="og:book:author" itemprop="author" content="Ian H. Witten"><meta property="og:book:tag" itemprop="about" content="Big Data"><meta property="og:book:tag" itemprop="about" content="Databases"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/ff49cd60-92d4-4bee-94ce-69a00f89e9c5/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.safaribooksonline.com/public/support" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.safaribooksonline.com/public/support" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Data Mining: Practical Machine Learning Tools and Techniques, 3rd Edition
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9780123748560/chapter/html/c0007.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0007.html&amp;text=Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0007.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0007.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%20Chapter%207.%20Data%20Transformations&amp;body=https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/html/c0007.html%0D%0Afrom%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques%2C%203rd%20Edition%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">6.8. Clustering</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 8. Ensemble Learning</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><div class="chp"><a id="c0007"></a><h1 class="chapterlabel" id="c0007tit1">
<strong>CHAPTER</strong> 7</h1>
<h1 class="chaptertitle" id="c0007tit">Data Transformations</h1>
<p id="p0010" class="noindent"><a id="p305"></a>In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#c0006">Chapter 6</a> we examined a vast array of machine learning methods: decision trees, classification and association rules, linear models, instance-based schemes, numeric prediction techniques, Bayesian networks, clustering algorithms, and semisupervised and multi-instance learning. All are sound, robust techniques that are eminently applicable to practical data mining problems.</p>
<p id="p0015" class="para_indented">But successful data mining involves far more than selecting a learning algorithm and running it over your data. For one thing, many learning schemes have various parameters, and suitable values must be chosen for these. In most cases, results can be improved markedly by a suitable choice of parameter values, and the appropriate choice depends on the data at hand. For example, decision trees can be pruned or unpruned, and in the former case a pruning parameter may have to be chosen. In the <em>k</em>-nearest-neighbor method of instance-based learning, a value for <em>k</em> will have to be chosen. More generally, the learning scheme itself will have to be chosen from the range of schemes that are available. In all cases, the right choices depend on the data itself.</p>
<p id="p0020" class="para_indented">It is tempting to try out several learning schemes and several parameter values on your data, and see which works best. But be careful! The best choice is not necessarily the one that performs best on the training data. We have repeatedly cautioned about the problem of overfitting, where a learned model is too closely tied to the particular training data from which it was built. It is incorrect to assume that performance on the training data faithfully represents the level of performance that can be expected on the fresh data to which the learned model will be applied in practice.</p>
<p id="p0025" class="para_indented">Fortunately, we have already encountered the solution to this problem in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#c0005">Chapter 5</a>. There are two good methods for estimating the expected true performance of a learning scheme: the use of a large dataset that is quite separate from the training data, in the case of plentiful data, and cross-validation (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0020">Section 5.3</a>), if data is scarce. In the latter case, a single tenfold cross-validation is typically used in practice, although to obtain a more reliable estimate the entire procedure should be repeated 10 times. Once suitable parameters have been chosen for the learning scheme, use the whole training set—all the available training instances—to produce the final learned model that is to be applied to fresh data.</p>
<p id="p0030" class="para_indented"><a id="p306"></a>Note that the performance obtained with the chosen parameter value during the tuning process is <em>not</em> a reliable estimate of the final model’s performance, because the final model potentially overfits the data that was used for tuning. To ascertain how well it will perform, you need yet another large dataset that is quite separate from any data used during learning and tuning. The same is true for cross-validation: You need an “inner” cross-validation for parameter tuning and an “outer” cross-validation for error estimation. With tenfold cross-validation, this involves running the learning scheme 100 times. To summarize: When assessing the performance of a learning scheme, any parameter tuning that goes on should be treated as though it were an integral part of the training process.</p>
<p id="p0035" class="para_indented">There are other important processes that can materially improve success when applying machine learning techniques to practical data mining problems, and these are the subject of this chapter. They constitute a kind of data engineering—engineering the input data into a form suitable for the learning scheme chosen and engineering the output to make it more effective. You can look on them as a bag of tricks that you can apply to practical data mining problems to enhance the chance of success. Sometimes they work; other times they don’t—and at the present state of the art, it’s hard to say in advance whether they will or not. In an area such as this, where trial and error is the most reliable guide, it is particularly important to be resourceful and have an understanding of what the tricks are.</p>
<p id="p0040" class="para_indented">In this chapter we examine six different ways in which the input can be massaged to make it more amenable for learning methods: attribute selection, attribute discretization, data projections, sampling, data cleansing, and converting multiclass problems to two-class ones. Consider the first, attribute selection. In many practical situations there are far too many attributes for learning schemes to handle, and some of them—perhaps the overwhelming majority—are clearly irrelevant or redundant. Consequently, the data must be preprocessed to select a subset of the attributes to use in learning. Of course, many learning schemes themselves try to select attributes appropriately and ignore irrelevant or redundant ones, but in practice their performance can frequently be improved by preselection. For example, experiments show that adding useless attributes causes the performance of learning schemes such as decision trees and rules, linear regression, instance-based learners, and clustering methods to deteriorate.</p>
<p id="p0045" class="para_indented">Discretization of numeric attributes is absolutely essential if the task involves numeric attributes but the chosen learning scheme can only handle categorical ones. Even schemes that can handle numeric attributes often produce better results, or work faster, if the attributes are prediscretized. The converse situation, in which categorical attributes must be represented numerically, also occurs (although less often), and we describe techniques for this case, too.</p>
<p id="p0050" class="para_indented">Data projection covers a variety of techniques. One transformation, which we have encountered before when looking at relational data in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#c0002">Chapter 2</a> and support vector machines in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#c0006">Chapter 6</a>, is to add new, synthetic attributes whose purpose is to present existing information in a form that is suitable for the machine learning scheme to pick up on. More general techniques that do not depend so intimately <a id="p307"></a>on the semantics of the particular data mining problem at hand include principal components analysis and random projections. We also cover partial least-squares regression as a data projection technique for regression problems.</p>
<p id="p0055" class="para_indented">Sampling the input is an important step in many practical data mining applications, and is often the only way in which really large-scale problems can be handled. Although it is fairly simple, we include a brief section on techniques of sampling, including a way of incrementally producing a random sample of a given size when the total size of the dataset is not known in advance.</p>
<p id="p0060" class="para_indented">Unclean data plagues data mining. We emphasized in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#c0002">Chapter 2</a> the necessity of getting to know your data: understanding the meaning of all the different attributes, the conventions used in coding them, the significance of missing values and duplicate data, measurement noise, typographical errors, and the presence of systematic errors—even deliberate ones. Various simple visualizations often help with this task. There are also automatic methods of cleansing data, of detecting outliers, and of spotting anomalies, which we describe, including a class of techniques referred to as <em>one-class learning</em> in which only a single class of instances is available at training time.</p>
<p id="p0065" class="para_indented">Finally, we examine techniques for refining the output of learning schemes that estimate class probabilities by recalibrating the estimates that they make. This is primarily of importance when accurate probabilities are required, as in cost-sensitive classification, though it can also improve classification performance.</p>
<div id="s0010">
<h2 id="st0010">7.1 Attribute selection</h2>
<p id="p0070" class="noindent">Most machine learning algorithms are designed to learn which are the most appropriate attributes to use for making their decisions. For example, decision tree methods choose the most promising attribute to split on at each point and should—in theory—never select irrelevant or unhelpful attributes. Having more features should surely—in theory—result in more discriminating power, never less. “What’s the difference between theory and practice?” an old question asks. The answer goes, “There is no difference between theory and practice—in theory. But in practice, there is.” Here there is too: In practice, adding irrelevant or distracting attributes to a dataset often confuses machine learning systems.</p>
<p id="p0075" class="para_indented">Experiments with a decision tree learner (C4.5) have shown that adding to standard datasets a random binary attribute generated by tossing an unbiased coin impacts classification performance, causing it to deteriorate (typically by 5 to 10% in the situations tested). This happens because at some point in the trees that are learned, the irrelevant attribute is invariably chosen to branch on, causing random errors when test data is processed. How can this be when decision tree learners are cleverly designed to choose the best attribute for splitting at each node? The reason is subtle. As you proceed further down the tree, less and less data is available to help make the selection decision. At some point, with little data, the random attribute will look good just by chance. Because the number of nodes at each level increases <a id="p308"></a>exponentially with depth, the chance of the rogue attribute looking good somewhere along the frontier multiplies up as the tree deepens. The real problem is that you inevitably reach depths at which only a small amount of data is available for attribute selection. If the dataset were bigger it wouldn’t necessarily help—you’d probably just go deeper.</p>
<p id="p0080" class="para_indented">Divide-and-conquer tree learners and separate-and-conquer rule learners both suffer from this effect because they inexorably reduce the amount of data on which they base judgments. Instance-based learners are very susceptible to irrelevant attributes because they always work in local neighborhoods, taking just a few training instances into account for each decision. Indeed, it has been shown that the number of training instances needed to produce a predetermined level of performance for instance-based learning increases exponentially with the number of irrelevant attributes present. Naïve Bayes, by contrast, does not fragment the instance space and robustly ignores irrelevant attributes. It assumes by design that all attributes are conditionally independent of one another, an assumption that is just right for random “distracter” attributes. But through this very same assumption, Naïve Bayes pays a heavy price in other ways because its operation is damaged by adding redundant attributes.</p>
<p id="p0085" class="para_indented">The fact that irrelevant distracters degrade the performance of state-of-the-art decision tree and rule learners is, at first, surprising. Even more surprising is that <em>relevant</em> attributes can also be harmful. For example, suppose that in a two-class dataset a new attribute was added that had the same value as the class to be predicted most of the time (65%) and the opposite value the rest of the time, randomly distributed among the instances. Experiments with standard datasets have shown that this can cause classification accuracy to deteriorate (by 1 to 5% in the situations tested). The problem is that the new attribute is (naturally) chosen for splitting high up in the tree. This has the effect of fragmenting the set of instances available at the nodes below so that other choices are based on sparser data.</p>
<p id="p0090" class="para_indented">Because of the negative effect of irrelevant attributes on most machine learning schemes, it is common to precede learning with an attribute selection stage that strives to eliminate all but the most relevant attributes. The best way to select relevant attributes is manually, based on a deep understanding of the learning problem and what the attributes actually mean. However, automatic methods can also be useful. Reducing the dimensionality of the data by deleting unsuitable attributes improves the performance of learning algorithms. It also speeds them up, although this may be outweighed by the computation involved in attribute selection. More important, dimensionality reduction yields a more compact, more easily interpretable representation of the target concept, focusing the user’s attention on the most relevant variables.</p>
<div id="s0015">
<h3 id="st0015">Scheme-Independent Selection</h3>
<p id="p0095" class="noindent">When selecting a good attribute subset, there are two fundamentally different approaches. One is to make an independent assessment based on general characteristics of the data; the other is to evaluate the subset using the machine learning algorithm <a id="p309"></a>that will ultimately be employed for learning. The first is called the <em>filter</em> method because the attribute set is filtered to produce the most promising subset before learning commences. The second is called the <em>wrapper</em> method because the learning algorithm is wrapped into the selection procedure. Making an independent assessment of an attribute subset would be easy if there were a good way of determining when an attribute was relevant to choosing the class. However, there is no universally accepted measure of relevance, although several different ones have been proposed.</p>
<p id="p0100" class="para_indented">One simple scheme-independent method of attribute selection is to use just enough attributes to divide up the instance space in a way that separates all the training instances. For example, if just one or two attributes are used, there will generally be several instances that have the same combination of attribute values. At the other extreme, the full set of attributes will likely distinguish the instances uniquely so that no two instances have the same values for all attributes. (This will not necessarily be the case, however; datasets sometimes contain instances with the same attribute values but different classes.) It makes intuitive sense to select the smallest attribute subset that serves to distinguish all instances uniquely. This can easily be found using an exhaustive search, although at considerable computational expense. Unfortunately, this strong bias toward consistency of the attribute set on the training data is statistically unwarranted and can lead to overfitting—the algorithm may go to unnecessary lengths to repair an inconsistency that was in fact merely caused by noise.</p>
<p id="p0105" class="para_indented">Machine learning algorithms can be used for attribute selection. For instance, you might first apply a decision tree algorithm to the full dataset and then select only those attributes that are actually used in the tree. While this selection would have no effect at all if the second stage merely built another tree, it will have an effect on a different learning algorithm. For example, the nearest-neighbor algorithm is notoriously susceptible to irrelevant attributes, and its performance can be improved by using a decision tree builder as a filter for attribute selection first. The resulting nearest-neighbor scheme can also perform better than the decision tree algorithm used for filtering.</p>
<p id="p9000" class="para_indented">As another example, the simple 1R scheme described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#c0004">Chapter 4</a> has been used to select the attributes for a decision tree learner by evaluating the effect of branching on different attributes (although an error-based method such as 1R may not be the optimal choice for ranking attributes, as we will see later when covering the related problem of supervised discretization). Often the decision tree performs just as well when only the two or three top attributes are used for its construction—and it is much easier to understand. In this approach, the user determines how many attributes to use for building the decision tree.</p>
<p id="p0110" class="para_indented">Another possibility is to use an algorithm that builds a linear model—for example, a linear support vector machine—and ranks the attributes based on the size of the coefficients. A more sophisticated variant applies the learning algorithm repeatedly. It builds a model, ranks the attributes based on the coefficients, removes the lowest-ranked one, and repeats the process until all attributes have been removed. This method of <em>recursive feature elimination</em> has been found to yield better results on certain datasets (e.g., when identifying important genes for cancer classification) than simply ranking attributes based on a single model. With both <a id="p310"></a>methods it is important to ensure that the attributes are measured on the same scale; otherwise, the coefficients are not comparable. Note that these techniques just produce a ranking; another method must be used to determine the appropriate number of attributes to use.</p>
<p id="p0115" class="para_indented">Attributes can be selected using instance-based learning methods too. You could sample instances randomly from the training set and check neighboring records of the same and different classes—“near hits” and “near misses.” If a near hit has a different value for a certain attribute, that attribute appears to be irrelevant and its weight should be decreased. On the other hand, if a near miss has a different value, the attribute appears to be relevant and its weight should be increased. Of course, this is the standard kind of procedure used for attribute weighting for instance-based learning, described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0180">Section 6.5</a>. After repeating this operation many times, selection takes place: Only attributes with positive weights are chosen. As in the standard incremental formulation of instance-based learning, different results will be obtained each time the process is repeated, because of the different ordering of examples. This can be avoided by using all training instances and taking into account all near hits and near misses of each.</p>
<p id="p0120" class="para_indented">A more serious disadvantage is that the method will not detect an attribute that is redundant because it is correlated with another attribute. In the extreme case, two identical attributes would be treated in the same way, either both selected or both rejected. A modification has been suggested that appears to go some way toward addressing this issue by taking the current attribute weights into account when computing the nearest hits and misses.</p><a id="p0125"></a><div class="boxg" id="b0010">
<p id="p0130" class="noindent">Another way of eliminating redundant attributes as well as irrelevant ones is to select a subset of attributes that individually correlate well with the class but have little intercorrelation. The correlation between two nominal attributes <em>A</em> and <em>B</em> can be measured using the <em>symmetric uncertainty</em>:</p>
<p class="figure" id="e0010"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si1.jpg" alt="image" width="244" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si1.jpg"></p>
<p>where <em>H</em> is the entropy function described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0045">Section 4.3</a>. The entropies are based on the probability associated with each attribute value; <em>H</em>(<em>A</em>, <em>B</em>), the joint entropy of <em>A</em> and <em>B</em>, is calculated from the joint probabilities of all combinations of values of <em>A</em> and <em>B</em>.</p>
<p id="p0135" class="para_indented">The symmetric uncertainty always lies between 0 and 1. Correlation-based feature selection determines the goodness of a set of attributes using</p>
<p class="figure" id="e0015"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si2.jpg" alt="image" width="213" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si2.jpg"></p>
<p>where <em>C</em> is the class attribute and the indices <em>i</em> and <em>j</em> range over all attributes in the set. If all <em>m</em> attributes in the subset correlate perfectly with the class and with one another, the numerator becomes <em>m</em> and the denominator<img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079if007-001-9780123748560.jpg" alt="image" width="35" height="25" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079if007-001-9780123748560.jpg">, which is also <em>m</em>. Thus, the measure is 1, which turns out to be the maximum value it can attain (the minimum is 0). Clearly, this is not ideal, because we want to avoid redundant attributes. However, any subset of this set will also have value 1. When using this criterion to search for a good subset of attributes, it makes sense to break ties in favor of the smallest subset.</p>
</div>
<p></p>
</div>
<div id="s0020">
<h3 id="st0020"><a id="p311"></a>Searching the Attribute Space</h3>
<p id="p0140" class="noindent">Most methods for attribute selection involve searching the space of attributes for the subset that is most likely to predict the class best. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0010">Figure 7.1</a> illustrates the attribute space for the—by now all-too-familiar—weather dataset. The number of possible attribute subsets increases exponentially with the number of attributes, making an exhaustive search impractical on all but the simplest problems.</p>
<p id="f0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-001-9780123748560.jpg" alt="image" width="514" height="385" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-001-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.1</span> Attribute space for the weather dataset.</p>
<p id="p0145" class="para_indented">Typically, the space is searched greedily in one of two directions: top to bottom and bottom to top in the figure. At each stage, a local change is made to the current attribute subset by either adding or deleting a single attribute. The downward direction, where you start with no attributes and add them one at a time, is called <em>forward selection</em>. The upward one, where you start with the full set and delete attributes one at a time, is <em>backward elimination</em>.</p>
<p id="p0150" class="para_indented">In forward selection, each attribute that is not already in the current subset is tentatively added to it, and the resulting set of attributes is evaluated—using, for example, cross-validation, as described in the following section. This evaluation produces a numeric measure of the expected performance of the subset. The effect of adding each attribute in turn is quantified by this measure, the best one is chosen, <a id="p312"></a>and the procedure continues. However, if no attribute produces an improvement when added to the current subset, the search ends. This is a standard greedy search procedure and guarantees to find a locally—but not necessarily globally—optimal set of attributes.</p>
<p id="p0155" class="para_indented">Backward elimination operates in an entirely analogous fashion. In both cases a slight bias is often introduced toward smaller attribute sets. This can be done for forward selection by insisting that if the search is to continue, the evaluation measure must not only increase, but must increase by at least a small predetermined quantity. A similar modification works for backward elimination.</p>
<p id="p0160" class="para_indented">More sophisticated search schemes exist. Forward selection and backward elimination can be combined into a bidirectional search; again, one can begin either with all the attributes or with none of them. Best-first search is a method that does not just terminate when the performance starts to drop but keeps a list of all attribute subsets evaluated so far, sorted in order of the performance measure, so that it can revisit an earlier configuration instead. Given enough time it will explore the entire space, unless this is prevented by some kind of stopping criterion. Beam search is similar but truncates its list of attribute subsets at each stage so that it only contains a fixed number—the beam width—of most promising candidates. Genetic algorithm search procedures are loosely based on the principle of natural selection: They “evolve” good feature subsets by using random perturbations of a current list of candidate subsets and combining them based on performance.</p>
</div>
<div id="s0025">
<h3 id="st0025">Scheme-Specific Selection</h3>
<p id="p0165" class="noindent">The performance of an attribute subset with scheme-specific selection is measured in terms of the learning scheme’s classification performance using just those attributes. Given a subset of attributes, accuracy is estimated using the normal procedure of cross-validation described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0020">Section 5.3</a>. Of course, other evaluation methods such as performance on a holdout set (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0020">Section 5.3</a>) or the bootstrap estimator (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0025">Section 5.4</a>) could be equally well used.</p>
<p id="p0170" class="para_indented">The entire attribute selection process is rather computation intensive. If each evaluation involves a tenfold cross-validation, the learning procedure must be executed 10 times. With <em>m</em> attributes, the heuristic forward selection or backward elimination multiplies evaluation time by a factor proportional to <em>m</em><sup>2</sup> in the worst case. For more sophisticated searches, the penalty will be far greater, up to 2<em>
<sup>m</sup>
</em> for an exhaustive algorithm that examines each of the 2<em>
<sup>m</sup>
</em> possible subsets.</p>
<p id="p0175" class="para_indented">Good results have been demonstrated on many datasets. In general terms, backward elimination produces larger attribute sets than forward selection but better classification accuracy in some cases. The reason is that the performance measure is only an estimate, and a single optimistic estimate will cause both of these search procedures to halt prematurely—backward elimination with too many attributes and forward selection with not enough. But forward selection is useful if the focus is on understanding the decision structures involved, because it often reduces the number of attributes with only a small effect on classification accuracy. Experience seems <a id="p313"></a>to show that more sophisticated search techniques are not generally justified, although they can produce much better results in certain cases.</p><a id="p0180"></a><div class="boxg" id="b0015">
<p id="p0185" class="noindent">One way to accelerate the search process is to stop evaluating a subset of attributes as soon as it becomes apparent that it is unlikely to lead to higher accuracy than another candidate subset. This is a job for a paired statistical significance test, performed between the classifier based on this subset and all the other candidate classifiers based on other subsets. The performance difference between two classifiers on a particular test instance can be −1, 0, or 1 depending on, respectively, whether the first classifier is worse than, the same as, or better than the second on that instance. A paired <em>t</em>-test (described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0040">Section 5.5</a>) can be applied to these figures over the entire test set, effectively treating the results for each instance as an independent estimate of the difference in performance. Then the cross-validation for a classifier can be prematurely terminated as soon as it turns out to be significantly worse than another, which, of course, may never happen. We might want to discard classifiers more aggressively by modifying the <em>t</em>-test to compute the probability that one classifier is better than another classifier by at least a small user-specified threshold. If this probability becomes very small, we can discard the former classifier on the basis that it is very unlikely to perform substantially better than the latter.</p>
<p id="p0190" class="para_indented">This methodology is called <em>race search</em> and can be implemented with different underlying search strategies. When it is used with forward selection, we race all possible single-attribute additions simultaneously and drop those that do not perform well enough. In backward elimination, we race all single-attribute deletions. <em>Schemata search</em> is a more complicated method specifically designed for racing; it runs an iterative series of races that each determine whether or not a particular attribute should be included. The other attributes for this race are included or excluded randomly at each point in the evaluation. As soon as one race has a clear winner, the next iteration of races begins, using the winner as the starting point. Another search strategy is to rank the attributes first using, for example, their information gain (assuming they are discrete), and then race the ranking. In this case the race includes no attributes, the top-ranked attribute, the top two attributes, the top three, and so on.</p>
</div>
<p></p>
<p id="p0195" class="para_indented">A simple method for accelerating a scheme-specific search is to preselect a given number of attributes by ranking them first using a criterion like the information gain and discarding the rest before applying scheme-specific selection. This has been found to work surprisingly well on high-dimensional datasets such as gene expression and text categorization data, where only a couple of hundred of attributes are used instead of several thousands. In the case of forward selection, a slightly more sophisticated variant is to restrict the number of attributes available for expanding the current attribute subset to a fixed-sized subset chosen from the ranked list of attributes—creating a sliding window of attribute choices—rather than making all (unused) attributes available for consideration in each step of the search process.</p>
<p id="p0200" class="para_indented">Whatever way you do it, scheme-specific attribute selection by no means yields a uniform improvement in performance. Because of the complexity of the process, which is greatly increased by the feedback effect of including a target machine learning algorithm in the attribution selection loop, it is quite hard to predict the conditions under which it will turn out to be worthwhile. As in many machine <a id="p314"></a>learning situations, trial and error using your own particular source of data is the final arbiter.</p>
<p id="p0205" class="para_indented">There is one type of classifier for which scheme-specific attribute selection is an essential part of the learning process: the decision table. As mentioned in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0003.html#s0010">Section 3.1</a>, the entire problem of learning decision tables consists of selecting the right attributes to be included. Usually this is done by measuring the table’s cross-validation performance for different subsets of attributes and choosing the best-performing subset. Fortunately, leave-one-out cross-validation is very cheap for this kind of classifier. Obtaining the cross-validation error from a decision table derived from the training data is just a matter of manipulating the class counts associated with each of the table’s entries, because the table’s structure doesn’t change when instances are added or deleted. The attribute space is generally searched by best-first search because this strategy is less likely to get stuck in a local maximum than others, such as forward selection.</p>
<p id="p0210" class="para_indented">Let’s end our discussion with a success story. Naïve Bayes is a learning method for which a simple scheme-specific attribute selection approach has shown good results. Although this method deals well with random attributes, it has the potential to be misled when there are dependencies among attributes, and particularly when redundant ones are added. However, good results have been reported using the forward selection algorithm—which is better able to detect when a redundant attribute is about to be added than the backward elimination approach—in conjunction with a very simple, almost “naïve,” metric that determines the quality of an attribute subset to be simply the performance of the learned algorithm on the <em>training</em> set. As was emphasized in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#c0005">Chapter 5</a>, training set performance is certainly not a reliable indicator of test set performance. Nevertheless, experiments show that this simple modification to Naïve Bayes markedly improves its performance on those standard datasets for which it does not do so well as tree- or rule-based classifiers, and does not have any negative effect on results on datasets on which Naïve Bayes already does well. <em>Selective Naïve Bayes</em>, as this learning method is called, is a viable machine learning technique that performs reliably and well in practice.</p>
</div>
</div>
<div id="s0030">
<h2 id="st0030">7.2 Discretizing numeric attributes</h2>
<p id="p0215" class="noindent">Some classification and clustering algorithms deal with nominal attributes only and cannot handle ones measured on a numeric scale. To use them on general datasets, numeric attributes must first be “discretized” into a small number of distinct ranges. Even learning algorithms that handle numeric attributes sometimes process them in ways that are not altogether satisfactory. Statistical clustering methods often assume that numeric attributes have a normal distribution—often not a very plausible assumption in practice—and the standard extension of the Naïve Bayes classifier for numeric attributes adopts the same assumption. Although most decision tree and decision rule learners can handle numeric attributes, some implementations work <a id="p315"></a>much more slowly when numeric attributes are present because they repeatedly sort the attribute values. For all these reasons the question arises, what is a good way to discretize numeric attributes into ranges before any learning takes place?</p>
<p id="p0220" class="para_indented">We have already encountered some methods for discretizing numeric attributes. The 1R learning scheme described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#c0004">Chapter 4</a> uses a simple but effective technique: Sort the instances by the attribute’s value and assign the value into ranges at the points that the class value changes—except that a certain minimum number of instances in the majority class (six) must lie in each of the ranges, which means that any given range may include a mixture of class values. This is a “global” method of discretization that is applied to all continuous attributes before learning starts.</p>
<p id="p0225" class="para_indented">Decision tree learners, on the other hand, deal with numeric attributes on a local basis, examining attributes at each node of the tree when it is being constructed to see whether they are worth branching on, and only at that point deciding on the best place to split continuous attributes. Although the tree-building method we examined in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#c0006">Chapter 6</a> only considers binary splits of continuous attributes, one can imagine a full discretization taking place at that point, yielding a multiway split on a numeric attribute. The pros and cons of the local versus global approach are clear. Local discretization is tailored to the actual context provided by each tree node, and will produce different discretizations of the same attribute at different places in the tree if that seems appropriate. However, its decisions are based on less data as tree depth increases, which compromises their reliability. If trees are developed all the way out to single-class leaves before being pruned back, as with the normal technique of backward pruning, it is clear that many discretization decisions will be based on data that is grossly inadequate.</p>
<p id="p0230" class="para_indented">When using global discretization before applying a learning scheme, there are two possible ways of presenting the discretized data to the learner. The most obvious is to treat discretized attributes like nominal ones: Each discretization interval is represented by one value of the nominal attribute. However, because a discretized attribute is derived from a numeric one, its values are ordered, and treating it as nominal discards this potentially valuable ordering information. Of course, if a learning scheme can handle ordered attributes directly, the solution is obvious: Each discretized attribute is declared to be of type “ordered.”</p>
<p id="p0235" class="para_indented">If the learning scheme cannot handle ordered attributes, there is still a simple way of enabling it to exploit the ordering information: Transform each discretized attribute into a set of binary attributes before the learning scheme is applied. If the discretized attribute has <em>k</em> values, it is transformed into <em>k</em> − 1 binary attributes. If the original attribute’s value is <em>i</em> for a particular instance, the first <em>i</em> − 1 of these new attributes are set to <em>false</em> and the remainder are set to <em>true</em>. In other words, the (<em>i</em> − 1)th binary attribute represents whether the discretized attribute is less than <em>i</em>. If a decision tree learner splits on this attribute, it implicitly utilizes the ordering information it encodes. Note that this transformation is independent of the particular discretization method being applied: It is simply a way of coding an ordered attribute using a set of binary attributes.</p>
<div id="s0035">
<h3 id="st0035"><a id="p316"></a>Unsupervised Discretization</h3>
<p id="p0240" class="noindent">There are two basic approaches to the problem of discretization. One is to quantize each attribute in the absence of any knowledge of the classes of the instances in the training set—so-called <em>unsupervised</em> discretization. The other is to take the classes into account when discretizing—<em>supervised</em> discretization. The former is the only possibility when dealing with clustering problems where the classes are unknown or nonexistent.</p>
<p id="p0245" class="para_indented">The obvious way of discretizing a numeric attribute is to divide its range into a predetermined number of equal intervals: a fixed, data-independent yardstick. This is frequently done at the time when data is collected. But, like any unsupervised discretization method, it runs the risk of destroying distinctions that would have turned out to be useful in the learning process by using gradations that are too coarse or, that by unfortunate choices of boundary, needlessly lump together many instances of different classes.</p>
<p id="p0250" class="para_indented"><em>Equal-width binning</em> often distributes instances very unevenly: Some bins contain many instances while others contain none. This can seriously impair the ability of the attribute to help build good decision structures. It is often better to allow the intervals to be of different sizes, choosing them so that the same number of training examples fall into each one. This method, called <em>equal-frequency binning</em>, divides the attribute’s range into a predetermined number of bins based on the distribution of examples along that axis—sometimes called <em>histogram equalization</em> because if you take a histogram of the contents of the resulting bins it will be completely flat. If you view the number of bins as a resource, this method makes the best use of it.</p>
<p id="p0255" class="para_indented">However, equal-frequency binning is still oblivious to the instances’ classes, and this can cause bad boundaries. For example, if all instances in a bin have one class, and all instances in the next higher bin have another except for the first, which has the original class, surely it makes sense to respect the class divisions and include that first instance in the previous bin, sacrificing the equal-frequency property for the sake of homogeneity. Supervised discretization—taking classes into account during the process—certainly has advantages. Nevertheless, it has been found that equal-frequency binning can yield excellent results, at least in conjunction with the Naïve Bayes learning scheme, when the number of bins is chosen in a data-dependent fashion by setting it to the square root of the number of instances. This method is called <em>proportional k-interval discretization</em>.</p>
</div>
<div id="s0040">
<h3 id="st0040">Entropy-Based Discretization</h3>
<p id="p0260" class="noindent">Because the criterion used for splitting a numeric attribute during the formation of a decision tree works well in practice, it seems a good idea to extend it to more general discretization by recursively splitting intervals until it is time to stop. In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#c0006">Chapter 6</a> we saw how to sort the instances by the attribute’s value and consider, for each possible splitting point, the information gain of the resulting split. To <a id="p317"></a>discretize the attribute, once the first split is determined, the splitting process can be repeated in the upper and lower parts of the range, and so on, recursively.</p>
<p id="p0265" class="para_indented">To see this working in practice, we revisit the example given in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0010">Section 6.1</a> for discretizing the temperature attribute of the weather data, of which the values are as follows:</p><a id="p0270"></a>
<p id="t0010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000079tabt0010.jpg" alt="Image" width="559" height="57" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000079tabt0010.jpg"></p>
<p></p>
<p id="p0275" class="para_indented">(Repeated values have been collapsed together.) The information gain for each of the 11 possible positions for the breakpoint is calculated in the usual way. For example, the information value of the test <em>temperature</em> &lt; 71.5, which splits the range into four <em>yes</em> and two <em>no</em> versus five <em>yes</em> and three <em>no</em>, is</p>
<p class="figure" id="e0020"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si3.jpg" alt="image" width="617" height="31" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si3.jpg"></p>
<p>This represents the amount of information required to specify the individual values of <em>yes</em> and <em>no</em> given the split. We seek a discretization that makes the subintervals as pure as possible; thus, we choose to split at the point where the information value is smallest. (This is the same as splitting where the information <em>gain</em>, defined as the difference between the information value without the split and that with the split, is largest.) As before, we place numeric thresholds halfway between the values that delimit the boundaries of a concept.</p>
<p id="p0280" class="para_indented">The graph labeled A in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0015">Figure 7.2</a> shows the information values at each possible cut point at this first stage. The cleanest division—the smallest information value—is at a temperature of 84 (0.827 bits), which separates off just the very final value, a <em>no</em> instance, from the preceding list. The instance classes are written below the horizontal axis to make interpretation easier. Invoking the algorithm again on the lower range of temperatures, from 64 to 83, yields the graph labeled B. This has a minimum at 80.5 (0.800 bits), which splits off the next two values, both <em>yes</em> instances. Again invoking the algorithm on the lower range, now from 64 to 80, produces the graph labeled C (shown dotted to help distinguish it from the others). The minimum is at 77.5 (0.801 bits), splitting off another <em>no</em> instance. Graph D has a minimum at 73.5 (0.764 bits), splitting off two <em>yes</em> instances. Graph E (again dashed, purely to make it more easily visible), for the temperature range 64 to 72, has a minimum at 70.5 (0.796 bits), which splits off two <em>no</em> and one <em>yes</em>. Finally, graph F, for the range 64 to 70, has a minimum at 66.5 (0.4 bits).</p>
<p id="f0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-002-9780123748560.jpg" alt="image" width="509" height="350" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-002-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.2</span> Discretizing the <em>temperature</em> attribute using the entropy method.</p>
<p id="p0285" class="para_indented">The final discretization of the <em>temperature</em> attribute is shown in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0020">Figure 7.3</a>. The fact that recursion only ever occurs in the first interval of each split is an artifact of this example: In general, both the upper and lower intervals will have to be split further. Underneath each division is the label of the graph in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0015">Figure 7.2</a> that is responsible for it, and below that the actual value of the split point.</p>
<p id="f0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-003-9780123748560.jpg" alt="image" width="404" height="83" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-003-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.3</span> The result of discretizing the <em>temperature</em> attribute.</p>
<p id="p0290" class="para_indented">It can be shown theoretically that a cut point that minimizes the information value will never occur between two instances of the same class. This leads to a useful <a id="p318"></a>optimization: It is only necessary to consider potential divisions that separate instances of different classes. Notice that if class labels were assigned to the intervals based on the majority class in the interval, there would be no guarantee that adjacent intervals would receive different labels. You might be tempted to consider merging intervals with the same majority class (e.g., the first two intervals of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0020">Figure 7.3</a>), but as we will see later this is not a good thing to do in general.</p>
<p id="p0295" class="para_indented">The only problem left to consider is the stopping criterion. In the temperature example most of the intervals that were identified were “pure” in that all their instances had the same class, and there is clearly no point in trying to split such an <a id="p319"></a>interval. (Exceptions were the final interval, which we tacitly decided not to split, and the interval from 70.5 to 73.5.) In general, however, things are not so straightforward.</p>
<p id="p0300" class="para_indented">A good way to stop the entropy-based splitting discretization procedure turns out to be the MDL principle that we encountered in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#c0005">Chapter 5</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#p183">page 183</a>). In accordance with that principle, we want to minimize the size of the “theory” plus the size of the information necessary to specify all the data given that theory. In this case, if we do split, the “theory” is the splitting point, and we are comparing the situation in which we split with that in which we do not. In both cases we assume that the instances are known but their class labels are not. If we do not split, the classes can be transmitted by encoding each instance’s label. If we do, we first encode the split point (in log<span class="sub">2</span>[<em>N</em> − 1] bits, where <em>N</em> is the number of instances), then the classes of the instances below that point, and then the classes of those above it.</p>
<p id="p0305" class="para_indented">You can imagine that if the split is a good one—say, all the classes below it are <em>yes</em> and all those above are <em>no</em>—then there is much to be gained by splitting. If there is an equal number of <em>yes</em> and <em>no</em> instances, each instance costs 1 bit without splitting but hardly more than 0 bits with splitting—it is not quite 0 because the class values associated with the split itself must be encoded, but this penalty is amortized across all the instances. In this case, if there are many examples, the penalty of having to encode the split point will be far outweighed by the information that is saved by splitting.</p><a id="p0310"></a><div class="boxg" id="b0020">
<p id="p0315" class="noindent">We emphasized in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0110">Section 5.9</a> that when applying the MDL principle, the devil is in the details. In the relatively straightforward case of discretization, the situation is tractable although not simple. The amounts of information can be obtained exactly under certain reasonable assumptions. We will not go into the details, but the upshot is that the split dictated by a particular cut point is worthwhile if the information gain for that split exceeds a certain value that depends on the number of instances <em>N</em>, the number of classes <em>k</em>, the entropy of the instances <em>E</em>, the entropy of the instances in each subinterval <em>E</em>
<span class="sub">1</span> and <em>E</em>
<span class="sub">2</span>, and the number of classes represented in each subinterval <em>k</em>
<span class="sub">1</span> and <em>k</em>
<span class="sub">2</span>:</p>
<p class="figure" id="e0025"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si4.jpg" alt="image" width="369" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si4.jpg"></p>
<p>The first component is the information needed to specify the splitting point; the second is a correction due to the need to transmit the classes that correspond to the upper and lower subintervals.</p>
<p id="p0320" class="para_indented">When applied to the temperature example, this criterion prevents any splitting at all. The first split removes just the final example, and, as you can imagine, very little actual information is gained by this when transmitting the classes—in fact, the MDL criterion will never create an interval containing just one example. Failure to discretize <em>temperature</em> effectively disbars it from playing any role in the final decision structure because the same discretized value will be given to all instances. In this situation, this is perfectly appropriate: <em>Temperature</em> does not occur in good decision trees or rules for the weather data. In effect, failure to discretize is tantamount to attribute selection.</p>
</div>
<p></p>
</div>
<div id="s0045">
<h3 id="st0045"><a id="p320"></a>Other Discretization Methods</h3>
<p id="p0325" class="noindent">The entropy-based method with the MDL stopping criterion is one of the best general techniques for supervised discretization. However, many other methods have been investigated. For example, instead of proceeding top-down by recursively splitting intervals until some stopping criterion is satisfied, you could work bottom-up, first placing each instance into its own interval and then considering whether to merge adjacent intervals. You could apply a statistical criterion to see which would be the best two intervals to merge, and merge them if the statistic exceeds a certain preset confidence level, repeating the operation until no potential merge passes the test. The χ<sup>2</sup> test is a suitable one and has been used for this purpose. Instead of specifying a preset significance threshold, more complex techniques are available to determine an appropriate level automatically.</p>
<p id="p0330" class="para_indented">A rather different approach is to count the number of errors that a discretization makes when predicting each training instance’s class, assuming that each interval receives the majority class. For example, the 1R method described earlier is error-based—it focuses on errors rather than the entropy. However, the best possible discretization in terms of error count is obtained by using the largest possible number of intervals, and this degenerate case should be avoided by restricting the number of intervals in advance.</p>
<p id="p0335" class="para_indented">Let’s consider the best way to discretize an attribute into <em>k</em> intervals in a way that minimizes the number of errors. The brute-force method of finding this is exponential in <em>k</em> and therefore infeasible. However, there are much more efficient schemes that are based on the idea of dynamic programming. Dynamic programming applies not just to the error count measure but to any given additive impurity function, and it can find the partitioning of <em>N</em> instances into <em>k</em> intervals in a way that minimizes the impurity in time proportional to <em>kN</em><sup>2</sup>. This gives a way of finding the best entropy-based discretization, yielding a potential improvement in the quality of the discretization (in practice a negligible one) over the recursive entropy-based method described previously. The news for error-based discretization is even better because there is an algorithm that can be used to minimize the error count in time linear in <em>N</em>.</p>
</div>
<div id="s0050">
<h3 id="st0050">Entropy-Based versus Error-Based Discretization</h3>
<p id="p0340" class="noindent">Why not use error-based discretization, since the optimal discretization can be found very quickly? The answer is that there is a serious drawback to error-based discretization: It cannot produce adjacent intervals with the same label (such as the first two of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0020">Figure 7.3</a>). The reason is that merging two such intervals will not affect the error count, but it will free up an interval that can be used elsewhere to reduce the error count.</p>
<p id="p0345" class="para_indented">Why would anyone want to generate adjacent intervals with the same label? The reason is best illustrated with an example. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0025">Figure 7.4</a> shows the instance space for a simple two-class problem with two numeric attributes ranging from 0 to 1. Instances <a id="p321"></a>belong to one class (the dots) if their first attribute (<em>a</em>1) is less than 0.3, or if the first attribute is less than 0.7 <em>and</em> their second attribute (<em>a</em>2) is less than 0.5. Otherwise, they belong to the other class (triangles). The data in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0025">Figure 7.4</a> has been artificially generated according to this rule.</p>
<p id="f0025" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-004-9780123748560.jpg" alt="image" width="750" height="678" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-004-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.4</span> Class distribution for a two-class, two-attribute problem.</p>
<p id="p0350" class="para_indented">Now suppose we are trying to discretize both attributes with a view to learning the classes from the discretized attributes. The very best discretization splits <em>a</em>1 into three intervals (0–0.3, 0.3–0.7, and 0.7–1) and <em>a</em>2 into two intervals (0–0.5 and 0.5–1). Given these nominal attributes, it will be easy to learn how to tell the classes apart with a simple decision tree or rule algorithm. Discretizing <em>a</em>2 is no problem. For <em>a</em>1, however, the first and last intervals will have opposite labels (<em>dot</em> and <em>triangle</em>, respectively). The second will have whichever label happens to occur most in the region from 0.3 to 0.7 (it is in fact <em>dot</em> for the data in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0025">Figure 7.4</a>). Either way, this label must inevitably be the same as one of the adjacent labels—of course, this is true whatever the class probability happens to be in the middle region. Thus, this discretization will not be achieved by any method that minimizes the error counts because such a method cannot produce adjacent intervals with the same label.</p>
<p id="p0355" class="para_indented">The point is that what changes as the value of <em>a</em>1 crosses the boundary at 0.3 is not the majority class but the class <em>distribution</em>. The majority class remains <em>dot</em>. The distribution, however, changes markedly, from 100% before the boundary to just over 50% after it. And the distribution changes again as the boundary at 0.7 is crossed, from 50 to 0%. Entropy-based discretization methods are sensitive to <a id="p322"></a>changes in the distribution even though the majority class does not change. Error-based methods are not sensitive.</p>
</div>
<div id="s0055">
<h3 id="st0055">Converting Discrete Attributes to Numeric Attributes</h3>
<p id="p0360" class="noindent">There is a converse problem to discretization. Some learning algorithms—notably the nearest-neighbor instance-based method and numeric prediction techniques involving regression—naturally handle only attributes that are numeric. How can they be extended to nominal attributes?</p>
<p id="p0365" class="para_indented">In instance-based learning, as described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0135">Section 4.7</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#p132">page 132</a>), discrete attributes can be treated as numeric by defining as 0 the “distance” between two nominal values that are the same and as 1 the distance between two values that are different, regardless of the actual values involved. Rather than modifying the distance function, this can be achieved by an attribute transformation: Replace a <em>k</em>-valued nominal attribute by <em>k</em> synthetic binary attributes, one for each value indicating whether the attribute has that value or not. If the attributes have equal weight, this achieves the same effect on the distance function. The distance is insensitive to the attribute values because only “same” or “different” information is encoded, not the shades of difference that may be associated with the various possible values of the attribute. More subtle distinctions can be made if the attributes have weights reflecting their relative importance.</p>
<p id="p0370" class="para_indented">If the values of the attribute can be ordered, more possibilities arise. For a numeric prediction problem, the average class value corresponding to each value of a nominal attribute can be calculated from the training instances and used to determine an ordering—this technique was introduced for model trees in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0220">Section 6.6</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#p253">page 253</a>). (It is hard to come up with an analogous way of ordering attribute values for a classification problem.) An ordered nominal attribute can be replaced by an integer in the obvious way, but this implies not just an ordering but also a metric on the attribute’s values. The implication of a metric can be avoided by creating <em>k</em> − 1 synthetic binary attributes for a <em>k</em>-valued nominal attribute, in the manner described on <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#p0220">page 315</a>. This encoding still implies an ordering among different values of the attribute—adjacent values differ in just one of the synthetic attributes whereas distant ones differ in several—but does not imply an equal distance between the attribute values.</p>
</div>
</div>
<div id="s0060">
<h2 id="st0060">7.3 Projections</h2>
<p id="p0375" class="noindent">Resourceful data miners have a toolbox full of techniques, such as discretization, for transforming data. As we emphasized in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#s0035">Section 2.4</a>, data mining is hardly ever a matter of simply taking a dataset and applying a learning algorithm to it. Every problem is different. You need to think about the data and what it means, and (creatively) examine it from diverse points of view to arrive at a suitable perspective. Transforming it in different ways can help you get started. In mathematics, a <em>projection</em> is a kind of function or mapping that transforms data in some way.</p>
<p id="p0380" class="para_indented"><a id="p323"></a>You don’t have to make your own toolbox by implementing the projections yourself. Comprehensive environments for data mining, such as the one described in Part III of this book, contain a wide range of suitable tools for you to use. You do not necessarily need a detailed understanding of how they are implemented. What you do need to understand is what the tools do and how they can be applied. In Part III we list, and briefly describe, all the transformations in the Weka data mining workbench.</p>
<p id="p0385" class="para_indented">Data often calls for general mathematical transformations of a set of attributes. It might be useful to define new attributes by applying specified mathematical functions to existing ones. Two <em>date</em> attributes might be subtracted to give a third attribute representing <em>age</em>—an example of a semantic transformation driven by the meaning of the original attributes. Other transformations might be suggested by known properties of the learning algorithm. If a linear relationship involving two attributes, A and B, is suspected, and the algorithm is only capable of axis-parallel splits (as most decision tree and rule learners are), the ratio A:B might be defined as a new attribute. The transformations are not necessarily mathematical ones, but may involve real-world knowledge such as days of the week, civic holidays, or chemical atomic numbers. They could be expressed as operations in a spreadsheet or as functions that are implemented by arbitrary computer programs.</p>
<p id="p9005" class="para_indented">Or you can reduce several nominal attributes to one by concatenating their values, producing a single <em>k</em>
<span class="sub">1</span> × <em>k</em>
<span class="sub">2</span>-valued attribute from attributes with <em>k</em>
<span class="sub">1</span> and <em>k</em>
<span class="sub">2</span> values, respectively. Discretization converts a numeric attribute to nominal, and we saw earlier how to convert in the other direction too.</p>
<p id="p0390" class="para_indented">As another kind of transformation, you might apply a clustering procedure to the dataset and then define a new attribute with a value for any given instance that is the cluster that contains it using an arbitrary labeling for the clusters. Alternatively, with probabilistic clustering, you could augment each instance with its membership probabilities for each cluster, including as many new attributes as there are clusters.</p>
<p id="p0395" class="para_indented">Sometimes it is useful to add noise to data, perhaps to test the robustness of a learning algorithm; to take a nominal attribute and change a given percentage of its values; to obfuscate data by renaming the relation, attribute names, and nominal and string attribute values (because it is often necessary to anonymize sensitive datasets); to randomize the order of instances or produce a random sample of the dataset by resampling it; to reduce a dataset by removing a given percentage of instances, or all instances that have certain values for nominal attributes, or numeric values above or below a certain threshold; or to remove outliers by applying a classification method to the dataset and deleting misclassified instances.</p>
<p id="p0400" class="para_indented">Different types of input call for their own transformations. If you can input sparse data files (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#s0035">Section 2.4</a>), you may need to be able to convert datasets to nonsparse form and vice versa. Textual input and time series input call for their own specialized conversions, described in the following sections. But first we look at two general techniques for transforming data with numeric attributes into a lower-dimensional form that may be more useful for mining.</p>
<div id="s0065">
<h3 id="st0065"><a id="p324"></a>Principal Components Analysis</h3>
<p id="p0405" class="noindent">In a dataset with <em>m</em> numeric attributes, you can visualize the data as a cloud of points in <em>m</em>-dimensional space—the stars in the sky, a swarm of flies frozen in time, a two-dimensional scatter plot on paper. The attributes represent the coordinates of the space. But the axes you use, the coordinate system itself, is arbitrary. You can place horizontal and vertical axes on the paper and represent the points of the scatter plot using those coordinates, or you could draw an arbitrary straight line to represent the <em>x</em>-axis and one perpendicular to it to represent the <em>y</em>-axis. To record the positions of the flies you could use a conventional coordinate system with a north–south axis, an east–west axis, and an up–down axis. But other coordinate systems would do equally well. Creatures like flies don’t know about north, south, east, and west, although, being subject to gravity, they may perceive up–down as something special. And as for the stars in the sky, who’s to say what the “right” coordinate system is? Over the centuries our ancestors moved from a geocentric perspective to a heliocentric one to a purely relativistic one, each shift of perspective being accompanied by turbulent religious–scientific upheavals and painful reexamination of humankind’s role in God’s universe.</p>
<p id="p0410" class="para_indented">Back to the dataset. Just as in these examples, there is nothing to stop you from transforming all the data points into a different coordinate system. But unlike these examples, in data mining there often <em>is</em> a preferred coordinate system, defined not by some external convention but by the very data itself. Whatever coordinates you use, the cloud of points has a certain variance in each direction, indicating the degree of spread around the mean value in that direction. It is a curious fact that if you add up the variances along each axis and then transform the points into a different coordinate system and do the same there, you get the same total variance in both cases. This is always true provided the coordinate systems are <em>orthogonal</em>—that is, each axis is at right angles to the others.</p>
<p id="p0415" class="para_indented">The idea of principal components analysis is to use a special coordinate system that depends on the cloud of points as follows: Place the first axis in the direction of greatest variance of the points to maximize the variance along that axis. The second axis is perpendicular to it. In two dimensions there is no choice—its direction is determined by the first axis—but in three dimensions it can lie anywhere in the plane perpendicular to the first axis, and in higher dimensions there is even more choice, though it is always constrained to be perpendicular to the first axis. Subject to this constraint, choose the second axis in the way that maximizes the variance along it. And so on, choosing each axis to maximize its share of the remaining variance.</p><a id="p0420"></a><div class="boxg" id="b0025">
<p id="p0425" class="noindent">How do you do this? It’s not hard, given an appropriate computer program, and it’s not hard to understand, given the appropriate mathematical tools. Technically—for those who understand the italicized terms—you calculate the <em>covariance matrix</em> of the mean-centered coordinates of the points and <em>diagonalize</em> it to find the <em>eigenvectors</em>. These are the axes of the transformed space, sorted in order of <em>eigenvalue</em>—because each eigenvalue gives the variance along its axis.</p>
</div>
<p></p>
<p id="p0430" class="para_indented"><a id="p325"></a><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0030">Figure 7.5</a> shows the result of transforming a particular dataset with 10 numeric attributes, corresponding to points in 10-dimensional space. Imagine the original dataset as a cloud of points in 10 dimensions—we can’t draw it! Choose the first axis along the direction of greatest variance, the second perpendicular to it along the direction of next greatest variance, and so on. The table in the figure gives the variance along each new coordinate axis in the order in which the axes were chosen. Because the sum of the variances is constant regardless of the coordinate system, they are expressed as percentages of that total. We call axes <em>components</em> and say that each one “accounts for” its share of the variance. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0030">Figure 7.5(b)</a> plots the variance that each component accounts for against the component’s number. You can use all the components as new attributes for data mining, or you might want to choose just the first few, the <em>principal components</em>, and discard the rest. In this case, three principal components account for 84% of the variance in the dataset; seven account for more than 95%.</p>
<p id="f0030" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-005ab-9780123748560.jpg" alt="image" width="513" height="319" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-005ab-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.5</span> Principal components transform of a dataset: (a) variance of each component and (b) variance plot.</p>
<p id="p0435" class="para_indented">On numeric datasets it is common to use principal components analysis prior to data mining as a form of data cleanup and attribute selection. For example, you might want to replace the numeric attributes with the principal component axes or with a subset of them that accounts for a given proportion—say, 95%—of the variance. Note that the scale of the attributes affects the outcome of principal components analysis, and it is common practice to standardize all attributes to zero mean and unit variance first.</p>
<p id="p0440" class="para_indented"><a id="p326"></a>Another possibility is to apply principal components analysis recursively in a decision tree learner. At each stage an ordinary decision tree learner chooses to split in a direction that is parallel to one of the axes. However, suppose a principal components transform is performed first, and the learner chooses an axis in the transformed space. This equates to a split along an oblique line in the original space. If the transform is performed afresh before each split, the result will be a multivariate decision tree with splits that are in directions that are not parallel with the axes or with one another.</p>
</div>
<div id="s0070">
<h3 id="st0070">Random Projections</h3>
<p id="p0445" class="noindent">Principal components analysis transforms the data linearly into a lower-dimensional space—but it’s expensive. The time taken to find the transformation (which is a matrix comprising the eigenvectors of the covariance matrix) is cubic in the number of dimensions. This makes it infeasible for datasets with a large number of attributes. A far simpler alternative is to use a random projection of the data into a subspace with a predetermined number of dimensions. It’s very easy to find a random projection matrix. But will it be any good?</p>
<p id="p0450" class="para_indented">In fact, theory shows that random projections preserve distance relationships quite well on average. This means that they could be used in conjunction with <em>k</em>D-trees or ball trees to do approximate nearest-neighbor search in spaces with a huge number of dimensions. First transform the data to reduce the number of attributes; then build a tree for the transformed space. In the case of nearest-neighbor classification you could make the result more stable, and less dependent on the choice of random projection, by building an ensemble classifier that uses multiple random matrices.</p>
<p id="p0455" class="para_indented">Not surprisingly, random projections perform worse than projections carefully chosen by principal components analysis when used to preprocess data for a range of standard classifiers. However, experimental results have shown that the difference is not too great—and it tends to decrease as the number of dimensions increases. And, of course, random projections are far cheaper computationally.</p>
</div>
<div id="s0075">
<h3 id="st0075">Partial Least-Squares Regression</h3>
<p id="p0460" class="noindent">As mentioned earlier, principal components analysis is often performed as a preprocessing step before applying a learning algorithm. When the learning algorithm is linear regression, the resulting model is known as <em>principal components regression</em>. Since principal components are themselves linear combinations of the original attributes, the output of principal components regression can be reexpressed in terms of the original attributes. In fact, if all the components are used—not just the “principal” ones—the result is the same as that obtained by applying least-squares regression to the original input data. Using fewer than the full set of components results in a reduced regression.</p>
<p id="p0465" class="para_indented"><a id="p327"></a>Partial least-squares differs from principal components analysis in that it takes the class attribute into account, as well as the predictor attributes, when constructing a coordinate system. The idea is to calculate derived directions that, as well as having high variance, are strongly correlated with the class. This can be advantageous when seeking as small a set of transformed attributes as possible to use for supervised learning.</p>
<p id="p0470" class="para_indented">There is a simple iterative method for computing the partial least-squares directions that involves only dot product operations. Starting with input attributes that have been standardized to have zero mean and unit variance, the attribute coefficients for the first partial least-squares direction are found by taking the dot product between each attribute vector and the class vector in turn. To find the second direction the same approach is used, but the original attribute values are replaced by the difference between the attribute’s value and the prediction from a simple univariate regression that uses the first direction as the single predictor of that attribute. These differences are called <em>residuals</em>. The process continues in the same fashion for each remaining direction, with residuals for the attributes from the previous iteration forming the input for finding the current partial least-squares direction.</p>
<p id="p0475" class="para_indented">Here is a simple worked example that should help make the procedure clear. For the first five instances from the CPU performance data in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0001.html#t0030">Table 1.5</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0001.html#p15">page 15</a>), <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0015">Table 7.1(a)</a> shows the values of CHMIN and CHMAX (after standardization to zero mean and unit variance) and PRP (not standardized). The task is to find an expression for the target attribute PRP in terms of the other two. The attribute coefficients for the first partial least-squares direction are found by taking the dot product between the class and each attribute in turn. The dot product between the PRP and CHMIN columns is −0.4472, and that between PRP and CHMAX is 22.981. Thus, the first partial least-squares direction is</p>
<p id="f9000" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079u007-001-9780123748560.jpg" alt="image" width="654" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079u007-001-9780123748560.jpg"></p>
<p><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0015">Table 7.1(b)</a> shows the values for PLS 1 obtained from this formula.</p>
<p class="table_caption"><span class="tab_num">Table 7.1. </span> First Five Instances from the CPU Performance Data</p>
<p id="t0015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000079tabt0015.jpg" alt="Image" width="561" height="169" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000079tabt0015.jpg"></p>
<p class="table_legend">(a) original values, (b) first partial least-squares direction, and (c) residuals from the first direction.</p>
<p id="p0480" class="para_indented">The next step is to prepare the input data for finding the second partial least-squares direction. To this end, PLS 1 is regressed onto CHMIN and CHMAX in <a id="p328"></a>turn, resulting in linear equations that predict each of these attributes individually from PLS 1. The coefficients are found by taking the dot product between PLS 1 and the attribute in question, and dividing the result by the dot product between PLS 1 and itself. The resulting univariate regression equations are</p>
<p id="f9005" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079u007-002-9780123748560.jpg" alt="image" width="379" height="58" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079u007-002-9780123748560.jpg"></p>
<p></p>
<p id="f9010" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079u007-003-9780123748560.jpg" alt="image" width="396" height="46" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079u007-003-9780123748560.jpg"></p>
<p></p>
<p id="p0485" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0015">Table 7.1(c)</a> shows the CPU data in preparation for finding the second partial least-squares direction. The original values of CHMIN and CHMAX have been replaced by residuals—that is, the difference between the original value and the output of the corresponding univariate regression equation given before (the target value PRP remains the same). The entire procedure is repeated using this data as input to yield the second partial least-squares direction, which is</p>
<p id="f9015" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079u007-004-9780123748560.jpg" alt="image" width="696" height="62" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079u007-004-9780123748560.jpg"></p>
<p>After this last partial least-squares direction has been found, the attribute residuals are all zero. This reflects the fact that, as with principal components analysis, the full set of directions account for all of the variance in the original data.</p>
<p id="p0490" class="para_indented">When the partial least-squares directions are used as input to linear regression, the resulting model is known as a <em>partial least-squares regression model</em>. As with principal components regression, if all the directions are used, the solution is the same as that obtained by applying linear regression to the original data.</p>
</div>
<div id="s0080">
<h3 id="st0080">Text to Attribute Vectors</h3>
<p id="p0495" class="noindent">In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0002.html#s0035">Section 2.4</a> we introduced string attributes that contain pieces of text, and there remarked that the value of a string attribute is often an entire document. String attributes are basically nominal, with an unspecified number of values. If they are treated simply as nominal attributes, models can be built that depend on whether the values of two string attributes are equal or not. But that does not capture any internal structure of the string or bring out any interesting aspects of the text it represents.</p>
<p id="p0500" class="para_indented">You could imagine decomposing the text in a string attribute into paragraphs, sentences, or phrases. Generally, however, the word is the most useful unit. The text in a string attribute is usually a sequence of words, and it is often best represented in terms of the words it contains. For example, you might transform the string attribute into a set of numeric attributes, one for each word, that represents how often each word appears. The set of words—that is, the set of new attributes—is determined from the dataset and is typically quite large. If there are several string attributes with properties that should be treated separately, the new attribute names must be distinguished, perhaps by a user-determined prefix.</p>
<p id="p0505" class="para_indented">Conversion into words—<em>tokenization</em>—is not as simple an operation as it sounds. Tokens may be formed from contiguous alphabetic sequences with nonalphabetic <a id="p329"></a>characters discarded. If numbers are present, numeric sequences may be retained too. Numbers may involve + or − signs, may contain decimal points, and may have exponential notation—in other words, they must be parsed according to a defined number syntax. An alphanumeric sequence may be regarded as a single token. Perhaps the space character is the token delimiter; perhaps whitespace (including the tab and new-line characters) and punctuation are too. Periods can be difficult: Sometimes they should be considered part of the word (e.g., with initials, titles, abbreviations, and numbers), but sometimes they should not (e.g., if they are sentence delimiters). Hyphens and apostrophes are similarly problematic.</p>
<p id="p0510" class="para_indented">All words may be converted to lowercase before being added to the dictionary. Words on a fixed, predetermined list of function words, or <em>stopwords</em>—such as <em>the</em>, <em>and</em>, and <em>but</em>—could be ignored. Note that stopword lists are language dependent. In fact, so are capitalization conventions (German capitalizes all nouns), number syntax (Europeans use the comma for a decimal point), punctuation conventions (Spanish has an initial question mark), and, of course, character sets. Text is complicated!</p>
<p id="p0515" class="para_indented">Low-frequency words such as <em>hapax legomena</em><sup><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#fn0010" id="cc000079fn0010" class="totri-footnote">1</a></sup> are often discarded. Sometimes it is found beneficial to keep the most frequent <em>k</em> words after stopwords have been removed—or perhaps the top <em>k</em> words for each class.</p>
<p id="p0520" class="para_indented">Along with all these tokenization options, there is the question of what the value of each word attribute should be. The value may be the word count—the number of times the word appears in the string—or it may simply indicate the word’s presence or absence. Word frequencies could be normalized to give each document’s attribute vector the same Euclidean length. Alternatively, the frequencies <em>f<span class="sub">ij</span>
</em> for word <em>i</em> in document <em>j</em> can be transformed in various standard ways. One standard logarithmic term-frequency measure is log (1 + <em>f<span class="sub">ij</span>
</em>). A measure that is widely used in information retrieval is TF × IDF, or “term frequency times inverse document frequency.”</p>
<p id="p0525" class="para_indented">Here, the term frequency is modulated by a factor that depends on how commonly the word is used in other documents. The TF × IDF metric is typically defined as</p>
<p class="figure" id="e0050"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si5.jpg" alt="image" width="373" height="56" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si5.jpg"></p>
<p>The idea is that a document is basically characterized by the words that appear often in it, which accounts for the first factor, except that words used in every document or almost every document are useless as discriminators, which accounts for the second. TF × IDF is used to refer not just to this particular formula but to a general class of measures of the same type. For example, the frequency factor <em>f<span class="sub">ij</span>
</em> may be replaced by a logarithmic term such as log (1 + <em>f<span class="sub">ij</span>
</em>).</p>
</div>
<div id="s0085">
<h3 id="st0085"><a id="p330"></a>Time Series</h3>
<p id="p0530" class="noindent">In time series data, each instance represents a different time step and the attributes give values associated with that time, such as in weather forecasting or stock market prediction. You sometimes need to be able to replace an attribute’s value in the current instance by the corresponding value in some other instance in the past or the future. Even more common is to replace an attribute’s value by the <em>difference</em> between the current value and the value in some previous instance. For example, the difference—often called the <em>Delta</em>—between the current value and the preceding one is often more informative than the value itself. The first instance, for which the time-shifted value is unknown, may be removed or replaced with a missing value. The Delta value is essentially the first derivative scaled by some constant that depends on the size of the time step. Successive Delta transformations take higher derivatives.</p>
<p id="p0535" class="para_indented">In some time series, instances do not represent regular samples; instead, the time of each instance is given by a <em>timestamp</em> attribute. The difference between timestamps is the step size for that instance, and if successive differences are taken for other attributes they should be divided by the step size to normalize the derivative. In other cases, each attribute may represent a different time, rather than each instance, so that the time series is from one attribute to the next rather than one instance to the next. Then, if differences are needed, they must be taken between one attribute’s value and the next attribute’s value for each instance.</p>
</div>
</div>
<div id="s0090">
<h2 id="st0090">7.4 Sampling</h2>
<p id="p0540" class="noindent">In many applications involving a large volume of data it is necessary to come up with a random sample of much smaller size for processing. A random sample is one in which each instance in the original dataset has an equal chance of being included. Given a batch of <em>N</em> instances, a sample of any desired size is easily created: Just generate uniform random integers between 1 and <em>N</em> and retrieve the corresponding instances until the appropriate number has been collected. This is sampling <em>with replacement</em>, because the same instance might be selected more than once. (In fact, we used sampling with replacement for the bootstrap algorithm in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0025">Section 5.4</a>—<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#p155">page 155</a>.) For sampling <em>without replacement</em>, simply note, when selecting each instance, whether it has already been chosen and, if so, discard the second copy. If the sample size is much smaller than the full dataset, there is little difference between sampling with and without replacement.</p>
<div id="s0095">
<h3 id="st0095">Reservoir Sampling</h3>
<p id="p0545" class="noindent">Sampling is such a simple procedure that it merits little discussion or explanation. But there is a situation in which producing a random sample of a given size becomes <a id="p331"></a>a little more challenging. What if the training instances arrive one by one but the total number of them—the value of <em>N</em>—is not known in advance? Or suppose we need to be able to run a learning algorithm on a sample of a given size from a continuous stream of instances at any time, without repeatedly performing an entire sampling operation? Or perhaps the number of training instances is so vast that it is impractical to store them all before taking a sample?</p>
<p id="p0550" class="para_indented">All these situations call for a way of generating a random sample of an input stream without storing up all the instances and waiting for the last one to arrive before beginning the sampling procedure. Is it possible to generate a random sample of a given size and still guarantee that each instance has an equal chance of being selected? The answer is yes. Furthermore, there is a simple algorithm to do so.</p>
<p id="p0555" class="para_indented">The idea is to use a “reservoir” of size <em>r</em>, the size of the sample that is to be generated. To begin, place successive instances from the input stream in the reservoir until it is full. If the stream were to stop there, we would have the trivial case of a random sample of size <em>r</em> from an input stream of the same size. But most likely more instances will come in. The next one should be included in the sample with probability <em>r</em>/(<em>r</em> + 1)—in fact, if the input stream were to stop there, (<em>N</em> = <em>r</em> + 1), <em>any</em> instance should be in the sample with this probability. Consequently, with probability <em>r</em>/(<em>r</em> + 1) we replace a random instance in the reservoir with this new instance. And we carry on in the same vein, replacing a random reservoir element with the next instance with probability <em>r</em>/(<em>r</em> + 2) and so on. In general, the <em>i</em>th instance in the input stream is placed into the reservoir at a random location with probability <em>r</em>/<em>i</em>. It is easy to show by induction that once this instance has been processed the probability of any particular instance being in the reservoir is just the same, namely <em>r</em>/<em>i</em>. Thus, at any point in the procedure, the reservoir contains a random sample of size <em>r</em> from the input stream. You can stop at any time, secure in the knowledge that the reservoir contains the desired random sample.</p>
<p id="p0560" class="para_indented">This method samples without replacement. Sampling with replacement is a little harder, although for large datasets and small reservoirs there is little difference between the two. But if you really want a sample of size <em>r</em> with replacement, you could set up <em>r</em> independent reservoirs, each with size 1. Run the algorithm concurrently for all of these, and at any time their union is a random sample with replacement.</p>
</div>
</div>
<div id="s0100">
<h2 id="st0100">7.5 Cleansing</h2>
<p id="p0565" class="noindent">A problem that plagues practical data mining is poor quality of the data. Errors in large databases are extremely common. Attribute values, and class values too, are frequently unreliable and corrupted. Although one way of addressing this problem is to painstakingly check through the data, data mining techniques themselves can sometimes help to solve the problem.</p>
<div id="s0105">
<h3 id="st0105"><a id="p332"></a>Improving Decision Trees</h3>
<p id="p0570" class="noindent">It is a surprising fact that decision trees induced from training data can often be simplified, without loss of accuracy, by discarding misclassified instances from the training set, relearning, and then repeating until there are no misclassified instances. Experiments on standard datasets have shown that this hardly affects the classification accuracy of C4.5, a standard decision tree–induction scheme. In some cases it improves slightly; in others it deteriorates slightly. The difference is rarely statistically significant—and even when it is, the advantage can go either way. What the technique does affect is decision tree size. The resulting trees are invariably much smaller than the original ones, even though they perform about the same.</p>
<p id="p0575" class="para_indented">What is the reason for this? When a decision tree–induction method prunes away a subtree, it applies a statistical test that decides whether that subtree is “justified” by the data. The decision to prune accepts a small sacrifice in classification accuracy on the training set in the belief that this will improve test-set performance. Some training instances that were classified correctly by the unpruned tree will now be misclassified by the pruned one. In effect, the decision has been taken to ignore these training instances.</p>
<p id="p0580" class="para_indented">But that decision has only been applied locally, in the pruned subtree. Its effect has not been allowed to percolate further up the tree, perhaps resulting in different choices being made of attributes to branch on. Removing the misclassified instances from the training set and relearning the decision tree is just taking the pruning decisions to their logical conclusion. If the pruning strategy is a good one, this should not harm performance. And it may improve it by allowing better attribute choices to be made.</p>
<p id="p0585" class="para_indented">It would no doubt be even better to consult a human expert. Misclassified training instances could be presented for verification, and those that were found to be wrong could be deleted—or, better still, corrected.</p>
<p id="p0590" class="para_indented">Notice that we are assuming that the instances are not misclassified in any systematic way. If instances are systematically corrupted in both training and test sets—for example, one class value might be substituted for another—it is only to be expected that training on the erroneous training set would yield better performance on the (also erroneous) test set.</p>
<p id="p0595" class="para_indented">Interestingly enough, it has been shown that when artificial noise is added to attributes (rather than added to classes), test-set performance is improved if the same noise is added in the same way to the training set. In other words, when attribute noise is the problem, it is not a good idea to train on a “clean” set if performance is to be assessed on a “dirty” one. A learning scheme can learn to compensate for attribute noise, in some measure, if given a chance. In essence, it can learn which attributes are unreliable and, if they are all unreliable, how best to use them together to yield a more reliable result. To remove noise from attributes for the training set denies the opportunity to learn how best to combat that noise. But with class noise (rather than attribute noise), it is best to train on noise-free instances if possible, if accurate classification is the goal.</p>
</div>
<div id="s0110">
<h3 id="st0110"><a id="p333"></a>Robust Regression</h3>
<p id="p0600" class="noindent">The problems caused by noisy data have been known in linear regression for years. Statisticians often check data for outliers and remove them manually. In the case of linear regression, outliers can be identified visually, although it is never completely clear whether an outlier is an error or just a surprising, but correct, value. Outliers dramatically affect the usual least-squares regression because the squared distance measure accentuates the influence of points far away from the regression line.</p>
<p id="p0605" class="para_indented">Statistical methods that address the problem of outliers are called <em>robust</em>. One way of making regression more robust is to use an absolute-value distance measure instead of the usual squared one. This weakens the effect of outliers. Another possibility is to try to identify outliers automatically and remove them from consideration. For example, one could form a regression line and then remove from consideration those 10% of points that lie furthest from the line. A third possibility is to minimize the <em>median</em> (rather than the mean) of the squares of the divergences from the regression line. It turns out that this estimator is very robust and actually copes with outliers in the <em>x</em>-direction as well as outliers in the <em>y</em>-direction, which is the normal direction one thinks for outliers.</p>
<p id="p0610" class="para_indented">A dataset that is often used to illustrate robust regression is a graph of international telephone calls made from Belgium during the years 1950 through 1973, shown in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0035">Figure 7.6</a>. This data is taken from the Belgian Statistical Survey published by the Ministry of Economy. The plot seems to show an upward trend over the years, but there is an anomalous group of points from 1964 to 1969. It turns out that during this period, results were mistakenly recorded as the total number of <em>minutes</em> of the calls. The years 1963 and 1970 are also partially affected. This error causes a large fraction of outliers in the <em>y</em>-direction.</p>
<p id="f0035" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-006-9780123748560.jpg" alt="image" width="393" height="243" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-006-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.6</span> Number of international phone calls from Belgium, 1950–1973.</p>
<p id="p0615" class="para_indented"><a id="p334"></a>Not surprisingly, the usual least-squares regression line is seriously affected by this anomalous data. However, the least <em>median</em> of squares line remains remarkably unperturbed. This line has a simple and natural interpretation. Geometrically, it corresponds to finding the narrowest strip covering half of the observations, where the thickness of the strip is measured in the vertical direction—this strip is marked gray in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0035">Figure 7.6</a>. The least median of squares line lies at the exact center of this band. Note that this notion is often easier to explain and visualize than the normal least-squares definition of regression. Unfortunately, there is a serious disadvantage to median-based regression techniques: They incur high computational cost, which often makes them infeasible for practical problems.</p>
</div>
<div id="s0115">
<h3 id="st0115">Detecting Anomalies</h3>
<p id="p0620" class="noindent">A serious problem with any form of automatic detection of apparently incorrect data is that the baby may be thrown out with the bathwater. Short of consulting a human expert, there is no way of telling whether a particular instance really is an error or whether it just does not fit the type of model that is being applied. In statistical regression, visualizations help. It will usually be visually apparent, even to the nonexpert, if the wrong kind of curve is being fitted—a straight line is being fitted to data that lies on a parabola, for example. The outliers in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0035">Figure 7.6</a> certainly stand out to the eye. But most classification problems cannot be so easily visualized: The notion of “model type” is more subtle than a regression line. And although it is known that good results are obtained on most standard datasets by discarding instances that do not fit a decision tree model, this is not necessarily of great comfort when dealing with a particular new dataset. The suspicion will remain that perhaps the new dataset is simply unsuited to decision tree modeling.</p>
<p id="p0625" class="para_indented">One solution that has been tried is to use several different learning schemes (e.g., a decision tree, a nearest-neighbor learner, and a linear discriminant function) to filter the data. A conservative approach is to ask that all three schemes fail to classify an instance correctly before it is deemed erroneous and removed from the data. In some cases, filtering the data in this way and using the filtered data as input to a final learning scheme gives better performance than simply using the three learning schemes and letting them vote on the outcome. Training all three schemes on the <em>filtered</em> data and letting them vote can yield even better results. However, there is a danger to voting techniques: Some learning algorithms are better suited to certain types of data than others, and the most appropriate scheme may simply get out-voted! We will examine a more subtle method of combining the output from different classifiers, called <em>stacking</em>, in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#s0090">Section 8.7</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#p369">page 369</a>). The lesson, as usual, is to get to know your data and look at it in many different ways.</p>
<p id="p0630" class="para_indented">One possible danger with filtering approaches is that they might conceivably just be sacrificing instances of a particular class (or group of classes) to improve accuracy on the remaining classes. Although there are no general ways to guard against this, it has not been found to be a common problem in practice.</p>
<p id="p0635" class="para_indented"><a id="p335"></a>Finally, it is worth noting once again that automatic filtering is a poor substitute for getting the data right in the first place. And if this is too time-consuming and expensive to be practical, human inspection could be limited to those instances that are identified by the filter as suspect.</p>
</div>
<div id="s0120">
<h3 id="st0120">One-Class Learning</h3>
<p id="p0640" class="noindent">In most classification problems, training data is available for all classes that can occur at prediction time, and the learning algorithm uses the data for the different classes to determine decision boundaries that discriminate between them. However, some problems exhibit only a single class of instances at training time, while at prediction time new instances with unknown class labels can belong either to this target class or to a new class that was not available during training. Then, two different predictions are possible: <em>target</em>, meaning that an instance belongs to the class experienced during training, and <em>unknown</em>, where the instance does not appear to belong to that class. This type of learning problem is known as <em>one-class classification</em>.</p>
<p id="p0645" class="para_indented">In many cases, one-class problems can be reformulated into two-class ones because there is data from other classes that can be used for training. However, there are genuine one-class applications where it is impossible or inappropriate to make use of negative data during training. For example, consider password hardening, a biometric system that strengthens a computer login process by not only requiring the correct password to be typed, but also requiring that it be typed with the correct rhythm. This is a one-class problem; a single user must be verified and during training time only data from that user is available—we cannot ask anyone else to provide data without supplying them with the password!</p>
<p id="p0650" class="para_indented">Even in applications where instances from several classes are available at training time, it may be best to focus solely on the target class under consideration—if, for example, new classes may occur at prediction time that differ from all those available during training. Continuing with the typing-rhythm scenario, suppose we are to recognize typists in a situation where the text is not fixed—the current typist is to be verified as who he or she claims to be from his or her rhythmic patterns on a block of free text. This task is fundamentally different from distinguishing one user from a group of other users because we must be prepared to refuse attackers that the system has never seen before.</p>
<div id="s0125">
<h4 class="h4" id="st0125">Outlier Detection</h4>
<p id="p0655" class="noindent">One-class classification is often called <em>outlier</em> (or novelty) detection because the learning algorithm is being used to differentiate between data that appears normal and abnormal with respect to the distribution of the training data. Earlier in this section we talked about making regression more robust against outliers by replacing the usual squared distance measure with the absolute-value one, and about trying to detect anomalies by using several different learning schemes.</p>
<p id="p0660" class="para_indented"><a id="p336"></a>A generic statistical approach to one-class classification is to identify outliers as instances that lie beyond a distance <em>d</em> from a given percentage <em>p</em> of the training data. Alternatively, a probability density can be estimated for the target class by fitting a statistical distribution, such as a Gaussian, to the training data; any test instances with a low probability value can be marked as outliers. The challenge is to identify an appropriate distribution for the data at hand. If this cannot be done, one can adopt a nonparametric approach such as kernel density estimation (mentioned at the end of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0025">Section 4.2</a>, <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#p99">page 99</a>). An advantage of the density estimation approach is that the threshold can be adjusted at prediction time to obtain a suitable rate of outliers.</p>
<p id="p0665" class="para_indented">Multiclass classifiers can be tailored to the one-class situation by fitting a boundary around the target data and deeming instances that fall outside it to be outliers. The boundary can be generated by adapting the inner workings of existing multiclass classifiers such as support vector machines. These methods rely heavily on a parameter that determines how much of the target data is likely to be classified as outliers. If it is chosen too conservatively, data in the target class will erroneously be rejected. If it is chosen too liberally, the model will overfit and reject too much legitimate data. The rejection rate usually cannot be adjusted during testing, because an appropriate parameter value needs to be chosen at training time.</p>
</div>
<div id="s0130">
<h4 class="h4" id="st0130">Generating Artificial Data</h4>
<p id="p0670" class="noindent">Rather than modify the internal workings of a multiclass classifier to form a one-class decision boundary directly, another possibility is to generate artificial data for the outlier class and apply any off-the-shelf classifier. Not only does this allow any classifier to be used, but if the classifier produces class probability estimates the rejection rate can be tuned by altering the threshold.</p>
<p id="p0675" class="para_indented">The most straightforward approach is to generate uniformly distributed data and learn a classifier that can discriminate this from the target. However, different decision boundaries will be obtained for different amounts of artificial data: If too much is generated it will overwhelm the target class and the learning algorithm will always predict the artificial class. This problem can be avoided if the objective of learning is viewed as accurate class probability estimation rather than minimizing the classification error. For example, bagged decision trees (described in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#s0015">Section 8.2</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#p352">page 352</a>), which have been shown to yield good class probability estimators, can be used.</p>
<p id="p0680" class="para_indented">Once a class probability estimation model has been obtained in this fashion, different thresholds on the probability estimates for the target class correspond to different decision boundaries surrounding the target class. This means that, as in the density estimation approach to one-class classification, the rate of outliers can be adjusted at prediction time to yield an outcome appropriate for the application at hand.</p>
<p id="p0685" class="para_indented">There is one significant problem. As the number of attributes increases, it quickly becomes infeasible to generate enough artificial data to obtain adequate coverage of <a id="p337"></a>the instance space, and the probability that a particular artificial instance occurs inside or close to the target class diminishes to a point that makes any kind of discrimination impossible.</p>
<p id="p0690" class="para_indented">The solution is to generate artificial data that is as close as possible to the target class. In this case, because it is no longer uniformly distributed, the distribution of this artificial data—call this the “reference” distribution—must be taken into account when computing the membership scores for the resulting one-class model. In other words, the class probability estimates of the two-class classifier must be combined with the reference distribution to obtain membership scores for the target class.</p><a id="p0695"></a><div class="boxg" id="b0030">
<p id="p0700" class="noindent">To elaborate a little further, let <em>T</em> denote the target class for which we have training data and seek a one-class model, and <em>A</em> the artificial class, for which we generate data using a known reference distribution. What we would like to obtain is Pr[<em>X</em> | <em>T</em>], the density function of the target class, for any instance <em>X</em>—of course, we know Pr[<em>X</em> | <em>A</em>], the density function of the reference distribution. Assume for the moment that we know the true class probability function Pr[<em>T</em> | <em>X</em>]. In practice, we need to estimate this function using a class probability estimator learned from the training data. A simple application of Bayes’ rule can be used to express Pr[<em>X</em> | <em>T</em>] in terms of Pr[<em>T</em>], Pr[<em>T</em> | <em>X</em>], and Pr[<em>X</em> | <em>A</em>]:</p>
<p class="figure" id="e0055"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079si6.jpg" alt="image" width="285" height="48" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079si6.jpg"></p>
<p></p>
<p id="p0705" class="para_indented">To use this equation in practice, choose Pr[<em>X</em> | <em>A</em>], generate a user-specified amount of artificial data from it, label it <em>A</em>, and combine it with instances in the training set for the target class, labeled <em>T</em>. The proportion of target instances is an estimate of Pr[<em>T</em>], and a standard learning algorithm can be applied to this two-class dataset to obtain a class probability estimator Pr[<em>T</em> | <em>X</em>]. Given that the value for Pr[<em>X</em> | <em>A</em>] can be computed for any particular instance <em>X</em>, everything is at hand to compute an estimate of the target density function Pr[<em>X</em> | <em>T</em>] for any instance <em>X</em>. To perform classification we choose an appropriate threshold, adjusted to tune the rejection rate to any desired value.</p>
<p id="p0710" class="para_indented">One question remains, namely, how to choose the reference density Pr[<em>X</em> | <em>A</em>]. We need to be able to generate artificial data from it and to compute its value for any instance <em>X</em>. Another requirement is that the data it generates should be close to the target class. In fact, ideally the reference density is identical to the target density, in which case Pr[<em>T</em> | <em>X</em>] becomes a constant function that any learning algorithm should be able to induce—the resulting two-class learning problem becomes trivial. This is unrealistic because it would require us to know the density of the target class. However, this observation gives a clue as to how to proceed: Apply any density estimation technique to the target data and use the resulting function to model the artificial class. The better the match between Pr[<em>X</em> | <em>A</em>] and Pr[<em>X</em> | <em>T</em>], the easier the resulting two-class class probability estimation task becomes.</p>
<p id="p0715" class="para_indented">In practice, given the availability of powerful methods for class probability estimation and the relative lack of such techniques for density estimation, it makes sense to apply a simple density estimation technique to the target data first to obtain Pr[<em>X</em> | <em>A</em>] and then employ a state-of-the-art class probability estimation method to the two-class problem that is obtained by combining the artificial data with the data from the target class.</p>
</div>
<p></p>
</div>
</div>
</div>
<div id="s0135">
<h2 id="st0135">7.6 <a id="p338"></a>Transforming multiple classes to binary ones</h2>
<p id="p0720" class="noindent">Recall from <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#c0006">Chapter 6</a> that some learning algorithms—for example, standard support vector machines—only work with two-class problems. In most cases, sophisticated multiclass variants have been developed, but they may be very slow or difficult to implement. As an alternative, it is common practice to transform multiclass problems into multiple two-class ones: The dataset is decomposed into several two-class problems, the algorithm is run on each one, and the outputs of the resulting classifiers are combined. Several popular techniques can implement this idea. We begin with a very simple one that was touched on when we were discussing how to use linear regression for classification; we then move on to pairwise classification and more advanced techniques—error-correcting output codes and ensembles of nested dichotomies—that can often be profitably applied even when the underlying learning algorithm is able to deal with multiclass problems directly.</p>
<div id="s0140">
<h3 id="st0140">Simple Methods</h3>
<p id="p0725" class="noindent">At the beginning of the Linear Classification section in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#c0004">Chapter 4</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#p125">page 125</a>) we learned how to transform a multiclass dataset for multiresponse linear regression to perform a two-class regression for each class. The idea essentially produces several two-class datasets by discriminating each class against the union of all the other classes. This technique is commonly called <em>one-vs.-rest</em> (or somewhat misleadingly, <em>one-vs.-all</em>). For each class, a dataset is generated containing a copy of each instance in the original data, but with a modified class value. If the instance has the class associated with the corresponding dataset, it is tagged <em>yes</em>; otherwise, <em>no</em>. Then classifiers are built for each of these binary datasets—classifiers that output a confidence figure with their predictions; for example, the estimated probability that the class is <em>yes</em>. During classification, a test instance is fed into each binary classifier, and the final class is the one associated with the classifier that predicts <em>yes</em> most confidently.</p>
<p id="p0730" class="para_indented">Of course, this method is sensitive to the accuracy of the confidence figures produced by the classifiers: If some classifiers have an exaggerated opinion of their own predictions, the overall result will suffer. That is why it can be important to carefully tune parameter settings in the underlying learning algorithm. For example, in standard support vector machines for classification, it is generally necessary to tune the parameter <em>C</em>, which provides an upper bound to the influence of each support vector and controls the closeness of fit to the training data, and the value of the kernel parameter—for example, the degree of the exponent in a polynomial kernel. This can be done based on internal cross-validation. It has been found empirically that the one-vs.-rest method can be very competitive, at least in the case of kernel-based classifiers, when appropriate parameter tuning is done. Note that it may also be useful to apply techniques for calibrating confidence scores, discussed in the next section, to the individual two-class models.</p>
<p id="p0735" class="para_indented"><a id="p339"></a>Another simple and general method for multiclass problems is <em>pairwise classification</em>. Here, a classifier is built for every pair of classes, using only the instances from these two classes. The output on an unknown test example is based on which class receives the most votes. This scheme generally yields accurate results in terms of classification error. It can also be used to produce probability estimates by applying a method called <em>pairwise coupling</em>, which calibrates the individual probability estimates from the different classifiers.</p>
<p id="p0740" class="para_indented">If there are <em>k</em> classes, pairwise classification builds a total of <em>k</em>(<em>k</em> − 1)/2 classifiers. Although this sounds unnecessarily computation intensive, it is not. In fact, if the classes are evenly populated, a pairwise classifier is at least as quick to train as any other multiclass method. The reason is that each of the pairwise learning problems only involves instances pertaining to the two classes under consideration. If <em>n</em> instances are divided evenly among <em>k</em> classes, this amounts to 2<em>n</em>/<em>k</em> instances per problem. Suppose the learning algorithm for a two-class problem with <em>n</em> instances takes time proportional to <em>n</em> seconds to execute. Then the runtime for pairwise classification is proportional to <em>k</em>(<em>k</em> − 1)/2 × 2<em>n</em>/<em>k</em> seconds, which is (<em>k</em> − 1)<em>n</em>. In other words, the method scales linearly with the number of classes. If the learning algorithm takes more time—say proportional to <em>n</em><sup>2</sup>—the advantage of the pairwise approach becomes even more pronounced.</p>
</div>
<div id="s0145">
<h3 id="st0145">Error-Correcting Output Codes</h3>
<p id="p0745" class="noindent">The simple methods discussed above are often very effective. Pairwise classification in particular can be a very useful technique. It has been found that it can in some cases improve accuracy even when the underlying learning algorithm, such as a decision tree learner, can deal with multiclass problems directly. This may be due to the fact that pairwise classification actually generates an ensemble of many classifiers. Ensemble learning is a well-known strategy for obtaining accurate classifiers, and we will discuss several ensemble learning methods in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html#c0008">Chapter 8</a>. It turns out that there are methods other than pairwise classification that can be used to generate an ensemble classifier by decomposing a multiclass problem into several two-class subtasks. The one we discuss next is based on error-correcting output codes.</p>
<p id="p0750" class="para_indented">Two-class decompositions of multiclass problems can be viewed in terms of the so-called “output codes” they correspond to. Let us revisit the simple one-vs.-rest method to see what such codes look like. Consider a multiclass problem with four classes <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em>. The transformation can be visualized as illustrated in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(a)</a>, where <em>yes</em> and <em>no</em> are mapped to 1 and 0, respectively. Each of the original class values is converted into a 4-bit code word, 1 bit per class, and the four classifiers predict the bits independently. Interpreting the classification process in terms of these code words, errors occur when the wrong binary bit receives the highest confidence.</p>
<p class="table_caption"><span class="tab_num">Table 7.2. </span> Transforming a Multiclass Problem into a Two-Class One</p>
<p id="t0020" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/t000079tabt0020.jpg" alt="Image" width="560" height="149" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/t000079tabt0020.jpg"></p>
<p class="table_legend">(a) standard method and (b) error-correcting code.</p>
<p id="p0755" class="para_indented">However, we do not have to use the particular code words shown. Indeed, there is no reason why each class must be represented by 4 bits. Look instead at the code of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a>, where classes are represented by 7 bits. When applied to a dataset, <a id="p340"></a>seven classifiers must be built instead of four. To see what that might buy, consider the classification of a particular instance. Suppose it belongs to class <em>a</em>, and that the predictions of the individual classifiers are 1 0 1 1 1 1 1, respectively. Obviously, comparing this code word with those in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a>, the second classifier has made a mistake: It predicted 0 instead of 1, <em>no</em> instead of <em>yes</em>.</p>
<p id="p9015" class="para_indented">Comparing the predicted bits with the code word associated with each class, the instance is clearly closer to <em>a</em> than to any other class. This can be quantified by the number of bits that must be changed to convert the predicted code word into those of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a>: The <em>Hamming distance</em>, or the discrepancy between the bit strings, is 1, 3, 3, and 5 for the classes <em>a</em>, <em>b</em>, <em>c</em>, and <em>d</em>, respectively. We can safely conclude that the second classifier made a mistake and correctly identify <em>a</em> as the instance’s true class.</p>
<p id="p0760" class="para_indented">The same kind of error correction is not possible with the code words shown in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(a)</a> because any predicted string of 4 bits, other than these four 4-bit words, has the same distance to at least two of them. Thus, the output codes are not “error correcting.”</p>
<p id="p0765" class="para_indented">What determines whether a code is error correcting or not? Consider the Hamming distance between the code words representing different classes. The number of errors that can be possibly corrected depends on the minimum distance between any pair of code words, say <em>d</em>. The code can guarantee to correct up to (<em>d</em> − <em>1</em>)/2 1-bit errors because if this number of bits of the correct code word are flipped, it will still be the closest and will therefore be identified correctly. In <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(a)</a> the Hamming distance for each pair of code words is 2. Thus, the minimum distance <em>d</em> is also 2, and we can correct no more than 0 errors! However, in the code of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a> the minimum distance is 4 (in fact, the distance is 4 for all pairs). That means it is guaranteed to correct 1-bit errors.</p>
<p id="p0770" class="para_indented">We have identified one property of a good error-correcting code: The code words must be well separated in terms of their Hamming distance. Because they comprise the rows of the code table, this property is called <em>row separation</em>. There is a second requirement that a good error-correcting code should fulfill: <em>column separation</em>. The Hamming distance between every pair of columns must be large, as must the distance between each column and the complement of every other column. The seven <a id="p341"></a>columns in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a> are separated from one another (and their complements) by at least 1 bit.</p>
<p id="p0775" class="para_indented">Column separation is necessary because if two columns are identical (or if one is the complement of another), the corresponding classifiers will make the same errors. Error correction is weakened if the errors are correlated—in other words, if many bit positions are simultaneously incorrect. The greater the distance between columns, the more errors are likely to be corrected.</p>
<p id="p0780" class="para_indented">With fewer than four classes it is impossible to construct an effective error-correcting code because good row separation and good column separation cannot be achieved simultaneously. For example, with three classes there are only eight possible columns (2<sup>3</sup>), four of which are complements of the other four. Moreover, columns with all 0s or all 1s provide no discrimination. This leaves just three possible columns, and the resulting code is not error correcting at all. (In fact, it is the standard “one-vs.-rest” encoding.)</p>
<p id="p0790" class="para_indented">If there are few classes, an exhaustive error-correcting code, such as the one in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0020">Table 7.2(b)</a>, can be built. In an exhaustive code for the <em>k</em> classes, the columns comprise every possible <em>k</em>-bit string, except for complements and the trivial all-0 or all-1 strings. Each of the code words contains 2<em>
<sup>k</sup>
</em><sup>−1</sup> − 1 bits. The code is constructed as follows: The code word for the first class consists of all 1s; that for the second class has 2<em>
<sup>k</sup>
</em><sup>−2</sup> 0s followed by 2<em>
<sup>k</sup>
</em><sup>−2</sup> − 1 1s; the third has 2<em>
<sup>k</sup>
</em><sup>−3</sup> 0s followed by 2<em>
<sup>k</sup>
</em><sup>−3</sup> 1s followed by 2<em>
<sup>k</sup>
</em><sup>−3</sup> 0s followed by 2<em>
<sup>k</sup>
</em><sup>−3</sup> − 1 1s; and so on. The <em>i</em>th code word consists of alternating runs of 2<em>
<sup>k</sup>
</em><sup>−</sup><em>
<sup>i</sup>
</em> 0s and 1s, the last run being one short.</p>
<p id="p0795" class="para_indented">With more classes, exhaustive codes are infeasible because the number of columns increases exponentially and too many classifiers have to be built. In that case, more sophisticated methods are employed, which can build a code with good error-correcting properties from a smaller number of columns.</p>
<p id="p0800" class="para_indented">Error-correcting output codes do not work for local learning algorithms such as instance-based learners, which predict the class of an instance by looking at nearby training instances. In the case of a nearest-neighbor classifier, all output bits would be predicted using the same training instance. The problem can be circumvented by using different attribute subsets to predict each output bit, decorrelating the predictions.</p>
</div>
<div id="s0150">
<h3 id="st0150">Ensembles of Nested Dichotomies</h3>
<p id="p0805" class="noindent">Error-correcting output codes often produce accurate classifiers for multiclass problems. However, the basic algorithm produces classifications, whereas often we would like class probability estimates as well—for example, to perform cost-sensitive classification using the minimum expected cost approach discussed in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0065">Section 5.7</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#p167">page 167</a>). Fortunately, there is a method for decomposing multiclass problems into two-class ones that provides a natural way of computing class probability estimates, so long as the underlying two-class models are able to produce probabilities for the corresponding two-class subtasks.</p>
<p id="p0810" class="para_indented"><a id="p342"></a>The idea is to recursively split the full set of classes from the original multiclass problem into smaller and smaller subsets, while splitting the full dataset of instances into subsets corresponding to these subsets of classes. This yields a binary tree of classes. Consider the hypothetical four-class problem discussed earlier. At the root node is the full set of classes {<em>a</em>, <em>b</em>, <em>c</em>, <em>d</em>}, which is split into disjoint subsets, say {<em>a</em>, <em>c</em>} and {<em>b</em>, <em>d</em>}, along with the instances pertaining to these two subsets of classes. The two subsets form the two successor nodes in the binary tree. These subsets are then split further into one-element sets, yielding successors {<em>a</em>} and {<em>c</em>} for the node {<em>a</em>, <em>c</em>} and successors {<em>b</em>} and {<em>d</em>} for the node {<em>b</em>, <em>d</em>}. Once we reach one-element subsets, the splitting process stops.</p>
<p id="p0815" class="para_indented">The resulting binary tree of classes is called a <em>nested dichotomy</em> because each internal node and its two successors define a dichotomy—for example, discriminating between classes {<em>a</em>, <em>c</em>} and {<em>b</em>, <em>d</em>} at the root node—and the dichotomies are nested in a hierarchy. We can view a nested dichotomy as a particular type of sparse output code. <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#t0025">Table 7.3</a> shows the output code matrix for the example just discussed. There is one dichotomy for each internal node of the tree structure. Thus, given that the example involves three internal nodes, there are three columns in the code matrix. In contrast to the class vectors considered before, the matrix contains elements marked <em>X</em> that indicate that instances of the corresponding classes are simply omitted from the associated two-class learning problems.</p>
<p class="table_caption"><span class="tab_num">Table 7.3. </span> Nested Dichotomy in the Form of a Code Matrix</p>
<table id="t0025" frame="box" rules="all">
<thead>
<tr><td class="tch">Class</td>
<td class="tch">Class Vector</td></tr>
</thead>
<tbody valign="top">
<tr><td class="tb"><em>a</em></td>
<td class="tb">0 0 X</td></tr>
<tr><td class="tb"><em>b</em></td>
<td class="tb">1 1 X</td></tr>
<tr><td class="tb"><em>c</em></td>
<td class="tb">0 X 0</td></tr>
<tr><td class="tb"><em>d</em></td>
<td class="tb">1 X 0</td></tr>
</tbody>
</table>
<p id="p0820" class="para_indented">What is the advantage of this kind of output code? It turns out that, because the decomposition is hierarchical and yields disjoint subsets, there is a simple method for computing class probability estimates for each element in the original set of multiple classes, assuming two-class estimates for each dichotomy in the hierarchy. The reason is the chain rule from probability theory, which we already encountered when discussing Bayesian networks in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#s0270">Section 6.7</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0006.html#p265">page 265</a>).</p>
<p id="p0825" class="para_indented">Suppose we want to compute the probability for class <em>a</em> given a particular instance <em>x—</em>that is, the conditional probability Pr[<em>a</em> | <em>x</em>]. This class corresponds to one of the four leaf nodes in the hierarchy of classes in the previous example. First, we learn two-class models that yield class probability estimates for the three two-class datasets at the internal nodes of the hierarchy. Then, from the two-class model at the root node, an estimate of the conditional probability Pr[{<em>a</em>, <em>b</em>} | <em>x</em>]—namely, that <em>x</em> belongs to either <em>a</em> or <em>b</em>—can be obtained. Moreover, we can obtain an estimate of Pr[{<em>a</em>} | <em>x</em>, {<em>a</em>, <em>b</em>}]—the probability that <em>x</em> belongs to <em>a</em> given that we already know <a id="p343"></a>that it belongs to either <em>a</em> or <em>b</em>—from the model that discriminates between the one-element sets {<em>a</em>} and {<em>b</em>}. Now, based on the chain rule, Pr[{<em>a</em>} | <em>x</em>] = Pr[{<em>a</em>} | {<em>a</em>, <em>b</em>}, <em>x</em> ] × Pr[{<em>a</em>, <em>b</em>} | <em>x</em>]. Thus, to compute the probability for any individual class of the original multiclass problem—any leaf node in the tree of classes—we simply multiply together the probability estimates collected from the internal nodes encountered when proceeding from the root node to this leaf node: the probability estimates for all subsets of classes that contain the target class.</p>
<p id="p0830" class="para_indented">Assuming that the individual two-class models at the internal nodes produce accurate probability estimates, there is reason to believe that the multiclass probability estimates obtained using the chain rule will generally be accurate. However, it is clear that estimation errors will accumulate, causing problems for very deep hierarchies. A more basic issue is that in the previous example we arbitrarily decided on a particular hierarchical decomposition of the classes. Perhaps there is some background knowledge regarding the domain concerned, in which case one particular hierarchy may be preferable because certain classes are known to be related, but this is generally not the case.</p>
<p id="p0835" class="para_indented">What can be done? If there is no reason a priori to prefer any particular decomposition, perhaps all of them should be considered, yielding an <em>ensemble</em> of nested dichotomies. Unfortunately, for any nontrivial number of classes there are too many potential dichotomies, making an exhaustive approach infeasible. But we could consider a subset, taking a random sample of possible tree structures, building two-class models for each internal node of each tree structure (with caching of models, given that the same two-class problem may occur in multiple trees), and then averaging the probability estimates for each individual class to obtain the final estimates.</p>
<p id="p0840" class="para_indented">Empirical experiments show that this approach yields accurate multiclass classifiers and is able to improve predictive performance even in the case of classifiers, such as decision trees, that can deal with multiclass problems directly. In contrast to standard error-correcting output codes, the technique often works well even when the base learner is unable to model complex decision boundaries. The reason is that, generally speaking, learning is easier with fewer classes so results become more successful the closer we get to the leaf nodes in the tree. This also explains why the pairwise classification technique described earlier works particularly well for simple models such as ones corresponding to hyperplanes: It creates the simplest possible dichotomies! Nested dichotomies appear to strike a useful balance between the simplicity of the learning problems that occur in pairwise classification—after all, the lowest-level dichotomies involve pairs of individual classes—and the power of the redundancy embodied in standard error-correcting output codes.</p>
</div>
</div>
<div id="s0155">
<h2 id="st0155">7.7 Calibrating class probabilities</h2>
<p id="p0845" class="noindent">Class probability estimation is obviously more difficult than classification. Given a way of generating class probabilities, classification error is minimized as long as the correct class is predicted with maximum probability. However, a method for accurate <a id="p344"></a>classification does not imply a method of generating accurate probability estimates: The estimates that yield the correct classification may be quite poor when assessed according to the quadratic (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#p160">page 160</a>) or informational (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#p161">page 161</a>) loss discussed in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0005.html#s0045">Section 5.6</a>. Yet—as we have stressed several times—it is often more important to obtain accurate conditional class probabilities for a given instance than to simply place the instance into one of the classes. Cost-sensitive prediction based on the minimum expected cost approach is one example where accurate class probability estimates are very useful.</p>
<p id="p0850" class="para_indented">Consider the case of probability estimation for a dataset with two classes. If the predicted probabilities are on the correct side of the 0.5 threshold commonly used for classification, no classification errors will be made. However, this does not mean that the probability estimates themselves are accurate. They may be systematically too optimistic—too close to either 0 or 1—or too pessimistic—not close enough to the extremes. This type of bias will increase the measured quadratic or informational loss, and will cause problems when attempting to minimize the expected cost of classifications based on a given cost matrix.</p>
<p id="p0855" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0040">Figure 7.7</a> demonstrates the effect of overoptimistic probability estimation for a two-class problem. The <em>x</em>-axis shows the predicted probability of the multinomial Naïve Bayes model from <a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#s0025">Section 4.2</a> (<a href="/site/library/view/data-mining-practical/9780123748560/html/c0004.html#p97">page 97</a>) for one of two classes in a text classification problem with about 1000 attributes representing word frequencies. The <em>y</em>-axis shows the observed relative frequency of the target class. The predicted probabilities and relative frequencies were collected by running a tenfold cross-validation. To estimate relative frequencies, the predicted probabilities were first discretized into 20 ranges using equal-frequency discretization. Observations corresponding to one interval were then pooled—predicted probabilities on the one hand and corresponding 0/1 values on the other—and the pooled values are shown as the 20 points in the plot.</p>
<p id="f0040" class="figure"><img src="https://www.safaribooksonline.com/library/view/data-mining-practical/9780123748560/images/f000079f007-007-9780123748560.jpg" alt="image" width="750" height="555" data-mfp-src="/library/view/data-mining-practical/9780123748560/images/f000079f007-007-9780123748560.jpg"></p>
<p class="figure_legend"><span class="fig_num">FIGURE 7.7</span> Overoptimistic probability estimation for a two-class problem.</p>
<p id="p0860" class="para_indented"><a id="p345"></a>This kind of plot, known as a <em>reliability diagram</em>, shows how reliable the estimated probabilities are. For a well-calibrated class probability estimator, the observed curve will coincide with the diagonal. This is clearly not the case here. The Naïve Bayes model is too optimistic, generating probabilities that are too close to 0 and 1. This is not the only problem: The curve is quite far from the line that corresponds to the 0.5 threshold that is used for classification. This means that classification performance will be affected by the poor probability estimates that the model generates.</p>
<p id="p0865" class="para_indented">The fact that we seek a curve that lies close to the diagonal makes the remedy clear: Systematic misestimation should be corrected by using post hoc calibration of the probability estimates to map the empirically observed curve into a diagonal. A coarse way of doing this is to use the data from the reliability diagram directly for calibration, and map the predicted probabilities to the observed relative frequencies in the corresponding discretization intervals. Data for this can be obtained using internal cross-validation or a holdout set so that the actual test data remains untouched.</p>
<p id="p0870" class="para_indented">Discretization-based calibration is very fast. However, determining appropriate discretization intervals is not easy. With too few, the mapping is too coarse; with too many, each interval contains insufficient data for a reliable estimate of relative frequencies. However, other ways of calibrating can be devised. The key is to realize that calibrating probability estimates for two-class problems is a function estimation problem with one input—the estimated class probability—and one output—the calibrated probability. In principle, complex functions could be used to estimate the mapping—perhaps arbitrary polynomials. However, it makes sense to assume that the observed relationship is at least monotonically increasing, in which case increasing functions should be used.</p>
<p id="p0875" class="para_indented">Assuming that the calibration function is piecewise constant and monotonically increasing, there is an efficient algorithm that minimizes the squared error between the observed class “probabilities” (which are either 0 or 1 when no binning is applied) and the resulting calibrated class probabilities. Estimating a piecewise constant monotonically increasing function is an instance of <em>isotonic regression</em>, for which there is a fast algorithm based on the pair-adjacent violators (PAV) approach. The data consists of estimated probabilities and 0/1 values; assume it has been sorted according to the estimated probabilities. The basic PAV algorithm iteratively merges pairs of neighboring data points that violate the monotonicity constraint by computing their weighted mean—initially this will be the mean of 0/1 values—and using it to replace the original data points. This is repeated until all conflicts have been resolved. It can be shown that the order in which data points are merged does not affect the outcome of the process. The result is a function that increases monotonically in a stepwise fashion. This naïve algorithm is quadratic in the number of data points, but there is a clever variant that operates in linear time.</p>
<p id="p0880" class="para_indented">Another popular calibration method, which also presupposes a monotonic relationship, is to assume a linear relation between the log-odds of the estimated class probabilities and the target class probabilities. The logistic function is appropriate <a id="p346"></a>here, and logistic regression can be used to estimate the calibration function, with the caveat that it is important to use log-odds of the estimated class probabilities rather than the raw values as the input for logistic regression.</p>
<p id="p0885" class="para_indented">Given that logistic regression, with only two parameters, uses a simpler model than the PAV approach, it can be more appropriate when little data is available for calibration. However, with a large volume of data, PAV-based calibration is generally preferable. Logistic regression has the advantage that it can be easily applied to calibrate probabilities for multiclass problems because multiclass versions of logistic regression exist. In the case of isotonic regression it is common to use the one-vs.-rest method for problems with more than two classes, but pairwise coupling or ensembles of nested dichotomies—discussed in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#s0135">Section 7.6</a>—offer an alternative.</p>
<p id="p0890" class="para_indented">Note that situations exist in which the relationship between the estimated and true probabilities is not monotonic. However, rather than switching to a more complex calibration method—or using discretization-based calibration, which does not assume monotonicity—this should perhaps be taken as an indication that the underlying class probability estimation method is not powerful enough for the problem at hand.</p>
</div>
<div id="s0160">
<h2 id="st0160">7.8 Further reading</h2>
<p id="p0895" class="noindent">Attribute selection, under the term <em>feature selection</em>, has been investigated in the field of pattern recognition for decades. Backward elimination, for example, was introduced in the early 1960s (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib217">Marill and Green, 1963</a>). <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib180">Kittler (1978)</a> surveys the feature-selection algorithms that have been developed for pattern recognition. Best-first search and genetic algorithms are standard artificial intelligence techniques (<a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib318">Winston, 1992</a>; <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib139">Goldberg, 1989</a>).</p>
<p id="p0900" class="para_indented">The experiments that show the performance of decision tree learners deteriorating when new attributes are added are reported by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib169">John (1997)</a>, who gives a nice explanation of attribute selection. The idea of finding the smallest attribute set that carves up the instances uniquely is from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib7">Almuallin and Dietterich (1991, 1992)</a> and was further developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib213">Liu and Setiono (1996)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib176">Kibler and Aha (1987)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib49">Cardie (1993)</a> both investigated the use of decision tree algorithms to identify features for nearest-neighbor learning; <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib162">Holmes and Nevill-Manning (1995)</a> used OneR to order features for selection. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib178">Kira and Rendell (1992)</a> used instance-based methods to select features, leading to a scheme called Relief for <em>Recursive Elimination of Features</em>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib136">Gilad-Bachrach et al. (2004)</a> show how this scheme can be modified to work better with redundant attributes. The correlation-based feature-selection method was developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib146">Hall (2000)</a>.</p>
<p id="p0905" class="para_indented">The use of wrapper methods for feature selection is from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib170">John et al. (1994)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib187">Kohavi and John (1997)</a>, and genetic algorithms have been applied within a wrapper framework by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib301">Vafaie and DeJong (1992)</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib59">Cherkauer and Shavlik (1996)</a>. The selective Naïve Bayes learning scheme is from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib202">Langley and Sage (1994)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib145">Guyon et al. (2002)</a> present and evaluate the recursive feature-elimination <a id="p347"></a>scheme in conjunction with support vector machines. The method of raced search was developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib232">Moore and Lee (1994)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib144">Gütlein et al. (2009)</a> investigate how to speed up scheme-specific selection for datasets with many attributes using simple ranking-based methods.</p>
<p id="p0910" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib86">Dougherty et al. (1995)</a> give a brief account of supervised and unsupervised discretization, along with experimental results comparing the entropy-based method with equal-width binning and the OneR method. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib114">Frank and Witten (1999)</a> describe the effect of using the ordering information in discretized attributes. Proportional <em>k</em>-interval discretization for Naïve Bayes was proposed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib329">Yang and Webb (2001)</a>. The entropy-based method for discretization, including the use of the MDL stopping criterion, was developed by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib96">Fayyad and Irani (1993)</a>. The bottom-up statistical method using the χ<sup>2</sup> test is from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib175">Kerber (1992)</a>, and its extension to an automatically determined significance level is described by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib214">Liu and Setiono (1997)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib125">Fulton et al. (1995)</a> investigate the use of dynamic programming for discretization and derive the quadratic time bound for a general impurity function (e.g., entropy) and the linear one for error-based discretization. The example used for showing the weakness of error-based discretization is adapted from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib190">Kohavi and Sahami (1996)</a>, who were the first to clearly identify this phenomenon.</p>
<p id="p0915" class="para_indented">Principal components analysis is a standard technique that can be found in most statistics textbooks. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib106">Fradkin and Madigan (2003)</a> analyze the performance of random projections. The algorithm for partial least-squares regression is from <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib156">Hastie et al. (2009)</a>. The TF × IDF metric is described by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib321">Witten et al. (1999b)</a>.</p>
<p id="p0920" class="para_indented">The experiments on using C4.5 to filter its own training data were reported by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib168">John (1995)</a>. The more conservative approach of a consensus filter involving several different learning algorithms has been investigated by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib44">Brodley and Friedl (1996)</a>. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib269">Rousseeuw and Leroy (1987)</a> describe the detection of outliers in statistical regression, including the least median of squares method; they also present the telephone data of <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#f0035">Figure 7.6</a>. It was <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib255">Quinlan (1986)</a> who noticed that removing noise from the training instance’s attributes can decrease a classifier’s performance on similarly noisy test instances, particularly at higher noise levels.</p>
<p id="p0925" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib16">Barnett and Lewis (1994)</a> address the general topic of outliers in statistical data, while <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib248">Pearson (2005)</a> describes the statistical approach of fitting a distribution to the target data. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib277">Schölkopf et al. (2000)</a> describe the use of support vector machines for novelty detection, while <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib1">Abe et al. (2006)</a>, among others, use artificial data as a second class. Combining density estimation and class probability estimation using artificial data is suggested as a generic approach to unsupervised learning by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib156">Hastie et al. (2009)</a>, and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib158">Hempstalk et al. (2008)</a> describe it in the context of one-class classification. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib159">Hempstalk and Frank (2008)</a> discuss the fair comparison of one-class and multiclass classification when several classes are available at training time and we want to discriminate against an entirely new class at prediction time.</p>
<p id="p0930" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib304">Vitter (1985)</a> explored the idea of reservoir sampling; he called the method we described algorithm <em>R</em>. Its computational complexity is O(<em>N</em>), where <em>N</em> is the number of instances in the stream, because a random number must be generated for every instance in order to determine whether, and where, to place it in the reservoir. Vitter <a id="p348"></a>describes several other algorithms that improve on <em>R</em> by reducing the number of random numbers that must be generated in order to produce the sample.</p>
<p id="p0935" class="para_indented"><a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib265">Rifkin and Klautau (2004)</a> show that the one-vs.-rest method for multiclass classification can work well if appropriate parameter tuning is applied. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib120">Friedman (1996)</a> describes the technique of pairwise classification, <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib126">Fürnkranz (2002)</a> further analyzes it, and <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib155">Hastie and Tibshirani (1998)</a> extend it to estimate probabilities using pairwise coupling. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib127">Fürnkranz (2003)</a> evaluates pairwise classification as a technique for ensemble learning. The idea of using error-correcting output codes for classification gained wide acceptance after a paper by <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib79">Dietterich and Bakiri (1995)</a>; <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib263">Ricci and Aha (1998)</a> showed how to apply such codes to nearest-neighbor classifiers. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib115">Frank and Kramer (2004)</a> introduce ensembles of nested dichotomies for multiclass problems. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib85">Dong et al. (2005)</a> considered using balanced nested dichotomies rather than unrestricted random hierarchies to reduce training time.</p>
<p id="p0940" class="para_indented">The importance of methods for calibrating class probability estimates is now well-established. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib332">Zadrozny and Elkan (2002)</a> applied the PAV approach and logistic regression to calibration, and also investigated how to deal with multiclass problems. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib238">Niculescu-Mizil and Caruana (2005)</a> compared a variant of logistic regression and the PAV-based method in conjunction with a large set of underlying class probability estimators, and found that the latter is preferable for sufficiently large calibration sets. They also found that multilayer perceptrons and bagged decision trees produce well-calibrated probabilities and do not require an extra calibration step. <a href="/site/library/view/data-mining-practical/9780123748560/html/bib00023.html#bib291">Stout (2008)</a> describes a linear-time algorithm for isotonic regression based on minimizing the squared error.</p>
</div>
<div id="s0165">
<h2 id="st0165">7.9 Weka implementations</h2><a id="p0945"></a><div class="none">
<p class="hang" id="u0010"><a id="p0950"></a>Attribute selection (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0265">Section 11.8</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0050">Tables 11.9</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0055">11.10</a>):</p>
<div class="none">
<p class="hang1" id="u0015">• <a id="p0955"></a><em>CfsSubsetEval</em> (correlation-based attribute subset evaluator)</p>
<p class="hang1" id="u0020">• <a id="p0960"></a><em>ConsistencySubsetEval</em> (measures class consistency for a given set of attributes)</p>
<p class="hang1" id="u0025">• <a id="p0965"></a><em>ClassifierSubsetEval</em> (uses a classifier for evaluating subsets of attributes)</p>
<p class="hang1" id="u0030">• <a id="p0970"></a><em>SVMAttributeEval</em> (ranks attributes according to the magnitude of the coefficients learned by a support vector machine)</p>
<p class="hang1" id="u0035">• <a id="p0975"></a><em>ReliefF</em> (instance-based approach for ranking attributes)</p>
<p class="hang1" id="u0040">• <a id="p0980"></a><em>WrapperSubsetEval</em> (uses a classifier plus cross-validation)</p>
<p class="hang1" id="u0045">• <a id="p0985"></a><em>GreedyStepwise</em> (forward selection and backward elimination search)</p>
<p class="hang1" id="u0050">• <a id="p0990"></a><em>LinearForwardSelection</em> (forward selection with a sliding window of attribute choices at each step of the search)</p>
<p class="hang1" id="u0055">• <a id="p0995"></a><em>BestFirst</em> (search method that uses greedy hill-climbing with backtracking)</p>
<p class="hang1" id="u0060">• <a id="p1000"></a><em>RaceSearch</em> (uses the race search methodology)</p>
<p class="hang1" id="u0065">• <a id="p1005"></a><em>Ranker</em> (ranks individual attributes according to their evaluation)</p>
</div>
<p class="hang" id="u0070"><a id="p1010"></a><a id="p349"></a>Learning decision tables—<em>DecisionTable</em> (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0175">Section 11.4</a> and <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0030">Table 11.5</a>)</p>
<div class="none">
<p class="hang1" id="u0075"><a id="p1020"></a>Discretization (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0100">Section 11.3</a>):</p>
<div class="none">
<p class="hang2" id="u0080">• <a id="p1025"></a><em>Discretize</em> in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0010">Table 11.1</a> (provides a variety of options for unsupervised discretization)</p>
<p class="hang2" id="u0085">• <a id="p9010"></a><em>PKIDiscretize</em> in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0010">Table 11.1</a> (proportional <em>k</em>-interval discretization)</p>
<p class="hang2" id="u0090">• <a id="p1030"></a><em>Discretize</em> in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0020">Table 11.3</a> (provides a variety of options for supervised discretization)</p>
</div>
</div>
<p class="hang" id="u0095"><a id="p1035"></a>Other data transformation operations (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#s0100">Section 11.3</a>):</p>
<div class="none">
<p class="hang1" id="u0100">• <a id="p1040"></a><em>PrincipalComponents</em> and <em>RandomProjection</em> in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0010">Table 11.1</a> (principal components analysis and random projections)</p>
<p class="hang1" id="u0105">• <a id="p1045"></a>Operations in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0010">Table 11.1</a> include arithmetic operations; time-series operations; obfuscation; generating cluster membership values; adding noise; various conversions between numeric, binary, and nominal attributes; and various data-cleansing operations.</p>
<p class="hang1" id="u0110">• <a id="p1050"></a>Operations in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0015">Table 11.2</a> include resampling and reservoir sampling.</p>
<p class="hang1" id="u0115">• <a id="p1055"></a>Operations in <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0020">Table 11.3</a> include partial least-squares transformation.</p>
<p class="hang1" id="u0120">• <a id="p1060"></a><em>MultiClassClassifier</em> (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0035">Table 11.6</a>; includes many ways of handling multiclass problems with two-class classifiers, including error-correcting output codes)</p>
<p class="hang1" id="u0125">• <a id="p1065"></a><a id="p350"></a><em>END</em> (see <a href="/site/library/view/data-mining-practical/9780123748560/html/c0011.html#t0035">Table 11.6</a>; ensembles of nested dichotomies)</p>
</div>
</div>
</div>
<div class="footnote">
<p class="footnote" id="fn0010"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#cc000079fn0010" class="totri-footnote"><span class="sup">1</span></a> A <em>hapax legomena</em> is a word that only occurs once in a given corpus of text.</p>
</div>
</div>
<div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0006a.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">6.8. Clustering</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/data-mining-practical/9780123748560/html/c0008.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Chapter 8. Ensemble Learning</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 68.0057px;">
    <a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    

<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.980972146466198"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.8516751073473849" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=2c780f5c-cb81-a5cb-9aed-c1c1aac97b49&amp;pi=-371479882&amp;lg=en-US&amp;sw=1280&amp;sh=800&amp;sc=24&amp;tl=Chapter%207.%20Data%20Transformations%20-%20Data%20Mining%3A%20Practical%20Machine%20Learning%20Tools%20and%20Techniques,%203rd%20Edition&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdata-mining-practical%2F9780123748560%2Fhtml%2Fc0007.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=660971"></div>



    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.042px; left: 1081px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/data-mining-practical/9780123748560/html/c0007.html#">Reset</a>
</div>
</div></body></html>