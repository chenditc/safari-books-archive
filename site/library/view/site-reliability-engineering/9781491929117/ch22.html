<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage"" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/site-reliability-engineering/9781491929117/ch22.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="859452"
  data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36"
  data-username="dchen267"
  data-account-type="B2B"
  
  data-activated-trial-date="04/25/2016"


  data-archive="9781491929117"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch22.html"
  data-epub-title="Site Reliability Engineering" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" "="" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/site-reliability-engineering/9781491929117/ch22.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="859452" data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36" data-username="dchen267" data-account-type="B2B" data-activated-trial-date="04/25/2016" data-archive="9781491929117" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch22.html" data-epub-title="Site Reliability Engineering" data-debug="0" data-testing="0"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491929117"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.8cc2fd27400e.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>22. Addressing Cascading Failures - Site Reliability Engineering</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/c1c7ad294784.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.min.fd58f69f4908.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content font,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}@page{margin:5px !important}#sbo-rt-content p{margin:8px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0}#sbo-rt-content div.preface p.byline+p.byline{margin:0}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content sup{font-size:x-small;vertical-align:super}#sbo-rt-content sub{font-size:smaller;vertical-align:sub}#sbo-rt-content span.lineannotation{font-style:italic;color:#A62A2A;font-family:serif,"DejaVuSerif"}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#FFF}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;padding:15px 5px 15px 5px !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding-top:10px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.informalfigure{text-align:center;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;background-color:#F7F7F7;font-size:90%;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:15px}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div.caution,#sbo-rt-content div.sidebar div.important{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content div.sidebar div.figure,#sbo-rt-content aside[data-type="sidebar"] figure{border:none}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-family:sans-serif;font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle{font-size:1em;font-weight:normal;text-align:center}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1em;font-family:serif,"DejaVuSerif";font-weight:bold;color:#8e0012;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{font-size:1em;font-family:serif,"DejaVuSerif";font-weight:normal;text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);position:absolute;bottom:0;max-width:100%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif,"DejaVuSerif";font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif,"DejaVuSerif";margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10pt}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-family:serif,"DejaVuSerif";font-style:italic}#sbo-rt-content blockquote div.attribution{margin:5px 0 0 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p{font-style:normal}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{font-size:85%;border-top:2px solid black;padding-left:1.5em;margin-top:1.5em}#sbo-rt-content p[data-type="footnote"]{margin-top:-.5em}#sbo-rt-content p[data-type="footnote"] sup{left:-1em;top:1.2em;position:relative;display:block}#sbo-rt-content div.refnamediv h2,#sbo-rt-content div.refnamediv h3,#sbo-rt-content div.refsynopsisdiv h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refentry div.refsect1 h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refsect2 h3{font-size:1em;color:#000;margin-top:10px !important;margin-bottom:0 !important}#sbo-rt-content div.refnamediv p{margin-left:15px !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dt span.term{font-weight:bold;font-style:italic}#sbo-rt-content dt span.term code.literal{font-style:normal;font-weight:normal}#sbo-rt-content dd{margin-left:1.5em !important}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ol{list-style-type:decimal;margin-top:8px !important;margin-bottom:8px !important;margin-left:20px !important;padding-left:25px !important}#sbo-rt-content ol ol{list-style-type:lower-alpha}#sbo-rt-content ol ol ol{list-style-type:lower-roman}#sbo-rt-content ul{list-style-type:square;margin-top:8px !important;margin-bottom:8px !important;margin-left:5px !important;padding-left:20px !important}#sbo-rt-content ul ul{list-style-type:none;padding-left:0 !important;margin-left:0 !important}#sbo-rt-content ol li,#sbo-rt-content ul li,#sbo-rt-content dd{margin-bottom:.25em}#sbo-rt-content ul ul li p:before{content:"— "}#sbo-rt-content ul ul ul li p:before{content:""}#sbo-rt-content ul ul ul{list-style-type:square;margin-left:20px !important;padding-left:30px !important}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist{margin-left:20px !important;margin-bottom:10px}#sbo-rt-content table.simplelist td{border:none;font-size:90%}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img{padding:0}#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;-webkit-border-radius:5px;border-radius:5px;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{font-weight:bold;font-size:110%;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;-webkit-border-radius:0;border-radius:0;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px auto 30px auto !important;max-width:95%;border-collapse:collapse;border-spacing:0}#sbo-rt-content div.table,#sbo-rt-content div.informaltable{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif,"DejaVuSans";color:#000;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{padding:.3em;text-align:left;vertical-align:baseline;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";margin:10px 0 10px 0 !important;text-align:center;padding:0;page-break-after:avoid}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{font-weight:bold;text-indent:0}#sbo-rt-content div.index li{line-height:140%}#sbo-rt-content div.index a.indexterm{color:#8e0012}#sbo-rt-content div.index ul{list-style-type:none;padding-left:0;margin-left:0}#sbo-rt-content div.index ul li{padding-left:0;margin-left:0}#sbo-rt-content div.index ul li ul li{margin-left:20px}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif,"DejaVuSerif";text-align:left}#sbo-rt-content span.roman_text{font-style:normal !important}
    </style><link rel="canonical" href="/library/view/site-reliability-engineering/9781491929117/ch22.html"><meta name="description" content=" Chapter 22. Addressing Cascading Failures Written by Mike Ulrich If at first you don’t succeed, back off exponentially. Dan Sandler, Google Software Engineer Why do people always forget that ... "><meta property="og:title" content="22. Addressing Cascading Failures"><meta itemprop="isPartOf" content="/library/view/site-reliability-engineering/9781491929117/"><meta itemprop="name" content="22. Addressing Cascading Failures"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch22.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491929117/"><meta property="og:description" itemprop="description" content=" Chapter 22. Addressing Cascading Failures Written by Mike Ulrich If at first you don’t succeed, back off exponentially. Dan Sandler, Google Software Engineer Why do people always forget that ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781491929124"><meta property="og:book:author" itemprop="author" content="Niall Richard Murphy"><meta property="og:book:author" itemprop="author" content="Jennifer Petoff"><meta property="og:book:author" itemprop="author" content="Chris Jones"><meta property="og:book:author" itemprop="author" content="Betsy Beyer"><meta property="og:book:tag" itemprop="about" content="Engineering"><meta property="og:book:tag" itemprop="about" content="Networking"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><!--[if lt IE 9]><script src="/static/js/src/respond.min.cf5c9b7980e5.js"></script><![endif]--></head>


<body class="reading sidenav nav-collapsed  js-show-related scalefonts" data-gr-c-s-loaded="true">

    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        




<a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="20" height="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z" fill="currentColor"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/s/" class="t-queue-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z" fill="currentColor"></path></g></svg><span>Queue</span></a></li><li class="search"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z" fill="currentColor"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z" fill="currentColor"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z" fill="currentColor"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z" fill="currentColor"></path></g></svg><span>Tutorials</span></a></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/003o000000t5q9fAAA/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z" fill="currentColor"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l1 no-icon">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l2">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Site Reliability Engineering
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781491929117/chapter/ch22.html"><span>Add to Queue</span></button></li><li class="js-font-control-panel font-control-activator"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch22.html&amp;text=Site%20Reliability%20Engineering&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch22.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch22.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%2022.%20Addressing%20Cascading%20Failures&amp;body=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch22.html%0D%0Afrom%20Site%20Reliability%20Engineering%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
      
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/library/view/site-reliability-engineering/9781491929117/ch21.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">21. Handling Overload</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/library/view/site-reliability-engineering/9781491929117/ch23.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">23. Managing Critical State: Distributed Consensus for Reliability</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 22. Addressing Cascading Failures"><div class="chapter" id="chapter_cascading-failure">
<h1><span class="label">Chapter 22. </span>Addressing Cascading Failures</h1>


<p class="byline">Written by Mike Ulrich</p>
<blockquote data-type="epigraph" epub:type="epigraph">
<p>If at first you don’t succeed, back off exponentially.</p>
<p data-type="attribution">Dan Sandler, Google Software Engineer</p>
</blockquote>
<blockquote data-type="epigraph" epub:type="epigraph">
<p>Why do people always forget that you need to add a little jitter?</p>
<p data-type="attribution">Ade Oshineye, Google Developer Advocate</p>
</blockquote>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="defined" id="idm140203533200608"></a>A cascading failure is a failure that grows over time as a result of
positive feedback.<sup><a data-type="noteref" id="idm140203533199440-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533199440">1</a></sup>  It can occur when a
portion of an overall system fails, increasing the probability that
other portions of the system fail.  For example, a single replica for
a service can fail due to overload, increasing load on remaining
replicas and increasing their probability of failing, causing
a domino effect that takes down all the replicas for a service.</p>

<p><a data-type="indexterm" data-primary="Shakespeare search service, example" data-secondary="cascading failure example" id="SSScascad22"></a>We’ll use the Shakespeare search service discussed in
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch02.html#xref_production-environment_shakespeare">“Shakespeare: A Sample Service”</a> as an example throughout
this chapter.  Its production configuration might look something like
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#fig_cascading-failure_basic-architecture">Figure&nbsp;22-1</a>.</p>

<figure><div id="fig_cascading-failure_basic-architecture" class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_2201.png" alt="Example production configuration for the Shakespeare search service." width="986" height="850">
<h6><span class="label">Figure 22-1. </span>Example production configuration for the Shakespeare search service</h6>
</div></figure>






<section data-type="sect1" data-pdf-bookmark="Causes of Cascading Failures and Designing to Avoid Them"><div class="sect1" id="idm140203533191264">
<h1>Causes of Cascading Failures and Designing to Avoid Them</h1>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="causes of" id="CFcause22"></a>Well-thought-out system design should take into account a few typical scenarios that account for the majority of cascading failures.</p>








<section data-type="sect2" data-pdf-bookmark="Server Overload"><div class="sect2" id="xref_cascading-failure_server-overload">
<h2>Server Overload</h2>

<p><a data-type="indexterm" data-primary="servers" data-secondary="overload scenario" id="idm140203533187008"></a>The most common cause of cascading failures is overload.
Most cascading failures described here are either directly
due to server overload, or due to extensions or variations of this
scenario.</p>

<p>Suppose the frontend in cluster A is handling 1,000 requests per second
(QPS), as in <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#fig_cascading-failure_load-distribution">Figure&nbsp;22-2</a>.</p>

<figure><div id="fig_cascading-failure_load-distribution" class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_2202.png" alt="Normal server load distribution between clusters A and B." width="986" height="565">
<h6><span class="label">Figure 22-2. </span>Normal server load distribution between clusters A and B</h6>
</div></figure>

<p>If cluster B fails (<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#fig_cascading-failure_failure-load-distribution">Figure&nbsp;22-3</a>),
requests to cluster A increase to 1,200
QPS.  The <span class="keep-together">frontends</span> in A are not able to handle requests at 1,200 QPS,
and therefore start running out of resources, which causes them to
crash, miss deadlines, or otherwise misbehave. As a result, the rate
of successfully handled requests in A dips well below 1,000 QPS.</p>

<figure><div id="fig_cascading-failure_failure-load-distribution" class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_2203.png" alt="Cluster B fails, sending all traffic to cluster A." width="986" height="565">
<h6><span class="label">Figure 22-3. </span>Cluster B fails, sending all traffic to cluster A</h6>
</div></figure>

<p>This reduction in the rate of useful work being done can spread into
other failure domains, potentially spreading globally.  For example,
local overload in one cluster may lead to its servers crashing; in
response, the load balancing controller sends requests to other
clusters, overloading their servers, leading to a service-wide
overload failure.  It may not take long for these events to transpire
(e.g., on the order of a couple minutes), because the load balancer and
task scheduling systems involved may act very quickly.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Resource Exhaustion"><div class="sect2" id="idm140203533176432">
<h2>Resource Exhaustion</h2>

<p><a data-type="indexterm" data-primary="resources" data-secondary="exhaustion" id="idm140203533175296"></a>Running out of a resource can result in higher latency, elevated error
rates, or the substitution of lower-quality results.  These are in
fact desired effects of running out of resources: something eventually
needs to give as the load increases beyond what a server can handle.</p>

<p>Depending on what resource becomes exhausted in a server and how the
server is built, resource
exhaustion can render the server less efficient or cause the server to
crash, prompting the load balancer to distribute the resource problems
to other servers.  When this happens, the rate of successfully handled
requests can drop and possibly send the cluster or an entire service
into a cascade failure.</p>

<p>Different types of resources can be exhausted, resulting in varying
effects on servers.</p>










<section data-type="sect3" data-pdf-bookmark="CPU"><div class="sect3" id="idm140203533172432">
<h3>CPU</h3>

<p><a data-type="indexterm" data-primary="CPU consumption" id="idm140203533171296"></a>If there is insufficient CPU to handle the request load, typically all
requests become slower.  This scenario can result in various secondary
effects, including the following:</p>
<dl>
<dt>Increased number of in-flight requests</dt>
<dd>
<p>Because requests take
longer to handle, more requests are handled concurrently (up to a
possible maximum capacity at which queuing may occur).  This
affects almost all resources, including memory, number of active
threads (in a thread-per-request server model), number of file
descriptors, and backend resources (which in turn can have other
effects).</p>
</dd>
<dt>Excessively long queue lengths</dt>
<dd>
<p>If there is insufficient capacity
to handle all the requests at steady state, the server will
saturate its queues.  This means that latency increases (the
requests are queued for longer amounts of time) and the queue uses
more memory.  See <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_queue-management">“Queue Management”</a> for a
discussion of mitigation strategies.</p>
</dd>
<dt>Thread starvation</dt>
<dd>
<p>When a thread can’t make progress because it’s
waiting for a lock, health checks may fail if the health check
endpoint can’t be served in time.</p>
</dd>
<dt>CPU or request starvation</dt>
<dd>
<p>Internal watchdogs<sup><a data-type="noteref" id="idm140203533163168-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533163168">2</a></sup> in the server detect that
the server isn’t making progress, causing the servers to crash due
to CPU starvation, or due to request starvation if watchdog events
are triggered remotely and processed as part of the request queue.</p>
</dd>
<dt>Missed RPC deadlines</dt>
<dd>
<p>As a server becomes overloaded, its responses to RPCs from its clients arrive later, which may exceed any deadlines those clients set.  The work the server did to respond is then wasted, and clients may retry the RPCs, leading to even more overload.</p>
</dd>
<dt>Reduced CPU caching benefits</dt>
<dd>
<p>As more CPU is used, the chance of
spilling on to more cores increases, resulting in decreased usage
of local caches and decreased CPU efficiency.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Memory"><div class="sect3" id="idm140203533158096">
<h3>Memory</h3>

<p><a data-type="indexterm" data-primary="memory exhaustion" id="idm140203533156960"></a>If nothing else, more in-flight requests consume more RAM from
allocating the request, response, and RPC objects.  Memory exhaustion
can cause the following effects:</p>
<dl>
<dt>Dying tasks</dt>
<dd>
<p>For example, a task might be evicted by the
container manager (VM or otherwise) for exceeding available
resource limits, or application-specific crashes may cause tasks
to die.</p>
</dd>
<dt>Increased rate of garbage collection (GC) in Java, resulting in increased CPU usage</dt>
<dd>
<p>A vicious cycle can occur in this scenario:
    less CPU is available, resulting in slower requests, resulting in
    increased RAM usage, resulting in more GC, resulting in even lower
    availability of CPU.  This is known colloquially as the “GC death
    spiral.”</p>
</dd>
<dt>Reduction in cache hit rates</dt>
<dd>
<p>Reduction in available RAM can
reduce application-level cache hit rates, resulting in more RPCs
to the backends, which can possibly cause the backends to become
overloaded.</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Threads"><div class="sect3" id="idm140203533150528">
<h3>Threads</h3>

<p><a data-type="indexterm" data-primary="thread starvation" id="idm140203533149392"></a>Thread starvation can directly cause errors or lead to health check
failures. If the server adds threads as needed, thread overhead can
use too much RAM. In extreme cases, thread starvation can also cause
you to run out of process IDs.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="File descriptors"><div class="sect3" id="idm140203533148016">
<h3>File descriptors</h3>

<p><a data-type="indexterm" data-primary="file descriptors" id="idm140203533146880"></a>Running out of file descriptors can lead to the inability to
initialize network connections, which in turn can cause health checks
to fail.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Dependencies among resources"><div class="sect3" id="idm140203533145600">
<h3>Dependencies among resources</h3>

<p><a data-type="indexterm" data-primary="dependency hierarchies" id="idm140203533144448"></a>Note that many of these resource exhaustion scenarios feed from one
another—a service experiencing overload often has a host of secondary
symptoms that can look like the root cause, making debugging
difficult.</p>

<p>For example, imagine the following scenario:</p>
<ol>
<li>
<p>A Java frontend has poorly tuned garbage collection (GC)
parameters.</p>
</li>
<li>
<p>Under high (but expected) load, the frontend runs out of CPU due to
GC.</p>
</li>
<li>
<p>CPU exhaustion slows down completion of requests.</p>
</li>
<li>
<p>The increased number of in-progress requests causes more RAM to be
used to process the requests.</p>
</li>
<li>
<p>Memory pressure due to requests, in combination with a fixed memory
allocation for the frontend process as a whole, leaves less RAM
available for caching.</p>
</li>
<li>
<p>The reduced cache size means fewer entries in the cache, in
addition to a lower hit rate.</p>
</li>
<li>
<p>The increase in cache misses means that more requests fall through
to the backend for servicing.</p>
</li>
<li>
<p>The backend, in turn, runs out of CPU or threads.</p>
</li>
<li>
<p>Finally, the lack of CPU causes basic health checks to fail,
starting a cascading failure.</p>
</li>

</ol>

<p>In situations as complex as the preceding scenario, it’s unlikely that the
causal chain will be fully diagnosed during an outage.  It might be
very hard to determine that the backend crash was caused by a decrease
in the cache rate in the frontend, particularly if the frontend and
backend components have different owners.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Service Unavailability"><div class="sect2" id="idm140203533132576">
<h2>Service Unavailability</h2>

<p><a data-type="indexterm" data-primary="service unavailability" id="idm140203533131440"></a>Resource exhaustion
can lead to servers crashing; for example, servers might crash when
too much RAM is allocated to a container.  Once a couple of servers
crash on overload, the load on the remaining servers can increase,
causing them to crash as well.  The problem tends to snowball and soon
all servers begin to crash-loop.  It’s often difficult to escape this
scenario because as soon as servers come back online they’re bombarded
with an extremely high rate of requests and fail almost immediately.</p>

<p>For example, if a service was healthy at 10,000 QPS, but started a
cascading failure due to crashes at 11,000 QPS, dropping the load to
9,000 QPS will almost certainly not stop the crashes.  This is because
the service will be handling increased demand with reduced capacity;
only a small fraction of servers will usually be healthy
enough to handle requests. The fraction of servers that will be
healthy depends on a few factors: how quickly the system is able to
start the tasks, how quickly the binary can start serving at full
capacity, and how long a freshly started task is able to survive the
load.  In this example, if 10% of the servers are healthy enough to
handle requests, the request rate would need to drop to about 1,000
QPS in order for the system to stabilize and recover.</p>

<p>Similarly, servers can appear unhealthy to the load balancing layer,
resulting in reduced load balancing capacity: servers
may go into “lame duck” state (see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch20.html#robust_approach_lame_duck">“A Robust Approach to Unhealthy Tasks: Lame Duck State”</a>) or fail health checks without crashing.
The effect can be very similar to crashing: more servers appear
unhealthy, the healthy servers tend to accept requests for a very
brief period of time before becoming unhealthy, and fewer servers
participate in handling requests.</p>

<p>Load balancing policies that avoid servers that have served errors can
exacerbate problems further—a few backends serve some errors, so they
don’t contribute to the available capacity for the service.  This
increases the load on the remaining servers, starting the snowball
effect.<a data-type="indexterm" data-primary="" data-startref="CFcause22" id="idm140203533126480"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Preventing Server Overload"><div class="sect1" id="xref_cascading-failure_preventing-server-overload">
<h1>Preventing Server Overload</h1>

<p><a data-type="indexterm" data-primary="servers" data-secondary="preventing overload" id="SPprevent22"></a><a data-type="indexterm" data-primary="cascading failures" data-secondary="preventing server overload" id="CFprevent22"></a>The following list presents strategies for avoiding server overload in
rough priority order:</p>
<dl>
<dt>Load test the server’s capacity limits, and test the failure mode for overload</dt>
<dd>
<p>This is the most important important exercise you
should conduct in order to prevent server overload. Unless you
test in a realistic environment, it’s very hard to predict exactly
which resource will be exhausted and how that resource exhaustion
will manifest.  For details, see
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_testing">“Testing for Cascading Failures”</a>.</p>
</dd>
<dt>Serve degraded results</dt>
<dd>
<p>Serve lower-quality, cheaper-to-compute
    results to the user. Your strategy here will be service-specific.
    See <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_load-shed-graceful-degredation">“Load Shedding and Graceful Degradation”</a>.</p>
</dd>
<dt>Instrument the server to reject requests when overloaded</dt>
<dd>
<p>Servers
    should protect themselves from becoming overloaded and crashing.
    When overloaded at either the frontend or backend layers, fail
    early and cheaply.  For details, see
    <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_load-shed-graceful-degredation">“Load Shedding and Graceful Degradation”</a>.</p>
</dd>
<dt>Instrument higher-level systems to reject requests, rather than overloading servers</dt>
<dd>
<p>Note that because rate limiting often
    doesn’t take overall service health into account, it may not be
    able to stop a failure that has already begun.  Simple
    rate-limiting implementations are also likely to leave capacity
    unused. Rate limiting can be implemented in a number of places:</p>

<ul>
<li>
<p><em>At the reverse proxies</em>, by limiting the volume of requests by
criteria such as IP address to mitigate attempted
denial-of-service attacks and abusive <span class="keep-together">clients</span>.</p>
</li>
<li>
<p><em>At the load balancers</em>, by dropping requests when the service
enters global overload.  Depending on the nature and complexity
of the service, this rate limiting can be indiscriminate (“drop
all traffic above X requests per second”) or more selective
(“drop requests that aren’t from users who have recently
interacted with the service” or “drop requests for low-priority
operations like background synchronization, but keep serving
interactive user <span class="keep-together">sessions</span>”).</p>
</li>
<li>
<p><em>At individual tasks</em>, to prevent random fluctuations in load
balancing from overwhelming the server.</p>
</li>
</ul>
</dd>
<dt>Perform capacity planning</dt>
<dd>
<p><a data-type="indexterm" data-primary="capacity planning" data-secondary="preventing server overload with" id="idm140203533104592"></a><a data-type="indexterm" data-primary="N + 2 configuration" id="idm140203533103632"></a>Good capacity planning can reduce the
    probability that a cascading failure will occur.  Capacity
    planning should be coupled with performance testing to determine
    the load at which the service will fail.  For instance, if every
    cluster’s breaking point is 5,000 QPS, the load is evenly spread across clusters,<sup><a data-type="noteref" id="idm140203533102496-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533102496">3</a></sup> and the service’s peak load
    is 19,000 QPS, then approximately six clusters are needed to run
    the service at <em>N</em> + 2.</p>
</dd>
</dl>

<p>Capacity planning reduces the probability of triggering a cascading
failure, but it is not sufficient to protect the service from
cascading failures.  When you lose major parts of your infrastructure
during a planned or unplanned event, no amount of capacity planning
may be sufficient to prevent cascading failures.  Load balancing
problems, network partitions, or unexpected traffic increases can
create pockets of high load beyond what was planned.  Some systems can
grow the number of tasks for your service on demand, which may prevent
overload; however, proper capacity planning is still needed.</p>








<section data-type="sect2" data-pdf-bookmark="Queue Management"><div class="sect2" id="xref_cascading-failure_queue-management">
<h2>Queue Management</h2>

<p><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="deadlines" data-tertiary="queue management" id="idm140203533097376"></a><a data-type="indexterm" data-primary="queuing" data-secondary="management of" id="idm140203533096144"></a>Most thread-per-request servers use a queue in front of a thread pool
to handle requests.  Requests come in, they sit on a queue, and then
threads pick requests off the queue and perform the actual work
(whatever actions are required by the server).  Usually, if the queue
is full, the server will reject new requests.</p>

<p>If the request rate and latency of a given task is constant, there is
no reason to queue requests: a constant number of threads should be
occupied.  Under this idealized scenario, requests will only be queued
if the steady state rate of incoming requests exceeds the rate at
which the server can process requests, which results in saturation of
both the thread pool and the queue.</p>

<p>Queued requests consume memory and increase latency.  For example, if
the queue size is 10x the number of threads, the time to handle the
request on a thread is 100 milliseconds. If the queue is full, then a request
will take 1.1 seconds to handle, most of which time is spent on the
queue.</p>

<p>For a system with fairly steady traffic over time, it is usually
better to have small queue lengths relative to the thread pool size
(e.g., 50% or less), which results in the server rejecting requests
early when it can’t sustain the rate of incoming requests.  For
example, Gmail often uses queueless servers, relying instead on
failover to other server tasks when the threads are full.  On the
other end of the spectrum, systems with “bursty” load for which
traffic patterns fluctuate drastically may do better with a queue size
based on the current number of threads in use, processing time for
each request, and the size and frequency of bursts.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Load Shedding and Graceful Degradation"><div class="sect2" id="xref_cascading-failure_load-shed-graceful-degredation">
<h2>Load Shedding and Graceful Degradation</h2>

<p><em>Load shedding</em> <a data-type="indexterm" data-primary="load shedding" id="idm140203533089888"></a>drops some proportion of load by dropping
 traffic as the server approaches overload conditions.  The goal is to
 keep the server from running out of RAM, failing health checks,
 serving with extremely high latency, or any of the other symptoms
 associated with overload, while still doing as much useful work
 as it can.</p>

<p>One straightforward way to shed load is to do per-task throttling
based on CPU, memory, or queue length; limiting queue length as
discussed in <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_queue-management">“Queue Management”</a> is a form of this strategy. For example,
one effective approach is to return an HTTP 503 (service unavailable)
to any incoming request when there are more than a given number of
client requests in flight.</p>

<p>Changing the queuing method from the standard <em>first-in, first-out</em>
(FIFO) <a data-type="indexterm" data-primary="queuing" data-secondary="first-in, first-out" id="idm140203533086192"></a> to <em>last-in, first-out</em>
(LIFO) <a data-type="indexterm" data-primary="queuing" data-secondary="last-in, first-out" id="idm140203533084640"></a> or using the <em>controlled
delay</em> (CoDel) algorithm <a data-type="indexterm" data-primary="queuing" data-secondary="controlled delay" id="idm140203533083088"></a> <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Nic12">[Nic12]</a> or similar approaches can reduce
load by removing requests that are unlikely to be worth processing
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Mau15">[Mau15]</a>.  If a user’s web search is slow because an RPC has been
queued for 10 seconds, there’s a good chance the user has given up and
refreshed their browser, issuing another request: there’s no point in
responding to the first one, since it will be ignored!  This strategy
works well when combined with propagating RPC deadlines
<a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="deadlines" data-tertiary="propagating" id="idm140203533079920"></a> throughout the stack, described in
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_latency-and-deadlines">“Latency and Deadlines”</a>.</p>

<p>More sophisticated approaches include identifying clients to be more
selective about what work is dropped, or picking requests that
are more important and prioritizing.  Such
strategies are more likely to be needed for shared services.</p>

<p><em>Graceful degradation</em> <a data-type="indexterm" data-primary="graceful degradation" id="idm140203533076240"></a>takes the concept of load shedding one step
 further by reducing the amount of work that needs to be performed. In
 some applications, it’s possible to significantly decrease the amount
 of work or time needed by decreasing the quality of responses.  For
 instance, a search application might only search a subset of data
 stored in an in-memory cache rather than the full on-disk database or
 use a less-accurate (but faster) ranking algorithm when overloaded.</p>

<p>When evaluating load shedding or graceful degradation options for your
service, consider the following:</p>

<ul>
<li>
<p>Which metrics should you use to determine when load shedding or
graceful degradation should kick in (e.g,. CPU usage, latency,
queue length, number of threads used, whether your service enters
degraded mode automatically or if manual intervention is
necessary)?</p>
</li>
<li>
<p>What actions should be taken when the server is in degraded mode?</p>
</li>
<li>
<p>At what layer should load shedding and graceful degradation be
implemented?  Does it make sense to implement these strategies at
every layer in the stack, or is it sufficient to have a high-level
choke-point?</p>
</li>
</ul>

<p>As you evaluate options and deploy, keep the following in mind:</p>

<ul>
<li>
<p>Graceful degradation shouldn’t trigger very often—usually
in cases of a capacity planning failure or
unexpected load shift.  Keep the system simple
and understandable, particularly if it isn’t used often.</p>
</li>
<li>
<p>Remember that the code path you never use is the code path that
(often) doesn’t work. In steady-state operation, graceful
degradation mode won’t be used, implying that you’ll have much less
operational experience with this mode and any of its quirks, which
<em>increases</em> the level of risk.  You can make sure that graceful
degradation stays working by regularly running a small subset of
servers near overload in order to exercise this code path.</p>
</li>
<li>
<p>Monitor and alert when too many servers enter these modes.</p>
</li>
<li>
<p>Complex load shedding and graceful degradation can cause
problems themselves—excessive complexity may cause the server to trip into a
degraded mode when it is not desired, or enter feedback cycles at undesired times.
Design a way to quickly turn off complex graceful degradation or
tune parameters if needed.  Storing this configuration
in a consistent system that each server can watch for changes, such
as Chubby, can increase deployment speed, but also introduces its
own risks of synchronized failure.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Retries"><div class="sect2" id="idm140203533064384">
<h2>Retries</h2>

<p><a data-type="indexterm" data-primary="retries, RPC" data-secondary="cascading failures due to" id="idm140203533063248"></a><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="retries" id="RPCretry22"></a>Suppose the code in the frontend that talks to the backend implements
retries naively.  It retries after encountering a failure and caps the
number of backend RPCs per logical request to 10.  Consider this code
in the frontend, using gRPC in Go:</p>

<pre data-type="programlisting">func exampleRpcCall(client pb.ExampleClient, request pb.Request) *pb.Response {

    // Set RPC timeout to 5 seconds.
    opts := grpc.WithTimeout(5 * time.Second)

    // Try up to 20 times to make the RPC call.
    attempts := 20
    for attempts &gt; 0 {
        conn, err := grpc.Dial(*serverAddr, opts...)
        if err != nil {
            // Something went wrong in setting up the connection.  Try again.
            attempts--
            continue
        }
        defer conn.Close()

        // Create a client stub and make the RPC call.
        client := pb.NewBackendClient(conn)
        response, err := client.MakeRequest(context.Background, request)
        if err != nil {
            // Something went wrong in making the call. Try again.
            attempts--
            continue
        }

        return response
    }

    grpclog.Fatalf("ran out of attempts")
}</pre>

<p>This system can cascade in the following way:</p>
<ol>
<li>
<p>Assume our backend has a known limit of 10,000 QPS per task, after
which point all further requests are rejected in an attempt at
graceful degradation.</p>
</li>
<li>
<p>The frontend calls <code>MakeRequest</code> at a constant rate of 10,100 QPS
and overloads the backend by 100 QPS, which the backend rejects.</p>
</li>
<li>
<p>Those 100 failed QPS are retried in <code>MakeRequest</code> every 1,000 ms,
and probably succeed.  But the retries are themselves adding to the
requests sent to the backend, which now receives 10,200 QPS—200
QPS of which are failing due to <span class="keep-together">overload</span>.</p>
</li>
<li>
<p>The volume of retries grows: 100 QPS of retries in the first second
leads to 200 QPS, then to 300 QPS, and so on. Fewer and fewer
requests are able to succeed on their first attempt, so less useful
work is being performed as a fraction of requests to the backend.</p>
</li>
<li>
<p>If the backend task is unable to handle the increase in load—which
is consuming file descriptors, memory, and CPU time on the backend—it
can melt down and crash under the sheer load of requests and retries.
This crash then redistributes the requests it was receiving across
the remaining backend tasks, in turn further overloading those tasks.</p>
</li>

</ol>

<p>Some simplifying assumptions were made here to illustrate this
scenario,<sup><a data-type="noteref" id="idm140203533050592-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533050592">4</a></sup> but the point remains that retries can destabilize a
system.  Note that both temporary load spikes and slow increases in
usage can cause this effect.</p>

<p>Even if the rate of calls to <code>MakeRequest</code> decreases to pre-meltdown
levels (9,000 QPS, for example), depending on how much returning a
failure costs the backend, the problem might not go away. Two factors
are at play here:</p>

<ul>
<li>
<p>If the backend spends a significant amount of resources processing
requests that will ultimately fail due to overload, then the
retries themselves may be keeping the backend in an overloaded
mode.</p>
</li>
<li>
<p>The backend servers themselves may not be stable.  Retries can
amplify the effects seen in
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_server-overload">“Server Overload”</a>.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="retries, RPC" data-secondary="considerations for automatic" id="idm140203533044848"></a>If either of these conditions is true, in order to dig out of this
outage, you must dramatically reduce or eliminate the load on the frontends until the retries stop and the backends stabilize.</p>

<p>This pattern has contributed to several cascading failures, whether
the frontends and backends communicate via RPC messages, the
“frontend” is client JavaScript code issuing <code>XmlHttpRequest</code> calls to
an endpoint and retries on failure, or the retries originate from
an offline sync protocol that retries aggressively when it encounters
a failure.</p>

<p>When issuing automatic retries, keep in mind the following considerations:</p>

<ul>
<li>
<p>Most of the backend protection strategies described in
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#xref_cascading-failure_preventing-server-overload">“Preventing Server Overload”</a> apply.  In
particular, testing the system can highlight problems, and graceful
degradation can reduce the effect of the retries on the backend.</p>
</li>
<li>
<p>Always use randomized exponential backoff when scheduling retries. See also <a href="http://www.awsarchitectureblog.com/2015/03/backoff.html">“Exponential Backoff and Jitter”</a> in the AWS Architecture Blog <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Bro15">[Bro15]</a>. If retries aren’t randomly distributed over the retry window, a
small perturbation (e.g., a network blip) can cause retry ripples
to schedule at the same time, which can then amplify themselves
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Flo94">[Flo94]</a>.</p>
</li>
<li>
<p>Limit retries per request.  Don’t retry a given request
indefinitely.</p>
</li>
<li>
<p>Consider having a server-wide retry budget.  For example, only
allow 60 retries per minute in a process, and if the retry budget
is exceeded, don’t retry; just fail the request.  This strategy can
contain the retry effect and be the difference between a capacity
planning failure that leads to some dropped queries and a global
cascading failure.</p>
</li>
<li>
<p>Think about the service holistically and decide if you really need
to perform retries at a given level.  In particular, avoid
amplifying retries by issuing retries at multiple levels: a single
request at the highest layer may produce a number of attempts as
large as the <em>product</em> of the number of attempts at each layer to
the lowest layer.  If the database can’t service
requests because it’s overloaded, and the backend, frontend, and
JavaScript layers all issue 3 retries (4 attempts), then a single
user action may create 64 attempts (<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/eq_32.png" alt="4 cubed" width="20" height="23">) on the database.  This
behavior is undesirable when the database is returning those errors
because it’s overloaded.</p>
</li>
<li>
<p>Use clear response codes and consider how different failure modes
should be handled.  For example, separate retriable and
nonretriable error conditions.  Don’t retry permanent errors or
malformed requests in a client, because neither will ever succeed.
Return a specific status when overloaded so that clients and other
layers back off and do not retry.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="retries, RPC" data-secondary="diagnosing outages due to" id="idm140203533029520"></a>In an emergency, it may not be obvious that an outage is due to bad
retry behavior.  Graphs of retry rates can be an indication of bad
retry behavior, but may be confused as a symptom instead of a
compounding cause.  In terms of mitigation, this is a special case of
the insufficient capacity problem, with the additional caveat that you
must either fix the retry behavior (usually requiring a code push),
reduce load significantly, or cut requests off entirely.<a data-type="indexterm" data-primary="" data-startref="RPCretry22" id="idm140203533027952"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Latency and Deadlines"><div class="sect2" id="xref_cascading-failure_latency-and-deadlines">
<h2>Latency and Deadlines</h2>

<p><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="deadlines" data-tertiary="selecting" id="idm140203533025248"></a>When a frontend sends an RPC to a backend server, the frontend
consumes resources waiting for a reply.  RPC deadlines define how long
a request can wait before the <span class="keep-together">frontend</span> gives up, limiting the
time that the backend may consume the frontend’s <span class="keep-together">resources</span>.</p>










<section data-type="sect3" data-pdf-bookmark="Picking a deadline"><div class="sect3" id="idm140203533022048">
<h3>Picking a deadline</h3>

<p>It’s usually wise to set a deadline.  Setting either no deadline or an
extremely high deadline may cause short-term problems that have long
since passed to continue to consume server resources until the server
restarts.</p>

<p>High deadlines can result in resource consumption in higher levels of
the stack when lower levels of the stack are having problems.  Short
deadlines can cause some more expensive requests to fail consistently.
Balancing these constraints to pick a good deadline can be something
of an art.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Missing deadlines"><div class="sect3" id="idm140203533019504">
<h3>Missing deadlines</h3>

<p><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="deadlines" data-tertiary="missing" id="idm140203533018368"></a>A common theme in many cascading outages is that servers spend
resources handling requests that will exceed their deadlines on the
client.  As a result, resources are spent while no progress is made:
you don’t get credit for late assignments with RPCs.</p>

<p>Suppose an RPC has a 10-second deadline, as set by the client.  The
server is very overloaded, and as a result, it takes 11 seconds to
move from a queue to a thread pool.  At this point, the client has
already given up on the request.  Under most circumstances, it would
be unwise for the server to attempt to handle this request, because it
would be doing work for which no credit will be granted—the client
doesn’t care what work the server does after the deadline has passed,
because it’s given up on the request already.</p>

<p>If handling a request is performed over multiple stages (e.g., there
are a few callbacks and RPC calls), the server should check the
deadline left at each stage before attempting to perform any more work
on the request.  For example, if a request is split into parsing,
backend request, and processing stages, it may make sense to check
that there is enough time left to handle the request before each
stage.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Deadline propagation"><div class="sect3" id="idm140203533014608">
<h3>Deadline propagation</h3>

<p><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="deadlines" data-tertiary="propagating" id="idm140203533013472"></a>Rather than inventing a deadline when sending RPCs to backends,
servers should employ deadline propagation and cancellation
propagation.</p>

<p>With deadline propagation, a deadline is set high in the stack (e.g.,
in the frontend).  The tree of RPCs emanating from an initial request
will all have the same absolute deadline.  For example, if server <em>A</em>
selects a 30-second deadline, and processes the request for 7 seconds
before sending an RPC to server <em>B</em>, the RPC from <em>A</em> to <em>B</em> will have a 23-second deadline.  If server <em>B</em> takes 4 seconds to handle the request
and sends an RPC to server <em>C</em>, the RPC from <em>B</em> to <em>C</em> will have a 19-second deadline, and so on.  Ideally, each server in the request tree
implements deadline propagation.</p>

<p>Without deadline propagation, the following scenario may occur:</p>
<ol>
<li>
<p>Server <em>A</em> sends an RPC to server <em>B</em> with a 10-second deadline.</p>
</li>
<li>
<p>Server <em>B</em> takes 8 seconds to start processing the request and then
sends an RPC to server <em>C</em>.</p>
</li>
<li>
<p>If server <em>B</em> uses deadline propagation, it should set a 2-second
deadline, but suppose it instead uses a hardcoded 20-second deadline
for the RPC to server <em>C</em>.</p>
</li>
<li>
<p>Server <em>C</em> pulls the request off its queue after 5 seconds.</p>
</li>

</ol>

<p>Had server <em>B</em> used deadline propagation, server <em>C</em> could immediately
give up on the request because the 2-second deadline was exceeded.
However, in this scenario, server <em>C</em> processes the request thinking it
has 15 seconds to spare, but is not doing useful work, since
the request from server <em>A</em> to server <em>B</em> has already exceeded its
deadline.</p>

<p>You may want to reduce the outgoing deadline a bit (e.g.,
a few hundred milliseconds) to account for network transit times and
post-processing in the client.</p>

<p>Also consider setting an upper bound for outgoing deadlines.  You may
want to limit how long the server waits for outgoing RPCs to
noncritical backends, or for RPCs to backends that typically complete
in a short duration. However, be sure to understand your traffic mix,
because you might otherwise inadvertently make particular types of requests
fail all the time (e.g., requests with large payloads, or requests that require responding to a lot of computation).</p>

<p>There are some exceptions for which servers may wish to continue
processing a request after the deadline has elapsed. For example, if a
server receives a request that involves performing some expensive
catchup operation and periodically checkpoints the progress of the
catchup, it would be a good idea to check the deadline only after
writing the checkpoint, instead of after the expensive operation.</p>

<p>Propagating cancellations avoids the potential RPC leakage that occurs
if an initial RPC has a long deadline, but RPCs between deeper layers
of the stack have short deadlines and time out. Using simple deadline
propagation, the initial RPC continues to use server resources until
it eventually times out, despite being unable to make progress.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Bimodal latency"><div class="sect3" id="idm140203532993200">
<h3>Bimodal latency</h3>

<p><a data-type="indexterm" data-primary="bimodal latency" id="idm140203532992064"></a><a data-type="indexterm" data-primary="Remote Procedure Call (RPC)" data-secondary="bimodal" id="idm140203532991392"></a>Suppose that the frontend from the preceding example consists of 10
servers, each with 100 worker threads.  This means that the frontend
has a total of 1,000 threads of capacity.  During usual operation, the
frontends perform 1,000 QPS and requests complete in 100 ms.  This
means that the frontends usually have 100 worker threads occupied out
of the 1,000 configured worker threads (1,000 QPS * 0.1 seconds).</p>

<p>Suppose an event causes 5% of the requests to never complete.  This
could be the result of the unavailability of some Bigtable row ranges,
which renders the requests corresponding to that Bigtable keyspace
unservable.  As a result, 5% of the requests hit the deadline, while
the remaining 95% of the requests take the usual 100 ms.</p>

<p>With a 100-second deadline, 5% of requests would consume 5,000 threads
(50 QPS * 100 seconds), but the frontend doesn’t have that many
threads available.  Assuming no other secondary effects, the frontend
will only be able to handle 19.6% of the requests (1,000 threads
available / (5,000 + 95) threads’ worth of work), resulting in an 80.4%
error rate.</p>

<p>Therefore, instead of only 5% of requests receiving an error (those
that didn’t complete due to keyspace unavailability), most requests
receive an error.</p>

<p>The following guidelines can help address this class of problems:</p>

<ul>
<li>
<p>Detecting this problem can be very hard.  In particular, it may not
be clear that bimodal latency is the cause of an outage when
you are looking at <em>mean</em> latency.  When you see a latency increase, try to
look at the <em>distribution</em> of latencies in addition to the
averages.</p>
</li>
<li>
<p>This problem can be avoided if the requests that don’t complete
return with an error early, rather than waiting the full deadline.
For example, if a backend is unavailable, it’s usually best to
immediately return an error for that backend, rather than consuming
resources until it the backend available.  If your RPC layer
supports a fail-fast option, use it.</p>
</li>
<li>
<p>Having deadlines several orders of magnitude longer than the mean
request latency is usually bad.  In the preceding example, a
small number of requests initially hit the deadline, but the
deadline was three orders of magnitude larger than the normal mean
latency, leading to thread exhaustion.</p>
</li>
<li>
<p>When using shared resources that can be exhausted by some keyspace,
consider either limiting in-flight requests by that keyspace or
using other kinds of abuse tracking.  Suppose your backend
processes requests for different clients that have wildly different
performance and request characteristics.  You might consider only
allowing 25% of your threads to be occupied by any one client in
order to provide fairness in the face of heavy load by any single
client misbehaving.</p>
</li>
</ul>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Slow Startup and Cold Caching"><div class="sect1" id="idm140203532980368">
<h1>Slow Startup and Cold Caching</h1>

<p><a data-type="indexterm" data-primary="slow startup" id="idm140203532979216"></a><a data-type="indexterm" data-primary="cold caching" id="idm140203532978544"></a>Processes are often slower at responding to requests immediately after
starting than they will be in steady state. This slowness can be
caused by either or both of the following:</p>
<dl>
<dt>Required initialization</dt>
<dd>
<p>Setting up connections upon receiving the
first request that needs a given <span class="keep-together">backend</span></p>
</dd>
<dt>Runtime performance improvements in some languages, particularly Java</dt>
<dd>
<p>Just-In-Time compilation, hotspot optimization, and deferred
class loading</p>
</dd>
</dl>

<p>Similarly, some binaries are less efficient when caches aren’t filled.
For example, in the case of some of Google’s services, most requests
are served out of caches, so requests that miss the cache are
significantly more expensive.  In steady-state operation with a warm
cache, only a few cache misses occur, but when the cache is completely
empty, 100% of requests are costly.  Other services might employ
caches to keep a user’s state in RAM. This might be accomplished
through hard or soft stickiness between reverse proxies and service
frontends.</p>

<p>If the service is not provisioned to handle requests under a cold
cache, it’s at greater risk of outages and should take steps to avoid
them.</p>

<p>The following scenarios can lead to a cold cache:</p>
<dl>
<dt>Turning up a new cluster</dt>
<dd>
<p>A recently added cluster will have an
empty cache.</p>
</dd>
<dt>Returning a cluster to service after maintenance</dt>
<dd>
<p>The cache may be
stale.</p>
</dd>
<dt>Restarts</dt>
<dd>
<p>If a task with a cache has recently restarted, filling
its cache will take some time.  It may be worthwhile to move
caching from a server to a separate binary like memcache, which
also allows cache sharing between many servers, albeit at the cost
of introducing another RPC and slight additional latency.</p>
</dd>
</dl>

<p>If caching has a significant effect on the service,<sup><a data-type="noteref" id="idm140203532966304-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203532966304">5</a></sup> you may want to
use one or some of the following strategies:</p>

<ul>
<li>
<p>Overprovision the service. It’s important to note the distinction
between a latency cache versus a capacity cache: when a latency
cache is employed, the service can sustain its expected load with
an empty cache, but a service using a capacity cache cannot sustain
its expected load under an empty cache. Service owners should be
vigilant about adding caches to their service, and make sure that
any new caches are either latency caches or are sufficiently well
engineered to safely function as capacity caches. Sometimes caches
are added to a service to improve performance, but actually wind up
being hard dependencies.</p>
</li>
<li>
<p>Employ general cascading failure prevention techniques. In
particular, servers should reject requests when they’re overloaded
or enter degraded modes, and testing should be performed to see how
the service behaves after events such as a large restart.</p>
</li>
<li>
<p>When adding load to a cluster, slowly increase the load. The
initially small request rate warms up the cache; once the cache is
warm, more traffic can be added. It’s a good idea to ensure that
all clusters carry nominal load and that the caches are kept warm.</p>
</li>
</ul>








<section data-type="sect2" data-pdf-bookmark="Always Go Downward in the Stack"><div class="sect2" id="idm140203532960736">
<h2>Always Go Downward in the Stack</h2>

<p>In the example Shakespeare service, the frontend talks to a backend,
which in turn talks to the storage layer.  A problem that manifests in
the storage layer can cause problems for servers that talk to it, but
fixing the storage layer will usually repair both the backend and
frontend layers.</p>

<p>However, suppose the backends cross-communicate amongst each
other. For example, the backends might proxy requests to one another
to change who owns a user when the storage layer can’t service a
request.  This intra-layer communication can be problematic for
several reasons:</p>

<ul>
<li>
<p>The communication is susceptible to a distributed deadlock.
Backends may use the same thread pool to wait on RPCs sent to
remote backends that are simultaneously receiving requests from
remote backends.  Suppose backend <em>A</em>’s thread pool is full.  Backend
<em>B</em> sends a request to backend <em>A</em> and uses a thread in backend <em>B</em> until
backend <em>A</em>’s thread pool clears.  This behavior can cause the thread
pool saturation to spread.</p>
</li>
<li>
<p>If intra-layer communication increases in response to some kind of
failure or heavy load condition (e.g., load rebalancing that is
more active under high load), intra-layer communication can quickly
switch from a low to high intra-layer request mode when the
load increases enough.</p>

<p>For example, suppose a user has a primary backend and a predetermined
hot standby secondary backend in a different cluster that can take
over the user.  The primary backend proxies requests to the secondary
backend as a result of errors from the lower layer or in response to
heavy load on the master.  If the entire system is overloaded, primary
to secondary proxying will likely increase and add even more load to
the system, due to the additional cost of parsing and waiting on the
request to the secondary in the primary.</p>
</li>
<li>
<p>Depending on the criticality of the cross-layer communication,
bootstrapping the system may become more complex.</p>

<p>It’s usually better to avoid intra-layer communication—i.e., possible
cycles in the communication path—in the user request path. Instead,
have the client do the communication.  For example, if a frontend
talks to a backend but guesses the wrong backend, the backend should
not proxy to the correct backend. Instead, the backend should tell the
frontend to retry its request on the correct backend.<a data-type="indexterm" data-primary="" data-startref="CFprevent22" id="idm140203532950912"></a><a data-type="indexterm" data-primary="" data-startref="SPprevent22" id="idm140203532949968"></a></p>
</li>
</ul>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Triggering Conditions for Cascading Failures"><div class="sect1" id="idm140203532948224">
<h1>Triggering Conditions for Cascading Failures</h1>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="factors triggering" id="idm140203532947056"></a>When a service is susceptible to cascading failures, there are several
possible disturbances that can initiate the domino effect.  This
section identifies some of the factors that trigger cascading
failures.</p>








<section data-type="sect2" data-pdf-bookmark="Process Death"><div class="sect2" id="idm140203532945600">
<h2>Process Death</h2>

<p><a data-type="indexterm" data-primary="process death" id="idm140203532944464"></a><a data-type="indexterm" data-primary="Query of Death" id="idm140203532943792"></a>Some server tasks may die, reducing the amount of available
capacity. Tasks might die because of a Query of Death (an RPC whose
contents trigger a failure in the process), cluster issues, assertion
failures, or a number of other reasons.  A very small event (e.g., a
couple of crashes or tasks rescheduled to other machines) may cause a
service on the brink of falling to break.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Process Updates"><div class="sect2" id="idm140203532942272">
<h2>Process Updates</h2>

<p><a data-type="indexterm" data-primary="process updates" id="idm140203532941136"></a><a data-type="indexterm" data-primary="configuration management" id="idm140203532940464"></a>Pushing a new version of the binary or updating its configuration may
initiate a cascading failure if a large number of tasks are affected
simultaneously. To prevent this scenario, either account for necessary
capacity overhead when setting up the service’s update infrastructure,
or push off-peak.  Dynamically adjusting the number of in-flight task
updates based on the volume of requests and available capacity may be
a workable approach.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="New Rollouts"><div class="sect2" id="idm140203532938864">
<h2>New Rollouts</h2>

<p><a data-type="indexterm" data-primary="rollouts" id="idm140203532937728"></a>A new binary, configuration changes, or a change to the underlying
infrastructure stack can result in changes to request profiles,
resource usage and limits, backends, or a number of other system
components that can trigger a cascading failure.</p>

<p>During a cascading failure, it’s usually wise to check for recent
changes and consider reverting them, particularly if those changes
affected capacity or altered the request profile.</p>

<p>Your service should implement some type of change logging, which can
help quickly identify recent changes.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Organic Growth"><div class="sect2" id="idm140203532935184">
<h2>Organic Growth</h2>

<p>In many cases, a cascading failure isn’t triggered by a specific
service change, but because a growth in usage wasn’t accompanied by an
adjustment to capacity.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Planned Changes, Drains, or Turndowns"><div class="sect2" id="idm140203532933424">
<h2>Planned Changes, Drains, or Turndowns</h2>

<p><a data-type="indexterm" data-primary="planned changes" id="idm140203532932272"></a><a data-type="indexterm" data-primary="drains" id="idm140203532931600"></a><a data-type="indexterm" data-primary="turndown automation" id="idm140203532930928"></a>If your service is multihomed, some of your capacity may be
unavailable because of maintenance or outages in a cluster.
Similarly, one of the service’s critical dependencies may be drained,
resulting in a reduction in capacity for the upstream service due to
drain dependencies, or an increase in latency due to having to send
the requests to a more distant cluster.</p>










<section data-type="sect3" data-pdf-bookmark="Request profile changes"><div class="sect3" id="idm140203532929584">
<h3>Request profile changes</h3>

<p><a data-type="indexterm" data-primary="request profile changes" id="idm140203532928448"></a>A backend service may receive requests from different clusters because
a frontend service shifted its traffic due to load balancing
configuration changes, changes in the traffic mix, or cluster
fullness.  Also, the average cost to handle an individual payload may
have changed due to frontend code or configuration changes.
Similarly, the data handled by the service may have changed
organically due to increased or differing usage by existing users: for
instance, both the number and size of images, <em>per user</em>, for a photo
storage service tend to increase over time.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Resource limits"><div class="sect3" id="idm140203532926320">
<h3>Resource limits</h3>

<p><a data-type="indexterm" data-primary="resources" data-secondary="limits" data-seealso="capacity planning" id="idm140203532925184"></a>Some cluster operating systems allow resource overcommitment. CPU is a
fungible resource; often, some machines have some amount of slack CPU
available, which provides a bit of a safety net against CPU spikes.
The availability of this slack CPU differs between cells, and also
between machines within the cell.</p>

<p>Depending upon this slack CPU as your safety net is dangerous. Its
availability is entirely dependent on the behavior of the other jobs
in the cluster, so it might suddenly drop out at any time. For
example, if a team starts a MapReduce that consumes a lot of CPU and
schedules on many machines, the aggregate amount of slack CPU can
suddenly decrease and trigger CPU starvation conditions for unrelated
jobs.  When performing load tests, make sure that you remain within
your committed resource limits.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Testing for Cascading Failures"><div class="sect1" id="xref_cascading-failure_testing">
<h1>Testing for Cascading Failures</h1>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="testing for" data-seealso="overload handling" id="CFtest22"></a>The specific ways in which a service will fail can be very hard to
predict from first principles.  This section discusses testing
strategies that can detect if services are susceptible to cascading
failures.</p>

<p>You should test your service to determine how it behaves under heavy
load in order to gain confidence that it won’t enter a cascading
failure under various circumstances.</p>








<section data-type="sect2" data-pdf-bookmark="Test Until Failure and Beyond"><div class="sect2" id="idm140203532917904">
<h2>Test Until Failure and Beyond</h2>

<p>Understanding the behavior of the service under heavy load is perhaps
the most important first step in avoiding cascading failures.  Knowing
how your system behaves when it is overloaded helps to identify what
engineering tasks are the most important for long-term fixes; at the
very least, this knowledge may help bootstrap the debugging process
for on-call engineers when an emergency arises.</p>

<p>Load test components until they break.  As load increases, a component
typically handles requests successfully until it reaches a point at
which it can’t handle more requests.  At this point, the component
should ideally start serving errors or degraded results in response to
additional load, but not significantly reduce the rate at which it
successfully handles requests.  A component that is highly susceptible
to a cascading failure will start crashing or serving a very high rate
of errors when it becomes overloaded; a better designed component will
instead be able to reject a few requests and survive.</p>

<p>Load testing also reveals where the breaking point is, knowledge
that’s fundamental to the capacity planning process.  It enables you
to test for regressions, provision for worst-case thresholds, and to
trade off utilization versus safety margins.</p>

<p>Because of caching effects, gradually ramping up load may yield
different results than immediately increasing to expected load
levels. Therefore, consider testing both gradual and impulse load
patterns.</p>

<p>You should also test and understand how the component behaves as it
returns to nominal load after having been pushed well beyond that
load. Such testing may answer questions such as:</p>

<ul>
<li>
<p>If a component enters a degraded mode on heavy load, is it capable
of exiting the degraded mode without human intervention?</p>
</li>
<li>
<p>If a couple of servers crash under heavy load, how much does the
load need to drop in order for the system to stabilize?</p>
</li>
</ul>

<p>If you’re load testing a stateful service or a service that employs
caching, your load test should track state between multiple
interactions and check correctness at high load, which is often where
subtle concurrency bugs hit.</p>

<p>Keep in mind that individual components may have different breaking
points, so load test each component separately.  You won’t know in
advance which component may hit the wall first, and you want to know
how your system behaves when it does.</p>

<p>If you believe your system has proper protections against being
overloaded, consider performing failure tests in a small slice of
production to find the point at which the components in your system
fail under real traffic. These limits may not be adequately reflected
by synthetic load test traffic, so real traffic tests may provide more
realistic results than load tests, at the risk of causing user-visible
pain.  Be careful when testing on real traffic: make sure that you
have extra capacity available in case your automatic protections don’t
work and you need to manually fail over.  You might consider some of
the following production tests:</p>

<ul>
<li>
<p>Reducing task counts quickly or slowly over time, beyond expected
traffic <span class="keep-together">patterns</span></p>
</li>
<li>
<p>Rapidly losing a cluster’s worth of capacity</p>
</li>
<li>
<p>Blackholing various backends</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Test Popular Clients"><div class="sect2" id="idm140203532904224">
<h2>Test Popular Clients</h2>

<p>Understand how large clients use your service.  For example, you want
to know if <span class="keep-together">clients</span>:</p>

<ul>
<li>
<p>Can queue work while the service is down</p>
</li>
<li>
<p>Use randomized exponential backoff on errors</p>
</li>
<li>
<p>Are vulnerable to external triggers that can create large amounts
of load (e.g., an externally triggered software update might
clear an offline client’s cache)</p>
</li>
</ul>

<p>Depending on your service, you may or may not be in control of all the
client code that talks to your service. However, it’s still a good
idea to have an understanding of how large clients that interact with
your service will behave.</p>

<p>The same principles apply to large internal clients.  Stage system
failures with the largest clients to see how they react.  Ask internal
clients how they access your service and what mechanisms they use to
handle backend failure.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Test Noncritical Backends"><div class="sect2" id="idm140203532897152">
<h2>Test Noncritical Backends</h2>

<p>Test your noncritical backends, and make sure their unavailability
does not interfere with the critical components of your service.</p>

<p>For example, suppose your frontend has critical and noncritical
backends.  Often, a given request includes both critical components
(e.g., query results) and noncritical components (e.g., spelling
suggestions). Your requests may significantly slow down and consume
resources waiting for noncritical backends to finish.</p>

<p>In addition to testing behavior when the noncritical backend is
unavailable, test how the frontend behaves if the noncritical backend
never responds (for example, if it is blackholing requests).  Backends
advertised as noncritical can still cause problems on frontends when
requests have long deadlines.  The frontend should not start rejecting
lots of requests, running out of resources, or serving with very high
latency when a noncritical backend blackholes.<a data-type="indexterm" data-primary="" data-startref="CFtest22" id="idm140203532894080"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Immediate Steps to Address Cascading Failures"><div class="sect1" id="idm140203532892656">
<h1>Immediate Steps to Address Cascading Failures</h1>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="addressing" id="CFaddress22"></a>Once you have identified that your service is experiencing a cascading
failure, you can use a few different strategies to remedy the
situation—and of course, a
cascading failure is a good opportunity to use your incident
management protocol (<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch14.html#chapter_managing-incidents">Chapter&nbsp;14</a>).</p>








<section data-type="sect2" data-pdf-bookmark="Increase Resources"><div class="sect2" id="idm140203532888880">
<h2>Increase Resources</h2>

<p>If your system is running at degraded capacity and you have idle
resources, adding tasks can be the most expedient way to recover from
the outage. However, if the service has entered a death spiral of some
sort, adding more resources may not be sufficient to recover.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Stop Health Check Failures/Deaths"><div class="sect2" id="idm140203532887008">
<h2>Stop Health Check Failures/Deaths</h2>

<p><a data-type="indexterm" data-primary="health checks" id="idm140203532885856"></a>Some cluster scheduling systems, such as Borg, check the health of
tasks in a job and restart tasks that are unhealthy. This practice may
create a failure mode in which health-checking itself makes the
service unhealthy.  For example, if half the tasks aren’t able to
accomplish any work because they’re starting up and the other half
will soon be killed because they’re overloaded and failing health
checks, temporarily disabling health checks may permit the system to
stabilize until all the tasks are running.</p>

<p><a data-type="indexterm" data-primary="process health checks" id="idm140203532884240"></a><a data-type="indexterm" data-primary="service health checks" id="idm140203532883568"></a>Process health checking (“is this binary responding <em>at all</em>?”) and
service health checking (“is this binary able to respond to <em>this
class of requests</em> right now?”) are two conceptually distinct
operations. Process health checking is relevant to the cluster scheduler,
whereas service health checking is relevant to the load balancer. Clearly distinguishing between the two types of health
checks can help avoid this scenario.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Restart Servers"><div class="sect2" id="idm140203532881168">
<h2>Restart Servers</h2>

<p>If servers are somehow wedged and not making progress, restarting them
may help.  Try restarting servers when:</p>

<ul>
<li>
<p>Java servers are in a GC death spiral</p>
</li>
<li>
<p>Some in-flight requests have no deadlines but are consuming
resources, leading them to block threads, for example</p>
</li>
<li>
<p>The servers are deadlocked</p>
</li>
</ul>

<p>Make sure that you identify the source of the cascading failure before
you restart your servers. Make sure that taking this action won’t
simply shift around load. Canary this change, and make it slowly.
Your actions may amplify an existing cascading failure if the outage
is actually due to an issue like a cold cache.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Drop Traffic"><div class="sect2" id="idm140203532875440">
<h2>Drop Traffic</h2>

<p>Dropping load is a big hammer, usually reserved for situations in
which you have a true cascading failure on your hands and you cannot
fix it by other means.  For example, if heavy load causes most servers to
crash as soon as they become healthy, you can get the service up and
running again by:</p>
<ol>
<li>
<p>Addressing the initial triggering condition (by adding capacity, for
example).</p>
</li>
<li>
<p>Reducing load enough so that the crashing stops. Consider being
aggressive here—if the entire service is crash-looping, only allow,
say, 1% of the traffic through.</p>
</li>
<li>
<p>Allowing the majority of the servers to become healthy.</p>
</li>
<li>
<p>Gradually ramping up the load.</p>
</li>

</ol>

<p>This strategy allows caches to warm up, connections to be established,
etc., before load returns to normal levels.</p>

<p>Obviously, this tactic will cause a lot of user-visible harm.  Whether
or not you’re able to (or if you even <em>should</em>) drop traffic
indiscriminately depends on how the service is configured. If you have
some mechanism to drop less important traffic (e.g., prefetching), use
that mechanism first.</p>

<p>It is important to keep in mind that this strategy enables you to
recover from a cascading outage once the underlying problem is fixed.
If the issue that started the cascading failure is not fixed (e.g.,
insufficient global capacity), then the cascading failure may trigger
shortly after all traffic returns. Therefore, before using this
strategy, consider fixing (or at least papering over) the root cause
or triggering condition.  For example, if the service ran out of
memory and is now in a death spiral, adding more memory or tasks
should be your first step.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Enter Degraded Modes"><div class="sect2" id="idm140203532866576">
<h2>Enter Degraded Modes</h2>

<p>Serve degraded results by doing less work or dropping unimportant
traffic.  This strategy must be engineered into your service, and can
be implemented only if you know which traffic can be degraded and you
have the ability to differentiate between the various payloads.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Eliminate Batch Load"><div class="sect2" id="idm140203532864704">
<h2>Eliminate Batch Load</h2>

<p><a data-type="indexterm" data-primary="batching" id="idm140203532863568"></a>Some services have load that is important, but not critical.
Consider turning off those sources of load.  For example, if index
updates, data copies, or statistics gathering consume resources of the
serving path, consider turning off those sources of load during an
outage.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Eliminate Bad Traffic"><div class="sect2" id="idm140203532862160">
<h2>Eliminate Bad Traffic</h2>

<p>If some queries are creating heavy load or crashes (e.g., queries of
death), consider blocking them or eliminating them via other means.</p>
<aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before"><div class="sidebar" id="idm140203532860576">
<h5>Cascading Failure and Shakespeare</h5>
<p><a data-type="indexterm" data-primary="Shakespeare search service, example" data-secondary="engagement" id="idm140203532859152"></a>A documentary about Shakespeare’s works airs in Japan, and explicitly
points to our Shakespeare service as an excellent place to conduct
further research. Following the broadcast, traffic to our Asian
datacenter surges beyond the service’s capacity. This capacity problem
is further compounded by a major update to the Shakespeare service
that simultaneously occurs in that datacenter.</p>

<p><a data-type="indexterm" data-primary="Production Readiness Review process" data-see="SRE engagement model" id="idm140203532857376"></a>Fortunately, a number of safeguards are in place that help mitigate
the potential for failure. The Production Readiness Review process
identified some issues that the team already addressed. For example,
the developers built graceful degradation into the service. As
capacity becomes scarce, the service no longer returns pictures
alongside text or small maps illustrating where a story takes
place. And depending on its purpose, an RPC that times out is either
not retried (for example, in the case of the aforementioned pictures),
or is retried with a randomized exponential backoff. Despite these
safeguards, the tasks fail one by one and are then restarted by Borg,
which drives the number of working tasks down even more.</p>

<p>As a result, some graphs on the service dashboard turn an alarming
shade of red and SRE is paged. In response, SREs temporarily add
capacity to the Asian datacenter by increasing the number of tasks
available for the Shakespeare job. By doing so, they’re able to
restore the Shakespeare service in the Asian cluster.</p>

<p>Afterward, the SRE team writes a postmortem detailing the chain of
events, what went well, what could have gone better, and a number of
action items to prevent this scenario from occurring again. For
example, in the case of a service overload, the GSLB load balancer
will redirect some traffic to neighboring datacenters.  Also, the SRE
team turns on autoscaling, so that the number of tasks automatically
increases with traffic, so they don’t have to worry about this type of
issue again.<a data-type="indexterm" data-primary="" data-startref="SSScascad22" id="idm140203532853872"></a><a data-type="indexterm" data-primary="" data-startref="CFaddress22" id="idm140203532852928"></a></p>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Closing Remarks"><div class="sect1" id="idm140203532851344">
<h1>Closing Remarks</h1>

<p><a data-type="indexterm" data-primary="cascading failures" data-secondary="overview of" id="idm140203532850208"></a>When systems are overloaded, something needs to give in order to
remedy the situation.  Once a service passes its breaking point, it is
better to allow some user-visible errors or lower-quality results to slip
through than try to fully serve every request.  Understanding where
those breaking points are and how the system behaves beyond them is
critical for service owners who want to avoid cascading failures.</p>

<p class="pagebreak-before">Without proper care, some system changes meant to reduce background
errors or otherwise improve the steady state can expose the service to
greater risk of a full outage.  Retrying on failures, shifting load
around from unhealthy servers, killing unhealthy servers, adding
caches to improve performance or reduce latency: all of these might be
implemented to improve the normal case, but can improve the chance of
causing a large-scale failure.  Be careful when evaluating changes to
ensure that one outage is not being traded for another.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm140203533199440"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533199440-marker">1</a></sup> See Wikipedia, “Positive feedback,” <a href="https://en.wikipedia.org/wiki/Positive_feedback"><em class="hyperlink">https://en.wikipedia.org/wiki/Positive_feedback</em></a>.</p><p data-type="footnote" id="idm140203533163168"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533163168-marker">2</a></sup> A watchdog is often implemented as a thread that wakes up periodically to see whether work has been done since the last time it checked. &nbsp;If not, it assumes that the server is stuck and kills it. &nbsp;For instance, requests of a known type can be sent to the server at regular intervals; if one hasn’t been received or processed when expected, this may indicate failure—of the server, the system sending requests, or the intermediate network.</p><p data-type="footnote" id="idm140203533102496"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533102496-marker">3</a></sup> This is often not a good assumption due to geography; see also <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch02.html#xref_production-environment_job-and-data-organization">“Job and Data Organization”</a>.</p><p data-type="footnote" id="idm140203533050592"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203533050592-marker">4</a></sup> An instructive exercise, left for the reader: write a simple simulator and see how the amount of useful work the backend can do varies with how much it’s overloaded and how many retries are permitted.</p><p data-type="footnote" id="idm140203532966304"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#idm140203532966304-marker">5</a></sup> Sometimes you find that a meaningful proportion of your actual serving capacity is as a function of serving from a cache, and if you lost access to that cache, you wouldn’t actually be able to serve that many queries. A similar observation holds for latency: a cache can help you achieve latency goals (by lowering the average response time when the query is servable from cache) that you possibly couldn’t meet without that cache.</p></div></div></section></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/library/view/site-reliability-engineering/9781491929117/ch21.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">21. Handling Overload</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/library/view/site-reliability-engineering/9781491929117/ch23.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">23. Managing Critical State: Distributed Consensus for Reliability</div>
        </a>
    
  
  </div>


      
    </section>
    <div class="reading-controls-bottom">
      <ul class="interface-controls js-bitlist">
        <li class="queue-control">
            <button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781491929117/chapter/ch22.html">
      <span>Add to Queue</span>
  </button>
        </li>
      </ul>
    </div>
  </div>
  <div class="js-related-container related"></div>
<section class="sbo-saved-archives"></section>



          
          
  





    
    



        
      </div>
      



  <footer class="pagefoot t-pagefoot">
    <a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li><a class="t-queue-footer" href="https://www.safaribooksonline.com/s/">Queue</a></li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li><a href="https://www.safaribooksonline.com/blog/">Blog</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://community.safaribooksonline.com/">Feedback</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2016 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    
    
    


  

<div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch22.html#">Reset</a>
</div>
</div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>