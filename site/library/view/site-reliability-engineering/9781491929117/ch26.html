<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage"" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/site-reliability-engineering/9781491929117/ch26.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="859452"
  data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36"
  data-username="dchen267"
  data-account-type="B2B"
  
  data-activated-trial-date="04/25/2016"


  data-archive="9781491929117"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch26.html"
  data-epub-title="Site Reliability Engineering" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" "="" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/site-reliability-engineering/9781491929117/ch26.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="859452" data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36" data-username="dchen267" data-account-type="B2B" data-activated-trial-date="04/25/2016" data-archive="9781491929117" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch26.html" data-epub-title="Site Reliability Engineering" data-debug="0" data-testing="0"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491929117"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.8cc2fd27400e.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>26. Data Integrity: What You Read Is What You Wrote - Site Reliability Engineering</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/c1c7ad294784.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.min.fd58f69f4908.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content font,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}@page{margin:5px !important}#sbo-rt-content p{margin:8px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0}#sbo-rt-content div.preface p.byline+p.byline{margin:0}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content sup{font-size:x-small;vertical-align:super}#sbo-rt-content sub{font-size:smaller;vertical-align:sub}#sbo-rt-content span.lineannotation{font-style:italic;color:#A62A2A;font-family:serif,"DejaVuSerif"}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#FFF}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;padding:15px 5px 15px 5px !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding-top:10px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.informalfigure{text-align:center;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;background-color:#F7F7F7;font-size:90%;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:15px}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div.caution,#sbo-rt-content div.sidebar div.important{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content div.sidebar div.figure,#sbo-rt-content aside[data-type="sidebar"] figure{border:none}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-family:sans-serif;font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle{font-size:1em;font-weight:normal;text-align:center}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1em;font-family:serif,"DejaVuSerif";font-weight:bold;color:#8e0012;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{font-size:1em;font-family:serif,"DejaVuSerif";font-weight:normal;text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);position:absolute;bottom:0;max-width:100%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif,"DejaVuSerif";font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif,"DejaVuSerif";margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10pt}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-family:serif,"DejaVuSerif";font-style:italic}#sbo-rt-content blockquote div.attribution{margin:5px 0 0 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p{font-style:normal}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{font-size:85%;border-top:2px solid black;padding-left:1.5em;margin-top:1.5em}#sbo-rt-content p[data-type="footnote"]{margin-top:-.5em}#sbo-rt-content p[data-type="footnote"] sup{left:-1em;top:1.2em;position:relative;display:block}#sbo-rt-content div.refnamediv h2,#sbo-rt-content div.refnamediv h3,#sbo-rt-content div.refsynopsisdiv h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refentry div.refsect1 h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refsect2 h3{font-size:1em;color:#000;margin-top:10px !important;margin-bottom:0 !important}#sbo-rt-content div.refnamediv p{margin-left:15px !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dt span.term{font-weight:bold;font-style:italic}#sbo-rt-content dt span.term code.literal{font-style:normal;font-weight:normal}#sbo-rt-content dd{margin-left:1.5em !important}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ol{list-style-type:decimal;margin-top:8px !important;margin-bottom:8px !important;margin-left:20px !important;padding-left:25px !important}#sbo-rt-content ol ol{list-style-type:lower-alpha}#sbo-rt-content ol ol ol{list-style-type:lower-roman}#sbo-rt-content ul{list-style-type:square;margin-top:8px !important;margin-bottom:8px !important;margin-left:5px !important;padding-left:20px !important}#sbo-rt-content ul ul{list-style-type:none;padding-left:0 !important;margin-left:0 !important}#sbo-rt-content ol li,#sbo-rt-content ul li,#sbo-rt-content dd{margin-bottom:.25em}#sbo-rt-content ul ul li p:before{content:"— "}#sbo-rt-content ul ul ul li p:before{content:""}#sbo-rt-content ul ul ul{list-style-type:square;margin-left:20px !important;padding-left:30px !important}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist{margin-left:20px !important;margin-bottom:10px}#sbo-rt-content table.simplelist td{border:none;font-size:90%}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img{padding:0}#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;-webkit-border-radius:5px;border-radius:5px;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{font-weight:bold;font-size:110%;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;-webkit-border-radius:0;border-radius:0;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px auto 30px auto !important;max-width:95%;border-collapse:collapse;border-spacing:0}#sbo-rt-content div.table,#sbo-rt-content div.informaltable{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif,"DejaVuSans";color:#000;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{padding:.3em;text-align:left;vertical-align:baseline;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";margin:10px 0 10px 0 !important;text-align:center;padding:0;page-break-after:avoid}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{font-weight:bold;text-indent:0}#sbo-rt-content div.index li{line-height:140%}#sbo-rt-content div.index a.indexterm{color:#8e0012}#sbo-rt-content div.index ul{list-style-type:none;padding-left:0;margin-left:0}#sbo-rt-content div.index ul li{padding-left:0;margin-left:0}#sbo-rt-content div.index ul li ul li{margin-left:20px}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif,"DejaVuSerif";text-align:left}#sbo-rt-content span.roman_text{font-style:normal !important}
    </style><link rel="canonical" href="/Library/view/site-reliability-engineering/9781491929117/ch26.html"><meta name="description" content=" Chapter 26. Data Integrity: What You Read Is What You Wrote Written by Raymond Blum and Rhandeev Singh Edited by Betsy Beyer What is “data integrity”? When users come first ... "><meta property="og:title" content="26. Data Integrity: What You Read Is What You Wrote"><meta itemprop="isPartOf" content="/library/view/site-reliability-engineering/9781491929117/"><meta itemprop="name" content="26. Data Integrity: What You Read Is What You Wrote"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch26.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491929117/"><meta property="og:description" itemprop="description" content=" Chapter 26. Data Integrity: What You Read Is What You Wrote Written by Raymond Blum and Rhandeev Singh Edited by Betsy Beyer What is “data integrity”? When users come first ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781491929124"><meta property="og:book:author" itemprop="author" content="Niall Richard Murphy"><meta property="og:book:author" itemprop="author" content="Jennifer Petoff"><meta property="og:book:author" itemprop="author" content="Chris Jones"><meta property="og:book:author" itemprop="author" content="Betsy Beyer"><meta property="og:book:tag" itemprop="about" content="Engineering"><meta property="og:book:tag" itemprop="about" content="Networking"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><!--[if lt IE 9]><script src="/static/js/src/respond.min.cf5c9b7980e5.js"></script><![endif]--><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  js-show-related scalefonts library" data-gr-c-s-loaded="true">

    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        




<a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="20" height="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z" fill="currentColor"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/s/" class="t-queue-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z" fill="currentColor"></path></g></svg><span>Queue</span></a></li><li class="search"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z" fill="currentColor"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z" fill="currentColor"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z" fill="currentColor"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z" fill="currentColor"></path></g></svg><span>Tutorials</span></a></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/003o000000t5q9fAAA/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z" fill="currentColor"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l1 no-icon">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l2">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Site Reliability Engineering
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781491929117/chapter/ch26.html" data-for-analytics="9781491929117:ch26.html"><span>Add to Queue</span></button></li><li class="js-font-control-panel font-control-activator"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch26.html&amp;text=Site%20Reliability%20Engineering&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch26.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch26.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%2026.%20Data%20Integrity%3A%20What%20You%20Read%20Is%20What%20You%20Wrote&amp;body=https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/ch26.html%0D%0Afrom%20Site%20Reliability%20Engineering%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
      
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/Library/view/site-reliability-engineering/9781491929117/ch25.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">25. Data Processing Pipelines</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/Library/view/site-reliability-engineering/9781491929117/ch27.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">27. Reliable Product Launches at Scale</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 26. Data Integrity: What You Read Is What You Wrote"><div class="chapter" id="chapter_data-integrity">
<h1><span class="label">Chapter 26. </span>Data Integrity: What You Read Is <span class="keep-together">What You Wrote</span></h1>


<p class="byline">Written by Raymond Blum and Rhandeev Singh</p>

<p class="byline">Edited by Betsy Beyer</p>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="defined" id="idm140203536673120"></a>What is “data integrity”? When users come first, data integrity is whatever users think it is.</p>

<p>We might say <em>data integrity is a measure of the accessibility and
accuracy of the datastores needed to provide users with an adequate
level of service</em>. But this definition is insufficient.</p>

<p>For instance, if a user interface bug in Gmail displays an empty
mailbox for too long, users might believe data has been lost. Thus,
even if no data was <em>actually</em> lost, the world would question
Google’s ability to act as a responsible steward of data, and the
viability of cloud computing would be threatened. Were Gmail to
display an error or maintenance message for too long while “only a
bit of metadata” is repaired, the trust of Google’s users would
similarly erode.</p>

<p>How long is “too long” for data to be unavailable? As demonstrated by an
actual Gmail incident in 2011 <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Hic11">[Hic11]</a>, four days is a long
time—perhaps “too long.” Subsequently, we believe 24 hours is a good
starting point for establishing the threshold of “too long” for Google
Apps.</p>

<p>Similar reasoning applies to applications like Google Photos, Drive,
Cloud Storage, and Cloud Datastore, because users don’t necessarily draw a
distinction between these discrete products (reasoning, “this
product is still Google” or “Google, Amazon, whatever; this
product is still part of the cloud”). Data loss, data corruption,
and extended unavailability are typically indistinguishable to users.
Therefore, data integrity applies to all types of data across all
services. When considering data integrity, what matters is that
<em>services in the cloud remain accessible to users. User access to data
is especially important</em>.</p>






<section data-type="sect1" data-pdf-bookmark="Data Integrity’s Strict Requirements"><div class="sect1" id="idm140203536666352">
<h1>Data Integrity’s Strict Requirements</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="strict requirements" id="idm140203536665200"></a>When considering the reliability needs of a given
system, it may seem that uptime (service availability) needs are
stricter than those of data integrity. For example, users may find an
hour of email downtime unacceptable, whereas they may live grumpily
with a four-day time window to recover a mailbox. However, there’s a
more appropriate way to consider the demands of uptime versus data
integrity.</p>

<p>An SLO of 99.99% uptime leaves room for only an hour of downtime in a
whole year. This SLO sets a rather high bar, which likely exceeds the
expectations of most Internet and Enterprise users.</p>

<p>In contrast, an SLO of 99.99% good bytes in a 2 GB artifact would
render documents, executables, and databases corrupt (up to 200 KB
garbled). This amount of corruption is <em>catastrophic</em> in the majority
of cases—resulting in executables with random opcodes and completely
unloadable databases.</p>

<p>From the user perspective, then, every service has independent uptime
and data integrity requirements, even if these requirements are
implicit. The worst time to disagree with users about these
requirements is after the demise of their data!</p>

<figure><div class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_26in01.png" alt="srle 26in01" width="1565" height="322" data-mfp-src="/library/view/site-reliability-engineering/9781491929117/assets/srle_26in01.png">
<h6></h6>
</div></figure>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="expanded definition of" id="idm140203536659344"></a><a data-type="indexterm" data-primary="cloud environment" data-secondary="definition of data integrity in" id="idm140203536658400"></a>To revise our earlier definition of data integrity, we might say that
<em>data integrity means that services in the cloud remain accessible to
users. User access to data is especially important, so this access
should remain in perfect shape</em>.</p>

<p>Now, suppose an artifact were corrupted or lost exactly once a year.
If the loss were unrecoverable, uptime of the affected artifact is
<em>lost</em> for that year.  The most likely means to avoid any such loss is
through proactive detection, coupled with rapid repair.</p>

<p>In an alternate universe, suppose the corruption were immediately
detected before users were affected and that the artifact was removed,
fixed, and returned to service within half an hour. Ignoring any other
downtime during that 30 minutes, such an object would be 99.99%
available that year.</p>

<p>Astonishingly, at least from the user perspective, in this scenario,
data integrity is still 100% (or close to 100%) during the accessible
lifetime of the object. As demonstrated by this example, <em>the secret
to superior data integrity is proactive detection and rapid repair and
recovery.</em></p>








<section data-type="sect2" data-pdf-bookmark="Choosing a Strategy for Superior Data Integrity"><div class="sect2" id="idm140203536653632">
<h2>Choosing a Strategy for Superior Data Integrity</h2>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="selecting strategy for" id="DIstrat26"></a>There are many possible strategies for rapid detection, repair, and
recovery of lost data. All of these strategies trade uptime against
data integrity with respect to affected users. Some strategies work
better than others, and some strategies require more complex
engineering investment than others. With so many options available,
which strategies should you utilize? The answer depends on your
computing paradigm.</p>

<p><a data-type="indexterm" data-primary="cloud environment" data-secondary="data integrity strategies" id="idm140203536650400"></a>Most cloud computing applications seek to optimize for some
combination of uptime, latency, scale, velocity, and privacy. To
provide a working definition for each of these terms:</p>
<dl>
<dt>Uptime</dt>
<dd>
<p><a data-type="indexterm" data-primary="uptime" id="idm140203536647808"></a><a data-type="indexterm" data-primary="availability" id="idm140203536647136"></a>Also referred to as <em>availability</em>, the proportion of
time a service is usable by its users.</p>
</dd>
<dt>Latency</dt>
<dd>
<p><a data-type="indexterm" data-primary="latency" data-secondary="defined" id="idm140203536644624"></a>How responsive a service appears to its users.</p>
</dd>
<dt>Scale</dt>
<dd>
<p><a data-type="indexterm" data-primary="scale" data-secondary="defined" id="idm140203536642304"></a>A service’s volume of users and the mixture of workloads
the service can handle before latency suffers or the service falls
apart.</p>
</dd>
<dt>Velocity</dt>
<dd>
<p><a data-type="indexterm" data-primary="velocity" id="idm140203536639904"></a>How fast a service can innovate to provide users with
superior value at reasonable cost.</p>
</dd>
<dt>Privacy</dt>
<dd>
<p><a data-type="indexterm" data-primary="privacy" id="idm140203536637808"></a>This concept imposes complex requirements. As a
simplification, this chapter limits its scope in discussing privacy to
data deletion: data must be destroyed within a reasonable time after
users delete it.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="ACID datastore semantics" id="idm140203536636176"></a><a data-type="indexterm" data-primary="cloud environment" data-secondary="evolution of applications in" id="idm140203536635488"></a><a data-type="indexterm" data-primary="datastores" data-secondary="ACID and BASE" id="idm140203536634528"></a>Many cloud applications continually evolve atop a mixture of
ACID<sup><a data-type="noteref" id="idm140203536633376-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536633376" class="totri-footnote">1</a></sup> and BASE<sup><a data-type="noteref" id="idm140203536631408-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536631408" class="totri-footnote">2</a></sup> APIs to meet the
demands of these five components.<sup><a data-type="noteref" id="idm140203536629360-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536629360" class="totri-footnote">3</a></sup> BASE allows for
higher availability than ACID, in exchange for a softer
distributed consistency guarantee. Specifically, BASE only guarantees
that once a piece of data is no longer updated, its value will
<em>eventually</em> become consistent across (potentially distributed)
storage locations.</p>

<p>The following scenario provides an example of how trade-offs between
uptime, latency, scale, velocity, and privacy might play out.</p>

<p>When velocity trumps other requirements, the resulting applications
rely on an arbitrary collection of APIs that are most familiar to the
particular developers working on the application.</p>

<p><a data-type="indexterm" data-primary="Blobstore" id="idm140203536624784"></a>For example, an application may take advantage of an efficient
BLOB<sup><a data-type="noteref" id="idm140203536623904-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536623904" class="totri-footnote">4</a></sup> storage API, such as Blobstore, that neglects
distributed consistency in favor of scaling to heavy workloads with
high uptime, low latency, and at low cost. To compensate:</p>

<ul>
<li>
<p>The same application may entrust small amounts of authoritative
metadata pertaining to its blobs to a higher latency, less
available, more costly Paxos-based service such as
Megastore <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Bak11">[Bak11]</a>, <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Lam98">[Lam98]</a>.</p>
</li>
<li>
<p>Certain clients of the application may cache some of that metadata
locally and access blobs directly, shaving latency still further
from the vantage point of users.</p>
</li>
<li>
<p>Another application may keep metadata in Bigtable, sacrificing
strong distributed consistency because its developers happened to
be familiar with Bigtable.</p>
</li>
</ul>

<p>Such cloud applications face a variety of data integrity challenges at
runtime, such as referential integrity between datastores (in the preceding
example, Blobstore, Megastore, and client-side caches). The
vagaries of high velocity dictate that schema changes, data
migrations, the piling of new features atop old features, rewrites,
and evolving integration points with other applications collude to
produce an environment riddled with complex relationships between
various pieces of data that no single engineer fully groks.</p>

<p><a data-type="indexterm" data-primary="out-of-band checks and balances" id="idm140203536615312"></a>To prevent such an application’s data from degrading before its users’
eyes, a system of out-of-band checks and balances is needed within and
between its datastores. <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#xref_data-integrity_early-detection">“Third Layer: Early Detection”</a> discusses such a system.</p>

<p>In addition, if such an application relies on independent,
uncoordinated backups of several datastores (in the preceding example,
Blobstore and Megastore), then its ability to make effective use of
restored data during a data recovery effort is complicated by the
variety of relationships between restored and live data. Our example
application would have to sort through and distinguish between
restored blobs versus live Megastore, restored Megastore versus live blobs,
restored blobs versus restored Megastore, and interactions with
client-side caches.</p>

<p>In consideration of these dependencies and complications, how many
resources should be invested in data integrity efforts, and where?<a data-type="indexterm" data-primary="" data-startref="DIstrat26" id="idm140203536611888"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Backups Versus Archives"><div class="sect2" id="idm140203536610624">
<h2>Backups Versus Archives</h2>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="backups vs. archives" id="idm140203536609488"></a><a data-type="indexterm" data-primary="backups" data-see="data integrity" id="idm140203536608544"></a>Traditionally, companies “protect” data against loss by investing in
backup strategies. However, the real focus of such backup efforts
should be data recovery, which distinguishes <em>real</em>
backups from archives. As is sometimes observed: No one really
<em>wants</em> to make backups; what people <em>really</em> want are
<em>restores</em>.</p>

<p>Is your “backup” really an archive, rather than appropriate for use in disaster <span class="keep-together">recovery</span>?</p>

<figure><div class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_26in02.png" alt="srle 26in02" width="1565" height="324" data-mfp-src="/library/view/site-reliability-engineering/9781491929117/assets/srle_26in02.png">
<h6></h6>
</div></figure>

<p>The most important difference between backups and archives is that
backups <em>can</em> be loaded back into an application, while archives
<em>cannot</em>. Therefore, backups and archives have quite differing use
cases.</p>

<p><em>Archives</em> <a data-type="indexterm" data-primary="archives vs. backups" id="idm140203536600320"></a>safekeep data for long periods of time to meet auditing,
discovery, and compliance needs. Data recovery for such purposes
generally doesn’t need to complete within uptime requirements of a
service. For example, you might need to retain financial transaction
data for seven years. To achieve this goal, you could move accumulated
audit logs to long-term archival storage at an offsite location once a
month. Retrieving and recovering the logs during a month-long
financial audit may take a week, and this weeklong time window for
recovery may be acceptable for an archive.</p>

<p><a data-type="indexterm" data-primary="real backups" id="idm140203536598640"></a>On the other hand, when disaster strikes, data must be recovered from
<em>real backups</em> quickly, preferably well within the uptime needs of a
service. Otherwise, affected users are left without useful access to
the application from the onset of the data integrity issue until the
completion of the recovery effort.</p>

<p>It’s also important to consider that because the most recent data is
at risk until safely backed up, it may be optimal to schedule real
backups (as opposed to archives) to occur daily, hourly, or more
frequently, using full and incremental or continuous (streaming)
approaches.</p>

<p>Therefore, when formulating a backup strategy, consider how quickly
you need to be able to recover from a problem, and how much recent
data you can afford to lose.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Requirements of the Cloud Environment in Perspective"><div class="sect2" id="idm140203536595488">
<h2>Requirements of the Cloud Environment in Perspective</h2>

<p><a data-type="indexterm" data-primary="cloud environment" data-secondary="technical challenges of" id="idm140203536594320"></a><a data-type="indexterm" data-primary="data integrity" data-secondary="technical challenges of" id="idm140203536593376"></a>Cloud environments introduce a unique combination of technical challenges:</p>

<ul>
<li>
<p>If the environment uses a mixture of transactional and nontransactional backup and restore solutions, recovered data won’t necessarily be correct.</p>
</li>
<li>
<p>If services must evolve without going down for maintenance, different versions of business logic may act on data in parallel.</p>
</li>
<li>
<p>If interacting services are versioned independently, incompatible versions of different services may interact momentarily, further increasing the chance of accidental data corruption or data loss.</p>
</li>
</ul>

<p>In addition, in order to maintain economy of scale, service providers must provide only a limited number of APIs. These APIs must be simple and easy to use for the vast majority of applications, or few customers will use them. At the same time, the APIs must be robust enough to understand the following:</p>

<ul>
<li>
<p>Data locality and caching</p>
</li>
<li>
<p>Local and global data distribution</p>
</li>
<li>
<p>Strong and/or eventual consistency</p>
</li>
<li>
<p>Data durability, backup, and recovery</p>
</li>
</ul>

<p>Otherwise, sophisticated customers can’t migrate applications to the cloud, and simple applications that grow complex and large will need complete rewrites in order to use different, more complex APIs.</p>

<p>Problems arise when the preceding API features are used in certain combinations. If the service provider doesn’t solve these problems, then the applications that run into these challenges must identify and solve them independently.</p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Google SRE Objectives in Maintaining Data Integrity and Availability"><div class="sect1" id="idm140203536582032">
<h1>Google SRE Objectives in Maintaining Data Integrity and Availability</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="SRE objectives for" id="DIobject26"></a>While SRE’s goal of “maintaining integrity of persistent data” is a good vision, we thrive on concrete objectives with measurable indicators.  SRE defines key metrics that we use to set expectations for the capabilities of our systems and processes through tests and to track their performance during an actual event.</p>








<section data-type="sect2" class="pagebreak-before" data-pdf-bookmark="Data Integrity Is the Means; Data Availability Is the Goal"><div class="sect2" id="idm140203536579008">
<h2>Data Integrity Is the Means; Data Availability Is the Goal</h2>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="from users’ perspective" id="idm140203536577552"></a>Data integrity refers to the accuracy and consistency of
data throughout its lifetime. Users need to know that information will
be correct and won’t change in some unexpected way from the time
it’s first recorded to the last time it’s observed. But is such
assurance enough?</p>

<p>Consider the case of an email provider who suffered a weeklong data
outage <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Kinc09">[Kinc09]</a>. Over the space of 10 days, users had to find other,
temporary methods of conducting their business with the expectation
that they’d soon return to their established email accounts,
identities, and accumulated histories.</p>

<p>Then, the worst possible news arrived: the provider announced that
despite earlier expectations, the trove of past email and contacts was
in fact gone—evaporated and never to be seen again. It seemed that a
series of mishaps in managing data integrity had conspired to leave
the service provider with no usable backups. Furious users either
stuck with their interim identities or established new identities,
abandoning their troubled former email provider.</p>

<p>But wait! Several days after the declaration of absolute loss, the
provider announced that the users’ personal information <em>could</em> be
recovered. There was no data loss; this was only an outage. All was
well!</p>

<p>Except, <em>all was not well</em>. User data had been preserved, but the data
was not accessible by the people who needed it for too long.</p>

<p>The moral of this example: From the user’s point of view, data
integrity without expected and regular data availability is
effectively the same as having no data at all.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Delivering a Recovery System, Rather Than a Backup System"><div class="sect2" id="idm140203536570720">
<h2>Delivering a Recovery System, Rather Than a Backup System</h2>

<p><a data-type="indexterm" data-primary="recovery systems" id="idm140203536569536"></a>Making backups is a classically neglected, delegated, and deferred
task of system administration. Backups aren’t a high priority for
anyone—they’re an ongoing drain on time and resources, and yield no
immediate visible benefit. For this reason, a lack of diligence in
implementing a backup strategy is typically met with a sympathetic eye
roll. One might argue that, like most measures of protection against
low-risk dangers, such an attitude is pragmatic. The fundamental
problem with this lackadaisical strategy is that the dangers it
entails may be low risk, but they are also high impact. When your
service’s data is unavailable, your response can make or break your
service, product, and even your company.</p>

<p>Instead of focusing on the thankless job of taking a backup, it’s much
more useful, not to mention easier, to motivate participation in
taking backups by concentrating on a task with a visible payoff: the
<em>restore</em>! <em>Backups are a tax</em>, one paid on an ongoing basis for the
municipal service of guaranteed data availability. Instead of
emphasizing the tax, draw attention to the service the tax funds: data
availability. We don’t make teams “practice” their backups, instead:</p>

<ul>
<li>
<p>Teams define service level objectives (SLOs) for data availability
in a variety of failure modes.</p>
</li>
<li>
<p>A team practices and demonstrates their ability to meet those SLOs.</p>
</li>
</ul>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Types of Failures That Lead to Data Loss"><div class="sect2" id="idm140203536563552">
<h2>Types of Failures That Lead to Data Loss</h2>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="conditions leading to failure" id="idm140203536562384"></a>As illustrated by <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#fig_data-integrity_failure-modes">Figure&nbsp;26-1</a>, at a very high
level, there are 24 distinct types of failures when the 3 factors can occur
in any combination. You should consider each of these potential failures when designing a data integrity program. The factors of
data integrity failure modes are as follows:</p>
<dl>
<dt>Root cause</dt>
<dd>
<p>An unrecoverable loss of data may be caused by a
number of factors: user action, operator error, application bugs,
defects in infrastructure, faulty hardware, or site catastrophes.</p>
</dd>
<dt>Scope</dt>
<dd>
<p>Some losses are widespread, affecting many entities. Some
losses are narrow and directed, deleting or corrupting data specific
to a small subset of users.</p>
</dd>
<dt>Rate</dt>
<dd>
<p>Some data losses are a big bang event (for example, 1
million rows are replaced by only 10 rows in a single minute), whereas some data losses are creeping (for example, 10 rows of data are
deleted every minute over the course of weeks).</p>
</dd>
</dl>

<figure><div id="fig_data-integrity_failure-modes" class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_2601.png" alt="The factors of data integrity failure modes." width="1289" height="523" data-mfp-src="/library/view/site-reliability-engineering/9781491929117/assets/srle_2601.png">
<h6><span class="label">Figure 26-1. </span>The factors of data integrity failure modes</h6>
</div></figure>

<p>An effective restore plan must account for any of these failure modes
occurring in any conceivable combination. What may be a perfectly
effective strategy for guarding against a data loss caused by a
creeping application bug may be of no help whatsoever when your
colocation datacenter catches fire.</p>

<p>A study of 19 data recovery efforts at Google found that the most
common user-visible data loss scenarios involved data deletion or loss
of referential integrity caused by software bugs. The most challenging
variants involved low-grade corruption or deletion that was discovered
weeks to months after the bugs were first released into the production
environment. Therefore, the safeguards Google employs should be well
suited to prevent or recover from these types of loss.</p>

<p>To recover from such scenarios, a large and successful application
needs to retrieve data for perhaps millions of users spread across
days, weeks, or months. The application may also need to recover each
affected artifact to a unique point in time. This data recovery
scenario is called “point-in-time recovery” outside Google, and “time-travel” inside
Google.</p>

<p><a data-type="indexterm" data-primary="datastores" data-secondary="ACID and BASE" id="idm140203536550272"></a>A backup and recovery solution that provides point-in-time recovery
for an application across its ACID and BASE datastores while meeting
strict uptime, latency, scalability, velocity, and cost goals is a
chimera today!</p>

<p>Solving this problem with your own engineers entails sacrificing
velocity. Many projects compromise by adopting a tiered backup
strategy without point-in-time recovery. For instance, the APIs
beneath your application may support a variety of data recovery
mechanisms. Expensive local “snapshots” may provide limited protection
from application bugs and offer quick restoration functionality, so
you might retain a few days of such local “snapshots,” taken several
hours apart. Cost-effective full and incremental copies every two days
may be retained longer. Point-in-time recovery is a very nice feature
to have if one or more of these strategies support it.</p>

<p>Consider the data recovery options provided by the cloud APIs you are
about to use. Trade point-in-time recovery against a tiered strategy
if necessary, but don’t resort to not using either! If you can have
both features, use both features. Each of these features (or both)
will be valuable at some point.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Challenges of Maintaining Data Integrity Deep and Wide"><div class="sect2" id="idm140203536546800">
<h2>Challenges of Maintaining Data Integrity Deep and Wide</h2>

<p><a data-type="indexterm" data-primary="replication" id="idm140203536545632"></a><a data-type="indexterm" data-primary="redundancy" id="idm140203536544960"></a><a data-type="indexterm" data-primary="recoverability" id="idm140203536544288"></a>In designing a data integrity program, it’s
important to recognize that <em>replication and
redundancy are not
recoverability</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Scaling issues: Fulls, incrementals, and the competing forces of backups and restores"><div class="sect3" id="idm140203536542752">
<h3>Scaling issues: Fulls, incrementals, and the competing forces of backups and restores</h3>

<p><a data-type="indexterm" data-primary="scale" data-secondary="issues in" id="idm140203536541552"></a>A classic but flawed response to the question “Do you have a backup?”
is “We have something even better than a backup—replication!”
Replication provides many benefits, including locality of data and
protection from a site-specific disaster, but it can’t protect you
from many sources of data loss. Datastores that automatically sync
multiple replicas guarantee that a corrupt database row or errant
delete are pushed to all of your copies, likely before you can isolate
the problem.</p>

<p>To address this concern, you might make nonserving copies of your
data in some other format, such as frequent database exports to a
native file. This additional measure adds protection from the types of
errors replication doesn’t protect against—user errors and
application-layer bugs—but does nothing to guard against losses
introduced at a lower layer. This measure also introduces a risk of
bugs during data conversion (in both directions) and during storage of
the native file, in addition to possible mismatches in semantics
between the two formats. Imagine a zero-day attack<sup><a data-type="noteref" id="idm140203536538944-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536538944" class="totri-footnote">5</a></sup> at some low level
of your stack, such as the filesystem or device driver. Any copies
that rely on the compromised software component, including the
database exports that were written to the same filesystem that backs
your database, are <span class="keep-together">vulnerable</span>.</p>

<p>Thus, we see that diversity is key: protecting against a failure at
layer X requires storing data on diverse components at that layer.
Media isolation protects against media flaws: a bug or attack in a
disk device driver is unlikely to affect tape drives. If we could,
we’d make backup copies of our valuable data on clay
tablets.<sup><a data-type="noteref" id="idm140203536535328-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536535328" class="totri-footnote">6</a></sup></p>

<p>The forces of data freshness and restore completion compete against
comprehensive protection. The further down the stack you push a
snapshot of your data, the longer it takes to make a copy, which means
that the frequency of copies decreases. At the database level, a
transaction may take on the order of seconds to replicate. Exporting a
database snapshot to the filesystem underneath may take 40 minutes. A
full backup of the underlying filesystem may take hours.</p>

<p>In this scenario, you may lose up to 40 minutes of the most recent
data when you restore the latest snapshot. A restore from the
filesystem backup might incur hours of missing transactions.
Additionally, restoring probably takes as long as backing up, so
actually loading the data might take hours. You’d obviously like to
have the freshest data back as quickly as possible, but depending on
the type of failure, that freshest and most immediately available copy
might not be an option.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Retention"><div class="sect3" id="idm140203536531680">
<h3>Retention</h3>

<p><a data-type="indexterm" data-primary="retention" id="idm140203536530544"></a>Retention—how long you keep copies of your data around—is yet another
factor to consider in your data recovery plans.</p>

<p>While it’s likely that you or your customers will quickly notice the
sudden emptying of an entire database, it might take days for a more
gradual loss of data to attract the right person’s attention.
Restoring the lost data in the latter scenario requires snapshots
taken further back in time. When reaching back this far, you’ll
likely want to merge the restored data with the current state. Doing
so significantly complicates the restore process.<a data-type="indexterm" data-primary="" data-startref="DIobject26" id="idm140203536528720"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="How Google SRE Faces the Challenges of Data Integrity"><div class="sect1" id="idm140203536527136">
<h1>How Google SRE Faces the Challenges of Data Integrity</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="SRE approach to" id="DIsreapp26"></a>Similar to our assumption that Google’s underlying systems are prone to failure, we assume that any of our protection mechanisms are also subject to the same forces and can fail in the same ways and at the most inconvenient of times. Maintaining a guarantee of data integrity at large scale, a challenge that is further complicated by the high rate of change of the involved software systems, requires a number of complementary but uncoupled practices, each chosen to offer a high degree of protection on its own.</p>








<section data-type="sect2" data-pdf-bookmark="The 24 Combinations of Data Integrity Failure Modes"><div class="sect2" id="idm140203536523936">
<h2>The 24 Combinations of Data Integrity Failure Modes</h2>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="failure modes" id="idm140203536522768"></a><a data-type="indexterm" data-primary="defense in depth, for data integrity" id="idm140203536521824"></a>Given the many ways data can be lost (as described previously), there is no
silver bullet that guards against the many combinations of failure
modes. Instead, you need defense in depth. Defense in depth comprises
multiple layers, with each successive layer of defense conferring
protection from progressively less common data loss scenarios. <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#object_journey_26">Figure&nbsp;26-2</a> illustrates an object’s journey from soft deletion to destruction, and the data recovery strategies that should be employed along this journey to ensure defense in depth.</p>

<p><a data-type="indexterm" data-primary="lazy deletion" id="idm140203536519376"></a>The first layer is <em>soft deletion</em> (or “lazy deletion” in the case of
developer API offerings), which has proven to be an effective defense
against inadvertent data deletion scenarios. The second line of
defense is <em>backups and their related recovery methods</em>. The third and
final layer is <em>regular data validation</em>, covered in
<a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#xref_data-integrity_early-detection">“Third Layer: Early Detection”</a>. Across all these layers, the
presence of <em>replication</em> is occasionally useful for data
recovery in specific scenarios (although data recovery plans should not rely upon replication).</p>

<figure><div id="object_journey_26" class="figure">
<img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/srle_2602.png" alt="An object’s journey from soft deletion to destruction." width="1493" height="480" data-mfp-src="/library/view/site-reliability-engineering/9781491929117/assets/srle_2602.png">
<h6><span class="label">Figure 26-2. </span>An object’s journey from soft deletion to destruction</h6>
</div></figure>
</div></section>













<section data-type="sect2" data-pdf-bookmark="First Layer: Soft Deletion"><div class="sect2" id="idm140203536513072">
<h2>First Layer: Soft Deletion</h2>

<p><a data-type="indexterm" data-primary="soft deletion" id="idm140203536511920"></a>When velocity is high and privacy matters, bugs in applications
account for the vast majority of data loss and corruption events. In
fact, data deletion bugs may become so common that the ability to
undelete data for a limited time becomes the primary line of defense
against the majority of otherwise permanent, inadvertent data loss.</p>

<p>Any product that upholds the privacy of its users must allow the users
to delete selected subsets and/or all of their data. Such products
incur a support burden due to accidental deletion. Giving users the
ability to undelete their data (for example, via a trash folder)
reduces but cannot completely eliminate this support burden,
particularly if your service also supports third-party add-ons that
can also delete data.</p>

<p>Soft deletion can dramatically reduce or even completely eliminate
this support burden. Soft deletion means that deleted data is
immediately marked as such, rendering it unusable by all but the
application’s administrative code paths. Administrative code paths
may include legal discovery, hijacked account recovery, enterprise
administration, user support, and problem troubleshooting and its
related features. Conduct soft deletion when a user empties his or her
trash, and provide a user support tool that enables authorized
administrators to undelete any items accidentally deleted by users.
Google implements this strategy for our most popular productivity
applications; otherwise, the user support engineering burden would be
untenable.</p>

<p>You can extend the soft deletion strategy even further by offering
users the option to recover deleted data. For example, the Gmail trash
bin allows users to access messages that were deleted fewer than 30
days ago.</p>

<p>Another common source of unwanted data deletion occurs as a result of
account hijacking. In account hijacking scenarios, a hijacker commonly
deletes the original user’s data before using the account for
spamming and other unlawful purposes. When you combine the commonality
of accidental user deletion with the risk of data deletion by
hijackers, the case for a programmatic soft deletion and undeletion
interface within and/or beneath your application becomes clear.</p>

<p>Soft deletion implies that once data is marked as such, it is
destroyed after a reasonable delay. The length of the delay depends
upon an organization’s policies and applicable laws, available
storage resources and cost, and product pricing and market
positioning, especially in cases involving much short-lived data.
Common choices of soft deletion delays are 15, 30, 45, or 60 days. In
Google’s experience, the majority of account hijacking and data
integrity issues are reported or detected within 60 days. Therefore,
the case for soft deleting data for longer than 60 days may not be
strong.</p>

<p><a data-type="indexterm" data-primary="batch processing pipelines" id="idm140203536505888"></a>Google has also found that the most devastating acute data deletion
cases are caused by application developers unfamiliar with existing
code but working on deletion-related code, especially batch processing
pipelines (e.g., an offline MapReduce or Hadoop pipeline). It’s
advantageous to design your interfaces to hinder developers unfamiliar
with your code from circumventing soft deletion features with new
code. One effective way of achieving this is to implement cloud
computing offerings <span class="keep-together">that include</span> built-in soft deletion and undeletion
APIs, making sure to <em>enable said feature</em>.<sup><a data-type="noteref" id="idm140203536503344-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536503344" class="totri-footnote">7</a></sup> Even the best armor is useless if you don’t put it on.</p>

<p>Soft deletion strategies cover data deletion features in consumer
products like Gmail or Google Drive, but what if you support a cloud
computing offering instead? Assuming your cloud computing offering
already supports a programmatic soft deletion and undeletion feature
with reasonable defaults, the remaining accidental data deletion
scenarios will originate in mistakes made by your own internal
developers or your developer customers.</p>

<p>In such cases, it can be useful to introduce an additional layer of
soft deletion, which we will refer to as “lazy deletion.” You can
think of lazy deletion as behind the scenes purging, controlled by the
storage system (whereas soft deletion is controlled by and expressed
to the client application or service). In a lazy deletion scenario,
data that is deleted by a cloud application becomes immediately
inaccessible to the application, but is preserved by the cloud
service provider for up to a few weeks before destruction. Lazy
deletion isn’t advisable in all defense in depth strategies: a long
lazy deletion period is costly in systems with much short-lived data,
and impractical in systems that must guarantee destruction of deleted
data within a reasonable time frame (i.e., those that offer privacy
guarantees).</p>

<p>To sum up the first layer of defense in depth:</p>

<ul>
<li>
<p>A trash folder that allows users to undelete data is the primary
defense against user error.</p>
</li>
<li>
<p>Soft deletion is the primary defense
against developer error and the secondary defense against user error.</p>
</li>
<li>
<p>In developer offerings, lazy deletion is the primary defense against
internal developer error and the secondary defense against external
developer error.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="revision history" id="idm140203536495280"></a>What about <em>revision history</em>? Some products provide the ability to
revert items to previous states. When such a feature is available to
users, it is a form of trash. When available to developers, it may or
may not substitute for soft deletion, depending on its implementation.</p>

<p>At Google, revision history has proven useful in recovering from
certain data corruption scenarios, but not in recovering from most
data loss scenarios involving accidental deletion, programmatic or
otherwise. This is because some revision history implementations treat
deletion as a special case in which previous states must be removed,
as opposed to mutating an item whose history may be retained for a
certain time period. To provide adequate protection against unwanted
deletion, apply the lazy and/or soft deletion principles to revision history also.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Second Layer: Backups and Their Related Recovery Methods"><div class="sect2" id="idm140203536492448">
<h2>Second Layer: Backups and Their Related Recovery Methods</h2>

<p>Backups
and data recovery are the second line of defense after soft deletion.
The most important principle in this layer is that backups don’t
matter; what matters is recovery. The factors supporting successful
recovery should drive your backup decisions, not the other way around.</p>

<p>In other words, the scenarios in which you want your backups to help
you recover should dictate the following:</p>

<ul>
<li>
<p>Which backup and recovery methods to use</p>
</li>
<li>
<p>How frequently you
establish restore points by taking full or incremental backups of your
data</p>
</li>
<li>
<p>Where you store backups</p>
</li>
<li>
<p>How long you retain backups</p>
</li>
</ul>

<p>How much recent data can you afford to lose during a recovery effort?
The less data you can afford to lose, the more serious you should be
about an incremental backup strategy. In one of Google’s most
extreme cases, we used a near-real-time streaming backup strategy for
an older version of Gmail.</p>

<p>Even if money isn’t a limitation, frequent full backups are
expensive in other ways. Most notably, they impose a compute burden on
the live datastores of your service while it’s serving users,
driving your service closer to its scalability and performance limits.
To ease this burden, you can take full backups during off-peak hours,
and then a series of incremental backups when your service is busier.</p>

<p>How quickly do you need to recover? The faster your users need to be
rescued, the more local your backups should be. Often Google retains
costly but quick-to-restore snapshots<sup><a data-type="noteref" id="idm140203536483952-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536483952" class="totri-footnote">8</a></sup> for very short periods of
time within the storage instance, and stores less recent backups on
random access distributed storage within the same (or nearby)
datacenter for a slightly longer time. Such a strategy alone would not
protect from site-level failures, so those backups are often
transferred to nearline or offline locations for a longer time period
before they’re expired in favor of newer backups.</p>

<p>How far back should your backups reach? Your backup strategy becomes
more costly the further back you reach, while the scenarios from which
you can hope to recover increase (although this increase is subject to
diminishing returns).</p>

<p>In Google’s experience, low-grade data mutation or deletion bugs
within application code demand the furthest reaches back in time, as
some of those bugs were noticed months after the first data loss
began. Such cases suggest that you’d like the ability to reach back
in time as far as possible.</p>

<p>On the flip side, in a high-velocity development environment, changes
to code and schema may render older backups expensive or impossible to
use. Furthermore, it is challenging to recover different subsets of
data to different restore points, because doing so would involve
multiple backups. Yet, that is exactly the sort of recovery effort
demanded by low-grade data corruption or deletion scenarios.</p>

<p>The
strategies described in <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#xref_data-integrity_early-detection">“Third Layer: Early Detection”</a> are meant to
speed detection of low-grade data mutation or deletion bugs within
application code, at least partly warding off the need for this type
of complex recovery effort. Still, how do you confer reasonable
protection before you know what kinds of issues to detect? Google
chose to draw the line between 30 and 90 days of backups for many
services. Where a service falls within this window depends on its
tolerance for data loss and its relative investments in early
detection.</p>

<p>To sum up our advice for guarding against the 24 combinations of data
integrity failure modes: addressing a broad range of scenarios at
reasonable cost demands a tiered backup strategy. The first tier
comprises many frequent and quickly restored backups stored closest to
the live datastores, perhaps using the same or similar storage
technologies as the data sources. Doing so confers protection from the
majority of scenarios involving software bugs and developer error. Due
to relative expense, backups are retained in this tier for anywhere
from hours to single-digit days, and may take minutes to restore.</p>

<p>The second tier comprises fewer backups retained for single-digit or
low double-digit days on random access distributed filesystems local
to the site. These backups may take hours to restore and confer
additional protection from mishaps affecting particular storage
technologies in your serving stack, but not the technologies used to
contain the backups. This tier also protects
against bugs in your application that are detected too late to rely
upon the first tier of your backup strategy. If you are introducing
new versions of your code to production twice a week, it may make
sense to retain these backups for at least a week or two before
deleting them.</p>

<p>Subsequent tiers take advantage of nearline storage such as dedicated
tape libraries and offsite storage of the backup media (e.g., tapes or
disk drives). Backups in these tiers confer protection against
site-level issues, such as a datacenter power outage or distributed
filesystem corruption due to a bug.</p>

<p>It is expensive to move large amounts of data to and from tiers. On
the other hand, storage capacity at the later tiers does not contend
with growth of the live production storage instances of your service.
As a result, backups in these tiers tend to be taken less frequently
but retained longer.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Overarching Layer: Replication"><div class="sect2" id="idm140203536474512">
<h2>Overarching Layer: Replication</h2>

<p><a data-type="indexterm" data-primary="replication" id="idm140203536473360"></a>In an ideal world, every storage
instance, including the instances containing your backups, would be
replicated. During a data recovery effort, the last thing you want is
to discover is that your backups themselves lost the needed data or
that the datacenter containing the most useful backup is under
maintenance.</p>

<p><a data-type="indexterm" data-primary="redundancy" id="idm140203536471936"></a><a data-type="indexterm" data-primary="RAID" id="idm140203536471264"></a><a data-type="indexterm" data-primary="Reed-Solomon erasure codes" id="idm140203536470592"></a><a data-type="indexterm" data-primary="GFS (Google File System)" id="idm140203536469904"></a>As the volume of data increases, replication of every storage instance
isn’t always feasible. In such cases, it makes sense to stagger
successive backups across different sites, each of which may fail
independently, and to write your backups using a redundancy method
such as RAID, Reed-Solomon erasure codes, or GFS-style
replication.<sup><a data-type="noteref" id="idm140203536468736-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536468736" class="totri-footnote">9</a></sup></p>

<p>When choosing a system of redundancy, don’t rely upon an
infrequently used scheme whose only “tests” of efficacy are your
own infrequent data recovery attempts. Instead, choose a popular
scheme that’s in common and continual use by many of its users.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="1T Versus 1E: Not “Just” a Bigger Backup"><div class="sect2" id="idm140203536465040">
<h2>1T Versus 1E: Not “Just” a Bigger Backup</h2>

<p>Processes and practices applied to volumes of data measured in T (terabytes) don’t scale well to data measured in E (exabytes).  Validating, copying,
and performing round-trip tests on a few gigabytes of structured data
is an interesting problem. However, assuming that you have sufficient
knowledge of your schema and transaction model, this exercise
doesn’t present any special challenges. You typically just need to
procure the machine resources to iterate over your data, perform some
validation logic, and delegate enough storage to hold a few copies of
your data.</p>

<p>Now let’s up the ante: instead of a few gigabytes, let’s try
securing and validating 700 petabytes of structured data. Assuming
ideal SATA 2.0 performance of 300 MB/s, a
single task that iterates over all of your data and performs even the
most basic of validation checks will take 8 decades. Making a few full
backups, assuming you have the media, is going to take at least as
long.  Restore time, with some post-processing, will take even longer.
We’re now looking at almost a full century to restore a backup that
was up to 80 years old when you started the restore. Obviously, such a
strategy needs to be rethought.</p>

<p>The most common and largely effective technique used to back up
massive amounts of data is to establish “trust points” in your
data—portions of your stored data that are verified after being
rendered immutable, usually by the passage of time. Once we know that
a given user profile or transaction is fixed and won’t be subject to
further change, we can verify its internal state and make suitable
copies for recovery purposes. You can then make incremental backups
that only include data that has been modified or added since your last
backup. This technique brings your backup time in line with your
“mainline” processing time, meaning that frequent incremental
backups can save you from the 80-year monolithic verify and copy job.</p>

<p><a data-type="indexterm" data-primary="restores" id="idm140203536460624"></a>However, remember that we care about <em>restores</em>, not backups. Let’s
say that we took a full backup three years ago and have been making
daily incremental backups since. A full restore of our data will
serially process a chain of over 1,000 highly interdependent
backups. Each independent backup incurs additional risk of failure,
not to mention the logistical burden of scheduling and the runtime
cost of those jobs.</p>

<p>Another way we can reduce the wall time of our copying and
verification jobs is to distribute the load. If we shard our data
well, it’s possible to run <em>N</em> tasks in parallel, with each task
responsible for copying and verifying 1/<em>N</em>th of our data. Doing so
requires some forethought and planning in the schema design and the
physical deployment of our data in order to:</p>

<ul>
<li>
<p>Balance the data correctly</p>
</li>
<li>
<p>Ensure the independence of each shard</p>
</li>
<li>
<p>Avoid contention among the concurrent sibling tasks</p>
</li>
</ul>

<p>Between distributing the load horizontally and restricting the work to
vertical slices of the data demarcated by time, we can reduce those
eight decades of wall time by several orders of magnitude, rendering
our restores relevant.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Third Layer: Early Detection"><div class="sect2" id="xref_data-integrity_early-detection">
<h2>Third Layer: Early Detection</h2>

<p>“Bad” <a data-type="indexterm" data-primary="early detection for data integrity" data-seealso="data integrity" id="idm140203536451520"></a>data doesn’t sit idly by, it propagates.  References to missing or corrupt data are copied, links fan out, and with every update the overall quality of your datastore goes down. Subsequent dependent transactions and potential data format changes make restoring from a given backup more difficult as the clock ticks.  The sooner you know about a data loss, the easier and more complete your recovery can be.</p>










<section data-type="sect3" data-pdf-bookmark="Challenges faced by cloud developers"><div class="sect3" id="idm140203536449840">
<h3>Challenges faced by cloud developers</h3>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="selecting strategy for" id="idm140203536448688"></a><a data-type="indexterm" data-primary="cloud environment" data-secondary="data integrity strategies" id="idm140203536447744"></a>In high-velocity environments, cloud application and
infrastructure services face many data integrity challenges at runtime, such as:</p>

<ul>
<li>
<p>Referential integrity between datastores</p>
</li>
<li>
<p>Schema changes</p>
</li>
<li>
<p>Aging code</p>
</li>
<li>
<p>Zero-downtime data migrations</p>
</li>
<li>
<p>Evolving integration
points with other services</p>
</li>
</ul>

<p>Without conscious engineering effort to track emerging relationships
in its data, the data quality of a successful and growing service
degrades over time.</p>

<p>Often, the novice
cloud developer who chooses a distributed consistent storage API (such
as Megastore) delegates the integrity of the application’s data to
the distributed consistent algorithm implemented beneath the API (such
as Paxos; see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch23.html#chapter_distributed-consensus">Chapter&nbsp;23</a>). The developer reasons that the selected API alone will keep
the application’s data in good shape. As a result, they unify all
application data into a single storage solution that guarantees
distributed consistency, avoiding referential integrity problems in
exchange for reduced performance and/or scale.</p>

<p>While such algorithms are infallible in theory, their implementations
are often riddled with hacks, optimizations, bugs, and educated
guesses. For example: in theory, Paxos ignores failed compute nodes
and can make progress as long as a quorum of functioning nodes is
maintained. In practice, however, ignoring a failed node may
correspond to timeouts, retries, and other failure-handling approaches
beneath the particular Paxos implementation <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Cha07">[Cha07]</a>. How
long should Paxos try to contact an unresponsive node before timing it
out? When a particular machine fails (perhaps intermittently) in a
certain way, with a certain timing, and at a particular datacenter,
unpredictable behavior results. The larger the scale of an
application, the more frequently the application is affected,
unbeknownst, by such inconsistencies. If this logic holds true even
when applied to Paxos implementations (as has been true for Google),
then it must be more true for eventually consistent implementations
such as Bigtable (which has also shown to be true). Affected
applications have no way to know that 100% of their data is good until
they check: trust storage systems, but <span class="keep-together">verify</span>!</p>

<p>To complicate this problem, in order to recover from low-grade
data corruption or deletion scenarios, we must recover different subsets of data to different restore points using different backups,
while changes to code and schema may render older backups ineffective
in high-velocity environments.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Out-of-band data validation"><div class="sect3" id="idm140203536434928">
<h3>Out-of-band data validation</h3>

<p><a data-type="indexterm" data-primary="out-of-band checks and balances" id="idm140203536433776"></a><a data-type="indexterm" data-primary="datacenters" data-secondary="data validation" id="idm140203536433088"></a>To prevent data quality from degrading before users’ eyes, and to
detect low-grade data corruption or data loss scenarios before they
become unrecoverable, a system of out-of-band checks and balances is
needed both within and between an application’s datastores.</p>

<p>Most often, these data validation pipelines are implemented as
collections of map-reductions or Hadoop jobs. Frequently, such
pipelines are added as an afterthought to services that are already
popular and successful. Sometimes, such pipelines are first attempted
when services reach scalability limits and are rebuilt from the ground
up. Google has built validators in response to each of these
situations.</p>

<p>Shunting some developers to work on a data validation pipeline can
slow engineering velocity in the short term. However, devoting
engineering resources to data validation endows other developers with
the courage to move faster in the long run, because the engineers know that
data corruption bugs are less likely to sneak into production
unnoticed. Similar to the effects enjoyed when units test are
introduced early in the project lifecycle, a data validation pipeline
results in an overall acceleration of software development projects.</p>

<p>To cite a specific example: Gmail sports a number of data validators,
each of which has detected actual data integrity problems in
production. Gmail developers derive comfort from the knowledge that
bugs introducing inconsistencies in production data are detected
within 24 hours, and shudder at the thought of running their data
validators less often than daily. These validators, along with a
culture of unit and regression testing and other best practices, have
given Gmail developers the courage to introduce code changes to
Gmail’s production storage implementation more frequently than once
a week.</p>

<p>Out-of-band data validation is tricky to implement correctly. When too
strict, even simple, appropriate changes cause validation to fail. As
a result, engineers abandon data validation altogether. If the data
validation isn’t strict enough, user experience–affecting data
corruption can slip through undetected. To find the right balance,
only validate invariants that cause devastation to users.</p>

<p>For example, Google Drive periodically validates that file contents
align with listings in Drive folders. If these two elements don’t
align, some files would be missing data—a disastrous outcome. Drive
infrastructure developers were so invested in data integrity that they
also enhanced their validators to automatically fix such
inconsistencies. This safeguard turned a potential emergency
“all-hands-on-deck-omigosh-files-are-disappearing!” data loss
situation in 2013 into a business as usual, “let’s go home and fix the
root cause on Monday,” situation. By transforming emergencies into
business as usual, validators improve engineering morale, quality of
life, and predictability.</p>

<p>Out-of-band validators can be expensive at scale. A significant
portion of Gmail’s compute resource footprint supports a collection
of daily validators. To compound this expense, these validators also
lower server-side cache hit rates, reducing server-side responsiveness
experienced by users. To mitigate this hit to responsiveness, Gmail
provides a variety of knobs for rate-limiting its validators and
periodically refactors the validators to reduce disk contention. In
one such refactoring effort, we cut the contention for disk spindles
by 60% without significantly reducing the scope of the invariants they
covered. While the majority of Gmail’s validators run daily, the
workload of the largest validator is divided into 10–14 shards, with
one shard validated per day for reasons of scale.</p>

<p>Google Compute Storage is another example of the challenges scale
entails to data validation. When its out-of-band validators could no
longer finish within a day, Compute Storage engineers had to devise a
more efficient way to verify its metadata than use of brute force
alone. Similar to its application in data recovery, a tiered strategy
can also be useful in out-of-band data validation. As a service
scales, sacrifice rigor in daily validators. Make sure that daily
validators continue to catch the most disastrous scenarios within 24
hours, but continue with more rigorous validation at reduced frequency
to contain costs and latency.</p>

<p>Troubleshooting failed validations can take significant effort. Causes
of an intermittent failed validation could vanish within minutes,
hours, or days. Therefore, the ability to rapidly drill down into
validation audit logs is essential. Mature Google services provide
on-call engineers with comprehensive documentation and tools to
troubleshoot. For example, on-call engineers for Gmail are provided
with:</p>

<ul>
<li>
<p>A suite of playbook entries describing how to respond to a
validation failure alert</p>
</li>
<li>
<p>A BigQuery-like investigation tool</p>
</li>
<li>
<p>A data validation dashboard</p>
</li>
</ul>

<p>Effective out-of-band data validation demands all of the following:</p>

<ul>
<li>
<p>Validation job management</p>
</li>
<li>
<p>Monitoring, alerts, and dashboards</p>
</li>
<li>
<p>Rate-limiting features</p>
</li>
<li>
<p>Troubleshooting tools</p>
</li>
<li>
<p>Production playbooks</p>
</li>
<li>
<p>Data validation APIs that make validators easy to
add and refactor</p>
</li>
</ul>

<p>The majority of small engineering teams operating at high velocity
can’t afford to design, build, and maintain all of these systems. If
they are pressured to do so, the result is often fragile, limited, and wasteful
one-offs that fall quickly into disrepair. Therefore, structure your
engineering teams such that a central infrastructure team provides a
data validation framework for multiple product engineering teams. The
central infrastructure team maintains the out-of-band data
validation framework, while the product engineering teams maintain the
custom business logic at the heart of the validator to keep pace with
their evolving products.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Knowing That Data Recovery Will Work"><div class="sect2" id="idm140203536412576">
<h2>Knowing That Data Recovery Will Work</h2>

<p><a data-type="indexterm" data-primary="data recovery" id="idm140203536411424"></a><a data-type="indexterm" data-primary="recovery" id="idm140203536410752"></a>When does a light bulb break? When flicking the switch fails to turn
on the light?  Not always—often the bulb had already failed, and you simply notice
the failure at the unresponsive flick of the switch. By then, the room is
dark and you’ve stubbed your toe.</p>

<p>Likewise, your recovery dependencies (meaning mostly, but not only,
your backup), may be in a latent broken state, which you aren’t
aware of until you attempt to recover data.</p>

<p>If you discover that your restore process is broken before you need to
rely upon it, you can address the vulnerability before you fall victim
to it: you can take another backup, provision additional resources,
and change your SLO. But to take these actions proactively, you
first have to know they’re needed. To detect these vulnerabilities:</p>

<ul>
<li>
<p>Continuously test the recovery process as part of your
normal operations</p>
</li>
<li>
<p>Set up alerts that fire when a recovery process
fails to provide a heartbeat indication of its success</p>
</li>
</ul>

<p>What can go wrong with your recovery process? Anything and
everything—which is why the only test that should let you sleep at
night is a full end-to-end test. Let the proof be in the pudding. Even
if you recently ran a successful recovery, parts of your recovery
process can still break. If you take away just one lesson from this
chapter, remember that <em>you only know that you can recover your recent
state if you actually do so</em>.</p>

<p>If recovery tests are a manual, staged event, testing becomes an
unwelcome bit of drudgery that isn’t performed either deeply or
frequently enough to deserve your confidence. Therefore, automate
these tests whenever possible and then run them continuously.</p>

<p>The aspects of your recovery plan you should confirm are myriad:</p>

<ul>
<li>
<p>Are your backups valid and complete, or are they empty?</p>
</li>
<li>
<p>Do you have sufficient machine resources to run all of the setup,
restore, and post-processing tasks that comprise your recovery?</p>
</li>
<li>
<p>Does the recovery process complete in reasonable wall time?</p>
</li>
<li>
<p>Are you able to monitor the state of your recovery process as it
progresses?</p>
</li>
<li>
<p>Are you free of critical dependencies on resources outside of your
control, such as access to an offsite media storage vault that isn’t
available 24/7?</p>
</li>
</ul>

<p>Our testing has discovered the aforementioned failures, as well as
failures of many other components of a successful data recovery. If we
hadn’t discovered these failures in regular tests—that is, if we
came across the failures only when we needed to recover user data in
real emergencies—it’s quite possible that some of Google’s most
successful products today may not have stood the test of time.</p>

<p>Failures are inevitable. If you wait to discover them when you’re
under the gun, facing a real data loss, you’re playing with fire. If
testing forces the failures to happen before actual catastrophe strikes,
you can fix problems before any harm comes to <span class="keep-together">fruition</span>.<a data-type="indexterm" data-primary="" data-startref="DIsreapp26" id="idm140203536395856"></a></p>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Case Studies"><div class="sect1" id="idm140203536394432">
<h1>Case Studies</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="case studies in" id="DIcase26"></a>Life imitates art (or in this case, science), and as we predicted, real life has given us unfortunate and inevitable opportunities to put our data recovery systems and processes to the test, under real-world pressure.  Two of the more notable and interesting of these opportunities are discussed here.</p>








<section data-type="sect2" data-pdf-bookmark="Gmail—February, 2011: Restore from GTape"><div class="sect2" id="idm140203536391472">
<h2>Gmail—February, 2011: Restore from GTape</h2>

<p><a data-type="indexterm" data-primary="GTape" id="idm140203536390304"></a><a data-type="indexterm" data-primary="Gmail" id="idm140203536389632"></a>The first recovery case study we’ll examine was unique in a couple of ways: the number of failures that coincided to bring about the data loss, and the fact that it was the largest use of our last line of defense, the GTape offline backup system.</p>










<section data-type="sect3" data-pdf-bookmark="Sunday, February 27, 2011, late in the evening"><div class="sect3" id="idm140203536388400">
<h3>Sunday, February 27, 2011, late in the evening</h3>

<p>The Gmail backup
system pager is triggered, displaying a phone number to join a
conference call. The event we had long feared—indeed, the reason for
the backup system’s existence—has come to pass: Gmail lost a
significant amount of user data. Despite the system’s many
safeguards and internal checks and redundancies, the data disappeared
from Gmail.</p>

<p>This was the first large-scale use of GTape, a global
backup system for Gmail, to restore live customer data. Fortunately,
it was not the first such restore, as similar situations had been
previously simulated many times. Therefore, we were able to:</p>

<ul>
<li>
<p>Deliver an estimate of how long it would take to restore the
majority of the affected user accounts</p>
</li>
<li>
<p>Restore all of the accounts within several hours of our initial
estimate</p>
</li>
<li>
<p>Recover 99%+ of the data before the estimated completion time</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="defense in depth, for data integrity" id="idm140203536382432"></a><a data-type="indexterm" data-primary="emergency preparedness" id="idm140203536381744"></a>Was the ability to formulate such an estimate luck? No—our success
was the fruit of planning, adherence to best practices, hard work, and
cooperation, and we were glad to see our investment in each of these
elements pay off as well as it did. Google was able to restore the
lost data in a timely manner by executing a plan designed according to
the best practices of <em>Defense in Depth</em> and <em>Emergency Preparedness</em>.</p>

<p>When Google publicly revealed that we recovered this data from our
previously undisclosed tape backup system <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Slo11">[Slo11]</a>, public reaction was a mix
of surprise and amusement.
Tape? Doesn’t Google have lots of disks and a fast network to
replicate data this important? Of course Google has such resources,
but the principle of Defense in Depth dictates providing multiple
layers of protection to guard against the breakdown or compromise of
any single protection mechanism. Backing up online systems such as
Gmail provides defense in depth at two layers:</p>

<ul>
<li>
<p>A failure of the internal Gmail redundancy and backup subsystems</p>
</li>
<li>
<p>A wide failure or zero-day vulnerability in a device driver or filesystem affecting the underlying storage medium (disk)</p>
</li>
</ul>

<p>This particular failure resulted from the first scenario—while Gmail
had internal means of recovering lost data, this loss went beyond what
internal means could recover.</p>

<p>One of the most internally celebrated aspects of the Gmail data
recovery was the degree of cooperation and smooth coordination that
comprised the recovery. Many teams, some completely unrelated to Gmail
or data recovery, pitched in to help. The recovery couldn’t have
succeeded so smoothly without a central plan to choreograph such a
widely distributed Herculean effort; this plan was the product of
regular dress rehearsals and dry runs. Google’s devotion to
emergency preparedness leads us to view such failures as inevitable.
Accepting this inevitability, we don’t hope or bet to avoid such
disasters, but anticipate that they will occur. Thus, we need a plan
for dealing not only with the foreseeable failures, but for some
amount of random undifferentiated breakage, as well.</p>

<p>In short, we always <em>knew</em> that adherence to best practices is
important, and it was good to see that maxim proven true.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Google Music—March 2012: Runaway Deletion Detection"><div class="sect2" id="idm140203536372192">
<h2>Google Music—March 2012: Runaway Deletion Detection</h2>

<p>The second failure we’ll examine entails challenges in logistics that are unique to the scale of the datastore being recovered: where do you store over 5,000 tapes, and how do you efficiently (or even feasibly) read that much data from offline media in a reasonable amount of time?</p>










<section data-type="sect3" data-pdf-bookmark="Tuesday, March 6th, 2012, mid-afternoon"><div class="sect3" id="idm140203536370432">
<h3>Tuesday, March 6th, 2012, mid-afternoon</h3>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Discovering the problem"><div class="sect3" id="idm140203536369248">
<h3>Discovering the problem</h3>

<p>A Google Music user reports that previously unproblematic
tracks are being skipped. The team responsible for interfacing with
Google Music’s users notifies Google Music engineers. The problem is
investigated as a possible media streaming issue.</p>

<p>On March 7th, the investigating engineer discovers that the unplayable
track’s metadata is missing a reference that should point to the
actual audio data. He is surprised. The obvious fix is to locate the
audio data and reinstate the reference to the data. However, Google
engineering prides itself for a culture of fixing issues at the root,
so the engineer digs deeper.</p>

<p>When he finds the cause of the data integrity lapse, he almost has a
heart attack: the audio reference was removed by a privacy-protecting
data deletion pipeline. This part of Google Music was designed to
delete very large numbers of audio tracks in record time.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Assessing the damage"><div class="sect3" id="idm140203536365920">
<h3>Assessing the damage</h3>

<p>Google’s privacy policy protects a user’s personal data. As applied to
Google Music specifically, our privacy policy means that music files
and relevant metadata are removed within reasonable time after users
delete them. As the popularity of Google Music soared, the amount of
data grew rapidly, so the original deletion implementation needed to
be redesigned in 2012 to be more efficient. On February 6th, the
updated data deletion pipeline enjoyed its maiden run, to remove
relevant metadata. Nothing seemed amiss at the time, so a second stage
of the pipeline was allowed to remove the associated audio data too.</p>

<p>Could the engineer’s worst nightmare be true? He immediately sounded
the alarm, raising the priority of the support case to Google’s most
urgent classification and reporting the issue to engineering
management and Site Reliability Engineering. A small team of Google
Music developers and SREs assembled to tackle the issue, and the
offending pipeline was temporarily disabled to stem the tide of
external user <span class="keep-together">casualties</span>.</p>

<p>Next, manually checking the metadata for millions to billions of files
organized across multiple datacenters would be unthinkable. So the
team whipped up a hasty <span class="keep-together">MapReduce</span> job to assess the damage and waited
desperately for the job to complete. They froze as its results came in
on March 8th: the refactored data deletion pipeline had removed
approximately 600,000 audio references that shouldn’t have been
removed, affecting audio files for 21,000 users. Since the hasty
diagnosis pipeline made a few simplifications, the true extent of the
damage could be worse.</p>

<p>It had been over a month since the buggy data deletion pipeline first
ran, and that maiden run itself removed hundreds of thousands of audio
tracks that should not have been removed. Was there any hope of
getting the data back? If the tracks weren’t recovered, or weren’t
recovered fast enough, Google would have to face the music from its
users. How could we not have noticed this glitch?</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Resolving the issue"><div class="sect3" id="idm140203536359600">
<h3>Resolving the issue</h3>












<section data-type="sect4" data-pdf-bookmark="Parallel bug identification and recovery efforts"><div class="sect4" id="idm140203536358592">
<h4>Parallel bug identification and recovery efforts</h4>

<p>The first step in resolving the issue was to identify the actual bug,
and determine how and why the bug happened. As long as the root cause
wasn’t identified and fixed, any recovery efforts would be in vain.
We would be under pressure to re-enable the pipeline to respect the
requests of users who deleted audio tracks, but doing so would hurt
innocent users who would continue to lose store-bought music, or
worse, their own painstakingly recorded audio files. The only way to
escape the Catch-22<sup><a data-type="noteref" id="idm140203536356784-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536356784">10</a></sup>  was to fix the issue
at its root, and fix it quickly.</p>

<p>Yet there was no time to waste before mounting the recovery effort.
The audio tracks themselves were backed up to tape, but unlike our
Gmail case study, the encrypted backup tapes for Google Music were
trucked to offsite storage locations, because that option offered more
space for voluminous backups of users’ audio data. To restore the
experience of affected users quickly, the team decided to troubleshoot
the root cause while retrieving the offsite backup tapes (a rather
time-intensive restore option) in parallel.</p>

<p>The engineers split into two groups. The most experienced SREs worked
on the recovery effort, while the developers analyzed the data
deletion code and attempted to fix the data loss bug at its root. Due
to incomplete knowledge of the root problem, the recovery would have
to be staged in multiple passes. The first batch of nearly half a
million audio tracks was identified, and the team that maintained the
tape backup system was notified of the emergency recovery effort at
4:34 p.m. Pacific Time on March 8th.</p>

<p>The recovery team had one factor working in their favor: this recovery
effort occurred just weeks after the company’s annual disaster
recovery testing exercise (see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Kri12">[Kri12]</a>). The tape backup team
already knew the capabilities and limitations of their subsystems that had
been the subjects of DiRT tests and began dusting off a new tool they’d tested
during a DiRT exercise. Using the new tool, the combined recovery team began the
painstaking effort of mapping hundreds of thousands of audio files to
backups registered in the tape backup system, and then mapping the
files from backups to actual tapes.</p>

<p>In this way, the team determined that the initial recovery effort
would involve the recall of over 5,000 backup tapes by truck.
Afterwards, datacenter technicians would have to clear out space for
the tapes at tape libraries. A long, complex process of registering
the tapes and extracting the data from the tapes would follow,
involving workarounds and mitigations in the event of bad tapes, bad
drives, and unexpected system interactions.</p>

<p>Unfortunately, only 436,223 of the approximately 600,000 lost audio
tracks were found on tape backups, which meant that about 161,000
other audio tracks were eaten before they could be backed up. The
recovery team decided to figure out how to recover the 161,000 missing
tracks after they initiated the recovery process for the tracks with
tape backups.</p>

<p>Meanwhile, the root cause team had pursued and abandoned a red
herring: they initially thought that a storage service on which Google
Music depended had provided buggy data that misled the data deletion
pipelines to remove the wrong audio data. Upon closer investigation,
that theory was proven false. The root cause team scratched their
heads and continued their search for the elusive bug.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="First wave of recovery"><div class="sect4" id="idm140203536348304">
<h4>First wave of recovery</h4>

<p>Once the
recovery team had identified the backup tapes, the first recovery wave
kicked off on March 8th. Requesting 1.5 petabytes of data distributed
among thousands of tapes from offsite storage was one matter, but
extracting the data from the tapes was quite another. The custom-built
tape backup software stack wasn’t designed to handle a single
restore operation of such a large size, so the initial recovery was
split into 5,475 restore jobs. It would take a human operator typing
in one restore command a minute more than three days to request that
many restores, and any human operator would no doubt make many
mistakes. Just requesting the restore from the tape backup system
needed SRE to develop a programmatic solution.<sup><a data-type="noteref" id="idm140203536346288-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536346288">11</a></sup></p>

<p>By midnight on March 9th, Music SRE finished requesting all 5,475
restores. The tape backup system began working its magic. Four hours
later, it spat out a list of 5,337 backup tapes to be recalled from
offsite locations. In another eight hours, the tapes arrived at a
datacenter in a series of truck deliveries.</p>

<p>While the trucks were en route, datacenter technicians took several
tape libraries down for maintenance and removed thousands of tapes to
make way for the massive data recovery operation. Then the technicians
began painstakingly loading the tapes by hand as thousands of tapes
arrived in the wee hours of the morning. In past DiRT exercises, this
manual process proved hundreds of times faster for massive restores
than the robot-based methods provided by the tape library vendors.
Within three hours, the libraries were back up scanning the tapes and
performing thousands of restore jobs onto distributed compute storage.</p>

<p>Despite the team’s DiRT experience, the massive 1.5 petabyte
recovery took longer than the two days estimated. By the morning of
March 10th, only 74% of the 436,223 audio files had been successfully
transferred from 3,475 recalled backup tapes to distributed filesystem
storage at a nearby compute cluster. The other 1,862 backup tapes had
been omitted from the tape recall process by a vendor. In addition,
the recovery process had been held up by 17 bad tapes. In anticipation
of a failure due to bad tapes, a redundant encoding had been used to
write the backup files. Additional truck deliveries were set off to
recall the redundancy tapes, along with the other 1,862 tapes that had
been omitted by the first offsite recall.</p>

<p>By the morning of March 11th, over 99.95% of the restore operation had
completed, and the recall of additional redundancy tapes for the
remaining files was in progress. Although the data was safely on
distributed filesystems, additional data recovery steps were necessary
in order to make them accessible to users. The Google Music Team began
exercising these final steps of the data recovery process in parallel
on a small sample of recovered audio files to make sure the process
still worked as expected.</p>

<p>At that moment, Google Music production pagers sounded due to an
unrelated but critical user-affecting production failure—a failure
that fully engaged the Google Music team for two days. The data
recovery effort resumed on March 13th, when all 436,223 audio tracks
were once again made accessible to their users. In just short of 7
days, 1.5 petabytes of audio data had been reinstated to users with
the help of offsite tape backups; 5 of the 7 days comprised the actual
data recovery effort.</p>
</div></section>













<section data-type="sect4" data-pdf-bookmark="Second wave of recovery"><div class="sect4" id="idm140203536339376">
<h4>Second wave of recovery</h4>

<p>With the first wave of the recovery process
behind them, the team shifted its focus to the other 161,000 missing
audio files that had been deleted by the bug before they were backed
up. The majority of these files were store-bought and promotional
tracks, and the original store copies were unaffected by the bug. Such
tracks were quickly reinstated so that the affected users could enjoy
their music again.</p>

<p>However, a small portion of the 161,000 audio files had been uploaded
by the users themselves. The Google Music Team prompted their servers
to request that the Google Music clients of affected users re-upload
files dating from March 14th onward. This process lasted more than a
week. Thus concluded the complete recovery effort for the incident.</p>
</div></section>

</div></section>













<section data-type="sect3" data-pdf-bookmark="Addressing the root cause"><div class="sect3" id="idm140203536336432">
<h3>Addressing the root cause</h3>

<p>Eventually, the Google Music Team
identified the flaw in their refactored data deletion pipeline. To
understand this flaw, you first need context about how offline data
processing systems evolve on a large scale.</p>

<p>For a large and complex service comprising several subsystems and
storage services, even a task as simple as removing deleted data needs
to be performed in stages, each involving different datastores.</p>

<p>For data processing to finish quickly, the processing is parallelized
to run across tens of thousands of machines that exert a large load on
various subsystems. This distribution can slow the service for users,
or cause the service to crash under the heavy load.</p>

<p>To avoid these undesirable scenarios, cloud computing engineers often
make a short-lived copy of data on secondary storage, where the data
processing is then performed. Unless the relative age of the secondary
copies of data is carefully coordinated, this practice introduces race
conditions.</p>

<p>For instance, two stages of a pipeline may be designed to run in
strict succession, three hours apart, so that the second stage can
make a simplifying assumption about the correctness of its inputs.
Without this simplifying assumption, the logic of the second stage may
be hard to parallelize. But the stages may take longer to complete as
the volume of data grows. Eventually, the original design assumptions
may no longer hold for certain pieces of data needed by the second
stage.</p>

<p>At first, this race condition may occur for a tiny fraction of data.
But as the volume of data increases, a larger
and larger fraction of the data is at risk for triggering a race
condition. Such a scenario is probabilistic—the pipeline works
correctly for the vast majority of data and for most of the time. When
such race conditions occur in a data deletion pipeline, the wrong data
can be deleted nondeterministically.</p>

<p>Google Music’s data deletion pipeline was designed with coordination
and large margins for error in place. But when upstream stages of the
pipeline began to require increased time as the service grew,
performance optimizations were put in place so Google Music could
continue to meet privacy requirements. As a result, the probability of
an inadvertent data-deleting race condition in this pipeline began to
increase. When the pipeline was refactored, this probability again
significantly increased, up to a point at which the race conditions
occurred more regularly.</p>

<p><a data-type="indexterm" data-primary="" data-startref="DIcase26" id="idm140203536329824"></a>In the wake of the recovery effort, Google Music redesigned its data
deletion pipeline to eliminate this type of race condition. In
addition, we enhanced production monitoring and alerting systems to
detect similar large-scale runaway deletion bugs with the aim of
detecting and fixing such issues before users notice any
problems.<sup><a data-type="noteref" id="idm140203536328400-marker" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536328400">12</a></sup></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="General Principles of SRE as Applied to Data Integrity"><div class="sect1" id="idm140203536326704">
<h1>General Principles of SRE as Applied to Data Integrity</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="SRE principles applied to" id="DIprinc26"></a>General principles of SRE can be applied to the specifics of
data integrity and cloud computing as described in this section.</p>








<section data-type="sect2" data-pdf-bookmark="Beginner’s Mind"><div class="sect2" id="idm140203536323872">
<h2>Beginner’s Mind</h2>

<p>Large-scale, complex services have inherent
bugs that can’t be fully grokked. Never think you understand enough
of a complex system to say it won’t fail in a certain way. Trust but
verify, and apply defense in depth. (Note:
“Beginner’s mind” does <em>not</em> suggest putting a new hire in
charge of that data deletion pipeline!)</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Trust but Verify"><div class="sect2" id="idm140203536321520">
<h2>Trust but Verify</h2>

<p>Any API upon which you depend won’t work
perfectly <em>all</em> of the time. It’s a given that regardless of your
engineering quality or rigor of testing, the API will have defects.
Check the correctness of the most critical elements of your data using
out-of-band data validators, even if API semantics suggest that you
need not do so. Perfect algorithms may not have perfect
implementations.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Hope Is Not a Strategy"><div class="sect2" id="idm140203536319104">
<h2>Hope Is Not a Strategy</h2>

<p>System components that aren’t
continually exercised fail when you need them most. Prove that data
recovery works with regular exercise, or data recovery won’t work.
Humans lack discipline to continually exercise system components, so
automation is your friend. However, when you staff such automation
efforts with engineers who have competing priorities, you may end up
with temporary stopgaps.</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Defense in Depth"><div class="sect2" id="idm140203536317104">
<h2>Defense in Depth</h2>

<p><a data-type="indexterm" data-primary="defense in depth, for data integrity" id="idm140203536315968"></a>Even the most bulletproof system is susceptible
to bugs and operator error. In order for data integrity issues to be
fixable, services must detect such issues quickly. Every strategy
eventually fails in changing environments. The best data integrity
strategies are multitiered—multiple strategies that fall back to
one another and address a broad swath of scenarios together at
reasonable cost.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="idm140203536314576">
<h5>Revisit and Reexamine</h5>
<p>The fact that your data “was safe yesterday” isn’t going to help you tomorrow, or even today. Systems and infrastructure change, and you’ve got to prove that your assumptions and processes remain relevant in the face of progress. Consider the following.</p>

<p>The Shakespeare service has received quite a bit of positive press,
and its user base is steadily increasing. No real attention was paid
to data integrity as the service was being built. Of course, we don’t
want to serve <em>bad</em> bits, but if the index Bigtable is lost, we can
easily re-create it from the original Shakespeare texts and a
MapReduce. Doing so would take very little time, so we never
made backups of the index.</p>

<p>Now a new feature allows users to make text annotations. Suddenly, our
dataset can no longer be easily re-created, while the user data is
increasingly valuable to our users. Therefore, we need to revisit our
replication options—we’re not just replicating for latency and
bandwidth, but for data integrity, as well. Therefore, we need to
create and test a backup and restore procedure. This procedure is also
periodically tested by a DiRT exercise to ensure that we can restore
users’ annotations from backups within the time set by the SLO.<a data-type="indexterm" data-primary="" data-startref="DIprinc26" id="idm140203536310784"></a></p>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Conclusion"><div class="sect1" id="idm140203536309200">
<h1>Conclusion</h1>

<p><a data-type="indexterm" data-primary="data integrity" data-secondary="overview of" id="idm140203536308064"></a>Data availability must be a foremost concern of any
data-centric system. Rather than focusing on the means to the end,
Google SRE finds it useful to borrow a page from test-driven
development by proving that our systems can maintain data availability
with a predicted maximum down time. The means and mechanisms that we
use to achieve this end goal are necessary evils. By keeping our eyes
on the goal, we avoid falling into the trap in which “The operation
was a success, but the system died.”</p>

<p>Recognizing that not just <em>anything</em> can go wrong, but that
<em>everything</em> will go wrong is a significant step toward preparation
for any real emergency. A matrix of all possible combinations of
disasters with plans to address each of these disasters permits you to
sleep soundly for at least one night; keeping your recovery plans
current and exercised permits you to sleep the other 364 nights of the
year.</p>

<p>As you get better at recovering from any breakage in reasonable time
<em>N</em>, find ways to whittle down that time through more rapid and
finer-grained loss detection, with the goal of approaching <img src="https://www.safaribooksonline.com/library/view/site-reliability-engineering/9781491929117/assets/eq_37.png" alt="upper N equals 0" width="73" height="20" data-mfp-src="/library/view/site-reliability-engineering/9781491929117/assets/eq_37.png">. You can then switch from planning recovery to planning
prevention, with the aim of achieving the holy grail of <em>all
the data, all the time</em>. Achieve this goal, and you can sleep on
the beach on that well-deserved vacation.</p>
</div></section>







<div data-type="footnotes"><p data-type="footnote" id="idm140203536633376"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536633376-marker" class="totri-footnote">1</a></sup> Atomicity, Consistency, Isolation, Durability; see <a href="https://en.wikipedia.org/wiki/ACID"><em class="hyperlink">https://en.wikipedia.org/wiki/ACID</em></a>. SQL databases such as MySQL and PostgreSQL strive to achieve these properties.</p><p data-type="footnote" id="idm140203536631408"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536631408-marker" class="totri-footnote">2</a></sup> Basically Available, Soft state, Eventual consistency; see <a href="https://en.wikipedia.org/wiki/Eventual_consistency"><em class="hyperlink">https://en.wikipedia.org/wiki/Eventual_consistency</em></a>. BASE systems, like Bigtable and Megastore, are often also described as “NoSQL.”</p><p data-type="footnote" id="idm140203536629360"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536629360-marker" class="totri-footnote">3</a></sup> For further reading on ACID and BASE APIs, see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Gol14">[Gol14]</a> and <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Bai13">[Bai13]</a>.</p><p data-type="footnote" id="idm140203536623904"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536623904-marker" class="totri-footnote">4</a></sup> Binary Large Object; see <a href="https://en.wikipedia.org/wiki/Binary_large_object"><em class="hyperlink">https://en.wikipedia.org/wiki/Binary_large_object</em></a>.</p><p data-type="footnote" id="idm140203536538944"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536538944-marker" class="totri-footnote">5</a></sup> See <a href="https://en.wikipedia.org/wiki/Zero-day_(computing)"><em class="hyperlink">https://en.wikipedia.org/wiki/Zero-day_(computing)</em></a>.</p><p data-type="footnote" id="idm140203536535328"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536535328-marker" class="totri-footnote">6</a></sup> Clay tablets are the oldest known examples of writing. For a broader discussion of preserving data for the long haul, see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Con96">[Con96]</a>.</p><p data-type="footnote" id="idm140203536503344"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536503344-marker" class="totri-footnote">7</a></sup> Upon reading this advice, one might ask: since you have to offer an API on top of the datastore to implement soft deletion, why stop at soft deletion, when you could offer many other features that protect against accidental data deletion by users? To take a specific example from Google’s experience, consider Blobstore: rather than allow customers to delete Blob data and metadata directly, the Blob APIs implement many safety features, including default backup policies (offline replicas), end-to-end checksums, and default tombstone lifetimes (soft deletion). It turns out that on multiple occasions, soft deletion saved Blobstore’s clients from data loss that could have been much, much worse. There are certainly many deletion protection features worth calling out, but for companies with required data deletion deadlines, soft deletion was the most pertinent protection against bugs and accidental deletion in the case of Blobstore’s clients.</p><p data-type="footnote" id="idm140203536483952"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536483952-marker" class="totri-footnote">8</a></sup> “Snapshot” here refers to a read-only, static view of a storage instance, such as snapshots of SQL databases. Snapshots are often implemented using copy-on-write semantics for storage efficiency. They can be expensive for two reasons: first, they contend for the same storage capacity as the live datastores, and second, the faster your data mutates, the less efficiency is gained from copying-on-write.</p><p data-type="footnote" id="idm140203536468736"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536468736-marker" class="totri-footnote">9</a></sup> For more information on GFS-style replication, see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Ghe03">[Ghe03]</a>. For more information on Reed-Solomon erasure codes, see <a href="https://en.wikipedia.org/wiki/Reed%E2%80%93Solomon_error_correction"><em class="hyperlink">https://en.wikipedia.org/wiki/Reed–Solomon_error_correction</em></a>.</p><p data-type="footnote" id="idm140203536356784"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536356784-marker">10</a></sup> See <a href="http://en.wikipedia.org/wiki/Catch-22_(logic)"><em class="hyperlink">http://en.wikipedia.org/wiki/Catch-22_(logic)</em></a>.</p><p data-type="footnote" id="idm140203536346288"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536346288-marker">11</a></sup> In practice, coming up with a programmatic solution was not a hurdle because the majority of SREs are experienced software engineers, as was the case here. The expectation of such experience makes SREs notoriously hard to find and hire, and from this case study and other data points, you can begin to appreciate why SRE hires practicing software engineers; see <a data-type="xref" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/bibliography01.html#Jon15">[Jon15]</a>.</p><p data-type="footnote" id="idm140203536328400"><sup><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#idm140203536328400-marker">12</a></sup> In our experience, cloud computing engineers are often reluctant to set up production alerts on data deletion rates due to natural variation of per-user data deletion rates with time. However, since the intent of such an alert is to detect global rather than local deletion rate anomalies, it would be more useful to alert when the global data deletion rate, aggregated across all users, crosses an extreme threshold (such as 10x the observed 95th percentile), as opposed to less useful per-user deletion rate alerts.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#">
			
				Add Note
			
		</a></li>
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/Library/view/site-reliability-engineering/9781491929117/ch25.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">25. Data Processing Pipelines</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/Library/view/site-reliability-engineering/9781491929117/ch27.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">27. Reliable Product Launches at Scale</div>
        </a>
    
  
  </div>


      
    </section>
    <div class="reading-controls-bottom">
      <ul class="interface-controls js-bitlist">
        <li class="queue-control">
            <button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781491929117/chapter/ch26.html" data-for-analytics="9781491929117:ch26.html">
      <span>Add to Queue</span>
  </button>
        </li>
      </ul>
    </div>
  </div>
  <div class="js-related-container related"></div>
<section class="sbo-saved-archives"></section>



          
          
  





    
    



        
      </div>
      



  <footer class="pagefoot t-pagefoot">
    <a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li><a class="t-queue-footer" href="https://www.safaribooksonline.com/s/">Queue</a></li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li><a href="https://www.safaribooksonline.com/blog/">Blog</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://community.safaribooksonline.com/">Feedback</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2016 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    
    
    


  

<div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://www.safaribooksonline.com//library/view/site-reliability-engineering/9781491929117/ch26.html#">Reset</a>
</div>
</div><div class="annotator-notice"></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>