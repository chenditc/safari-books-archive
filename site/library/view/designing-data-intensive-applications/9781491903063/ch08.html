<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/designing-data-intensive-applications/9781491903063/ch08.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3905629"
  data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99"
  data-username="safaribooksonline122"
  data-account-type="Trial"
  
  data-activated-trial-date="12/09/2018"


  data-archive="9781491903063"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch08.html"
  data-epub-title="Designing Data-Intensive Applications" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/designing-data-intensive-applications/9781491903063/ch08.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3905629" data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99" data-username="safaribooksonline122" data-account-type="Trial" data-activated-trial-date="12/09/2018" data-archive="9781491903063" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch08.html" data-epub-title="Designing Data-Intensive Applications" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491903063"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>8. The Trouble with Distributed Systems - Designing Data-Intensive Applications</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:1.5em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.map-ebook{page-break-after:always}
    </style><link rel="canonical" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html"><meta name="description" content=" Chapter 8. The Trouble with Distributed Systems Hey I just met you The network’s laggy But here’s my data So store it maybe Kyle Kingsbury, Carly Rae Jepsen ... "><meta property="og:title" content="8. The Trouble with Distributed Systems"><meta itemprop="isPartOf" content="/library/view/designing-data-intensive-applications/9781491903063/"><meta itemprop="name" content="8. The Trouble with Distributed Systems"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch08.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491903063/"><meta property="og:description" itemprop="description" content=" Chapter 8. The Trouble with Distributed Systems Hey I just met you The network’s laggy But here’s my data So store it maybe Kyle Kingsbury, Carly Rae Jepsen ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449373320"><meta property="og:book:author" itemprop="author" content="Martin Kleppmann"><meta property="og:book:tag" itemprop="about" content="Core Programming"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/f04af719-1c84-4fc3-9be3-1f1b4622ab99/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Designing Data-Intensive Applications
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781491903063/chapter/ch08.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch08.html&amp;text=Designing%20Data-Intensive%20Applications&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch08.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch08.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%208.%20The%20Trouble%20with%20Distributed%20Systems&amp;body=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch08.html%0D%0Afrom%20Designing%20Data-Intensive%20Applications%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">7. Transactions</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">9. Consistency and Consensus</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. The Trouble with Distributed Systems"><div class="chapter" id="ch_distributed">
<h1><span class="label">Chapter 8. </span>The Trouble with Distributed Systems</h1>

<blockquote data-type="epigraph" epub:type="epigraph"><p><em>Hey I just met you<br>
The network’s laggy<br>
But here’s my data<br>
So store it maybe</em></p>
<p data-type="attribution">Kyle Kingsbury, <em>Carly Rae Jepsen and the Perils of Network Partitions</em> (2013)</p>
</blockquote>

<div class="map-ebook">
 <img id="c274" src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ch08-map-ebook.png" width="2756" height="2100" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ch08-map-ebook.png">
</div>

<p><a data-type="indexterm" data-primary="distributed systems" id="ix_distsys"></a>
A recurring theme in the last few chapters has been how systems handle things going
wrong. For example, we discussed replica failover (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>), replication lag
(<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_lag">“Problems with Replication Lag”</a>), and concurrency control for transactions
(<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>). As we come to understand various edge cases that can occur
in real systems, we get better at handling them.</p>

<p>However, even though we have talked a lot about faults, the last few chapters have still been too
optimistic. The reality is even darker. We will now turn our pessimism to the maximum and assume
that anything that <em>can</em> go wrong <em>will</em> go wrong.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417552019744-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417552019744" class="totri-footnote">i</a></sup> (Experienced systems operators
will tell you that is a reasonable assumption. If you ask nicely, they might tell you some
frightening stories while nursing their scars of past battles.)</p>

<p>Working with distributed systems is fundamentally different from writing software on a single
computer—and the main difference is that there are lots of new and exciting ways for things to go
wrong [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cavage2013ez-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Cavage2013ez" class="totri-footnote">1</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2012td_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kreps2012td_ch8" class="totri-footnote">2</a>].
In this chapter, we will get a taste of the problems that arise in practice, and an understanding of
the things we can and cannot rely on.</p>

<p>In the end, our task as engineers is to build systems that do their job (i.e., meet the guarantees
that users are expecting), in spite of everything going wrong. In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>, we will look
at some examples of algorithms that can provide such guarantees in a distributed system. But first,
in this chapter, we must understand what challenges we are up against.</p>

<p>This chapter is a thoroughly pessimistic and depressing overview of things that may go wrong in a
distributed system. We will look into problems with networks (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_networks">“Unreliable Networks”</a>); clocks
and timing issues (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks">“Unreliable Clocks”</a>); and we’ll discuss to what degree they are avoidable.
The consequences of all <span class="keep-together">these issues</span> are disorienting, so
we’ll explore how to think about the state of a distributed system and how to reason about things
that have happened (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_truth">“Knowledge, Truth, and Lies”</a>).</p>






<section data-type="sect1" data-pdf-bookmark="Faults and Partial Failures"><div class="sect1" id="sec_distributed_partial_failure">
<h1>Faults and Partial Failures</h1>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="faults and partial failures" id="ix_distsysfaultfail"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="in distributed systems" id="ix_faultsDS"></a>
When you are writing a program on a single computer, it normally behaves in a fairly predictable
way: either it works or it doesn’t. Buggy software may give the appearance that the computer is
sometimes “having a bad day” (a problem that is often fixed by a reboot), but that is mostly just
a consequence of badly written software.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="dealing with partial failures" id="idm140417552000400"></a>
<a data-type="indexterm" data-primary="deterministic operations" id="idm140417551999280"></a>
There is no fundamental reason why software on a single computer should be flaky: when the hardware
is working correctly, the same operation always produces the same result (it is <em>deterministic</em>). If
there is a hardware problem (e.g., memory corruption or a loose connector), the consequence is usually a
total system failure (e.g., kernel panic, “blue screen of death,” failure to start up). An individual
computer with good software is usually either fully functional or entirely broken, but not something
in between.</p>

<p>This is a deliberate choice in the design of computers: if an internal fault occurs, we prefer a
computer to crash completely rather than returning a wrong result, because wrong results are difficult
and confusing to deal with. Thus, computers hide the fuzzy physical reality on which they are
implemented and present an idealized system model that operates with mathematical perfection. A CPU
instruction always does the same thing; if you write some data to memory or disk, that data remains
intact and doesn’t get randomly corrupted. This design goal of always-correct computation goes all
the way back to the very first digital computer
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Padua2015um-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Padua2015um" class="totri-footnote">3</a>].</p>

<p>When you are writing software that runs on several computers, connected by a network, the situation
is fundamentally different. In distributed systems, we are no longer operating in an idealized
system model—we have no choice but to confront the messy reality of the physical world. And in
the physical world, a remarkably wide range of things can go wrong, as illustrated by this anecdote
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hale2010we-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hale2010we" class="totri-footnote">4</a>]:</p>
<blockquote>
<p><a data-type="indexterm" data-primary="incidents" data-secondary="network partitions and whole-datacenter failures" id="idm140417551991088"></a>
In my limited experience I’ve dealt with long-lived network partitions in a single data center (DC),
PDU [power distribution unit] failures, switch failures, accidental power cycles of whole racks,
whole-DC backbone failures, whole-DC power failures, and a hypoglycemic driver smashing his Ford
pickup truck into a DC’s HVAC [heating, ventilation, and air conditioning] system. And I’m not even
an ops guy.</p>
<p data-type="attribution">Coda Hale</p>
</blockquote>

<p><a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" id="ix_failpartdistsys"></a>
<a data-type="indexterm" data-primary="partial failures" id="idm140417551986784"></a>
<a data-type="indexterm" data-primary="nondeterministic operations" data-secondary="partial failures in distributed systems" id="idm140417551985952"></a>
In a distributed system, there may well be some parts of the system that are broken in some
unpredictable way, even though other parts of the system are working fine. This is known as a
<em>partial failure</em>. The difficulty is that partial failures are <em>nondeterministic</em>: if you try to do
anything involving multiple nodes and the network, it may sometimes work and sometimes unpredictably
fail. As we shall see, you may not even <em>know</em> whether something succeeded or not, as the time it takes for a
message to travel across a network is also nondeterministic!</p>

<p>This nondeterminism and possibility of partial failures is what makes distributed systems hard to
work with [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hodges2013tj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hodges2013tj" class="totri-footnote">5</a>].</p>








<section data-type="sect2" data-pdf-bookmark="Cloud Computing and Supercomputing"><div class="sect2" id="sec_distributed_cloud_vs_hpc">
<h2>Cloud Computing and Supercomputing</h2>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="cloud versus supercomputing" id="idm140417551978512"></a>
There is a spectrum of philosophies on how to build large-scale computing systems:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="supercomputers" id="idm140417551976352"></a>
<a data-type="indexterm" data-primary="high-performance computing (HPC)" id="idm140417551975520"></a>
<a data-type="indexterm" data-primary="compute-intensive applications" id="idm140417551974672"></a>
At one end of the scale is the field of <em>high-performance computing</em> (HPC). Supercomputers
with thousands of CPUs are typically used for computationally intensive scientific computing
tasks, such as weather forecasting or molecular dynamics (simulating the movement of atoms and
molecules).</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="cloud computing" id="idm140417551972336"></a>
At the other extreme is <em>cloud computing</em>, which is not very well defined
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Regalado2011vn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Regalado2011vn" class="totri-footnote">6</a>]
but is often associated with multi-tenant datacenters, commodity computers connected with an IP
network (often Ethernet), elastic/on-demand resource allocation, and metered billing.</p>
</li>
<li>
<p>Traditional enterprise datacenters lie somewhere between these extremes.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="faults" data-secondary="handling in supercomputers and cloud computing" id="idm140417551967136"></a>
<a data-type="indexterm" data-primary="checkpointing" data-secondary="in high-performance computing" id="idm140417551965936"></a>
With these philosophies come very different approaches to handling faults. In a supercomputer, a job
typically checkpoints the state of its computation to durable storage from time to time. If one node
fails, a common solution is to simply stop the entire cluster workload. After the faulty node is
repaired, the computation is restarted from the last checkpoint
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Barroso2013ba-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Barroso2013ba" class="totri-footnote">7</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fiala2012ti-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Fiala2012ti" class="totri-footnote">8</a>].
Thus, a supercomputer is more like a single-node computer than a distributed system: it deals with
partial failure by letting it escalate into total failure—if any part of the system fails, just
let everything crash (like a kernel panic on a single machine).</p>

<p><a data-type="indexterm" data-primary="internet services, systems for implementing" id="idm140417551958320"></a>
In this book we focus on systems for implementing internet services, which usually look very
different from supercomputers:</p>

<ul>
<li>
<p>Many internet-related applications are <em>online</em>, in the sense that they need to be able to serve
users with low latency at any time. Making the service unavailable—for example, stopping the
cluster for repair—is not acceptable. In contrast, offline (batch) jobs like weather
simulations can be stopped and restarted with fairly low impact.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="RDMA (Remote Direct Memory Access)" id="idm140417551954544"></a>
Supercomputers are typically built from specialized hardware, where each node is quite reliable,
and nodes communicate through shared memory and remote direct memory access (RDMA). On the other
hand, nodes in cloud services are built from commodity machines, which can provide equivalent
performance at lower cost due to economies of scale, but also have higher failure rates.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="datacenters" data-secondary="network architecture" id="idm140417551952592"></a>
<a data-type="indexterm" data-primary="Ethernet (networks)" id="idm140417551951488"></a>
<a data-type="indexterm" data-primary="networks" data-secondary="datacenter network topologies" id="idm140417551950656"></a>
Large datacenter networks are often based on IP and Ethernet, arranged in Clos topologies to
provide high bisection bandwidth
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Singh2015fc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Singh2015fc" class="totri-footnote">9</a>].
Supercomputers often use specialized network topologies, such as multi-dimensional meshes and toruses
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lockwood2014uz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lockwood2014uz">10</a>],
which yield better performance for HPC workloads with known communication patterns.</p>
</li>
<li>
<p>The bigger a system gets, the more likely it is that one of its components is broken. Over time,
broken things get fixed and new things break, but in a system with thousands of nodes, it is
reasonable to assume that <em>something</em> is always broken
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Barroso2013ba" class="totri-footnote">7</a>]. When the error handling strategy
consists of simply giving up, a large system can end up spending a lot of its time recovering from
faults rather than doing useful work [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Fiala2012ti" class="totri-footnote">8</a>].</p>
</li>
<li>
<p>If the system can tolerate failed nodes and still keep working as a whole, that is a very useful
feature for operations and maintenance: for example, you can perform a rolling upgrade (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#ch_encoding">Chapter&nbsp;4</a>), restarting one node at a time, while the service continues serving users without
interruption. In cloud environments, if one virtual machine is not performing well, you can just
kill it and request a new one (hoping that the new one will be faster).</p>
</li>
<li>
<p>In a geographically distributed deployment (keeping data geographically close to your users to
reduce access latency), communication most likely goes over the internet, which is slow and
unreliable compared to local networks. Supercomputers generally assume that all of their nodes are
close together.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="reliability" data-secondary="building a reliable system from unreliable components" id="idm140417551937056"></a>
If we want to make distributed systems work, we must accept the possibility of partial failure and
build fault-tolerance mechanisms into the software. In other words, we need to build a reliable
system from unreliable components. (As discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch01.html#sec_introduction_reliability">“Reliability”</a>, there is no
such thing as perfect reliability, so we’ll need to understand the limits of what we can
realistically promise.)</p>

<p>Even in smaller systems consisting of only a few nodes, it’s important to think about partial
failure. In a small system, it’s quite likely that most of the components are working correctly most
of the time. However, sooner or later, some part of the system <em>will</em> become faulty, and the
software will have to somehow handle it. The fault handling must be part of the software design, and
you (as operator of the software) need to know what behavior to expect from the software in the case
of a fault.</p>

<p>It would be unwise to assume that faults are rare and simply hope for the best. It is important to
consider a wide range of possible faults—even fairly unlikely ones—and to artificially create
such situations in your testing environment to see what happens. In distributed systems,
suspicion, pessimism, and paranoia pay off.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_unreliable_components">
<h5>Building a Reliable System from Unreliable Components</h5>
<p>You may wonder whether this makes any sense—intuitively it may seem like a system can only be as
reliable as its least reliable component (its <em>weakest link</em>). This is not the case: in fact, it is
an old idea in computing to construct a more reliable system from a less reliable underlying base
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vonNeumann1956vm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#vonNeumann1956vm">11</a>]. For example:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="error-correcting codes" id="idm140417551926656"></a>
Error-correcting codes allow digital data to be transmitted accurately across a communication
channel that occasionally gets some bits wrong, for example due to radio interference on a
wireless network [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hamming1997wd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hamming1997wd">12</a>].</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="IP (Internet Protocol)" data-secondary="unreliability of" id="idm140417551923200"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" id="idm140417551922096"></a>
IP (the Internet Protocol) is unreliable: it may drop, delay, duplicate, or reorder packets.
TCP (the Transmission Control Protocol) provides a more reliable transport layer on top of IP: it
ensures that missing packets are retransmitted, duplicates are eliminated, and packets are
reassembled into the order in which they were sent.</p>
</li>
</ul>

<p>Although the system can be more reliable than its underlying parts, there is always a limit to how
much more reliable it can be. For example, error-correcting codes can deal with a small number of
single-bit errors, but if your signal is swamped by interference, there is a fundamental limit to
how much data you can get through your communication channel
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shannon1948wk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Shannon1948wk">13</a>].
TCP can hide packet loss, duplication, and reordering from you, but it cannot magically remove delays
in the network.</p>

<p><a data-type="indexterm" data-primary="end-to-end argument" id="idm140417551917168"></a>
Although the more reliable higher-level system is not perfect, it’s still useful because it takes
care of some of the tricky low-level faults, and so the remaining faults are usually easier to
reason about and deal with. We will explore this matter further in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_e2e_argument">“The end-to-end argument”</a>.
<a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" data-startref="ix_failpartdistsys" id="idm140417551915232"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="in distributed systems" data-startref="ix_faultsDS" id="idm140417551913840"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="faults and partial failures" data-startref="ix_distsysfaultfail" id="idm140417551912464"></a></p>
</div></aside>
</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Unreliable Networks"><div class="sect1" id="sec_distributed_networks">
<h1>Unreliable Networks</h1>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="network problems" id="ix_distsysunrelnet"></a>
<a data-type="indexterm" data-primary="shared-nothing architecture" data-secondary="use of network" id="idm140417551908320"></a>
As discussed in the introduction to <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/part02.html#part_distributed_data">Part&nbsp;II</a>, the distributed systems we focus on
in this book are <em>shared-nothing systems</em>: i.e., a bunch of machines connected by a network. The
network is the only way those machines can communicate—we assume that each machine has its
own memory and disk, and one machine cannot access another machine’s memory or disk (except by
making requests to a service over the network).</p>

<p><a data-type="indexterm" data-primary="datacenters" data-secondary="geographically distributed" id="idm140417551905408"></a>
<a data-type="indexterm" data-primary="geographically distributed datacenters" id="idm140417551904240"></a>
Shared-nothing is not the only way of building systems, but it has become the dominant approach for
building internet services, for several reasons: it’s comparatively cheap because it
requires no special hardware, it can make use of commoditized cloud computing services, and it can
achieve high reliability through redundancy across multiple geographically distributed datacenters.</p>

<p><a data-type="indexterm" data-primary="asynchronous networks" id="idm140417551902736"></a>
<a data-type="indexterm" data-primary="Ethernet (networks)" id="idm140417551901904"></a>
The internet and most internal networks in datacenters (often Ethernet) are <em>asynchronous packet
networks</em>. In this kind of network, one node can send a message (a packet) to another node, but the
network gives no guarantees as to when it will arrive, or whether it will arrive at all. If you send
a request and expect a response, many things could go wrong (some of which are illustrated in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_network">Figure&nbsp;8-1</a>):</p>
<ol>
<li>
<p>Your request may have been lost (perhaps someone unplugged a network cable).</p>
</li>
<li>
<p>Your request may be waiting in a queue and will be delivered later (perhaps the network or the
recipient is overloaded).</p>
</li>
<li>
<p>The remote node may have failed (perhaps it crashed or it was powered down).</p>
</li>
<li>
<p>The remote node may have temporarily stopped responding (perhaps it is experiencing a long
garbage collection pause; see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_pauses">“Process Pauses”</a>), but it will start responding
again later.</p>
</li>
<li>
<p>The remote node may have processed your request, but the response has been lost on the network
(perhaps a network switch has been misconfigured).</p>
</li>
<li>
<p>The remote node may have processed your request, but the response has been delayed and will be
delivered later (perhaps the network or your own machine is overloaded).</p>
</li>

</ol>

<figure><div id="fig_distributed_network" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0801.png" alt="ddia 0801" width="2880" height="898" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0801.png">
<h6><span class="label">Figure 8-1. </span>If you send a request and don’t get a response, it’s not possible to distinguish whether (a) the request was lost, (b) the remote node is down, or (c) the response was lost.</h6>
</div></figure>

<p>The sender can’t even tell whether the packet was delivered: the only option is for the recipient to
send a response message, which may in turn be lost or delayed. These issues are indistinguishable in
an asynchronous network: the only information you have is that you haven’t received a response yet.
If you send a request to another node and don’t receive a response, it is <em>impossible</em> to tell why.</p>

<p><a data-type="indexterm" data-primary="timeouts" id="idm140417551888624"></a>
The usual way of handling this issue is a <em>timeout</em>: after some time you give up waiting and assume that
the response is not going to arrive. However, when a timeout occurs, you still don’t know whether
the remote node got your request or not (and if the request is still queued somewhere, it may still
be delivered to the recipient, even if the sender has given up on it).</p>








<section data-type="sect2" data-pdf-bookmark="Network Faults in Practice"><div class="sect2" id="sec_distributed_network_faults">
<h2>Network Faults in Practice</h2>

<p><a data-type="indexterm" data-primary="faults" data-secondary="network faults" id="ix_faultnetpractice"></a>
We have been building computer networks for decades—one might hope that by now we would have
figured out how to make them reliable. However, it seems that we have not yet succeeded.</p>

<p><a data-type="indexterm" data-primary="datacenters" data-secondary="network faults" id="idm140417551883456"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="network faults" id="idm140417551882128"></a>
<a data-type="indexterm" data-primary="human errors" id="idm140417551881088"></a>
There are some systematic studies, and plenty of anecdotal evidence, showing that network problems
can be surprisingly common, even in controlled environments like a datacenter operated by one
company [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014jx-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bailis2014jx">14</a>].
One study in a medium-sized datacenter found about 12 network faults per month, of which half
disconnected a single machine, and half disconnected an entire rack
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Leners2015gv-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Leners2015gv">15</a>].
Another study measured the failure rates of components like top-of-rack switches, aggregation
switches, and load balancers
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gill2011ku-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gill2011ku">16</a>].
It found that adding redundant networking gear doesn’t reduce faults as much as you might hope, since it
doesn’t guard against human error (e.g., misconfigured switches), which is a major cause of outages.</p>

<p><a data-type="indexterm" data-primary="cloud computing" data-secondary="network glitches" id="idm140417551870576"></a>
<a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-secondary="network reliability" id="idm140417551869344"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="network interface dropping only inbound packets" id="idm140417551868176"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="split brain due to 1-minute packet delay" id="idm140417551867040"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="sharks biting undersea cables" id="idm140417551865904"></a>
<a data-type="indexterm" data-primary="sharks" data-secondary="biting undersea cables" id="idm140417551864784"></a>
Public cloud services such as EC2 are notorious for having frequent transient network glitches
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bailis2014jx">14</a>], and well-managed private datacenter
networks can be stabler environments. Nevertheless, nobody is immune from network problems: for
example, a problem during a software upgrade for a switch could trigger a network topology
reconfiguration, during which network packets could be delayed for more than a minute
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Imbriaco2012tx_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Imbriaco2012tx_ch8">17</a>]. Sharks might bite undersea cables and damage them
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oremus2014ty-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Oremus2014ty">18</a>].
Other surprising faults include a network interface that sometimes drops all inbound packets but
sends outbound packets successfully [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Donges2012tt-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Donges2012tt">19</a>]:
just because a network link works in one direction doesn’t guarantee it’s also working in the
opposite direction.</p>
<div data-type="note" epub:type="note"><h1>Network partitions</h1>
<p><a data-type="indexterm" data-primary="networks" data-secondary="network partitions" id="idm140417551854048"></a>
When one part of the network is cut off from the rest due to a network fault, that is sometimes
called a <em>network partition</em> or <em>netsplit</em>. In this book we’ll generally stick with the more general term
<em>network fault</em>, to avoid confusion with partitions (shards) of a storage system, as discussed in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#ch_partitioning">Chapter&nbsp;6</a>.</p>
</div>

<p>Even if network faults are rare in your environment, the fact that faults <em>can</em> occur means that
your software needs to be able to handle them. Whenever any communication happens over a network, it
may fail—there is no way around it.</p>

<p><a data-type="indexterm" data-primary="error handling" data-secondary="for network faults" id="idm140417551849040"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="poor handling of network faults" id="idm140417551847936"></a>
If the error handling of network faults is not defined and tested, arbitrarily bad things could
happen: for example, the cluster could become deadlocked and permanently unable to serve requests,
even when the network recovers [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2014vi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2014vi">20</a>], or it could even delete all of
your data [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sanfilippo2014ty-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sanfilippo2014ty">21</a>].
If software is put in an unanticipated situation, it may do arbitrary unexpected things.</p>

<p>Handling network faults doesn’t necessarily mean <em>tolerating</em> them: if your network is normally
fairly reliable, a valid approach may be to simply show an error message to users while your network
is experiencing problems. However, you do need to know how your software reacts to network problems
and ensure that the system can recover from them.
<a data-type="indexterm" data-primary="Chaos Monkey" id="idm140417551840800"></a><a data-type="indexterm" data-primary="Netflix Chaos Monkey" id="idm140417551840096"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="introducing deliberately" id="idm140417551839296"></a>
It may make sense to deliberately trigger network problems and test the system’s response (this is
the idea behind Chaos Monkey; see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch01.html#sec_introduction_reliability">“Reliability”</a>).</p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Detecting Faults"><div class="sect2" id="idm140417551837072">
<h2>Detecting Faults</h2>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="detecting network faults" id="idm140417551835696"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-tertiary="detecting" id="idm140417551834624"></a>
<a data-type="indexterm" data-primary="failures" data-secondary="failure detection" id="idm140417551833248"></a>
Many systems need to automatically detect faulty nodes. For example:</p>

<ul>
<li>
<p>A load balancer needs to stop sending requests to a node that is dead (i.e., take it <em>out of rotation</em>).</p>
</li>
<li>
<p>In a distributed database with single-leader replication, if the leader fails, one of the
followers needs to be promoted to be the new leader (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>).</p>
</li>
</ul>

<p>Unfortunately, the uncertainty about the network makes it difficult to tell whether a node is
working or not. In some specific circumstances you might get some feedback to explicitly tell you
that something is not working:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="connection failures" id="idm140417551826688"></a>
If you can reach the machine on which the node should be running, but no process is listening on
the destination port (e.g., because the process crashed), the operating system will helpfully close
or refuse TCP connections by sending a <code>RST</code> or <code>FIN</code> packet in reply. However, if the node
crashed while it was handling your request, you have no way of knowing how much data was actually
processed by the remote node [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hubert2009wf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hubert2009wf">22</a>].</p>
</li>
<li>
<p>If a node process crashed (or was killed by an administrator) but the node’s operating system is
still running, a script can notify other nodes about the crash so that another node can take over
quickly without having to wait for a timeout to expire. For example, HBase does this
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Liochon2015ux-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Liochon2015ux">23</a>].</p>
</li>
<li>
<p>If you have access to the management interface of the network switches in your datacenter, you can
query them to detect link failures at a hardware level (e.g., if the remote machine is powered
down). This option is ruled out if you’re connecting via the internet, or if you’re in a shared
datacenter with no access to the switches themselves, or if you can’t reach the management
interface due to a network problem.</p>
</li>
<li>
<p>If a router is sure that the IP address you’re trying to connect to is unreachable, it may reply
to you with an ICMP Destination Unreachable packet. However, the router doesn’t have a magic
failure detection capability either—it is subject to the same limitations as other participants
of the network.</p>
</li>
</ul>

<p>Rapid feedback about a remote node being down is useful, but you can’t count on it. Even if TCP
acknowledges that a packet was delivered, the application may have crashed before handling it. If
you want to be sure that a request was successful, you need a positive response from the application
itself [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saltzer1984do_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Saltzer1984do_ch8">24</a>].</p>

<p>Conversely, if something has gone wrong, you may get an error response at some level of the stack,
but in general you have to assume that you will get no response at all. You can retry a few times
(TCP retries transparently, but you may also retry at the application level), wait for a timeout to
elapse, and eventually declare the node dead if you don’t hear back within the timeout.
<a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-startref="ix_faultnetpractice" id="idm140417551811328"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Timeouts and Unbounded Delays"><div class="sect2" id="sec_distributed_queueing">
<h2>Timeouts and Unbounded Delays</h2>

<p><a data-type="indexterm" data-primary="networks" data-secondary="timeouts and unbounded delays" id="idm140417551808272"></a>
<a data-type="indexterm" data-primary="timeouts" data-secondary="length of" id="idm140417551807104"></a>
If a timeout is the only sure way of detecting a fault, then how long should the timeout be? There
is unfortunately no simple answer.</p>

<p>A long timeout means a long wait until a node is declared dead (and during this time, users may have
to wait or see error messages). A short timeout detects faults faster, but carries a higher risk of
incorrectly declaring a node dead when in fact it has only suffered a temporary slowdown (e.g., due
to a load spike on the node or the network).</p>

<p>Prematurely declaring a node dead is problematic: if the node is actually alive and in the middle of
performing some action (for example, sending an email), and another node takes over, the action may
end up being performed twice. We will discuss this issue in more detail in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_truth">“Knowledge, Truth, and Lies”</a>, and in
Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency" class="totri-footnote">9</a>
and <a data-type="xref" data-xrefstyle="select:labelnumber" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">11</a>.</p>

<p><a data-type="indexterm" data-primary="cascading failures" id="idm140417551800752"></a>
When a node is declared dead, its responsibilities need to be transferred to other nodes, which
places additional load on other nodes and the network. If the system is already struggling with high
load, declaring nodes dead prematurely can make the problem worse. In particular, it could happen
that the node actually wasn’t dead but only slow to respond due to overload; transferring its load
to other nodes can cause a cascading failure (in the extreme case, all nodes declare each other
dead, and everything stops working).</p>

<p>Imagine a fictitious system with a network that guaranteed a maximum delay for packets—every packet
is either delivered within some time <em>d</em>, or it is lost, but delivery never takes longer than <em>d</em>.
Furthermore, assume that you can guarantee that a non-failed node always handles a request within
some time <em>r</em>. In this case, you could guarantee that every successful request receives a response
within time 2<em>d</em>&nbsp;+&nbsp;<em>r</em>—and if you don’t receive a response within that time, you know
that either the network or the remote node is not working. If this was true,
2<em>d</em>&nbsp;+&nbsp;<em>r</em> would be a reasonable timeout to use.</p>

<p><a data-type="indexterm" data-primary="unbounded delays" data-secondary="in networks" id="idm140417551795104"></a>
<a data-type="indexterm" data-primary="delays" data-secondary="unbounded network delays" id="idm140417551793968"></a>
<a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="timeouts and unbounded delays" id="idm140417551792896"></a>
Unfortunately, most systems we work with have neither of those guarantees: asynchronous networks
have <em>unbounded delays</em> (that is, they try to deliver packets as quickly as possible, but there is
no upper limit on the time it may take for a packet to arrive), and most server implementations
cannot guarantee that they can handle requests within some maximum time (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_realtime">“Response time guarantees”</a>). For failure detection, it’s not sufficient for the system to
be fast most of the time: if your timeout is low, it only takes a transient spike in round-trip
times to throw the system off-balance.</p>










<section data-type="sect3" data-pdf-bookmark="Network congestion and queueing"><div class="sect3" id="sec_distributed_congestion">
<h3>Network congestion and queueing</h3>

<p><a data-type="indexterm" data-primary="congestion (networks)" data-secondary="queueing delays" id="idm140417551787760"></a>
<a data-type="indexterm" data-primary="networks" data-secondary="congestion and queueing" id="idm140417551786656"></a>
<a data-type="indexterm" data-primary="queueing delays (networks)" id="idm140417551785552"></a>
When driving a car, travel times on road networks often vary most due to traffic congestion.
Similarly, the variability of packet delays on computer networks is most often due to queueing
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Grosvenor2015vz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Grosvenor2015vz">25</a>]:</p>

<ul>
<li>
<p>If several different nodes simultaneously try to send packets to the same destination, the network
switch must queue them up and feed them into the destination network link one by one (as illustrated
in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_switch_queueing">Figure&nbsp;8-2</a>). On a busy network link, a packet may have to wait a while
until it can get a slot (this is called <em>network congestion</em>). If there is so much incoming data
that the switch queue fills up, the packet is dropped, so it needs to be resent—even though
the network is functioning fine.</p>
</li>
<li>
<p>When a packet reaches the destination machine, if all CPU cores are currently busy, the incoming
request from the network is queued by the operating system until the application is ready to
handle it. Depending on the load on the machine, this may take an arbitrary length of time.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="virtual machines" data-secondary="network performance" id="idm140417551777520"></a>
In virtualized environments, a running operating system is often paused for tens of milliseconds
while another virtual machine uses a CPU core. During this time, the VM cannot consume any data
from the network, so the incoming data is queued (buffered) by the virtual machine monitor
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wang2010ja-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Wang2010ja">26</a>],
further increasing the variability of network delays.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="flow control" id="idm140417551772288"></a>
<a data-type="indexterm" data-primary="congestion (networks)" data-secondary="avoidance" id="idm140417551771232"></a>
<a data-type="indexterm" data-primary="backpressure" data-secondary="in TCP" id="idm140417551770128"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="flow control" id="idm140417551769024"></a>
TCP performs <em>flow control</em> (also known as <em>congestion avoidance</em> or <em>backpressure</em>), in which a
node limits its own rate of sending in order to avoid overloading a network link or the receiving
node [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jacobson1988gl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Jacobson1988gl">27</a>].
This means additional queueing at the sender before the data even enters the network.</p>
</li>
</ul>

<figure><div id="fig_distributed_switch_queueing" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0802.png" alt="ddia 0802" width="2880" height="1081" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0802.png">
<h6><span class="label">Figure 8-2. </span>If several machines send network traffic to the same destination, its switch queue can fill up. Here, ports 1, 2, and 4 are all trying to send packets to port 3.</h6>
</div></figure>

<p>Moreover, TCP considers a packet to be lost if it is not acknowledged within some timeout (which is
calculated from observed round-trip times), and lost packets are automatically retransmitted.
Although the application does not see the packet loss and retransmission, it does see the resulting
delay (waiting for the timeout to expire, and then waiting for the retransmitted packet to be
acknowledged).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_tcp_udp">
<h5>TCP Versus UDP</h5>
<p><a data-type="indexterm" data-primary="UDP (User Datagram Protocol)" data-secondary="comparison to TCP" id="idm140417551758912"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="comparison to UDP" id="idm140417551757792"></a>
<a data-type="indexterm" data-primary="Voice over IP (VoIP)" id="idm140417551756672"></a>
Some latency-sensitive applications, such as videoconferencing and Voice over IP (VoIP), use UDP
rather than TCP. It’s a trade-off between reliability and variability of delays: as UDP does not
perform flow control and does not retransmit lost packets, it avoids some of the reasons for
variable network delays (although it is still susceptible to switch queues and scheduling delays).</p>

<p>UDP is a good choice in situations where delayed data is worthless. For example, in a VoIP phone
call, there probably isn’t enough time to retransmit a lost packet before its data is due to be
played over the loudspeakers. In this case, there’s no point in retransmitting the packet—the
application must instead fill the missing packet’s time slot with silence (causing a brief
interruption in the sound) and move on in the stream. The retry happens at the human layer instead.
(“Could you repeat that please? The sound just cut out for a moment.”)</p>
</div></aside>

<p>All of these factors contribute to the variability of network delays. Queueing delays have an
especially wide range when a system is close to its maximum capacity: a system with plenty of spare
capacity can easily drain queues, whereas in a highly utilized system, long queues can build up very
quickly.</p>

<p><a data-type="indexterm" data-primary="cloud computing" data-secondary="shared resources" id="idm140417551753328"></a>
<a data-type="indexterm" data-primary="datacenters" data-secondary="multi-tenancy and shared resources" id="idm140417551752000"></a>
<a data-type="indexterm" data-primary="virtual machines" data-secondary="noisy neighbors" id="idm140417551750928"></a>
<a data-type="indexterm" data-primary="noisy neighbors" id="idm140417551749824"></a>
<a data-type="indexterm" data-primary="multi-tenancy" id="idm140417551748992"></a>
In public clouds and multi-tenant datacenters, resources are shared among many customers: the
network links and switches, and even each machine’s network interface and CPUs (when running on
virtual machines), are shared. Batch workloads such as MapReduce (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ch_batch">Chapter&nbsp;10</a>) can easily
saturate network links. As you have no control over or insight into other customers’ usage of the shared
resources, network delays can be highly variable if someone near you (a <em>noisy neighbor</em>) is
using a lot of resources [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Philips2014tr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Philips2014tr">28</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Newman2012vf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Newman2012vf">29</a>].</p>

<p><a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="timeouts and unbounded delays" id="idm140417551742064"></a>
In such environments, you can only choose timeouts experimentally: measure the distribution of
network round-trip times over an extended period, and over many machines, to determine the expected
variability of delays. Then, taking into account your application’s characteristics, you can
determine an appropriate trade-off between failure detection delay and risk of premature timeouts.</p>

<p><a data-type="indexterm" data-primary="jitter (network delay)" id="idm140417551739936"></a>
<a data-type="indexterm" data-primary="timeouts" data-secondary="dynamic configuration of" id="idm140417551738880"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="retransmission timeouts" id="idm140417551737808"></a>
Even better, rather than using configured constant timeouts, systems can continually measure
response times and their variability (<em>jitter</em>), and automatically adjust timeouts according to the
observed response time distribution. This can be done with a Phi Accrual failure detector
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hayashibara2004vw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hayashibara2004vw">30</a>],
which is used for example in Akka and Cassandra [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wang2013wa-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Wang2013wa">31</a>].
TCP retransmission timeouts also work similarly
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Jacobson1988gl">27</a>].</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Synchronous Versus Asynchronous Networks"><div class="sect2" id="sec_distributed_sync_networks">
<h2>Synchronous Versus Asynchronous Networks</h2>

<p><a data-type="indexterm" data-primary="asynchronous networks" data-secondary="comparison to synchronous networks" id="idm140417551729664"></a>
<a data-type="indexterm" data-primary="synchronous networks" data-secondary="comparison to asynchronous networks" id="idm140417551728544"></a>
Distributed systems would be a lot simpler if we could rely on the network to deliver packets with
some fixed maximum delay, and not to drop packets. Why can’t we solve this at the hardware level
and make the network reliable so that the software doesn’t need to worry about it?</p>

<p>To answer this question, it’s interesting to compare datacenter networks to the traditional fixed-line
telephone network (non-cellular, non-VoIP), which is extremely reliable: delayed audio
frames and dropped calls are very rare. A phone call requires a constantly low end-to-end latency
and enough bandwidth to transfer the audio samples of your voice. Wouldn’t it be nice to have
similar reliability and predictability in computer networks?</p>

<p><a data-type="indexterm" data-primary="circuit-switched networks" id="idm140417551725984"></a>
<a data-type="indexterm" data-primary="ISDN (Integrated Services Digital Network)" id="idm140417551724960"></a>
When you make a call over the telephone network, it establishes a <em>circuit</em>: a fixed, guaranteed
amount of bandwidth is allocated for the call, along the entire route between the two callers. This
circuit remains in place until the call ends
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Keshav1997wb-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Keshav1997wb">32</a>].
For example, an ISDN network runs at a fixed rate of 4,000 frames per second. When a call is
established, it is allocated 16 bits of space within each frame (in each direction). Thus, for the
duration of the call, each side is guaranteed to be able to send exactly 16 bits of audio data every
250 microseconds
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CiscoISDN-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#CiscoISDN">33</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kyas1995ug-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kyas1995ug">34</a>].</p>

<p><a data-type="indexterm" data-primary="synchronous networks" id="idm140417551717712"></a>
<a data-type="indexterm" data-primary="bounded delays" data-secondary="in networks" id="idm140417551716848"></a>
<a data-type="indexterm" data-primary="delays" data-secondary="bounded network delays" id="idm140417551715744"></a>
This kind of network is <em>synchronous</em>: even as data passes through several routers, it does not
suffer from queueing, because the 16 bits of space for the call have already been reserved in the
next hop of the network. And because there is no queueing, the maximum end-to-end latency of the
network is fixed. We call this a <em>bounded delay</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Can we not simply make network delays predictable?"><div class="sect3" id="idm140417551713408">
<h3>Can we not simply make network delays predictable?</h3>

<p><a data-type="indexterm" data-primary="packet switching" id="idm140417551712032"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="comparison to circuit switching" id="idm140417551711200"></a>
Note that a circuit in a telephone network is very different from a TCP connection: a circuit is a
fixed amount of reserved bandwidth which nobody else can use while the circuit is established,
whereas the packets of a TCP connection opportunistically use whatever network bandwidth is
available. You can give TCP a variable-sized block of data (e.g., an email or a web page), and it
will try to transfer it in the shortest time possible. While a TCP connection is idle, it doesn’t
use any bandwidth.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417551709504-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551709504">ii</a></sup></p>

<p><a data-type="indexterm" data-primary="Ethernet (networks)" id="idm140417551708448"></a>
If datacenter networks and the internet were circuit-switched networks, it would be possible to
establish a guaranteed maximum round-trip time when a circuit was set up. However, they are not:
Ethernet and IP are packet-switched protocols, which suffer from queueing and thus unbounded delays
in the network. These protocols do not have the concept of a circuit.</p>

<p><a data-type="indexterm" data-primary="bursty network traffic patterns" id="idm140417551706976"></a>
Why do datacenter networks and the internet use packet switching? The answer is that they are
optimized for <em>bursty traffic</em>. A circuit is good for an audio or video call, which needs to
transfer a fairly constant number of bits per second for the duration of the call. On the other
hand, requesting a web page, sending an email, or transferring a file doesn’t have any particular
bandwidth requirement—we just want it to complete as quickly as possible.</p>

<p>If you wanted to transfer a file over a circuit, you would have to guess a bandwidth allocation. If
you guess too low, the transfer is unnecessarily slow, leaving network capacity unused. If you guess
too high, the circuit cannot be set up (because the network cannot allow a circuit to be created if
its bandwidth allocation cannot be guaranteed). Thus, using circuits for bursty data transfers
wastes network capacity and makes transfers unnecessarily slow. By contrast, TCP dynamically adapts
the rate of data transfer to the available network capacity.</p>

<p><a data-type="indexterm" data-primary="quality of service (QoS)" id="idm140417551703888"></a>
<a data-type="indexterm" data-primary="Asynchronous Transfer Mode (ATM)" id="idm140417551702864"></a>
<a data-type="indexterm" data-primary="InfiniBand (networks)" id="idm140417551702016"></a>
There have been some attempts to build hybrid networks that support both circuit switching and
packet switching, such as ATM.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417551701040-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551701040">iii</a></sup>
InfiniBand has some similarities
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mellanox2014ux-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Mellanox2014ux">35</a>]: it implements end-to-end
flow control at the link layer, which reduces the need for queueing in the network, although it can
still suffer from delays due to link congestion
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Santos2003ci-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Santos2003ci">36</a>].
With careful use of <em>quality of service</em> (QoS, prioritization and scheduling of packets) and <em>admission
control</em> (rate-limiting senders), it is possible to emulate circuit switching on packet networks, or
provide statistically bounded delay [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Grosvenor2015vz">25</a>,
<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Keshav1997wb">32</a>].</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_latency_utilization">
<h5>Latency and Resource Utilization</h5>
<p><a data-type="indexterm" data-primary="latency" data-secondary="network latency and resource utilization" id="idm140417551689184"></a>
More generally, you can think of variable delays as a consequence of dynamic resource partitioning.</p>

<p>Say you have a wire between two telephone switches that can carry up to 10,000 simultaneous calls.
Each circuit that is switched over this wire occupies one of those call slots. Thus, you can think of
the wire as a resource that can be shared by up to 10,000 simultaneous users. The resource is
divided up in a <em>static</em> way: even if you’re the only call on the wire right now, and all other
9,999 slots are unused, your circuit is still allocated the same fixed amount of bandwidth as when
the wire is fully utilized.</p>

<p>By contrast, the internet shares network bandwidth <em>dynamically</em>. Senders push and jostle with each
other to get their packets over the wire as quickly as possible, and the network switches decide
which packet to send (i.e., the bandwidth allocation) from one moment to the next. This approach has the
downside of queueing, but the advantage is that it maximizes utilization of the wire. The wire has a
fixed cost, so if you utilize it better, each byte you send over the wire is cheaper.</p>

<p><a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" id="idm140417551684960"></a>
A similar situation arises with CPUs: if you share each CPU core dynamically between several
threads, one thread sometimes has to wait in the operating system’s run queue while another thread
is running, so a thread can be paused for varying lengths of time. However, this utilizes the
hardware better than if you allocated a static number of CPU cycles to each thread (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_realtime">“Response time guarantees”</a>). Better hardware utilization is also a significant motivation
for using virtual machines.</p>

<p>Latency guarantees are achievable in certain environments, if resources are statically partitioned
(e.g., dedicated hardware and exclusive bandwidth allocations). However, it comes at the cost of
reduced utilization—in other words, it is more expensive. On the other hand, multi-tenancy with
dynamic resource partitioning provides better utilization, so it is cheaper, but it has the downside
of variable delays.</p>

<p>Variable delays in networks are not a law of nature, but simply the result of a cost/benefit
trade-off.
<a data-type="indexterm" data-primary="distributed systems" data-secondary="network problems" data-startref="ix_distsysunrelnet" id="idm140417551681120"></a></p>
</div></aside>

<p>However, such quality of service is currently not enabled in multi-tenant datacenters and public
clouds, or when communicating via the internet.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417551679328-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551679328">iv</a></sup>
Currently deployed technology does not allow us to make any guarantees about delays or reliability
of the network: we have to assume that network congestion, queueing, and unbounded delays will
happen. Consequently, there’s no “correct” value for timeouts—they need to be determined
experimentally.</p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Unreliable Clocks"><div class="sect1" id="sec_distributed_clocks">
<h1>Unreliable Clocks</h1>

<p><a data-type="indexterm" data-primary="data systems" data-secondary="unreliable clocks" id="ix_distsysunrelclock"></a>
<a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-seealso="clocks" id="ix_timeclock"></a>
<a data-type="indexterm" data-primary="clocks" id="ix_clocks"></a>
Clocks and time are important. Applications depend on clocks in various ways to answer questions
like the following:</p>
<ol>
<li>
<p>Has this request timed out yet?</p>
</li>
<li>
<p>What’s the 99th percentile response time of this service?</p>
</li>
<li>
<p>How many queries per second did this service handle on average in the last five minutes?</p>
</li>
<li>
<p>How long did the user spend on our site?</p>
</li>
<li>
<p>When was this article published?</p>
</li>
<li>
<p>At what date and time should the reminder email be sent?</p>
</li>
<li>
<p>When does this cache entry expire?</p>
</li>
<li>
<p>What is the timestamp on this error message in the log file?</p>
</li>

</ol>

<p><a data-type="indexterm" data-primary="duration (time)" id="idm140417551663584"></a>
<a data-type="indexterm" data-primary="point in time" id="idm140417551662720"></a>
Examples 1–4 measure <em>durations</em> (e.g., the time interval between a request being sent and a
response being received), whereas examples 5–8 describe <em>points in time</em> (events that occur on a
particular date, at a particular time).</p>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="use of clocks and time" id="idm140417551660592"></a>
In a distributed system, time is a tricky business, because communication is not instantaneous: it
takes time for a message to travel across the network from one machine to another. The time when a
message is received is always later than the time when it is sent, but due to variable delays in the
network, we don’t know how much later. This fact sometimes makes it difficult to determine the order
in which things happened when multiple machines are involved.</p>

<p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" id="idm140417551658752"></a>
<a data-type="indexterm" data-primary="Network Time Protocol" data-see="NTP" id="idm140417551657952"></a>
<a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm140417551656848"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm140417551655712"></a>
Moreover, each machine on the network has its own clock, which is an actual hardware device: usually
a quartz crystal oscillator. These devices are not perfectly accurate, so each machine has its own
notion of time, which may be slightly faster or slower than on other machines. It is possible to
synchronize clocks to some degree: the most commonly used mechanism is the Network Time Protocol (NTP), which
allows the computer clock to be adjusted according to the time reported by a group of servers
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Windl2006uo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Windl2006uo">37</a>]. The servers in turn get their time from a more accurate time source, such
as a GPS receiver.</p>








<section data-type="sect2" data-pdf-bookmark="Monotonic Versus Time-of-Day Clocks"><div class="sect2" id="sec_distributed_monotonic_timeofday">
<h2>Monotonic Versus Time-of-Day Clocks</h2>

<p><a data-type="indexterm" data-primary="clocks" data-secondary="time-of-day versus monotonic clocks" id="idm140417551649952"></a>
Modern computers have at least two different kinds of clocks: a <em>time-of-day clock</em> and a <em>monotonic
clock</em>. Although they both measure time, it is important to distinguish the two, since they serve
different purposes.</p>










<section data-type="sect3" data-pdf-bookmark="Time-of-day clocks"><div class="sect3" id="idm140417551647616">
<h3>Time-of-day clocks</h3>

<p><a data-type="indexterm" data-primary="time-of-day clocks" id="idm140417551646416"></a>
<a data-type="indexterm" data-primary="epoch (Unix timestamps)" id="idm140417551645360"></a>
<a data-type="indexterm" data-primary="real-time" data-secondary="time-of-day clocks" id="idm140417551644528"></a>
A time-of-day clock does what you intuitively expect of a clock: it returns the current date and
time according to some calendar (also known as <em>wall-clock time</em>). For example,
<code>clock_gettime(CLOCK_REALTIME)</code> on Linux<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417551642432-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551642432" class="totri-footnote">v</a></sup> and
<code>System.currentTimeMillis()</code> in Java return the number of seconds (or milliseconds) since the
<em>epoch</em>: midnight UTC on January 1, 1970, according to the Gregorian calendar, not counting leap
seconds. Some systems use other dates as their reference point.</p>

<p><a data-type="indexterm" data-primary="leap seconds" data-secondary="in time-of-day clocks" id="idm140417551638976"></a>
Time-of-day clocks are usually synchronized with NTP, which means that a timestamp from one machine
(ideally) means the same as a timestamp on another machine. However, time-of-day clocks also have
various oddities, as described in the next section. In particular, if the local clock is too far
ahead of the NTP server, it may be forcibly reset and appear to jump back to a previous point in
time. These jumps, as well as the fact that they often ignore leap seconds, make time-of-day clocks
unsuitable for measuring elapsed time
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GrahamCumming2017db-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#GrahamCumming2017db">38</a>].</p>

<p>Time-of-day clocks have also historically had quite a coarse-grained resolution, e.g., moving forward
in steps of 10&nbsp;ms on older Windows systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Holmes2006uj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Holmes2006uj">39</a>]. On recent systems, this is less of a problem.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Monotonic clocks"><div class="sect3" id="idm140417551631776">
<h3>Monotonic clocks</h3>

<p><a data-type="indexterm" data-primary="monotonic clocks" id="idm140417551630400"></a>
<a data-type="indexterm" data-primary="duration (time)" data-secondary="measurement with monotonic clocks" id="idm140417551629568"></a>
A monotonic clock is suitable for measuring a duration (time interval), such as a timeout or a
service’s response time: <code>clock_gettime(CLOCK_MONOTONIC)</code> on Linux and
<code>System.nanoTime()</code> in Java are monotonic clocks, for example. The name comes from the fact that they are
guaranteed to always move forward (whereas a time-of-day clock may jump back in time).</p>

<p>You can check the value of the monotonic clock at one point in time, do something, and then check
the clock again at a later time. The <em>difference</em> between the two values tells you how much time
elapsed between the two checks. However, the <em>absolute</em> value of the clock is meaningless: it might
be the number of nanoseconds since the computer was started, or something similarly arbitrary. In
particular, it makes no sense to compare monotonic clock values from two different computers,
because they don’t mean the same thing.</p>

<p>On a server with multiple CPU sockets, there may be a separate timer per CPU, which is not
necessarily synchronized with other CPUs. Operating systems compensate for any discrepancy and try
to present a monotonic view of the clock to application threads, even as they are scheduled across
different CPUs. However, it is wise to take this guarantee of monotonicity with a pinch of salt
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Loughran2015wi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Loughran2015wi">40</a>].</p>

<p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="adjustments to monotonic clocks" id="idm140417551621968"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="slewing" id="idm140417551620896"></a>
NTP may adjust the frequency at which the monotonic clock moves forward (this is known as <em>slewing</em>
the clock) if it detects that the computer’s local quartz is moving faster or slower than the NTP
server. By default, NTP allows the clock rate to be speeded up or slowed down by up to 0.05%, but
NTP cannot cause the monotonic clock to jump forward or backward. The resolution of monotonic
clocks is usually quite good: on most systems they can measure time intervals in microseconds or
less.</p>

<p>In a distributed system, using a monotonic clock for measuring elapsed time (e.g., timeouts) is
usually fine, because it doesn’t assume any synchronization between different nodes’ clocks and is
not sensitive to slight inaccuracies of measurement.</p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Clock Synchronization and Accuracy"><div class="sect2" id="sec_distributed_clock_accuracy">
<h2>Clock Synchronization and Accuracy</h2>

<p><a data-type="indexterm" data-primary="clocks" data-secondary="synchronization and accuracy" id="ix_clocksync"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="of time" id="ix_correcttime"></a>
<a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="clock synchronization and accuracy" id="idm140417551613712"></a>
<a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="accuracy" id="idm140417551612320"></a>
Monotonic clocks don’t need synchronization, but time-of-day clocks need to be set according to an
NTP server or other external time source in order to be useful. Unfortunately, our methods for
getting a clock to tell the correct time aren’t nearly as reliable or accurate as you might
hope—hardware clocks and NTP can be fickle beasts. To give just a few examples:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="drift (clocks)" id="idm140417551609824"></a>
The quartz clock in a computer is not very accurate: it <em>drifts</em> (runs faster or slower than it
should). Clock drift varies depending on the temperature of the machine. Google assumes a clock
drift of 200&nbsp;ppm (parts per million) for its servers
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Corbett2012uz_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Corbett2012uz_ch8">41</a>],
which is equivalent to 6&nbsp;ms drift for a clock that is resynchronized with a server every 30
seconds, or 17 seconds drift for a clock that is resynchronized once a day. This drift limits the best
possible accuracy you can achieve, even if everything is working correctly.</p>
</li>
<li>
<p>If a computer’s clock differs too much from an NTP server, it may refuse to synchronize, or the
local clock will be forcibly reset [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Windl2006uo">37</a>]. Any
applications observing the time before and after this reset may see time go backward or suddenly
jump forward.</p>
</li>
<li>
<p>If a node is accidentally firewalled off from NTP servers, the misconfiguration may go
unnoticed for some time. Anecdotal evidence suggests that this does happen in practice.</p>
</li>
<li>
<p>NTP synchronization can only be as good as the network delay, so there is a limit to its
accuracy when you’re on a congested network with variable packet delays. One experiment showed
that a minimum error of 35&nbsp;ms is achievable when synchronizing over the internet
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Caporaloni2012jn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Caporaloni2012jn">42</a>],
though occasional spikes in network delay lead to errors of around a second. Depending on the
configuration, large network delays can cause the NTP client to give up entirely.</p>
</li>
<li>
<p>Some NTP servers are wrong or misconfigured, reporting time that is off by hours
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Minar1999vf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Minar1999vf">43</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Holub2014uc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Holub2014uc">44</a>].
NTP clients are quite robust, because they query several servers and ignore outliers.
Nevertheless, it’s somewhat worrying to bet the correctness of your systems on the time that you
were told by a stranger on the internet.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="leap seconds" id="idm140417551591904"></a>
<a data-type="indexterm" data-primary="Linux, leap second bug" id="idm140417551590832"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="crashes due to leap seconds" id="idm140417551590000"></a>
Leap seconds result in a minute that is 59 seconds or 61 seconds long, which messes up timing
assumptions in systems that are not designed with leap seconds in mind
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kamp2011cr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kamp2011cr">45</a>].
<a data-type="indexterm" data-primary="smearing (leap seconds adjustments)" id="idm140417551585840"></a>
The fact that leap seconds have crashed many large systems
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#GrahamCumming2017db">38</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Minar2012vh_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Minar2012vh_ch8">46</a>]
shows how easy it is for incorrect assumptions about clocks to sneak into a system. The best
way of handling leap seconds may be to make NTP servers “lie,” by performing the leap second
adjustment gradually over the course of a day (this is known as <em>smearing</em>)
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pascoe2011uj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Pascoe2011uj">47</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhao2015ws-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Zhao2015ws">48</a>],
although actual NTP server behavior varies in practice
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Veitch2016jw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Veitch2016jw">49</a>].</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="virtual machines" data-secondary="virtualized clocks in" id="idm140417551573424"></a>
In virtual machines, the hardware clock is virtualized, which raises additional challenges for
applications that need accurate timekeeping
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="VMware2011vm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#VMware2011vm">50</a>].
When a CPU core is shared between virtual machines, each VM is paused for tens of milliseconds
while another VM is running. From an application’s point of view, this pause manifests itself as
the clock suddenly jumping forward [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Wang2010ja">26</a>].</p>
</li>
<li>
<p>If you run software on devices that you don’t fully control (e.g., mobile or embedded devices), you
probably cannot trust the device’s hardware clock at all. Some users deliberately set their
hardware clock to an incorrect date and time, for example to circumvent timing limitations in
games. As a result, the clock might be set to a time wildly in the past or the future.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="high-frequency trading" id="idm140417551567680"></a>
It is possible to achieve very good clock accuracy if you care about it sufficiently to invest
significant resources. For example, the MiFID II draft European regulation for financial
institutions requires all high-frequency trading funds to synchronize their clocks to within 100
microseconds of UTC, in order to help debug market anomalies such as “flash crashes” and to help
detect market manipulation
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MiFID2015wn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#MiFID2015wn">51</a>].</p>

<p><a data-type="indexterm" data-primary="Precision Time Protocol (PTP)" id="idm140417551564352"></a>
<a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm140417551563504"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm140417551562368"></a>
Such accuracy can be achieved using GPS receivers, the Precision Time Protocol (PTP)
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bigum2015ux-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bigum2015ux">52</a>],
and careful deployment and monitoring. However, it requires significant effort and expertise, and
there are plenty of ways clock synchronization can go wrong. If your NTP daemon is
misconfigured, or a firewall is blocking NTP traffic, the clock error due to drift can quickly
become large.
<a data-type="indexterm" data-primary="clocks" data-secondary="synchronization and accuracy" data-startref="ix_clocksync" id="idm140417551558512"></a></p>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Relying on Synchronized Clocks"><div class="sect2" id="sec_distributed_clocks_relying">
<h2>Relying on Synchronized Clocks</h2>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="synchronized clocks, relying on" id="ix_distsysclockrely"></a>
<a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="relying on synchronized clocks" id="ix_timeclocksync"></a>
The problem with clocks is that while they seem simple and easy to use, they have a surprising
number of pitfalls: a day may not have exactly 86,400 seconds, time-of-day clocks may move backward
in time, and the time on one node may be quite different from the time on another node.</p>

<p>Earlier in this chapter we discussed networks dropping and arbitrarily delaying packets. Even though
networks are well behaved most of the time, software must be designed on the assumption that the
network will occasionally be faulty, and the software must handle such faults gracefully. The same
is true with clocks: although they work quite well most of the time, robust software needs to be
prepared to deal with incorrect clocks.</p>

<p><a data-type="indexterm" data-primary="clocks" data-secondary="skew" id="ix_clockskew"></a>
<a data-type="indexterm" data-primary="skew" data-secondary="clock skew" id="ix_skewclock"></a>
Part of the problem is that incorrect clocks easily go unnoticed. If a machine’s CPU is defective or
its network is misconfigured, it most likely won’t work at all, so it will quickly be noticed and
fixed. On the other hand, if its quartz clock is defective or its NTP client is misconfigured, most
things will seem to work fine, even though its clock gradually drifts further and further away from
reality. If some piece of software is relying on an accurately synchronized clock, the result is
more likely to be silent and subtle data loss than a dramatic crash
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2013ti_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013ti_ch8">53</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Daily2013te_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Daily2013te_ch8">54</a>].</p>

<p>Thus, if you use software that requires synchronized clocks, it is essential that you also carefully
monitor the clock offsets between all the machines. Any node whose clock drifts too far from the
others should be declared dead and removed from the cluster. Such monitoring ensures that you notice
the broken clocks before they can cause too much damage.</p>










<section data-type="sect3" data-pdf-bookmark="Timestamps for ordering events"><div class="sect3" id="sec_distributed_lww">
<h3>Timestamps for ordering events</h3>

<p><a data-type="indexterm" data-primary="timestamps" data-secondary="ordering events" id="idm140417551540464"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-secondary="use of timestamps" id="idm140417551539360"></a>
Let’s consider one particular situation in which it is tempting, but dangerous, to rely on clocks:
ordering of events across multiple nodes. For example, if two clients write to a distributed
database, who got there first? Which write is the more recent one?</p>

<p><a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_timestamps">Figure&nbsp;8-3</a> illustrates a dangerous use of time-of-day clocks in a database with
multi-leader replication (the example is similar to <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#fig_replication_causality">Figure&nbsp;5-9</a>). Client A writes
<em>x</em>&nbsp;=&nbsp;1 on node 1; the write is replicated to node 3; client B increments <em>x</em> on node
3 (we now have <em>x</em>&nbsp;=&nbsp;2); and finally, both writes are replicated to node 2.</p>

<figure><div id="fig_distributed_timestamps" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0803.png" alt="ddia 0803" width="2880" height="1325" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0803.png">
<h6><span class="label">Figure 8-3. </span>The write by client B is causally later than the write by client A, but B’s write has an earlier timestamp.</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="causality" data-secondary="mismatch with clocks" id="idm140417551532080"></a>
In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_timestamps">Figure&nbsp;8-3</a>, when a write is replicated to other nodes, it is tagged with a
timestamp according to the time-of-day clock on the node where the write originated. The clock
synchronization is very good in this example: the skew between node 1 and node 3 is less than
3&nbsp;ms, which is probably better than you can expect in practice.</p>

<p>Nevertheless, the timestamps in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_timestamps">Figure&nbsp;8-3</a> fail to order the events correctly:
the write <em>x</em>&nbsp;=&nbsp;1 has a timestamp of 42.004 seconds, but the write <em>x</em>&nbsp;=&nbsp;2
has a timestamp of 42.003 seconds, even though <em>x</em>&nbsp;=&nbsp;2 occurred unambiguously later.
When node 2 receives these two events, it will incorrectly conclude that <em>x</em>&nbsp;=&nbsp;1 is the
more recent value and drop the write <em>x</em>&nbsp;=&nbsp;2. In effect, client B’s increment operation
will be lost.</p>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="last write wins (LWW)" id="idm140417551525824"></a>
<a data-type="indexterm" data-primary="last write wins (LWW)" data-secondary="problems with" id="idm140417551524144"></a>
<a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="last-write-wins conflict resolution" id="idm140417551523008"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="data loss due to last-write-wins" id="idm140417551521936"></a>
This conflict resolution strategy is called <em>last write wins</em> (LWW), and it is widely used in both
multi-leader replication and leaderless databases such as Cassandra
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013ti_ch8">53</a>] and Riak
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Daily2013te_ch8">54</a>] (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_lww">“Last write wins (discarding concurrent writes)”</a>). Some implementations generate timestamps on the client rather than
the server, but this doesn’t change the fundamental problems with LWW:</p>

<ul>
<li>
<p>Database writes can mysteriously disappear: a node with a lagging clock is unable to overwrite
values previously written by a node with a fast clock until the clock skew between the nodes has
elapsed [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Daily2013te_ch8">54</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2013vs-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013vs">55</a>]. This scenario can cause arbitrary
amounts of data to be silently dropped without any error being reported to the application.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="causality" data-secondary="violations of" id="idm140417551512320"></a>
LWW cannot distinguish between writes that occurred sequentially in quick succession (in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_timestamps">Figure&nbsp;8-3</a>, client B’s increment definitely occurs <em>after</em> client A’s write)
and writes that were truly concurrent (neither writer was aware of the other). Additional
causality tracking mechanisms, such as version vectors, are needed in order to prevent violations
of causality (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>).</p>
</li>
<li>
<p>It is possible for two nodes to independently generate writes with the same timestamp, especially
when the clock only has millisecond resolution. An additional tiebreaker value (which can simply
be a large random number) is required to resolve such conflicts, but this approach can also lead to
violations of causality [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013ti_ch8">53</a>].</p>
</li>
</ul>

<p>Thus, even though it is tempting to resolve conflicts by keeping the most “recent” value and
discarding others, it’s important to be aware that the definition of “recent” depends on a local
time-of-day clock, which may well be incorrect. Even with tightly NTP-synchronized clocks, you could
send a packet at timestamp 100&nbsp;ms (according to the sender’s clock) and have it arrive at
timestamp 99&nbsp;ms (according to the recipient’s clock)—so it appears as though the packet
arrived before it was sent, which is impossible.</p>

<p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="accuracy" id="idm140417551505280"></a>
Could NTP synchronization be made accurate enough that such incorrect orderings cannot occur?
Probably not, because NTP’s synchronization accuracy is itself limited by the network round-trip
time, in addition to other sources of error such as quartz drift. For correct ordering, you would
need the clock source to be significantly more accurate than the thing you are measuring (namely
network delay).</p>

<p><a data-type="indexterm" data-primary="clocks" data-secondary="logical" data-see="logical clocks" id="idm140417551503488"></a>
<a data-type="indexterm" data-primary="logical clocks" id="idm140417551501888"></a>
<a data-type="indexterm" data-primary="physical clocks" data-see="clocks" id="idm140417551501056"></a>
So-called <em>logical clocks</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1978jq_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lamport1978jq_ch8">56</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kulkarni2014ws-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kulkarni2014ws">57</a>],
which are based on incrementing counters rather than an oscillating quartz crystal, are a safer
alternative for ordering events (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>). Logical clocks do not measure
the time of day or the number of seconds elapsed, only the relative ordering of events (whether one
event happened before or after another). In contrast, time-of-day and monotonic clocks, which
measure actual elapsed time, are also known as <em>physical clocks</em>. We’ll look at ordering a bit more
in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_ordering">“Ordering Guarantees”</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Clock readings have a confidence interval"><div class="sect3" id="idm140417551541664">
<h3>Clock readings have a confidence interval</h3>

<p><a data-type="indexterm" data-primary="clocks" data-secondary="confidence interval" id="ix_clockconfid"></a>
<a data-type="indexterm" data-primary="congestion (networks)" data-secondary="limiting accuracy of clocks" id="idm140417551489728"></a>
You may be able to read a machine’s time-of-day clock with microsecond or even nanosecond
resolution. But even if you can get such a fine-grained measurement, that doesn’t mean the value is
actually accurate to such precision. In fact, it most likely is not—as mentioned previously, the
drift in an imprecise quartz clock can easily be several milliseconds, even if you synchronize with
an NTP server on the local network every minute. With an NTP server on the public internet, the best
possible accuracy is probably to the tens of milliseconds, and the error may easily spike to over
100 ms when there is network congestion [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kulkarni2014ws">57</a>].</p>

<p>Thus, it doesn’t make sense to think of a clock reading as a point in time—it is more like a
range of times, within a confidence interval: for example, a system may be 95% confident that the
time now is between 10.3 and 10.5 seconds past the minute, but it doesn’t know any more precisely
than that [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sheehy2015jm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sheehy2015jm">58</a>].
If we only know the time +/–&nbsp;100&nbsp;ms, the microsecond digits in the timestamp are
essentially meaningless.</p>

<p><a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm140417551483232"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm140417551482096"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="atomic (caesium) clocks" id="idm140417551480976"></a>
<a data-type="indexterm" data-primary="atomic clocks (caesium clocks)" data-seealso="clocks" id="idm140417551479872"></a>
The uncertainty bound can be calculated based on your time source. If you have a GPS receiver or
atomic (caesium) clock directly attached to your computer, the expected error range is reported by
the manufacturer. If you’re getting the time from a server, the uncertainty is based on the expected
quartz drift since your last sync with the server, plus the NTP server’s uncertainty, plus the
network round-trip time to the server (to a first approximation, and assuming you trust the server).</p>

<p>Unfortunately, most systems don’t expose this uncertainty: for example, when you call
<code>clock_gettime()</code>, the return value doesn’t tell you the expected error of the timestamp, so you
don’t know if its confidence interval is five milliseconds or five years.</p>

<p><a data-type="indexterm" data-primary="Spanner (database)" data-secondary="TrueTime API" id="idm140417551476864"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="TrueTime (clock API)" id="idm140417551475600"></a>
An interesting exception is Google’s <em>TrueTime</em> API in Spanner
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Corbett2012uz_ch8">41</a>], which explicitly reports the
confidence interval on the local clock. When you ask it for the current time, you get back two
values: <code>[<em>earliest</em>, <em>latest</em>]</code>, which are the <em>earliest possible</em> and the <em>latest possible</em>
timestamp. Based on its uncertainty calculations, the clock knows that the actual current time is
somewhere within that interval. The width of the interval depends, among other things, on how long
it has been since the local quartz clock was last synchronized with a more accurate clock source.
<a data-type="indexterm" data-primary="clocks" data-secondary="skew" data-startref="ix_clockskew" id="idm140417551470624"></a>
<a data-type="indexterm" data-primary="skew" data-secondary="clock skew" data-startref="ix_skewclock" id="idm140417551469248"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Synchronized clocks for global snapshots"><div class="sect3" id="sec_distributed_spanner">
<h3>Synchronized clocks for global snapshots</h3>

<p><a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="synchronized clocks for global snapshots" id="idm140417551466416"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="for global snapshots" id="idm140417551465216"></a>
In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_snapshot_isolation">“Snapshot Isolation and Repeatable Read”</a> we discussed <em>snapshot isolation</em>, which is a very useful
feature in databases that need to support both small, fast read-write transactions and large,
long-running read-only transactions (e.g., for backups or analytics). It allows read-only
transactions to see the database in a consistent state at a particular point in time, without
locking and interfering with read-write transactions.</p>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" id="idm140417551462144"></a>
The most common implementation of snapshot isolation requires a monotonically increasing transaction
ID. If a write happened later than the snapshot (i.e., the write has a greater transaction ID than
the snapshot), that write is invisible to the snapshot transaction. On a single-node database, a
simple counter is sufficient for generating transaction IDs.</p>

<p><a data-type="indexterm" data-primary="causality" data-secondary="with synchronized clocks" id="idm140417551460416"></a>
<a data-type="indexterm" data-primary="coordination" data-secondary="cross-partition ordering" id="idm140417551459296"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-secondary="generators" id="idm140417551458176"></a>
<a data-type="indexterm" data-primary="Twitter" data-secondary="Snowflake (sequence number generator)" id="idm140417551457056"></a>
However, when a database is distributed across many machines, potentially in multiple datacenters, a
global, monotonically increasing transaction ID (across all partitions) is difficult to generate,
because it requires coordination. The transaction ID must reflect causality: if transaction B reads
a value that was written by transaction A, then B must have a higher transaction ID than
A—otherwise, the snapshot would not be consistent. With lots of small, rapid transactions, creating
transaction IDs in a distributed system becomes an untenable
bottleneck.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417551455360-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551455360">vi</a></sup></p>

<p><a data-type="indexterm" data-primary="timestamps" data-secondary="for transaction ordering" id="idm140417551452480"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-secondary="use of timestamps" id="idm140417551451360"></a>
Can we use the timestamps from synchronized time-of-day clocks as transaction IDs? If we could get
the synchronization good enough, they would have the right properties: later transactions have a
higher timestamp. The problem, of course, is the uncertainty about clock accuracy.</p>

<p><a data-type="indexterm" data-primary="Spanner (database)" data-secondary="snapshot isolation using clocks" id="idm140417551449696"></a>
Spanner implements snapshot isolation across datacenters in this way
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Demirbas2013uz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Demirbas2013uz">59</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Malkhi2013bl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Malkhi2013bl">60</a>].
It uses the clock’s confidence interval as reported by the TrueTime API, and is based on the
following observation: if you have two confidence intervals, each consisting of an earliest and
latest possible timestamp (<em>A</em> = [<em>A<sub>earliest</sub></em>, <em>A<sub>latest</sub></em>] and
<em>B</em> = [<em>B<sub>earliest</sub></em>, <em>B<sub>latest</sub></em>]), and those two intervals do not overlap (i.e.,
<em>A<sub>earliest</sub></em> &lt; <em>A<sub>latest</sub></em> &lt; <em>B<sub>earliest</sub></em> &lt; <em>B<sub>latest</sub></em>), then B definitely happened after A—there
can be no doubt. Only if the intervals overlap are we unsure in which order A and B happened.</p>

<p><a data-type="indexterm" data-primary="GPS (Global Positioning System)" data-secondary="use for clock synchronization" id="idm140417551436048"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="synchronization using GPS" id="idm140417551434928"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="atomic (caesium) clocks" id="idm140417551433808"></a>
<a data-type="indexterm" data-primary="atomic clocks (caesium clocks)" id="idm140417551432704"></a>
In order to ensure that transaction timestamps reflect causality, Spanner deliberately waits for the
length of the confidence interval before committing a read-write transaction. By doing so, it
ensures that any transaction that may read the data is at a sufficiently later time, so their
confidence intervals do not overlap. In order to keep the wait time as short as possible, Spanner
needs to keep the clock uncertainty as small as possible; for this purpose, Google deploys a GPS
receiver or atomic clock in each datacenter, allowing clocks to be synchronized to within about
7&nbsp;ms [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Corbett2012uz_ch8">41</a>].</p>

<p>Using clock synchronization for distributed transaction semantics is an area of active research
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kulkarni2014ws">57</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bravo2015uy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bravo2015uy">61</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kimball2016wi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kimball2016wi">62</a>].
These ideas are interesting, but they have not yet been implemented in mainstream databases outside
of Google.
<a data-type="indexterm" data-primary="clocks" data-secondary="confidence interval" data-startref="ix_clockconfid" id="idm140417551424320"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="of time" data-startref="ix_correcttime" id="idm140417551422944"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="synchronized clocks, relying on" data-startref="ix_distsysclockrely" id="idm140417551421568"></a>
<a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-tertiary="relying on synchronized clocks" data-startref="ix_timeclocksync" id="idm140417551420128"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Process Pauses"><div class="sect2" id="sec_distributed_clocks_pauses">
<h2>Process Pauses</h2>

<p><a data-type="indexterm" data-primary="time" data-secondary="process pauses" id="ix_timeclockproc"></a>
<a data-type="indexterm" data-primary="process pauses" id="ix_procpause"></a>
Let’s consider another example of dangerous clock use in a distributed system. Say you have a
database with a single leader per partition. Only the leader is allowed to accept writes. How does a
node know that it is still leader (that it hasn’t been declared dead by the others), and that it may
safely accept writes?</p>

<p><a data-type="indexterm" data-primary="leases" id="idm140417551413776"></a>
One option is for the leader to obtain a <em>lease</em> from the other nodes, which is similar to a lock
with a timeout [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1989cu-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gray1989cu">63</a>].
Only one node can hold the lease at any one time—thus, when a node obtains a lease, it knows that
it is the leader for some amount of time, until the lease expires. In order to remain leader, the
node must periodically renew the lease before it expires. If the node fails, it stops renewing the
lease, so another node can take over when it expires.</p>

<p>You can imagine the request-handling loop looking something like this:</p>

<pre data-type="programlisting" data-code-language="java"><code class="k">while</code> <code class="o">(</code><code class="kc">true</code><code class="o">)</code> <code class="o">{</code>
    <code class="n">request</code> <code class="o">=</code> <code class="n">getIncomingRequest</code><code class="o">();</code>

    <code class="c1">// Ensure that the lease always has at least 10 seconds remaining</code>
    <code class="k">if</code> <code class="o">(</code><code class="n">lease</code><code class="o">.</code><code class="na">expiryTimeMillis</code> <code class="o">-</code> <code class="n">System</code><code class="o">.</code><code class="na">currentTimeMillis</code><code class="o">()</code> <code class="o">&lt;</code> <code class="mi">10000</code><code class="o">)</code> <code class="o">{</code>
        <code class="n">lease</code> <code class="o">=</code> <code class="n">lease</code><code class="o">.</code><code class="na">renew</code><code class="o">();</code>
    <code class="o">}</code>

    <code class="k">if</code> <code class="o">(</code><code class="n">lease</code><code class="o">.</code><code class="na">isValid</code><code class="o">())</code> <code class="o">{</code>
        <code class="n">process</code><code class="o">(</code><code class="n">request</code><code class="o">);</code>
    <code class="o">}</code>
<code class="o">}</code></pre>

<p>What’s wrong with this code? Firstly, it’s relying on synchronized clocks: the expiry time on the
lease is set by a different machine (where the expiry may be calculated as the current time plus 30
seconds, for example), and it’s being compared to the local system clock. If the clocks are out of
sync by more than a few seconds, this code will start doing strange things.</p>

<p>Secondly, even if we change the protocol to only use the local monotonic clock, there is another
problem: the code assumes that very little time passes between the point that it checks the time
(<code>System.currentTimeMillis()</code>) and the time when the request is processed (<code>process(request)</code>).
Normally this code runs very quickly, so the 10 second buffer is more than enough to ensure that the
lease doesn’t expire in the middle of processing a request.</p>

<p><a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" id="ix_threadexecpause"></a>
However, what if there is an unexpected pause in the execution of the program? For example, imagine
the thread stops for 15 seconds around the line <code>lease.isValid()</code> before finally continuing. In
that case, it’s likely that the lease will have expired by the time the request is processed, and
another node has already taken over as leader. However, there is nothing to tell this thread that it
was paused for so long, so this code won’t notice that the lease has expired until the next
iteration of the loop—by which time it may have already done something unsafe by processing the
request.</p>

<p><a data-type="indexterm" data-primary="unbounded delays" data-secondary="process pauses" id="idm140417551350944"></a>
<a data-type="indexterm" data-primary="delays" data-secondary="unbounded process pauses" id="idm140417551349840"></a>
Is it crazy to assume that a thread might be paused for so long? Unfortunately not. There are
various reasons why this could happen:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" data-seealso="process pauses" id="ix_GCprocpause"></a>
<a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="garbage collection pauses" id="idm140417551345632"></a>
<a data-type="indexterm" data-primary="stop-the-world" data-see="garbage collection" id="idm140417551344592"></a>
Many programming language runtimes (such as the Java Virtual Machine) have a <em>garbage collector</em>
(GC) that occasionally needs to stop all running threads. These <em>“stop-the-world” GC pauses</em> have
sometimes been known to last for several minutes [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lipcon2011tn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lipcon2011tn">64</a>]!
Even so-called “concurrent” garbage collectors like the HotSpot JVM’s CMS cannot fully run in
parallel with the application code—even they need to stop the world from time to time
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thompson2013vj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Thompson2013vj">65</a>].
Although the pauses can often be reduced by changing allocation patterns or tuning GC settings
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ragozin2011wr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Ragozin2011wr">66</a>],
we must assume the worst if we want to offer robust guarantees.</p>
</li>
<li>
<p>In virtualized environments, a virtual machine can be <em>suspended</em> (pausing the execution of all
processes and saving the contents of memory to disk) and <em>resumed</em> (restoring the contents of
memory and continuing execution). This pause can occur at any time in a process’s execution and can
last for an arbitrary length of time. This feature is sometimes used for <em>live migration</em> of
virtual machines from one host to another without a reboot, in which case the length of the pause
depends on the rate at which processes are writing to memory
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Clark2005ud-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Clark2005ud">67</a>].</p>
</li>
<li>
<p>On end-user devices such as laptops, execution may also be suspended and resumed arbitrarily, e.g.,
when the user closes the lid of their laptop.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="virtual machines" data-secondary="context switches" id="idm140417551329504"></a>
<a data-type="indexterm" data-primary="context switches" id="idm140417551328400"></a>
When the operating system context-switches to another thread, or when the hypervisor switches to a
different virtual machine (when running in a virtual machine), the currently running thread can be
paused at any arbitrary point in the code. In the case of a virtual machine, the CPU time spent in
other virtual machines is known as <em>steal time</em>. If the machine is under heavy load—i.e., if
there is a long queue of threads waiting to run—it may take some time before the paused thread
gets to run again.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="I/O operations, waiting for" id="idm140417551325920"></a>
If the application performs synchronous disk access, a thread may be paused waiting for a slow
disk I/O operation to complete [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shaver2008ug-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Shaver2008ug">68</a>]. In many languages, disk access can happen
surprisingly, even if the code doesn’t explicitly mention file access—for example, the Java
classloader lazily loads class files when they are first used, which could happen at any time in
the program execution. I/O pauses and GC pauses may even conspire to combine their delays
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhuang2016ui-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Zhuang2016ui">69</a>]. If the disk is actually a network filesystem or network block device (such as
Amazon’s EBS), the I/O latency is further subject to the variability of network delays
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Newman2012vf">29</a>].</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="virtual memory" data-secondary="process pauses due to page faults" id="idm140417551318320"></a>
<a data-type="indexterm" data-primary="swapping to disk" data-see="virtual memory" id="idm140417551316976"></a>
<a data-type="indexterm" data-primary="paging" data-see="virtual memory" id="idm140417551315872"></a>
<a data-type="indexterm" data-primary="thrashing (out of memory)" id="idm140417551314768"></a>
If the operating system is configured to allow <em>swapping to disk</em> (<em>paging</em>), a simple memory
access may result in a page fault that requires a page from disk to be loaded into memory. The
thread is paused while this slow I/O operation takes place. If memory pressure is high, this may
in turn require a different page to be swapped out to disk. In extreme circumstances, the
operating system may spend most of its time swapping pages in and out of memory and getting little
actual work done (this is known as <em>thrashing</em>). To avoid this problem, paging is often disabled
on server machines (if you would rather kill a process to free up memory than risk thrashing).</p>
</li>
<li>
<p>A Unix process can be paused by sending it the <code>SIGSTOP</code> signal, for example by pressing Ctrl-Z in
a shell. This signal immediately stops the process from getting any more CPU cycles until it is
resumed with <code>SIGCONT</code>, at which point it continues running where it left off. Even if your
environment does not normally use <code>SIGSTOP</code>, it might be sent accidentally by an operations
engineer.</p>
</li>
</ul>

<p class="pagebreak-before"><a data-type="indexterm" data-primary="preemption" data-secondary="of threads" id="idm140417551308880"></a>
<a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="preemption" id="idm140417551307776"></a>
All of these occurrences can <em>preempt</em> the running thread at any point and resume it at some later time,
without the thread even noticing. The problem is similar to making multi-threaded code on a single
machine thread-safe: you can’t assume anything about timing, because arbitrary context switches and
parallelism may occur.</p>

<p>When writing multi-threaded code on a single machine, we have fairly good tools for making it
thread-safe: mutexes, semaphores, atomic counters, lock-free data structures, blocking queues, and
so on. Unfortunately, these tools don’t directly translate to distributed systems, because a
distributed system has no shared memory—only messages sent over an unreliable network.</p>

<p>A node in a distributed system must assume that its execution can be paused for a significant length
of time at any point, even in the middle of a function. During the pause, the rest of the world
keeps moving and may even declare the paused node dead because it’s not responding. Eventually,
the paused node may continue running, without even noticing that it was asleep until it checks its
clock sometime later.
<a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="execution pauses" data-startref="ix_threadexecpause" id="idm140417551304320"></a></p>










<section data-type="sect3" data-pdf-bookmark="Response time guarantees"><div class="sect3" id="sec_distributed_clocks_realtime">
<h3>Response time guarantees</h3>

<p><a data-type="indexterm" data-primary="response time" data-secondary="guarantees on" id="idm140417551301328"></a>
<a data-type="indexterm" data-primary="bounded delays" data-secondary="process pauses" id="idm140417551300224"></a>
<a data-type="indexterm" data-primary="delays" data-secondary="bounded process pauses" id="idm140417551299120"></a>
In many programming languages and operating systems, threads and processes may pause for an
unbounded amount of time, as discussed. Those reasons for pausing <em>can</em> be eliminated if you try
hard enough.</p>

<p><a data-type="indexterm" data-primary="real-time" data-secondary="response time guarantees" id="idm140417551297120"></a>
Some software runs in environments where a failure to respond within a specified time can cause
serious damage: computers that control aircraft, rockets, robots, cars, and other physical objects
must respond quickly and predictably to their sensor inputs. In these systems, there is a specified
<em>deadline</em> by which the software must respond; if it doesn’t meet the deadline, that may cause a
failure of the entire system. These are so-called <em>hard real-time</em> systems.</p>
<div data-type="note" epub:type="note"><h1>Is real-time really real?</h1>
<p>In embedded systems, <em>real-time</em> means that a system is carefully designed and tested to meet
specified timing guarantees in all circumstances. This meaning is in contrast to the more vague use of the
term <em>real-time</em> on the web, where it describes servers pushing data to clients and stream
processing without hard response time constraints (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>).</p>
</div>

<p>For example, if your car’s onboard sensors detect that you are currently experiencing a crash, you
wouldn’t want the release of the airbag to be delayed due to an inopportune GC pause in the airbag
release system.</p>

<p>Providing real-time guarantees in a system requires support from all levels of the software stack: a
<em>real-time operating system</em> (RTOS) that allows processes to be scheduled with a guaranteed
allocation of CPU time in specified intervals is needed; library functions must document their
worst-case execution times; dynamic memory allocation may be restricted or disallowed entirely
(real-time garbage collectors exist, but the application must still ensure that it doesn’t give the
GC too much work to do); and an enormous amount of testing and measurement must be done to ensure
that guarantees are being met.</p>

<p>All of this requires a large amount of additional work and severely restricts the range of
programming languages, libraries, and tools that can be used (since most languages and tools do not
provide real-time guarantees). For these reasons, developing real-time systems is very expensive,
and they are most commonly used in safety-critical embedded devices. Moreover, “real-time” is not the
same as “high-performance”—in fact, real-time systems may have lower throughput, since they have to
prioritize timely responses above all else (see also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sidebar_distributed_latency_utilization">“Latency and Resource Utilization”</a>).</p>

<p>For most server-side data processing systems, real-time guarantees are simply not economical or
appropriate. Consequently, these systems must suffer the pauses and clock instability that come from
operating in a non-real-time environment.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Limiting the impact of garbage collection"><div class="sect3" id="idm140417551286448">
<h3>Limiting the impact of garbage collection</h3>

<p>The negative effects of process pauses can be mitigated without resorting to expensive real-time
scheduling guarantees. Language runtimes have some flexibility around when they schedule garbage
collections, because they can track the rate of object allocation and the remaining free memory over
time.</p>

<p><a data-type="indexterm" data-primary="high-frequency trading" id="idm140417551284304"></a>
An emerging idea is to treat GC pauses like brief planned outages of a node, and to let other nodes
handle requests from clients while one node is collecting its garbage. If the runtime can warn the
application that a node soon requires a GC pause, the application can stop sending new requests to
that node, wait for it to finish processing outstanding requests, and then perform the GC while no
requests are in progress. This trick hides GC pauses from clients and reduces the high percentiles of
response time [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terei2015va-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Terei2015va">70</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Maas2015vf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Maas2015vf">71</a>].
Some latency-sensitive financial trading systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cinnober2013up-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Cinnober2013up">72</a>]
use this approach.</p>

<p>A variant of this idea is to use the garbage collector only for short-lived objects (which are fast
to collect) and to restart processes periodically, before they accumulate enough long-lived objects
to require a full GC of long-lived objects [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Thompson2013vj">65</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fowler2011wp_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Fowler2011wp_ch8">73</a>]. One node can be restarted at a time, and traffic can
be shifted away from the node before the planned restart, like in a rolling upgrade (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#ch_encoding">Chapter&nbsp;4</a>).</p>

<p>These measures cannot fully prevent garbage collection pauses, but they can usefully reduce their
impact on the application.
<a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" data-startref="ix_GCprocpause" id="idm140417551271152"></a>
<a data-type="indexterm" data-primary="time" data-secondary="process pauses" data-startref="ix_timeclockproc" id="idm140417551269776"></a>
<a data-type="indexterm" data-primary="process pauses" data-startref="ix_procpause" id="idm140417551268400"></a>
<a data-type="indexterm" data-primary="data systems" data-secondary="unreliable clocks" data-startref="ix_distsysunrelclock" id="idm140417551267296"></a>
<a data-type="indexterm" data-primary="time" data-secondary="in distributed systems" data-startref="ix_timeclock" id="idm140417551265920"></a>
<a data-type="indexterm" data-primary="clocks" data-startref="ix_clocks" id="idm140417551264544"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Knowledge, Truth, and Lies"><div class="sect1" id="sec_distributed_truth">
<h1>Knowledge, Truth, and Lies</h1>

<p>So far in this chapter we have explored the ways in which distributed systems are different from
programs running on a single computer: there is no shared memory, only message passing via an
unreliable network with variable delays, and the systems may suffer from partial failures, unreliable clocks,
and processing pauses.</p>

<p>The consequences of these issues are profoundly disorienting if you’re not used to distributed
systems. A node in the network cannot <em>know</em> anything for sure—it can only make guesses based on
the messages it receives (or doesn’t receive) via the network. A node can only find out what state
another node is in (what data it has stored, whether it is correctly functioning, etc.) by
exchanging messages with it. If a remote node doesn’t respond, there is no way of knowing what state
it is in, because problems in the network cannot reliably be distinguished from problems at a node.</p>

<p>Discussions of these systems border on the philosophical: What do we know to be true or false in our
system? How sure can we be of that knowledge, if the mechanisms for perception and measurement are
unreliable? Should software systems obey the laws that we expect of the physical world, such as
cause and effect?</p>

<p><a data-type="indexterm" data-primary="system models" id="idm140417551259120"></a>
Fortunately, we don’t need to go as far as figuring out the meaning of life. In a distributed
system, we can state the assumptions we are making about the behavior (the <em>system model</em>) and
design the actual system in such a way that it meets those assumptions. Algorithms can be proved to
function correctly within a certain system model. This means that reliable behavior is achievable,
even if the underlying system model provides very few guarantees.</p>

<p>However, although it is possible to make software well behaved in an unreliable system model, it
is not straightforward to do so. In the rest of this chapter we will further explore the notions of
knowledge and truth in distributed systems, which will help us think about the kinds of assumptions
we can make and the guarantees we may want to provide. In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a> we will proceed to
look at some examples of distributed systems, algorithms that provide particular guarantees
under particular assumptions.</p>








<section data-type="sect2" data-pdf-bookmark="The Truth Is Defined by the Majority"><div class="sect2" id="sec_distributed_majority">
<h2>The Truth Is Defined by the Majority</h2>

<p><a data-type="indexterm" data-primary="faults" data-secondary="network faults" data-tertiary="asymmetric faults" id="idm140417551253648"></a>
Imagine a network with an asymmetric fault: a node is able to receive all messages sent to it, but
any outgoing messages from that node are dropped or delayed
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Donges2012tt">19</a>]. Even though that node is working
perfectly well, and is receiving requests from other nodes, the other nodes cannot hear its
responses. After some timeout, the other nodes declare it dead, because they haven’t heard from the
node. The situation unfolds like a nightmare: the semi-disconnected node is dragged to the
graveyard, kicking and screaming “I’m not dead!”—but since nobody can hear its screaming, the
funeral procession continues with stoic determination.</p>

<p>In a slightly less nightmarish scenario, the semi-disconnected node may notice that the messages it
is sending are not being acknowledged by other nodes, and so realize that there must be a fault
in the network. Nevertheless, the node is wrongly declared dead by the other nodes, and the
semi-disconnected node cannot do anything about it.</p>

<p><a data-type="indexterm" data-primary="garbage collection" data-secondary="process pauses for" id="idm140417551249776"></a>
As a third scenario, imagine a node that experiences a long stop-the-world garbage collection pause.
All of the node’s threads are preempted by the GC and paused for one minute, and consequently, no
requests are processed and no responses are sent. The other nodes wait, retry, grow impatient, and
eventually declare the node dead and load it onto the hearse. Finally, the GC finishes and the
node’s threads continue as if nothing had happened. The other nodes are surprised as the supposedly
dead node suddenly raises its head out of the coffin, in full health, and starts cheerfully chatting
with bystanders. At first, the GCing node doesn’t even realize that an entire minute has passed and
that it was declared dead—from its perspective, hardly any time has passed since it was last talking
to the other nodes.</p>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="quorums, relying on" id="idm140417551247584"></a>
<a data-type="indexterm" data-primary="quorums" data-secondary="making decisions in distributed systems" id="idm140417551246480"></a>
The moral of these stories is that a node cannot necessarily trust its own judgment of a situation.
A distributed system cannot exclusively rely on a single node, because a node may fail at any time,
potentially leaving the system stuck and unable to recover. Instead, many distributed algorithms
rely on a <em>quorum</em>, that is, voting among the nodes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>):
decisions require some minimum number of votes from several nodes in order to reduce the dependence
on any one particular node.</p>

<p>That includes decisions about declaring nodes dead. If a quorum of nodes declares another node
dead, then it must be considered dead, even if that node still very much feels alive. The individual
node must abide by the quorum decision and step down.</p>

<p>Most commonly, the quorum is an absolute majority of more than half the nodes (although other kinds
of quorums are possible). A majority quorum allows the system to continue working if individual nodes
have failed (with three nodes, one failure can be tolerated; with five nodes, two failures can be
tolerated). However, it is still safe, because there can only be only one majority in the
system—there cannot be two majorities with conflicting decisions at the same time. We will discuss
the use of quorums in more detail when we get to <em>consensus algorithms</em> in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>










<section data-type="sect3" data-pdf-bookmark="The leader and the lock"><div class="sect3" id="sec_distributed_lock_fencing">
<h3>The leader and the lock</h3>

<p><a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" id="ix_locksdistrib"></a>
<a data-type="indexterm" data-primary="leader-based replication" data-secondary="failover" id="idm140417551207440"></a>
<a data-type="indexterm" data-primary="failover" data-secondary="leader election" id="idm140417551206320"></a>
Frequently, a system requires there to be only one of some thing. For example:</p>

<ul>
<li>
<p>Only one node is allowed to be the leader for a database partition, to avoid split brain (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>).</p>
</li>
<li>
<p>Only one transaction or client is allowed to hold the lock for a particular resource or object, to
prevent concurrently writing to it and corrupting it.</p>
</li>
<li>
<p>Only one user is allowed to register a particular username, because a username must uniquely
identify a user.</p>
</li>
</ul>

<p>Implementing this in a distributed system requires care: even if a node believes that it is “the
chosen one” (the leader of the partition, the holder of the lock, the request handler of the user
who successfully grabbed the username), that doesn’t necessarily mean a quorum of nodes agrees!
A node may have formerly been the leader, but if the other nodes declared it dead in the meantime
(e.g., due to a network interruption or GC pause), it may have been demoted and another leader may
have already been elected.</p>

<p><a data-type="indexterm" data-primary="fencing (preventing split brain)" id="ix_fencing"></a>
<a data-type="indexterm" data-primary="split brain" data-secondary="using fencing tokens to avoid" id="ix_splitbrainfencing"></a>
If a node continues acting as the chosen one, even though the majority of nodes have declared it
dead, it could cause problems in a system that is not carefully designed. Such a node could send
messages to other nodes in its self-appointed capacity, and if other nodes believe it, the system as
a whole may do something incorrect.</p>

<p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="bug due to lack of fencing" id="idm140417551196624"></a>
<a data-type="indexterm" data-primary="leases" data-secondary="need for fencing" id="idm140417551195328"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="due to split brain" id="idm140417551194224"></a>
For example, <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_io_fencing">Figure&nbsp;8-4</a> shows a data corruption bug due to an incorrect
implementation of locking. (The bug is not theoretical: HBase used to have this problem
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2013wi_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Junqueira2013wi_ch8">74</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Soztutar2013vj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Soztutar2013vj">75</a>].) Say you want to ensure that a file in a storage service can only be
accessed by one client at a time, because if multiple clients tried to write to it, the file would
become corrupted. You try to implement this by requiring a client to obtain a lease from a lock
service before accessing the file.</p>

<figure><div id="fig_distributed_io_fencing" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0804.png" alt="ddia 0804" width="2880" height="1057" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0804.png">
<h6><span class="label">Figure 8-4. </span>Incorrect implementation of a distributed lock: client 1 believes that it still has a valid lease, even though it has expired, and thus corrupts a file in storage.</h6>
</div></figure>

<p>The problem is an example of what we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_pauses">“Process Pauses”</a>: if the client
holding the lease is paused for too long, its lease expires. Another client can obtain a lease for
the same file, and start writing to the file. When the paused client comes back, it believes
(incorrectly) that it still has a valid lease and proceeds to also write to the file. As a result,
the clients’ writes clash and corrupt the file.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Fencing tokens"><div class="sect3" id="sec_distributed_fencing_tokens">
<h3>Fencing tokens</h3>

<p><a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-tertiary="fencing tokens" id="idm140417551182752"></a>
When using a lock or lease to protect access to some resource, such as the file storage in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_io_fencing">Figure&nbsp;8-4</a>, we need to ensure that a node that is under a false belief of being
“the chosen one” cannot disrupt the rest of the system. A fairly simple technique that achieves this
goal is called <em>fencing</em>, and is illustrated in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_io_fencing_tokens">Figure&nbsp;8-5</a>.</p>

<figure><div id="fig_distributed_io_fencing_tokens" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0805.png" alt="ddia 0805" width="2880" height="1057" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0805.png">
<h6><span class="label">Figure 8-5. </span>Making access to storage safe by allowing writes only in the order of increasing fencing tokens.</h6>
</div></figure>

<p>Let’s assume that every time the lock server grants a lock or lease, it also returns a <em>fencing
token</em>, which is a number that increases every time a lock is granted (e.g., incremented by the lock
service). We can then require that every time a client sends a write request to the storage service,
it must include its current fencing token.</p>

<p>In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_io_fencing_tokens">Figure&nbsp;8-5</a>, client 1 acquires the lease with a token of 33, but then
it goes into a long pause and the lease expires. Client 2 acquires the lease with a token of 34 (the
number always increases) and then sends its write request to the storage service, including the
token of 34. Later, client 1 comes back to life and sends its write to the storage service,
including its token value 33. However, the storage server remembers that it has already processed a
write with a higher token number (34), and so it rejects the request with token 33.</p>

<p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="generating fencing tokens" id="idm140417551173568"></a>
If ZooKeeper is used as lock service, the transaction ID <code>zxid</code> or the node version
<span class="keep-together"><code>cversion</code></span> can
be used as fencing token. Since they are guaranteed to be monotonically increasing, they have the
required properties [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Junqueira2013wi_ch8">74</a>].</p>

<p>Note that this mechanism requires the resource itself to take an active role in checking tokens by rejecting any
writes with an older token than one that has already been processed—it is not sufficient to rely on clients checking
their lock status themselves. For resources that do not explicitly support fencing tokens, you might
still be able work around the limitation (for example, in the case of a file storage service you
could include the fencing token in the filename). However, some kind of check is necessary to avoid
processing requests outside of the lock’s protection.</p>

<p>Checking a token on the server side may seem like a downside, but it is arguably a good thing: it is
unwise for a service to assume that its clients will always be well behaved, because the clients are
often run by people whose priorities are very different from the priorities of the people running
the service [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McCaffrey2015ui-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#McCaffrey2015ui">76</a>]. Thus, it is a good idea for any service to protect itself from accidentally
abusive clients.
<a data-type="indexterm" data-primary="fencing (preventing split brain)" data-startref="ix_fencing" id="idm140417551165872"></a>
<a data-type="indexterm" data-primary="split brain" data-secondary="using fencing tokens to avoid" data-startref="ix_splitbrainfencing" id="idm140417551164752"></a>
<a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-startref="ix_locksdistrib" id="idm140417551163360"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Byzantine Faults"><div class="sect2" id="sec_distributed_byzantine">
<h2>Byzantine Faults</h2>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="Byzantine faults" id="ix_distsysknowByz"></a>
<a data-type="indexterm" data-primary="Byzantine faults" id="ix_Byzfaults"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="Byzantine faults" id="ix_faultsByz"></a>
Fencing tokens can detect and block a node that is <em>inadvertently</em> acting in error (e.g., because it
hasn’t yet found out that its lease has expired). However, if the node deliberately wanted to
subvert the system’s guarantees, it could easily do so by sending messages with a fake fencing
token.</p>

<p>In this book we assume that nodes are unreliable but honest: they may be slow or never respond (due
to a fault), and their state may be outdated (due to a GC pause or network delays), but we assume
that if a node <em>does</em> respond, it is telling the “truth”: to the best of its knowledge, it is
playing by the rules of the protocol.</p>

<p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="Byzantine Generals Problem" id="idm140417551154368"></a>
Distributed systems problems become much harder if there is a risk that nodes may “lie” (send
arbitrary faulty or corrupted responses)—for example, if a node may claim to have received a
particular message when in fact it didn’t. Such behavior is known as a <em>Byzantine fault</em>, and the
problem of reaching consensus in this untrusting environment is known as the <em>Byzantine Generals Problem</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1982fr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lamport1982fr">77</a>].</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_distributed_byzantine_generals">
<h5>The Byzantine Generals Problem</h5>
<p>The Byzantine Generals Problem is a generalization of the so-called <em>Two Generals Problem</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1978vv-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gray1978vv">78</a>],
which imagines a situation in which two army generals need to agree on a battle plan. As they
have set up camp on two different sites, they can only communicate by messenger, and the messengers
sometimes get delayed or lost (like packets in a network). We will discuss this problem of
<em>consensus</em> in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>.</p>

<p>In the Byzantine version of the problem, there are <em>n</em> generals who need to agree, and their
endeavor is hampered by the fact that there are some traitors in their midst. Most of the generals
are loyal, and thus send truthful messages, but the traitors may try to deceive and confuse the
others by sending fake or untrue messages (while trying to remain undiscovered). It is not known in
advance who the traitors are.</p>

<p>Byzantium was an ancient Greek city that later became Constantinople, in the place which is now
Istanbul in Turkey. There isn’t any historic evidence that the generals of Byzantium were any more
prone to intrigue and conspiracy than those elsewhere. Rather, the name is derived from <em>Byzantine</em>
in the sense of <em>excessively complicated, bureaucratic, devious</em>, which was used in politics long
before computers [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Palmer2011uh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Palmer2011uh">79</a>].
Lamport wanted to choose a nationality that would not offend any readers, and he was advised that
calling it <em>The Albanian Generals Problem</em> was not such a good idea
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="LamportPubs-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#LamportPubs">80</a>].</p>
</div></aside>

<p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="Byzantine fault-tolerant systems" id="idm140417551133824"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="Byzantine fault tolerance" id="idm140417551132704"></a>
A system is <em>Byzantine fault-tolerant</em> if it continues to operate correctly even if some of the
nodes are malfunctioning and not obeying the protocol, or if malicious attackers are interfering
with the network. This concern is relevant in certain specific circumstances. For example:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="rockets" id="idm140417551130032"></a><a data-type="indexterm" data-primary="aerospace systems" id="idm140417551129328"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="due to radiation" id="idm140417551128528"></a>
In aerospace environments, the data in a computer’s memory or CPU register could become corrupted
by radiation, leading it to respond to other nodes in arbitrarily unpredictable ways. Since a
system failure would be very expensive (e.g., an aircraft crashing and killing everyone on board,
or a rocket colliding with the International Space Station), flight control systems must tolerate
Byzantine faults [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rushby2001vu-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Rushby2001vu">81</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Edge2013wn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Edge2013wn">82</a>].</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="Bitcoin (cryptocurrency)" data-secondary="Byzantine fault tolerance" id="idm140417551121632"></a>
<a data-type="indexterm" data-primary="blockchains" data-secondary="Byzantine fault tolerance" id="idm140417551120448"></a>
In a system with multiple participating organizations, some participants may attempt to cheat or
defraud others. In such circumstances, it is not safe for a node to simply trust another node’s
messages, since they may be sent with malicious intent. For example, peer-to-peer networks like
Bitcoin and other blockchains can be considered to be a way of getting mutually untrusting parties
to agree whether a transaction happened or not, without relying on a central authority
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Miller2014wd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Miller2014wd">83</a>].</p>
</li>
</ul>

<p>However, in the kinds of systems we discuss in this book, we can usually safely assume that there
are no Byzantine faults. In your datacenter, all the nodes are controlled by your organization (so
they can hopefully be trusted) and radiation levels are low enough that memory corruption is not a
major problem. Protocols for making systems Byzantine fault-tolerant are quite complicated
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mickens2013tp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Mickens2013tp">84</a>],
and fault-tolerant embedded systems rely on support from the hardware level
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Rushby2001vu">81</a>]. In most server-side data systems, the
cost of deploying Byzantine fault-tolerant solutions makes them impractical.</p>

<p><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="SQL injection vulnerability" id="idm140417551112656"></a>
Web applications do need to expect arbitrary and malicious behavior of clients that are under
end-user control, such as web browsers. This is why input validation, sanitization, and output
escaping are so important: to prevent SQL injection and cross-site scripting, for example. However,
we typically don’t use Byzantine fault-tolerant protocols here, but simply make the server the
authority on deciding what client behavior is and isn’t allowed. In peer-to-peer networks, where
there is no such central authority, Byzantine fault tolerance is more relevant.</p>

<p>A bug in the software could be regarded as a Byzantine fault, but if you deploy the same software to
all nodes, then a Byzantine fault-tolerant algorithm cannot save you. Most Byzantine fault-tolerant
algorithms require a supermajority of more than two-thirds of the nodes to be functioning correctly
(i.e., if you have four nodes, at most one may malfunction). To use this approach against bugs, you
would have to have four independent implementations of the same software and hope that a bug only
appears in one of the four implementations.</p>

<p><a data-type="indexterm" data-primary="cryptography" data-secondary="defense against attackers" id="idm140417551109712"></a>
Similarly, it would be appealing if a protocol could protect us from vulnerabilities, security
compromises, and malicious attacks. Unfortunately, this is not realistic either: in most systems, if
an attacker can compromise one node, they can probably compromise all of them, because they are
probably running the same software. Thus, traditional mechanisms (authentication, access control,
encryption, firewalls, and so on) continue to be the main protection against attackers.</p>










<section data-type="sect3" data-pdf-bookmark="Weak forms of lying"><div class="sect3" id="sec_distributed_weak_lying">
<h3>Weak forms of lying</h3>

<p>Although we assume that nodes are generally honest, it can be worth adding mechanisms to software
that guard against weak forms of “lying”—for example, invalid messages due to hardware issues,
software bugs, and misconfiguration. Such protection mechanisms are not full-blown Byzantine fault
tolerance, as they would not withstand a determined adversary, but they are nevertheless simple and
pragmatic steps toward better reliability. For example:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="packets" data-secondary="corruption of" id="idm140417551104896"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="network packets" id="idm140417551103568"></a>
<a data-type="indexterm" data-primary="Ethernet (networks)" data-secondary="packet checksums" id="idm140417551102464"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="packet checksums" id="idm140417551101360"></a>
Network packets do sometimes get corrupted due to hardware issues or bugs in operating systems,
drivers, routers, etc. Usually, corrupted packets are caught by the checksums built into TCP and
UDP, but sometimes they evade detection [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gilman2015vp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gilman2015vp">85</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stone2000fc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Stone2000fc">86</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jones2015uy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Jones2015uy">87</a>].
Simple measures are usually sufficient protection against such corruption, such as checksums in
the application-level protocol.</p>
</li>
<li>
<p>A publicly accessible application must carefully sanitize any inputs from users, for example
checking that a value is within a reasonable range and limiting the size of strings to prevent
denial of service through large memory allocations. An internal service behind a firewall may be
able to get away with less strict checks on inputs, but some basic sanity-checking of values (e.g.,
in protocol parsing [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gilman2015vp">85</a>]) is a good idea.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="NTP (Network Time Protocol)" data-secondary="multiple server addresses" id="idm140417551089648"></a>
  NTP clients can be configured with multiple server addresses. When synchronizing, the client
  contacts all of them, estimates their errors, and checks that a majority of servers agree on some
  time range. As long as most of the servers are okay, a misconfigured NTP server that is reporting an
  incorrect time is detected as an outlier and is excluded from synchronization
  [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Windl2006uo">37</a>]. The use of multiple servers makes NTP
  more robust than if it only uses a single server.
<a data-type="indexterm" data-primary="faults" data-secondary="Byzantine faults" data-startref="ix_faultsByz" id="idm140417551087200"></a>
<a data-type="indexterm" data-primary="Byzantine faults" data-startref="ix_Byzfaults" id="idm140417551085856"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="Byzantine faults" data-startref="ix_distsysknowByz" id="idm140417551084752"></a></p>
</li>
</ul>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="System Model and Reality"><div class="sect2" id="sec_distributed_system_model">
<h2>System Model and Reality</h2>

<p><a data-type="indexterm" data-primary="system models" id="ix_sysmodreal"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="system models" id="ix_distsysknowsysmod"></a>
<a data-type="indexterm" data-primary="algorithms" data-secondary="for distributed systems" id="idm140417551078896"></a>
Many algorithms have been designed to solve distributed systems problems—for example, we will
examine solutions for the consensus problem in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>. In order to be useful, these
algorithms need to tolerate the various faults of distributed systems that we discussed in this
chapter.</p>

<p>Algorithms need to be written in a way that does not depend too heavily on the details of the
hardware and software configuration on which they are run. This in turn requires that we somehow
formalize the kinds of faults that we expect to happen in a system. We do this by defining a <em>system
model</em>, which is an abstraction that describes what things an algorithm may assume.</p>

<p><a data-type="indexterm" data-primary="time" data-secondary="system models for distributed systems" id="idm140417551075296"></a>
With regard to timing assumptions, three system models are in common use:</p>
<dl>
<dt>Synchronous model</dt>
<dd>
<p><a data-type="indexterm" data-primary="synchronous networks" data-secondary="formal model" id="idm140417551072608"></a>
The synchronous model assumes bounded network delay, bounded process pauses, and bounded clock
error. This does not imply exactly synchronized clocks or zero network delay; it just means you
know that network delay, pauses, and clock drift will never exceed some fixed upper bound
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dwork1988dr_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Dwork1988dr_ch8">88</a>].
The synchronous model is not a realistic model of most practical
systems, because (as discussed in this chapter) unbounded delays and pauses do occur.</p>
</dd>
<dt>Partially synchronous model</dt>
<dd>
<p>Partial synchrony means that a system behaves like a synchronous system <em>most of the time</em>, but it
sometimes exceeds the bounds for network delay, process pauses, and clock drift
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Dwork1988dr_ch8">88</a>]. This is a realistic model of many
systems: most of the time, networks and processes are quite well behaved—otherwise we would never
be able to get anything done—but we have to reckon with the fact that any timing assumptions
may be shattered occasionally. When this happens, network delay, pauses, and clock error may become
arbitrarily large.</p>
</dd>
<dt>Asynchronous model</dt>
<dd>
<p><a data-type="indexterm" data-primary="asynchronous networks" data-secondary="formal model" id="idm140417551063984"></a>
In this model, an algorithm is not allowed to make any timing assumptions—in fact, it does not
even have a clock (so it cannot use timeouts). Some algorithms can be designed for the
asynchronous model, but it is very restrictive.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="nodes (processes)" data-secondary="system models for failure" id="idm140417551062240"></a>
Moreover, besides timing issues, we have to consider node failures. The three most common system
models for nodes are:</p>
<dl>
<dt>Crash-stop faults</dt>
<dd>
<p>In the crash-stop model, an algorithm may assume that a node can fail in only one way, namely by
crashing. This means that the node may suddenly stop responding at any moment, and thereafter that
node is gone forever—it never comes back.</p>
</dd>
<dt>Crash-recovery faults</dt>
<dd>
<p>We assume that nodes may crash at any moment, and perhaps start responding again after some
unknown time. In the crash-recovery model, nodes are assumed to have stable storage (i.e.,
nonvolatile disk storage) that is preserved across crashes, while the in-memory state is assumed
to be lost.</p>
</dd>
<dt>Byzantine (arbitrary) faults</dt>
<dd>
<p><a data-type="indexterm" data-primary="Byzantine faults" id="idm140417551056272"></a>
Nodes may do absolutely anything, including trying to trick and deceive other nodes, as described
in the last section.</p>
</dd>
</dl>

<p>For modeling real systems, the partially synchronous model with crash-recovery faults is generally
the most useful model. But how do distributed algorithms cope with that model?</p>










<section data-type="sect3" data-pdf-bookmark="Correctness of an algorithm"><div class="sect3" id="idm140417551054480">
<h3>Correctness of an algorithm</h3>

<p><a data-type="indexterm" data-primary="algorithms" data-secondary="algorithm correctness" id="idm140417551053136"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="of algorithm within system model" id="idm140417551052032"></a>
<a data-type="indexterm" data-primary="system models" data-secondary="correctness of algorithms" id="idm140417551050912"></a>
To define what it means for an algorithm to be <em>correct</em>, we can describe its <em>properties</em>. For
example, the output of a sorting algorithm has the property that for any two distinct elements of
the output list, the element further to the left is smaller than the element further to the right.
That is simply a formal way of defining what it means for a list to be sorted.</p>

<p><a data-type="indexterm" data-primary="fencing (preventing split brain)" data-secondary="properties of fencing tokens" id="idm140417551048304"></a>
Similarly, we can write down the properties we want of a distributed algorithm to define what it
means to be correct. For example, if we are generating fencing tokens for a lock (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_fencing_tokens">“Fencing tokens”</a>), we may require the algorithm to have the following properties:</p>
<dl>
<dt>Uniqueness</dt>
<dd>
<p>No two requests for a fencing token return the same value.</p>
</dd>
<dt>Monotonic sequence</dt>
<dd>
<p>If request <em>x</em> returned token <em>t</em><sub><em>x</em></sub>, and request <em>y</em> returned token <em>t</em><sub><em>y</em></sub>, and
<em>x</em> completed before <em>y</em> began, then <em>t</em><sub><em>x</em></sub>&nbsp;&lt;&nbsp;<em>t</em><sub><em>y</em></sub>.</p>
</dd>
<dt>Availability</dt>
<dd>
<p>A node that requests a fencing token and does not crash eventually receives a response.</p>
</dd>
</dl>

<p>An algorithm is correct in some system model if it always satisfies its properties in all situations
that we assume may occur in that system model. But how does this make sense? If all nodes crash, or
all network delays suddenly become infinitely long, then no algorithm will be able to get anything
done.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Safety and liveness"><div class="sect3" id="sec_distributed_safety_liveness">
<h3>Safety and liveness</h3>

<p><a data-type="indexterm" data-primary="system models" data-secondary="safety and liveness" id="idm140417551033712"></a>
<a data-type="indexterm" data-primary="safety and liveness properties" id="idm140417551032384"></a>
<a data-type="indexterm" data-primary="liveness properties" id="idm140417551031584"></a>
To clarify the situation, it is worth distinguishing between two different kinds of properties:
<em>safety</em> and <em>liveness</em> properties. In the example just given, <em>uniqueness</em> and <em>monotonic sequence</em> are
safety properties, but <em>availability</em> is a liveness property.</p>

<p><a data-type="indexterm" data-primary="eventual consistency" id="idm140417551028336"></a>
What distinguishes the two kinds of properties? A giveaway is that liveness properties often include
the word “eventually” in their definition. (And yes, you guessed it—<em>eventual consistency</em> is a
liveness property [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2013jc_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bailis2013jc_ch8">89</a>].)</p>

<p>Safety is often informally defined as <em>nothing bad happens</em>, and liveness as <em>something good
eventually happens</em>. However, it’s best to not read too much into those informal definitions,
because the meaning of good and bad is subjective. The actual definitions of safety and liveness are
precise and mathematical
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alpern1985dg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Alpern1985dg">90</a>]:</p>

<ul>
<li>
<p>If a safety property is violated, we can point at a particular point in time at which it was
broken (for example, if the uniqueness property was violated, we can identify the particular
operation in which a duplicate fencing token was returned). After a safety property has been
violated, the violation cannot be undone—the damage is already done.</p>
</li>
<li>
<p>A liveness property works the other way round: it may not hold at some point in time (for example,
a node may have sent a request but not yet received a response), but there is always hope that it
may be satisfied in the future (namely by receiving a response).</p>
</li>
</ul>

<p>An advantage of distinguishing between safety and liveness properties is that it helps us deal with
difficult system models. For distributed algorithms, it is common to require that safety properties
<em>always</em> hold, in all possible situations of a system model
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Dwork1988dr_ch8">88</a>]. That is, even if all nodes crash, or
the entire network fails, the algorithm must nevertheless ensure that it does not return a wrong
result (i.e., that the safety properties remain satisfied).</p>

<p>However, with liveness properties we are allowed to make caveats: for example, we could say that a
request needs to receive a response only if a majority of nodes have not crashed, and only if the
network eventually recovers from an outage. The definition of the partially synchronous model
requires that eventually the system returns to a synchronous state—that is, any period of network
interruption lasts only for a finite duration and is then repaired.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Mapping system models to the real world"><div class="sect3" id="sec_distributed_model_real_world">
<h3>Mapping system models to the real world</h3>

<p><a data-type="indexterm" data-primary="system models" data-secondary="mapping to the real world" id="idm140417551011952"></a>
Safety and liveness properties and system models are very useful for reasoning about the correctness
of a distributed algorithm. However, when implementing an algorithm in practice, the messy facts of
reality come back to bite you again, and it becomes clear that the system model is a simplified
abstraction of reality.</p>

<p><a data-type="indexterm" data-primary="incidents" data-secondary="data on disks unreadable" id="idm140417551010192"></a>
<a data-type="indexterm" data-primary="GitHub, postmortems" id="idm140417551009072"></a>
For example, algorithms in the crash-recovery model generally assume that data in stable storage
survives crashes. However, what happens if the data on disk is corrupted, or the data is wiped out
due to hardware error or misconfiguration
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2015wf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Junqueira2015wf">91</a>]? What happens if a server has a firmware bug and fails to recognize
its hard drives on reboot, even though the drives are correctly attached to the server
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sanders2016tl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sanders2016tl">92</a>]?</p>

<p><a data-type="indexterm" data-primary="quorums" data-secondary="relying on durability" id="idm140417551003120"></a>
Quorum algorithms (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>) rely on a node remembering the data
that it claims to have stored. If a node may suffer from amnesia and forget previously stored data,
that breaks the quorum condition, and thus breaks the correctness of the algorithm. Perhaps a new
system model is needed, in which we assume that stable storage mostly survives crashes, but may
sometimes be lost. But that model then becomes harder to reason about.</p>

<p>The theoretical description of an algorithm can declare that certain things are simply assumed not
to happen—and in non-Byzantine systems, we do have to make some assumptions about faults that can
and cannot happen. However, a real implementation may still have to include code to handle the
case where something happens that was assumed to be impossible, even if that handling boils down to
<code>printf("Sucks to be you")</code> and <code>exit(666)</code>—i.e., letting a human operator clean up the mess
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2013ud-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kreps2013ud">93</a>].
(This is arguably the difference between computer science and software engineering.)</p>

<p><a data-type="indexterm" data-primary="complexity" data-secondary="distilling in theoretical models" id="idm140417550996560"></a>
That is not to say that theoretical, abstract system models are worthless—quite the opposite.
They are incredibly helpful for distilling down the complexity of real systems to a manageable set
of faults that we can reason about, so that we can understand the problem and try to solve it
systematically. We can prove algorithms correct by showing that their properties always hold in some
system model.</p>

<p>Proving an algorithm correct does not mean its <em>implementation</em> on a real system will necessarily
always behave correctly. But it’s a very good first step, because the theoretical analysis can
uncover problems in an algorithm that might remain hidden for a long time in a real system, and that
only come to bite you when your assumptions (e.g., about timing) are defeated due to unusual
circumstances. Theoretical analysis and empirical testing are equally important.
<a data-type="indexterm" data-primary="distributed systems" data-secondary="system models" data-startref="ix_distsysknowsysmod" id="idm140417550993696"></a>
<a data-type="indexterm" data-primary="system models" data-startref="ix_sysmodreal" id="idm140417550992320"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm140417551263568">
<h1>Summary</h1>

<p>In this chapter we have discussed a wide range of problems that can occur in distributed systems,
including:</p>

<ul>
<li>
<p>Whenever you try to send a packet over the network, it may be lost or arbitrarily delayed.
Likewise, the reply may be lost or delayed, so if you don’t get a reply, you have no idea whether
the message got through.</p>
</li>
<li>
<p>A node’s clock may be significantly out of sync with other nodes (despite your best efforts to set
up NTP), it may suddenly jump forward or back in time, and relying on it is dangerous because you
most likely don’t have a good measure of your clock’s error interval.</p>
</li>
<li>
<p>A process may pause for a substantial amount of time at any point in its execution (perhaps due to
a stop-the-world garbage collector), be declared dead by other nodes, and then come back to life
again without realizing that it was paused.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="partial failures" id="idm140417550985552"></a>
<a data-type="indexterm" data-primary="failures" data-secondary="partial failures in distributed systems" id="idm140417550984720"></a>
The fact that such <em>partial failures</em> can occur is the defining characteristic of distributed
systems. Whenever software tries to do anything involving other nodes, there is the possibility that
it may occasionally fail, or randomly go slow, or not respond at all (and eventually time out). In
distributed systems, we try to build tolerance of partial failures into software, so that the system
as a whole may continue functioning even when some of its constituent parts are broken.</p>

<p><a data-type="indexterm" data-primary="incidents" data-secondary="gigabit network interface with 1 Kb/s throughput" id="idm140417550982480"></a>
<a data-type="indexterm" data-primary="partial failures" data-secondary="limping" id="idm140417550981280"></a>
<a data-type="indexterm" data-primary="limping (partial failure)" id="idm140417550980176"></a>
To tolerate faults, the first step is to <em>detect</em> them, but even that is hard. Most systems
don’t have an accurate mechanism of detecting whether a node has failed, so most distributed
algorithms rely on timeouts to determine whether a remote node is still available. However, timeouts
can’t distinguish between network and node failures, and variable network delay sometimes causes a
node to be falsely suspected of crashing. Moreover, sometimes a node can be in a degraded state: for
example, a Gigabit network interface could suddenly drop to 1 Kb/s throughput due to a driver
bug [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Do2013hc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Do2013hc">94</a>].
Such a node that is “limping” but not dead can be even more difficult to deal with than a
cleanly failed node.</p>

<p>Once a fault is detected, making a system tolerate it is not easy either: there is no global
variable, no shared memory, no common knowledge or any other kind of shared state between the
machines. Nodes can’t even agree on what time it is, let alone on anything more profound. The only way
information can flow from one node to another is by sending it over the unreliable network. Major
decisions cannot be safely made by a single node, so we require protocols that enlist help from
other nodes and try to get a quorum to agree.</p>

<p>If you’re used to writing software in the idealized mathematical perfection of a single computer,
where the same operation always deterministically returns the same result, then moving to the messy
physical reality of distributed systems can be a bit of a shock. Conversely, distributed systems
engineers will often regard a problem as trivial if it can be solved on a single computer
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hodges2013tj" class="totri-footnote">5</a>],
and indeed a single computer can do a lot nowadays
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2015vx_ch8-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#McSherry2015vx_ch8">95</a>]. If you can avoid opening Pandora’s box and simply keep things on a
single machine, it is generally worth doing so.</p>

<p>However, as discussed in the introduction to <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/part02.html#part_distributed_data">Part&nbsp;II</a>, scalability is not the only
reason for wanting to use a distributed system. Fault tolerance and low latency (by placing data
geographically close to users) are equally important goals, and those things cannot be achieved with
a single node.</p>

<p>In this chapter we also went on some tangents to explore whether the unreliability of networks,
clocks, and processes is an inevitable law of nature. We saw that it isn’t: it is possible to give
hard real-time response guarantees and bounded delays in networks, but doing so is very expensive and
results in lower utilization of hardware resources. Most non-safety-critical systems choose cheap
and unreliable over expensive and reliable.</p>

<p>We also touched on supercomputers, which assume reliable components and thus have to be stopped and
restarted entirely when a component does fail. By contrast, distributed systems can run forever
without being interrupted at the service level, because all faults and maintenance can be handled at
the node level—at least in theory. (In practice, if a bad configuration change is rolled out to
all nodes, that will still bring a distributed system to its knees.)</p>

<p>This chapter has been all about problems, and has given us a bleak outlook. In the next chapter we
will move on to solutions, and discuss some algorithms that have been designed to cope with all the
problems in distributed systems.
<a data-type="indexterm" data-primary="distributed systems" data-startref="ix_distsys" id="idm140417550966560"></a></p>
</div></section>







<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417552019744"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417552019744-marker" class="totri-footnote">i</a></sup> With
one exception: we will assume that faults are <em>non-Byzantine</em> (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_byzantine">“Byzantine Faults”</a>).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417551709504"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551709504-marker">ii</a></sup> Except perhaps for an
occasional keepalive packet, if TCP keepalive is enabled.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417551701040"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551701040-marker">iii</a></sup> <em>Asynchronous
Transfer Mode</em> (ATM) was a competitor to Ethernet in the 1980s
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Keshav1997wb">32</a>], but it didn’t gain much adoption
outside of telephone network core switches. It has nothing to do with automatic teller machines
(also known as cash machines), despite sharing an acronym. Perhaps, in some parallel universe, the
internet is based on something like ATM—in that universe, internet video calls are probably a lot
more reliable than they are in ours, because they don’t suffer from dropped and delayed packets.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417551679328"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551679328-marker">iv</a></sup> Peering
agreements between internet service providers and the establishment of routes through the Border
Gateway Protocol (BGP), bear closer resemblance to circuit switching than IP itself. At this level,
it is possible to buy dedicated bandwidth. However, internet routing operates at the level of
networks, not individual connections between hosts, and at a much longer timescale.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417551642432"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551642432-marker" class="totri-footnote">v</a></sup> Although
the clock is called <em>real-time</em>, it has nothing to do with real-time operating systems, as
discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_realtime">“Response time guarantees”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417551455360"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#idm140417551455360-marker">vi</a></sup> There are distributed sequence
number generators, such as Twitter’s Snowflake, that generate <em>approximately</em> monotonically
increasing unique IDs in a scalable way (e.g., by allocating blocks of the ID space to different
nodes).  However, they typically cannot guarantee an ordering that is consistent with causality,
because the timescale at which blocks of IDs are assigned is longer than the timescale of database
reads and writes. See also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_ordering">“Ordering Guarantees”</a>.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cavage2013ez">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Cavage2013ez-marker" class="totri-footnote">1</a>] Mark Cavage:
“<a href="http://queue.acm.org/detail.cfm?id=2482856">There’s Just No Getting Around It: You’re
Building a Distributed System</a>,” <em>ACM Queue</em>, volume 11, number 4, pages 80-89, April 2013.
<a href="http://dx.doi.org/10.1145/2466486.2482856">doi:10.1145/2466486.2482856</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2012td_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kreps2012td_ch8-marker" class="totri-footnote">2</a>] Jay Kreps:
“<a href="http://blog.empathybox.com/post/19574936361/getting-real-about-distributed-system-reliability">Getting
Real About Distributed System Reliability</a>,” <em>blog.empathybox.com</em>, March 19, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Padua2015um">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Padua2015um-marker" class="totri-footnote">3</a>] Sydney Padua: <em>The Thrilling Adventures of
Lovelace and Babbage: The (Mostly) True Story of the First Computer</em>. Particular Books, April
2015.  ISBN: 978-0-141-98151-2</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hale2010we">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hale2010we-marker" class="totri-footnote">4</a>] Coda Hale:
“<a href="http://codahale.com/you-cant-sacrifice-partition-tolerance/">You Can’t Sacrifice
Partition Tolerance</a>,” <em>codahale.com</em>, October 7, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hodges2013tj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hodges2013tj-marker" class="totri-footnote">5</a>] Jeff Hodges:
“<a href="http://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/">Notes
on Distributed Systems for Young Bloods</a>,” <em>somethingsimilar.com</em>, January 14, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Regalado2011vn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Regalado2011vn-marker" class="totri-footnote">6</a>] Antonio Regalado:
“<a href="http://www.technologyreview.com/news/425970/who-coined-cloud-computing/">Who Coined
‘Cloud Computing’?</a>,” <em>technologyreview.com</em>, October 31, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Barroso2013ba">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Barroso2013ba-marker" class="totri-footnote">7</a>] Luiz André Barroso, Jimmy Clidaras, and Urs Hölzle:
“<a href="http://www.morganclaypool.com/doi/abs/10.2200/S00516ED2V01Y201306CAC024">The Datacenter
as a Computer: An Introduction to the Design of Warehouse-Scale Machines, Second Edition</a>,”
<em>Synthesis Lectures on Computer Architecture</em>, volume 8, number 3,
Morgan &amp; Claypool Publishers, July 2013.
<a href="http://dx.doi.org/10.2200/S00516ED2V01Y201306CAC024">doi:10.2200/S00516ED2V01Y201306CAC024</a>,
ISBN: 978-1-627-05010-4</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fiala2012ti">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Fiala2012ti-marker" class="totri-footnote">8</a>] David Fiala, Frank Mueller, Christian Engelmann, et al.:
“<a href="http://moss.csc.ncsu.edu/~mueller/ftp/pub/mueller/papers/sc12.pdf">Detection and
Correction of Silent Data Corruption for Large-Scale High-Performance Computing</a>,” at
<em>International Conference for High Performance Computing, Networking, Storage and
Analysis</em> (SC12), November 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Singh2015fc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Singh2015fc-marker" class="totri-footnote">9</a>] Arjun Singh, Joon Ong, Amit Agarwal, et al.:
“<a href="http://conferences.sigcomm.org/sigcomm/2015/pdf/papers/p183.pdf">Jupiter Rising: A
Decade of Clos Topologies and Centralized Control in Google’s Datacenter Network</a>,” at
<em>Annual Conference of the ACM Special Interest Group on Data Communication</em> (SIGCOMM), August 2015.
<a href="http://dx.doi.org/10.1145/2785956.2787508">doi:10.1145/2785956.2787508</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lockwood2014uz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lockwood2014uz-marker">10</a>] Glenn K. Lockwood:
“<a href="http://glennklockwood.blogspot.co.uk/2014/05/hadoops-uncomfortable-fit-in-hpc.html">Hadoop’s
Uncomfortable Fit in HPC</a>,” <em>glennklockwood.blogspot.co.uk</em>, May 16, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vonNeumann1956vm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#vonNeumann1956vm-marker">11</a>] John von Neumann:
“<a href="https://ece.uwaterloo.ca/~ssundara/courses/prob_logics.pdf">Probabilistic Logics and the
Synthesis of Reliable Organisms from Unreliable Components</a>,” in <em>Automata Studies (AM-34)</em>,
edited by Claude E. Shannon and John McCarthy, Princeton University Press, 1956.
ISBN: 978-0-691-07916-5</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hamming1997wd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hamming1997wd-marker">12</a>] Richard W. Hamming:
<em>The Art of Doing Science and Engineering</em>. Taylor &amp; Francis, 1997.
ISBN: 978-9-056-99500-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shannon1948wk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Shannon1948wk-marker">13</a>] Claude E. Shannon:
“<a href="http://cs.brynmawr.edu/Courses/cs380/fall2012/shannon1948.pdf">A Mathematical Theory of
Communication</a>,” <em>The Bell System Technical Journal</em>, volume 27, number 3,
pages 379–423 and 623–656, July 1948.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014jx">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bailis2014jx-marker">14</a>] Peter Bailis and Kyle Kingsbury:
“<a href="https://queue.acm.org/detail.cfm?id=2655736">The Network Is Reliable</a>,”
<em>ACM Queue</em>, volume 12, number 7, pages 48-55, July 2014.
<a href="http://dx.doi.org/10.1145/2639988.2639988">doi:10.1145/2639988.2639988</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Leners2015gv">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Leners2015gv-marker">15</a>] Joshua B. Leners, Trinabh Gupta, Marcos K. Aguilera, and Michael Walfish:
“<a href="http://www.cs.nyu.edu/~mwalfish/papers/albatross-eurosys15.pdf">Taming Uncertainty in
Distributed Systems with Help from the Network</a>,” at <em>10th European Conference on
Computer Systems</em> (EuroSys), April 2015.
<a href="http://dx.doi.org/10.1145/2741948.2741976">doi:10.1145/2741948.2741976</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gill2011ku">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gill2011ku-marker">16</a>] Phillipa Gill, Navendu Jain, and Nachiappan Nagappan:
“<a href="http://conferences.sigcomm.org/sigcomm/2011/papers/sigcomm/p350.pdf">Understanding
Network Failures in Data Centers: Measurement, Analysis, and Implications</a>,” at
<em>ACM SIGCOMM Conference</em>, August 2011.
<a href="http://dx.doi.org/10.1145/2018436.2018477">doi:10.1145/2018436.2018477</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Imbriaco2012tx_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Imbriaco2012tx_ch8-marker">17</a>] Mark Imbriaco:
“<a href="https://github.com/blog/1364-downtime-last-saturday">Downtime Last Saturday</a>,”
<em>github.com</em>, December 26, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oremus2014ty">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Oremus2014ty-marker">18</a>] Will Oremus:
“<a href="http://www.slate.com/blogs/future_tense/2014/08/15/shark_attacks_threaten_google_s_undersea_internet_cables_video.html">The
Global Internet Is Being Attacked by Sharks, Google Confirms</a>,” <em>slate.com</em>, August 15,
2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Donges2012tt">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Donges2012tt-marker">19</a>] Marc A. Donges:
“<a href="http://www.spinics.net/lists/netdev/msg210485.html">Re: bnx2 cards Intermittantly Going
Offline</a>,” Message to Linux <em>netdev</em> mailing list, <em>spinics.net</em>, September 13, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2014vi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2014vi-marker">20</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/317-call-me-maybe-elasticsearch">Call Me Maybe:
Elasticsearch</a>,” <em>aphyr.com</em>, June 15, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sanfilippo2014ty">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sanfilippo2014ty-marker">21</a>] Salvatore Sanfilippo:
“<a href="http://antirez.com/news/80">A Few Arguments About Redis Sentinel Properties and Fail
Scenarios</a>,” <em>antirez.com</em>, October 21, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hubert2009wf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hubert2009wf-marker">22</a>] Bert Hubert:
“<a href="http://blog.netherlabs.nl/articles/2009/01/18/the-ultimate-so_linger-page-or-why-is-my-tcp-not-reliable">The
Ultimate SO_LINGER Page, or: Why Is My TCP Not Reliable</a>,” <em>blog.netherlabs.nl</em>, January 18, 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Liochon2015ux">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Liochon2015ux-marker">23</a>] Nicolas Liochon:
“<a href="http://blog.thislongrun.com/2015/05/CAP-theorem-partition-timeout-zookeeper.html">CAP:
If All You Have Is a Timeout, Everything Looks Like a Partition</a>,” <em>blog.thislongrun.com</em>,
May 25, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saltzer1984do_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Saltzer1984do_ch8-marker">24</a>] Jerome H. Saltzer, David P. Reed, and
David D. Clark: “<a href="http://www.ece.drexel.edu/courses/ECE-C631-501/SalRee1984.pdf">End-To-End
Arguments in System Design</a>,” <em>ACM Transactions on Computer Systems</em>, volume 2, number 4,
pages 277–288, November 1984.
<a href="http://dx.doi.org/10.1145/357401.357402">doi:10.1145/357401.357402</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Grosvenor2015vz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Grosvenor2015vz-marker">25</a>] Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, et al.:
“<a href="https://www.usenix.org/system/files/conference/nsdi15/nsdi15-paper-grosvenor_update.pdf">Queues
Don’t Matter When You Can JUMP Them!</a>,” at <em>12th USENIX Symposium on Networked
Systems Design and Implementation</em> (NSDI), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wang2010ja">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Wang2010ja-marker">26</a>] Guohui Wang and T. S. Eugene Ng:
“<a href="http://www.cs.rice.edu/~eugeneng/papers/INFOCOM10-ec2.pdf">The Impact of
Virtualization on Network Performance of Amazon EC2 Data Center</a>,” at <em>29th IEEE
International Conference on Computer Communications</em> (INFOCOM), March 2010.
<a href="http://dx.doi.org/10.1109/INFCOM.2010.5461931">doi:10.1109/INFCOM.2010.5461931</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jacobson1988gl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Jacobson1988gl-marker">27</a>] Van Jacobson:
“<a href="http://www.cs.usask.ca/ftp/pub/discus/seminars2002-2003/p314-jacobson.pdf">Congestion
Avoidance and Control</a>,” at <em>ACM Symposium on Communications Architectures and
Protocols</em> (SIGCOMM), August 1988.
<a href="http://dx.doi.org/10.1145/52324.52356">doi:10.1145/52324.52356</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Philips2014tr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Philips2014tr-marker">28</a>] Brandon Philips:
“<a href="https://www.youtube.com/watch?v=HJIjTTHWYnE">etcd: Distributed Locking and Service
Discovery</a>,” at <em>Strange Loop</em>, September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Newman2012vf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Newman2012vf-marker">29</a>] Steve Newman:
“<a href="http://blog.scalyr.com/2012/10/a-systematic-look-at-ec2-io/">A Systematic Look at EC2
I/O</a>,” <em>blog.scalyr.com</em>, October 16, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hayashibara2004vw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Hayashibara2004vw-marker">30</a>] Naohiro Hayashibara, Xavier Défago, Rami Yared, and
Takuya Katayama: “<a href="http://hdl.handle.net/10119/4784">The ϕ Accrual Failure
Detector</a>,” Japan Advanced Institute of Science and Technology, School of Information
Science, Technical Report IS-RR-2004-010, May 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wang2013wa">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Wang2013wa-marker">31</a>] Jeffrey Wang:
“<a href="http://ternarysearch.blogspot.co.uk/2013/08/phi-accrual-failure-detector.html">Phi
Accrual Failure Detector</a>,” <em>ternarysearch.blogspot.co.uk</em>, August 11, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Keshav1997wb">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Keshav1997wb-marker">32</a>] Srinivasan Keshav: <em>An Engineering Approach
to Computer Networking: ATM Networks, the Internet, and the Telephone Network</em>.
Addison-Wesley Professional, May 1997. ISBN: 978-0-201-63442-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CiscoISDN">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#CiscoISDN-marker">33</a>] Cisco, “<a href="http://docwiki.cisco.com/wiki/Integrated_Services_Digital_Network">Integrated
Services Digital Network</a>,” <em>docwiki.cisco.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kyas1995ug">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kyas1995ug-marker">34</a>] Othmar Kyas: <em>ATM Networks</em>.
International Thomson Publishing, 1995. ISBN: 978-1-850-32128-6</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mellanox2014ux">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Mellanox2014ux-marker">35</a>] “<a href="http://www.mellanox.com/related-docs/whitepapers/InfiniBandFAQ_FQ_100.pdf">InfiniBand
FAQ</a>,” Mellanox Technologies, December 22, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Santos2003ci">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Santos2003ci-marker">36</a>] Jose Renato Santos, Yoshio Turner, and G. (John) Janakiraman:
“<a href="http://www.hpl.hp.com/techreports/2002/HPL-2002-359.pdf">End-to-End Congestion Control
for InfiniBand</a>,” at <em>22nd Annual Joint Conference of the IEEE Computer and
Communications Societies</em> (INFOCOM), April 2003. Also published by HP Laboratories Palo
Alto, Tech Report HPL-2002-359.
<a href="http://dx.doi.org/10.1109/INFCOM.2003.1208949">doi:10.1109/INFCOM.2003.1208949</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Windl2006uo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Windl2006uo-marker">37</a>] Ulrich Windl, David Dalton, Marc Martinec, and Dale R. Worley:
“<a href="http://www.ntp.org/ntpfaq/NTP-a-faq.htm">The NTP FAQ and HOWTO</a>,” <em>ntp.org</em>,
November 2006.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GrahamCumming2017db">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#GrahamCumming2017db-marker">38</a>] John Graham-Cumming:
“<a href="https://blog.cloudflare.com/how-and-why-the-leap-second-affected-cloudflare-dns/">How and
why the leap second affected Cloudflare DNS</a>,” <em>blog.cloudflare.com</em>, January 1, 2017.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Holmes2006uj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Holmes2006uj-marker">39</a>] David Holmes:
“<a href="https://blogs.oracle.com/dholmes/entry/inside_the_hotspot_vm_clocks">Inside the
Hotspot VM: Clocks, Timers and Scheduling Events – Part I – Windows</a>,” <em>blogs.oracle.com</em>,
October 2, 2006.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Loughran2015wi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Loughran2015wi-marker">40</a>] Steve Loughran:
“<a href="http://steveloughran.blogspot.co.uk/2015/09/time-on-multi-core-multi-socket-servers.html">Time
on Multi-Core, Multi-Socket Servers</a>,” <em>steveloughran.blogspot.co.uk</em>, September 17, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Corbett2012uz_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Corbett2012uz_ch8-marker">41</a>] James C. Corbett, Jeffrey Dean, Michael Epstein, et al.:
“<a href="http://research.google.com/archive/spanner.html">Spanner: Google’s Globally-Distributed
Database</a>,” at <em>10th USENIX Symposium on Operating System Design and
Implementation</em> (OSDI), October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Caporaloni2012jn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Caporaloni2012jn-marker">42</a>] M. Caporaloni and R. Ambrosini:
“<a href="https://iopscience.iop.org/0143-0807/23/4/103/">How Closely Can a Personal Computer
Clock Track the UTC Timescale Via the Internet?</a>,” <em>European Journal of
Physics</em>, volume 23, number 4, pages L17–L21, June 2012.
<a href="http://dx.doi.org/10.1088/0143-0807/23/4/103">doi:10.1088/0143-0807/23/4/103</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Minar1999vf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Minar1999vf-marker">43</a>] Nelson Minar:
“<a href="http://alumni.media.mit.edu/~nelson/research/ntp-survey99/">A Survey of the NTP Network</a>,”
<em>alumni.media.mit.edu</em>, December 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Holub2014uc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Holub2014uc-marker">44</a>] Viliam Holub:
“<a href="https://blog.logentries.com/2014/03/synchronizing-clocks-in-a-cassandra-cluster-pt-1-the-problem/">Synchronizing
Clocks in a Cassandra Cluster Pt. 1 – The Problem</a>,” <em>blog.logentries.com</em>, March 14, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kamp2011cr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kamp2011cr-marker">45</a>] Poul-Henning Kamp:
“<a href="http://queue.acm.org/detail.cfm?id=1967009">The One-Second War (What Time Will You
Die?)</a>,” <em>ACM Queue</em>, volume 9, number 4, pages 44–48, April 2011.
<a href="http://dx.doi.org/10.1145/1966989.1967009">doi:10.1145/1966989.1967009</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Minar2012vh_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Minar2012vh_ch8-marker">46</a>] Nelson Minar:
“<a href="http://www.somebits.com/weblog/tech/bad/leap-second-2012.html">Leap Second Crashes Half
the Internet</a>,” <em>somebits.com</em>, July 3, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pascoe2011uj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Pascoe2011uj-marker">47</a>] Christopher Pascoe:
“<a href="http://googleblog.blogspot.co.uk/2011/09/time-technology-and-leaping-seconds.html">Time,
Technology and Leaping Seconds</a>,” <em>googleblog.blogspot.co.uk</em>, September 15, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhao2015ws">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Zhao2015ws-marker">48</a>] Mingxue Zhao and Jeff Barr:
“<a href="https://aws.amazon.com/blogs/aws/look-before-you-leap-the-coming-leap-second-and-aws/">Look
Before You Leap – The Coming Leap Second and AWS</a>,” <em>aws.amazon.com</em>, May 18, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Veitch2016jw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Veitch2016jw-marker">49</a>] Darryl Veitch and Kanthaiah Vijayalayan:
“<a href="http://crin.eng.uts.edu.au/~darryl/Publications/LeapSecond_camera.pdf">Network Timing
and the 2015 Leap Second</a>,” at <em>17th International Conference on Passive and Active
Measurement</em> (PAM), April 2016.
<a href="http://dx.doi.org/10.1007/978-3-319-30505-9_29">doi:10.1007/978-3-319-30505-9_29</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="VMware2011vm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#VMware2011vm-marker">50</a>] “<a href="http://www.vmware.com/resources/techresources/238">Timekeeping
in VMware Virtual Machines</a>,” Information Guide, VMware, Inc., December 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MiFID2015wn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#MiFID2015wn-marker">51</a>] “<a href="https://www.esma.europa.eu/sites/default/files/library/2015/11/2015-esma-1464_annex_i_-_draft_rts_and_its_on_mifid_ii_and_mifir.pdf">MiFID
II / MiFIR: Regulatory Technical and Implementing Standards – Annex I (Draft)</a>,”
European Securities and Markets Authority, Report ESMA/2015/1464, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bigum2015ux">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bigum2015ux-marker">52</a>] Luke Bigum:
“<a href="https://www.lmax.com/blog/staff-blogs/2015/11/27/solving-mifid-ii-clock-synchronisation-minimum-spend-part-1/">Solving
MiFID II Clock Synchronisation With Minimum Spend (Part 1)</a>,” <em>lmax.com</em>, November 27, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2013ti_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013ti_ch8-marker">53</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/294-call-me-maybe-cassandra/">Call Me Maybe:
Cassandra</a>,” <em>aphyr.com</em>, September 24, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Daily2013te_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Daily2013te_ch8-marker">54</a>] John Daily:
“<a href="http://basho.com/clocks-are-bad-or-welcome-to-distributed-systems/">Clocks Are Bad, or,
Welcome to the Wonderful World of Distributed Systems</a>,” <em>basho.com</em>,
November 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2013vs">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kingsbury2013vs-marker">55</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/299-the-trouble-with-timestamps">The Trouble with
Timestamps</a>,” <em>aphyr.com</em>, October 12, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1978jq_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lamport1978jq_ch8-marker">56</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-US/um/people/Lamport/pubs/time-clocks.pdf">Time, Clocks,
and the Ordering of Events in a Distributed System</a>,” <em>Communications of the ACM</em>, volume
21, number 7, pages 558–565, July 1978.
<a href="http://dx.doi.org/10.1145/359545.359563">doi:10.1145/359545.359563</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kulkarni2014ws">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kulkarni2014ws-marker">57</a>] Sandeep Kulkarni, Murat Demirbas, Deepak Madeppa, et al.:
“<a href="http://www.cse.buffalo.edu/tech-reports/2014-04.pdf">Logical Physical Clocks and
Consistent Snapshots in Globally Distributed Databases</a>,” State University of New York at
Buffalo, Computer Science and Engineering Technical Report 2014-04, May 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sheehy2015jm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sheehy2015jm-marker">58</a>] Justin Sheehy:
“<a href="https://queue.acm.org/detail.cfm?id=2745385">There Is No Now: Problems With Simultaneity
in Distributed Systems</a>,” <em>ACM Queue</em>, volume 13, number 3, pages 36–41, March 2015.
<a href="http://dx.doi.org/10.1145/2733108">doi:10.1145/2733108</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Demirbas2013uz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Demirbas2013uz-marker">59</a>] Murat Demirbas:
“<a href="http://muratbuffalo.blogspot.co.uk/2013/07/spanner-googles-globally-distributed_4.html">Spanner:
Google’s Globally-Distributed Database</a>,” <em>muratbuffalo.blogspot.co.uk</em>, July 4, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Malkhi2013bl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Malkhi2013bl-marker">60</a>] Dahlia Malkhi and Jean-Philippe Martin:
“<a href="http://www.cs.cornell.edu/~ie53/publications/DC-col51-Sep13.pdf">Spanner’s Concurrency
Control</a>,” <em>ACM SIGACT News</em>, volume 44, number 3, pages 73–77, September 2013.
<a href="http://dx.doi.org/10.1145/2527748.2527767">doi:10.1145/2527748.2527767</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bravo2015uy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bravo2015uy-marker">61</a>] Manuel Bravo, Nuno Diegues, Jingna Zeng, et al.:
“<a href="http://sites.computer.org/debull/A15mar/p18.pdf">On the Use of Clocks
to Enforce Consistency in the Cloud</a>,” <em>IEEE Data Engineering Bulletin</em>,
volume 38, number 1, pages 18–31, March 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kimball2016wi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kimball2016wi-marker">62</a>] Spencer Kimball:
“<a href="http://www.cockroachlabs.com/blog/living-without-atomic-clocks/">Living Without Atomic
Clocks</a>,” <em>cockroachlabs.com</em>, February 17, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1989cu">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gray1989cu-marker">63</a>] Cary G. Gray and David R. Cheriton:
“<a href="http://web.stanford.edu/class/cs240/readings/89-leases.pdf">Leases: An Efficient
Fault-Tolerant Mechanism for Distributed File Cache Consistency</a>,” at
<em>12th ACM Symposium on Operating Systems Principles</em> (SOSP), December 1989.
<a href="http://dx.doi.org/10.1145/74850.74870">doi:10.1145/74850.74870</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lipcon2011tn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lipcon2011tn-marker">64</a>] Todd Lipcon:
“<a href="http://blog.cloudera.com/blog/2011/02/avoiding-full-gcs-in-hbase-with-memstore-local-allocation-buffers-part-1/">Avoiding
Full GCs in Apache HBase with MemStore-Local Allocation Buffers: Part 1</a>,”
<em>blog.cloudera.com</em>, February 24, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thompson2013vj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Thompson2013vj-marker">65</a>] Martin Thompson:
“<a href="http://mechanical-sympathy.blogspot.co.uk/2013/07/java-garbage-collection-distilled.html">Java
Garbage Collection Distilled</a>,” <em>mechanical-sympathy.blogspot.co.uk</em>, July 16, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ragozin2011wr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Ragozin2011wr-marker">66</a>] Alexey Ragozin:
“<a href="http://java.dzone.com/articles/how-tame-java-gc-pauses">How to Tame Java GC Pauses?
Surviving 16GiB Heap and Greater</a>,” <em>java.dzone.com</em>, June 28, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Clark2005ud">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Clark2005ud-marker">67</a>] Christopher Clark, Keir Fraser, Steven Hand, et al.:
“<a href="http://www.cl.cam.ac.uk/research/srg/netos/papers/2005-nsdi-migration.pdf">Live
Migration of Virtual Machines</a>,” at <em>2nd USENIX Symposium on Symposium on
Networked Systems Design &amp; Implementation</em> (NSDI), May 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shaver2008ug">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Shaver2008ug-marker">68</a>] Mike Shaver:
“<a href="http://shaver.off.net/diary/2008/05/25/fsyncers-and-curveballs/">fsyncers and
Curveballs</a>,” <em>shaver.off.net</em>, May 25, 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhuang2016ui">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Zhuang2016ui-marker">69</a>] Zhenyun Zhuang and Cuong Tran:
“<a href="https://engineering.linkedin.com/blog/2016/02/eliminating-large-jvm-gc-pauses-caused-by-background-io-traffic">Eliminating
Large JVM GC Pauses Caused by Background IO Traffic</a>,” <em>engineering.linkedin.com</em>, February 10,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terei2015va">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Terei2015va-marker">70</a>] David Terei and Amit Levy:
“<a href="http://arxiv.org/pdf/1504.02578.pdf">Blade: A Data Center Garbage Collector</a>,”
arXiv:1504.02578, April 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Maas2015vf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Maas2015vf-marker">71</a>] Martin Maas, Tim Harris, Krste Asanović, and John Kubiatowicz:
“<a href="https://timharris.uk/papers/2015-hotos.pdf">Trash Day: Coordinating Garbage Collection
in Distributed Systems</a>,” at <em>15th USENIX Workshop on Hot Topics in Operating
Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cinnober2013up">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Cinnober2013up-marker">72</a>] “<a href="http://cdn2.hubspot.net/hubfs/1624455/Website_2016/content/White%20papers/Cinnober%20on%20GC%20pause%20free%20Java%20applications.pdf">Predictable
Low Latency</a>,” Cinnober Financial Technology AB, <em>cinnober.com</em>, November 24, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fowler2011wp_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Fowler2011wp_ch8-marker">73</a>] Martin Fowler:
“<a href="http://martinfowler.com/articles/lmax.html">The LMAX Architecture</a>,”
<em>martinfowler.com</em>, July 12, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2013wi_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Junqueira2013wi_ch8-marker">74</a>] Flavio P. Junqueira and Benjamin Reed:
<em>ZooKeeper: Distributed Process Coordination</em>. O’Reilly Media, 2013.
ISBN: 978-1-449-36130-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Soztutar2013vj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Soztutar2013vj-marker">75</a>] Enis Söztutar:
“<a href="http://www.slideshare.net/enissoz/hbase-and-hdfs-understanding-filesystem-usage">HBase
and HDFS: Understanding Filesystem Usage in HBase</a>,” at <em>HBaseCon</em>,
June 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McCaffrey2015ui">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#McCaffrey2015ui-marker">76</a>] Caitie McCaffrey:
“<a href="http://caitiem.com/2015/06/23/clients-are-jerks-aka-how-halo-4-dosed-the-services-at-launch-how-we-survived/">Clients
Are Jerks: AKA How Halo 4 DoSed the Services at Launch &amp; How We Survived</a>,” <em>caitiem.com</em>,
June 23, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1982fr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Lamport1982fr-marker">77</a>] Leslie Lamport, Robert Shostak, and Marshall Pease:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/byz.pdf">The Byzantine
Generals Problem</a>,” <em>ACM Transactions on Programming Languages and
Systems</em> (TOPLAS), volume 4, number 3, pages 382–401, July 1982.
<a href="http://dx.doi.org/10.1145/357172.357176">doi:10.1145/357172.357176</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1978vv">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gray1978vv-marker">78</a>] Jim N. Gray:
“<a href="http://research.microsoft.com/en-us/um/people/gray/papers/DBOS.pdf">Notes on Data Base
Operating Systems</a>,” in <em>Operating Systems: An Advanced Course</em>, Lecture
Notes in Computer Science, volume 60, edited by R. Bayer, R. M. Graham, and G. Seegmüller,
pages 393–481, Springer-Verlag, 1978. ISBN: 978-3-540-08755-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Palmer2011uh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Palmer2011uh-marker">79</a>] Brian Palmer:
“<a href="http://www.slate.com/articles/news_and_politics/explainer/2011/10/the_byzantine_tax_code_how_complicated_was_byzantium_anyway_.html">How
Complicated Was the Byzantine Empire?</a>,” <em>slate.com</em>, October 20, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="LamportPubs">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#LamportPubs-marker">80</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/pubs.html">My
Writings</a>,” <em>research.microsoft.com</em>, December 16, 2014. This page can be found by searching the
web for the 23-character string obtained by removing the hyphens from the string
<code>allla-mport-spubso-ntheweb</code>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rushby2001vu">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Rushby2001vu-marker">81</a>] John Rushby:
“<a href="http://www.csl.sri.com/papers/emsoft01/emsoft01.pdf">Bus Architectures for
Safety-Critical Embedded Systems</a>,” at <em>1st International Workshop on Embedded Software</em>
(EMSOFT), October 2001.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Edge2013wn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Edge2013wn-marker">82</a>] Jake Edge:
“<a href="http://lwn.net/Articles/540368/">ELC: SpaceX Lessons Learned</a>,” <em>lwn.net</em>,
March 6, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Miller2014wd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Miller2014wd-marker">83</a>] Andrew Miller and Joseph J. LaViola, Jr.:
“<a href="http://nakamotoinstitute.org/static/docs/anonymous-byzantine-consensus.pdf">Anonymous
Byzantine Consensus from Moderately-Hard Puzzles: A Model for Bitcoin</a>,” University of Central
Florida, Technical Report CS-TR-14-01, April 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mickens2013tp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Mickens2013tp-marker">84</a>] James Mickens:
“<a href="https://www.usenix.org/system/files/login-logout_1305_mickens.pdf">The Saddest
Moment</a>,” <em>USENIX ;login: logout</em>, May 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gilman2015vp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Gilman2015vp-marker">85</a>] Evan Gilman:
“<a href="http://www.pagerduty.com/blog/the-discovery-of-apache-zookeepers-poison-packet/">The
Discovery of Apache ZooKeeper’s Poison Packet</a>,” <em>pagerduty.com</em>, May 7, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stone2000fc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Stone2000fc-marker">86</a>] Jonathan Stone and Craig Partridge:
“<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.27.7611&amp;rep=rep1&amp;type=pdf">When
the CRC and TCP Checksum Disagree</a>,” at <em>ACM Conference on Applications,
Technologies, Architectures, and Protocols for Computer Communication</em> (SIGCOMM), August 2000.
<a href="http://dx.doi.org/10.1145/347059.347561">doi:10.1145/347059.347561</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jones2015uy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Jones2015uy-marker">87</a>] Evan Jones:
“<a href="http://www.evanjones.ca/tcp-and-ethernet-checksums-fail.html">How Both TCP and Ethernet
Checksums Fail</a>,” <em>evanjones.ca</em>, October 5, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dwork1988dr_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Dwork1988dr_ch8-marker">88</a>] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
“<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf">Consensus in the
Presence of Partial Synchrony</a>,” <em>Journal of the ACM</em>, volume 35, number 2, pages 288–323,
April 1988. <a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2013jc_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Bailis2013jc_ch8-marker">89</a>] Peter Bailis and Ali Ghodsi:
“<a href="http://queue.acm.org/detail.cfm?id=2462076">Eventual Consistency Today: Limitations,
Extensions, and Beyond</a>,” <em>ACM Queue</em>, volume 11, number 3, pages 55-63, March 2013.
<a href="http://dx.doi.org/10.1145/2460276.2462076">doi:10.1145/2460276.2462076</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alpern1985dg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Alpern1985dg-marker">90</a>] Bowen Alpern and Fred B. Schneider:
“<a href="https://www.cs.cornell.edu/fbs/publications/DefLiveness.pdf">Defining Liveness</a>,”
<em>Information Processing Letters</em>, volume 21, number 4, pages 181–185, October 1985.
<a href="http://dx.doi.org/10.1016/0020-0190(85)90056-0">doi:10.1016/0020-0190(85)90056-0</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2015wf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Junqueira2015wf-marker">91</a>] Flavio P. Junqueira:
“<a href="http://fpj.me/2015/05/28/dude-wheres-my-metadata/">Dude, Where’s My Metadata?</a>,”
<em>fpj.me</em>, May 28, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sanders2016tl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Sanders2016tl-marker">92</a>] Scott Sanders:
“<a href="https://github.com/blog/2106-january-28th-incident-report">January 28th Incident
Report</a>,” <em>github.com</em>, February 3, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2013ud">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Kreps2013ud-marker">93</a>] Jay Kreps:
“<a href="http://blog.empathybox.com/post/62279088548/a-few-notes-on-kafka-and-jepsen">A Few Notes
on Kafka and Jepsen</a>,” <em>blog.empathybox.com</em>, September 25, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Do2013hc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#Do2013hc-marker">94</a>] Thanh Do, Mingzhe Hao, Tanakorn
Leesatapornwongsa, et al.:
“<a href="http://ucare.cs.uchicago.edu/pdf/socc13-limplock.pdf">Limplock: Understanding the Impact
of Limpware on Scale-out Cloud Systems</a>,” at <em>4th ACM Symposium on Cloud Computing</em>
(SoCC), October 2013.
<a href="http://dx.doi.org/10.1145/2523616.2523627">doi:10.1145/2523616.2523627</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2015vx_ch8">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#McSherry2015vx_ch8-marker">95</a>] Frank McSherry, Michael Isard, and Derek G. Murray:
“<a href="http://www.frankmcsherry.org/assets/COST.pdf">Scalability! But at What COST?</a>,”
at <em>15th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS),
May 2015.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">7. Transactions</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">9. Consistency and Consensus</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 69px;">
    <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/preferences/">Settings</a></li>
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    
    
      <img src="https://www.oreilly.com/library/view/oreilly_set_cookie/" alt="" style="display:none;">
    
    
    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.009px; left: 1356px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#">Reset</a>
</div>
</div>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.403475799025395"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.4411630421767636" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=a4389142-ba47-a28b-26bc-616cf1bf0750&amp;pi=1200101525&amp;lg=en-US&amp;sw=1440&amp;sh=900&amp;sc=24&amp;tl=8.%20The%20Trouble%20with%20Distributed%20Systems%20-%20Designing%20Data-Intensive%20Applications&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdesigning-data-intensive-applications%2F9781491903063%2Fch08.html&amp;r=&amp;lt=31640&amp;evt=pageLoad&amp;msclkid=N&amp;rn=872626"></div></body></html>