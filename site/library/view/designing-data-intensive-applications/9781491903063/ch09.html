<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/designing-data-intensive-applications/9781491903063/ch09.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3905629"
  data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99"
  data-username="safaribooksonline122"
  data-account-type="Trial"
  
  data-activated-trial-date="12/09/2018"


  data-archive="9781491903063"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch09.html"
  data-epub-title="Designing Data-Intensive Applications" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/designing-data-intensive-applications/9781491903063/ch09.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3905629" data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99" data-username="safaribooksonline122" data-account-type="Trial" data-activated-trial-date="12/09/2018" data-archive="9781491903063" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch09.html" data-epub-title="Designing Data-Intensive Applications" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491903063"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>9. Consistency and Consensus - Designing Data-Intensive Applications</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:1.5em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.map-ebook{page-break-after:always}
    </style><link rel="canonical" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html"><meta name="description" content=" Chapter 9. Consistency and Consensus Is it better to be alive and wrong or right and dead? Jay Kreps, A Few Notes on Kafka and Jepsen (2013) Lots of things ... "><meta property="og:title" content="9. Consistency and Consensus"><meta itemprop="isPartOf" content="/library/view/designing-data-intensive-applications/9781491903063/"><meta itemprop="name" content="9. Consistency and Consensus"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch09.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491903063/"><meta property="og:description" itemprop="description" content=" Chapter 9. Consistency and Consensus Is it better to be alive and wrong or right and dead? Jay Kreps, A Few Notes on Kafka and Jepsen (2013) Lots of things ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449373320"><meta property="og:book:author" itemprop="author" content="Martin Kleppmann"><meta property="og:book:tag" itemprop="about" content="Core Programming"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/f04af719-1c84-4fc3-9be3-1f1b4622ab99/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Designing Data-Intensive Applications
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781491903063/chapter/ch09.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch09.html&amp;text=Designing%20Data-Intensive%20Applications&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch09.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch09.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%209.%20Consistency%20and%20Consensus&amp;body=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch09.html%0D%0Afrom%20Designing%20Data-Intensive%20Applications%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. The Trouble with Distributed Systems</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/part03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">III. Derived Data</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Consistency and Consensus"><div class="chapter" id="ch_consistency">
<h1><span class="label">Chapter 9. </span>Consistency and Consensus</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
  <p><em>Is it better to be alive and wrong or right and dead?</em></p>
  <p data-type="attribution">Jay Kreps, <em>A Few Notes on Kafka and Jepsen</em> (2013)</p>
</blockquote>

<div class="map-ebook">
 <img id="c275" src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ch09-map-ebook.png" width="2756" height="2100" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ch09-map-ebook.png">
</div>

<p>Lots of things can go wrong in distributed systems, as discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>. The simplest
way of handling such faults is to simply let the entire service fail, and show the user an error
message. If that solution is unacceptable, we need to find ways of <em>tolerating</em> faults—that is, of
keeping the service functioning correctly, even if some internal component is faulty.</p>

<p>In this chapter, we will talk about some examples of algorithms and protocols for building
fault-tolerant distributed systems. We will assume that all the problems from <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a> can
occur: packets can be lost, reordered, duplicated, or arbitrarily delayed in the network; clocks are
approximate at best; and nodes can pause (e.g., due to garbage collection) or crash at any time.</p>

<p><a data-type="indexterm" data-primary="fault tolerance" data-secondary="abstractions for" id="idm140417550954736"></a>
The best way of building fault-tolerant systems is to find some general-purpose abstractions with
useful guarantees, implement them once, and then let applications rely on those guarantees. This is
the same approach as we used with transactions in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>: by using a transaction, the
application can pretend that there are no crashes (atomicity), that nobody else is concurrently
accessing the database (isolation), and that storage devices are perfectly reliable (durability).
Even though crashes, race conditions, and disk failures do occur, the transaction abstraction hides
those problems so that the application doesn’t need to worry about them.</p>

<p><a data-type="indexterm" data-primary="consensus" id="idm140417550951888"></a><a data-type="indexterm" data-primary="abstraction" id="idm140417550951184"></a>
We will now continue along the same lines, and seek abstractions that can allow an application to
ignore some of the problems with distributed systems. For example, one of the most important
abstractions for distributed systems is <em>consensus</em>: that is, getting all of the nodes to agree on
something. As we shall see in this chapter, reliably reaching consensus in spite of network faults
and process failures is a surprisingly tricky problem.</p>

<p><a data-type="indexterm" data-primary="split brain" data-secondary="preventing" id="idm140417550949232"></a>
Once you have an implementation of consensus, applications can use it for various purposes. For
example, say you have a database with single-leader replication. If the leader dies and you need to
fail over to another node, the remaining database nodes can use consensus to elect a new leader. As
discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>, it’s important that there is only one leader, and that
all nodes agree who the leader is. If two nodes both believe that they are the leader, that
situation is called <em>split brain</em>, and it often leads to data loss. Correct implementations of
consensus help avoid such problems.</p>

<p>Later in this chapter, in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus">“Distributed Transactions and Consensus”</a>, we <span class="keep-together">will look into</span>
<span class="keep-together">algorithms</span> to solve consensus and related problems. But we first need to explore the range of guarantees and abstractions that can be provided in a
distributed system.</p>

<p>We need to understand the scope of what can and cannot be done: in some situations, it’s possible
for the system to tolerate faults and continue working; in other situations, that is not possible.
The limits of what is and isn’t possible have been explored in depth, both in theoretical proofs and
in practical implementations. We will get an overview of those fundamental limits in this chapter.</p>

<p>Researchers in the field of distributed systems have been studying these topics for decades, so
there is a lot of material—we’ll only be able to scratch the surface. In this book we don’t have
space to go into details of the formal models and proofs, so we will stick with informal intuitions.
The literature references offer plenty of additional depth if you’re interested.</p>






<section data-type="sect1" data-pdf-bookmark="Consistency Guarantees"><div class="sect1" id="idm140417550941632">
<h1>Consistency Guarantees</h1>

<p>In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_lag">“Problems with Replication Lag”</a> we looked at some timing issues that occur in a replicated database. If
you look at two database nodes at the same moment in time, you’re likely to see different data on
the two nodes, because write requests arrive on different nodes at different times. These
inconsistencies occur no matter what replication method the database uses (single-leader,
multi-leader, or leaderless replication).</p>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="eventual" id="idm140417550938432"></a>
<a data-type="indexterm" data-primary="convergence (conflict resolution)" id="idm140417550937104"></a>
<a data-type="indexterm" data-primary="eventual consistency" id="idm140417550936304"></a>
Most replicated databases provide at least <em>eventual consistency</em>, which means that if you stop
writing to the database and wait for some unspecified length of time, then eventually all read
requests will return the same value [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2013jc_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2013jc_ch9" class="totri-footnote">1</a>].
In other words, the inconsistency is temporary, and it eventually resolves itself (assuming that any
faults in the network are also eventually repaired). A better name for eventual consistency may be
<em>convergence</em>, as we expect all replicas to eventually converge to the same value
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mahajan2011wz-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mahajan2011wz" class="totri-footnote">2</a>].</p>

<p>However, this is a very weak guarantee—it doesn’t say anything about <em>when</em> the replicas will
converge. Until the time of convergence, reads could return anything or nothing
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2013jc_ch9" class="totri-footnote">1</a>]. For example, if you write a value and
then immediately read it again, there is no guarantee that you will see the value you just wrote,
because the read may be routed to a different replica (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_ryw">“Reading Your Own Writes”</a>).</p>

<p>Eventual consistency is hard for application developers because it is so different from the behavior
of variables in a normal single-threaded program. If you assign a value to a variable and then read
it shortly afterward, you don’t expect to read back the old value, or for the read to fail. A
database looks superficially like a variable that you can read and write, but in fact it has much
more complicated semantics [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Scotti2015uc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Scotti2015uc" class="totri-footnote">3</a>].</p>

<p>When working with a database that provides only weak guarantees, you need to be constantly aware of
its limitations and not accidentally assume too much. Bugs are often subtle and hard to find by
testing, because the application may work well most of the time. The edge cases of eventual
consistency only become apparent when there is a fault in the system (e.g., a network interruption)
or at high concurrency.</p>

<p>In this chapter we will explore stronger consistency models that data systems may choose to provide.
They don’t come for free: systems with stronger guarantees may have worse performance or be less
fault-tolerant than systems with weaker guarantees. Nevertheless, stronger guarantees can be
appealing because they are easier to use correctly. Once you have seen a few different consistency
models, you’ll be in a better position to decide which one best fits your needs.</p>

<p>There is some similarity between distributed consistency models and the hierarchy of transaction
isolation levels we discussed previously
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014vc_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014vc_ch9" class="totri-footnote">4</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Viotti2016wr-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Viotti2016wr" class="totri-footnote">5</a>]
(see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>).
But while there is some overlap, they are mostly independent concerns: transaction isolation is
primarily about avoiding race conditions due to concurrently executing transactions, whereas
distributed consistency is mostly about coordinating the state of replicas in the face of delays and
faults.</p>

<p>This chapter covers a broad range of topics, but as we shall see, these areas are in fact deeply
linked:</p>

<ul>
<li>
<p>We will start by looking at one of the strongest consistency models in common use,
<em>linearizability</em>, and examine its pros and cons.</p>
</li>
<li>
<p>We’ll then examine the issue of ordering events in a distributed system
(<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_ordering">“Ordering Guarantees”</a>), particularly around causality and total ordering.</p>
</li>
<li>
<p>In the third section (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus">“Distributed Transactions and Consensus”</a>) we will explore how to atomically commit a
distributed transaction, which will finally lead us toward solutions for the consensus problem.</p>
</li>
</ul>
</div></section>













<section data-type="sect1" data-pdf-bookmark="Linearizability"><div class="sect1" id="sec_consistency_linearizability">
<h1>Linearizability</h1>

<p><a data-type="indexterm" data-primary="concurrency" data-secondary="in replicated systems" id="ix_concrepllinear"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="linearizability" id="ix_consislinear"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="strong" data-see="linearizability" id="idm140417550905232"></a>
<a data-type="indexterm" data-primary="strong consistency" data-see="linearizability" id="idm140417550903856"></a>
<a data-type="indexterm" data-primary="linearizability" id="ix_linear"></a>
In an eventually consistent database, if you ask two different replicas the same question at the
same time, you may get two different answers. That’s confusing. Wouldn’t it be a lot simpler if the
database could give the illusion that there is only one replica (i.e., only one copy of the data)?
Then every client would have the same view of the data, and you wouldn’t have to worry about
replication lag.</p>

<p>This is the idea behind <em>linearizability</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Herlihy1990jq-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1990jq" class="totri-footnote">6</a>]
(also known as <em>atomic consistency</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1986cg-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1986cg" class="totri-footnote">7</a>],
<em>strong consistency</em>, <em>immediate consistency</em>, or <em>external consistency</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gifford1981tu-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gifford1981tu" class="totri-footnote">8</a>]).
The exact definition of linearizability is quite subtle, and we will explore it in the rest of this
section. But the basic idea is to make a system appear as if there were only one copy of the data,
and all operations on it are atomic. With this guarantee, even though there may be multiple replicas
in reality, the application does not need to worry about them.</p>

<p><a data-type="indexterm" data-primary="recency guarantee" id="idm140417550890240"></a>
<a data-type="indexterm" data-primary="staleness (old data)" data-secondary="versus linearizability" id="idm140417550889504"></a>
<a data-type="indexterm" data-primary="caches" data-secondary="linearizability" id="idm140417550888400"></a>
In a linearizable system, as soon as one client successfully completes a write, all clients reading
from the database must be able to see the value just written. Maintaining the illusion of a single
copy of the data means guaranteeing that the value read is the most recent, up-to-date value, and
doesn’t come from a stale cache or replica. In other words, linearizability is a <em>recency
guarantee</em>. To clarify this idea, let’s look at an example of a system that is not linearizable.</p>

<figure><div id="fig_consistency_linearizability_0" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0901.png" alt="ddia 0901" width="2880" height="2063" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0901.png">
<h6><span class="label">Figure 9-1. </span>This system is not linearizable, causing football fans to be confused.</h6>
</div></figure>

<p><a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a> shows an example of a nonlinearizable sports website
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2015un-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015un" class="totri-footnote">9</a>].
Alice and Bob are sitting in the same room, both checking their phones to see the outcome of the
2014 FIFA World Cup final. Just after the final score is announced, Alice refreshes the page,
sees the winner announced, and excitedly tells Bob about it. Bob incredulously hits <em>reload</em> on his
own phone, but his request goes to a database replica that is lagging, and so his phone shows that
the game is still ongoing.</p>

<p>If Alice and Bob had hit reload at the same time, it would have been less surprising if they had gotten
two different query results, because they wouldn’t know at exactly what time their respective requests
were processed by the server. However, Bob knows that he hit the reload button (initiated his query)
<em>after</em> he heard Alice exclaim the final score, and therefore he expects his query result to be at
least as recent as Alice’s. The fact that his query returned a stale result is a violation of
linearizability.</p>








<section data-type="sect2" data-pdf-bookmark="What Makes a System Linearizable?"><div class="sect2" id="sec_consistency_lin_definition">
<h2>What Makes a System Linearizable?</h2>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="definition" id="ix_lineardef"></a>
The basic idea behind linearizability is simple: to make a system appear as if there is only a single
copy of the data. However, nailing down precisely what that means actually requires some care. In
order to understand linearizability better, let’s look at some more examples.</p>

<p><a data-type="indexterm" data-primary="register (data structure)" id="idm140417550874416"></a>
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_1">Figure&nbsp;9-2</a> shows three clients concurrently reading and writing the same
key <em>x</em> in a linearizable database. In the distributed systems literature, <em>x</em> is called a
<em>register</em>—in practice, it could be one key in a key-value store, one row in a relational
database, or one document in a document database, for example.</p>

<figure><div id="fig_consistency_linearizability_1" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0902.png" alt="ddia 0902" width="2880" height="829" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0902.png">
<h6><span class="label">Figure 9-2. </span>If a read request is concurrent with a write request, it may return either the old or the new value.</h6>
</div></figure>

<p>For simplicity, <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_1">Figure&nbsp;9-2</a> shows only the requests from the clients’
point of view, not the internals of the database. Each bar is a request made by a client, where the
start of a bar is the time when the request was sent, and the end of a bar is when the response was
received by the client. Due to variable network delays, a client doesn’t know exactly when the
database processed its request—it only knows that it must have happened sometime between the
client sending the request and receiving the response.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550867248-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550867248" class="totri-footnote">i</a></sup></p>

<p>In this example, the register has two types of operations:</p>

<ul>
<li>
<p><em>read</em>(<em>x</em>)&nbsp;⇒&nbsp;<em>v</em> means the client requested to read the value of register
<em>x</em>, and the database returned the value <em>v</em>.</p>
</li>
<li>
<p><em>write</em>(<em>x</em>,&nbsp;<em>v</em>)&nbsp;⇒&nbsp;<em>r</em> means the client requested to set the
register <em>x</em> to value <em>v</em>, and the database returned response <em>r</em> (which could be <em>ok</em> or <em>error</em>).</p>
</li>
</ul>

<p>In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_1">Figure&nbsp;9-2</a>, the value of <em>x</em> is initially 0, and client C performs a
write request to set it to 1. While this is happening, clients A and B are repeatedly polling the
database to read the latest value. What are the possible responses that A and B might get for their
read requests?</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="concurrency" data-secondary="ordering of operations" id="idm140417550852464"></a>
The first read operation by client A completes before the write begins, so it must definitely
return the old value 0.</p>
</li>
<li>
<p>The last read by client A begins after the write has completed, so it must definitely return the
new value 1 if the database is linearizable: we know that the write must have been processed
sometime between the start and end of the write operation, and the read must have been processed
sometime between the start and end of the read operation. If the read started after the write
ended, then the read must have been processed after the write, and therefore it must see the new
value that was written.</p>
</li>
<li>
<p>Any read operations that overlap in time with the write operation might return either 0 or 1,
because we don’t know whether or not the write has taken effect at the time when the read
operation is processed. These operations are <em>concurrent</em> with the write.</p>
</li>
</ul>

<p>However, that is not yet sufficient to fully describe linearizability: if reads that are concurrent
with a write can return either the old or the new value, then readers could see a value flip back
and forth between the old and the new value several times while a write is going on. That is not
what we expect of a system that emulates a “single copy of the
data.”<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550847280-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550847280">ii</a></sup></p>

<p>To make the system linearizable, we need to add another constraint, illustrated in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_2">Figure&nbsp;9-3</a>.</p>

<figure><div id="fig_consistency_linearizability_2" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0903.png" alt="ddia 0903" width="2880" height="829" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0903.png">
<h6><span class="label">Figure 9-3. </span>After any one read has returned the new value, all following reads (on the same or other clients) must also return the new value.</h6>
</div></figure>

<p>In a linearizable system we imagine that there must be some point in time (between the start and end
of the write operation) at which the value of <em>x</em> atomically flips from 0 to 1. Thus, if one
client’s read returns the new value 1, all subsequent reads must also return the new value, even if
the write operation has not yet completed.</p>

<p>This timing dependency is illustrated with an arrow in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_2">Figure&nbsp;9-3</a>.
Client A is the first to read the new value, 1. Just after A’s read returns, B begins a new read.
Since B’s read occurs strictly after A’s read, it must also return 1, even though the write by C is
still ongoing. (It’s the same situation as with Alice and Bob in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>: after Alice has read the new value, Bob also expects to read
the new value.)</p>

<p>We can further refine this timing diagram to visualize each operation taking effect atomically at
some point in time. A more complex example is shown in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2015uh-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uh">10</a>].</p>

<p>In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a> we add a third type of operation besides <em>read</em> and
<em>write</em>:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="atomicity (concurrency)" data-secondary="compare-and-set" id="idm140417550830384"></a>
<a data-type="indexterm" data-primary="compare-and-set operations" id="idm140417550828944"></a>
<em>cas</em>(<em>x</em>,&nbsp;<em>v</em><sub>old</sub>,&nbsp;<em>v</em><sub>new</sub>)&nbsp;⇒&nbsp;<em>r</em> means the client
requested an atomic <em>compare-and-set</em> operation (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_compare_and_set">“Compare-and-set”</a>). If the
current value of the register <em>x</em> equals <em>v</em><sub>old</sub>, it should be atomically set to <em>v</em><sub>new</sub>. If
<em>x</em>&nbsp;≠&nbsp;<em>v</em><sub>old</sub> then the operation should leave the register unchanged and
return an error. <em>r</em> is the database’s response (<em>ok</em> or <em>error</em>).</p>
</li>
</ul>

<p>Each operation in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a> is marked with a vertical line (inside the
bar for each operation) at the time when we think the operation was executed. Those markers are
joined up in a sequential order, and the result must be a valid sequence of reads and writes for a
register (every read must return the value set by the most recent write).</p>

<p>The requirement of linearizability is that the lines joining up the operation markers always move
forward in time (from left to right), never backward. This requirement ensures the recency guarantee we
discussed earlier: once a new value has been written or read, all subsequent reads see the value
that was written, until it is overwritten again.</p>

<figure><div id="fig_consistency_linearizability_3" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0904.png" alt="ddia 0904" width="2880" height="1389" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0904.png">
<h6><span class="label">Figure 9-4. </span>Visualizing the points in time at which the reads and writes appear to have taken effect. The final read by B is not linearizable.</h6>
</div></figure>

<p>There are a few interesting details to point out in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a>:</p>

<ul>
<li>
<p>First client B sent a request to read <em>x</em>, then client D sent a request to set <em>x</em> to 0, and then
client A sent a request to set <em>x</em> to 1. Nevertheless, the value returned to B’s read is 1 (the
value written by A). This is okay: it means that the database first processed D’s write, then A’s
write, and finally B’s read. Although this is not the order in which the requests were sent, it’s
an acceptable order, because the three requests are concurrent. Perhaps B’s read request was
slightly delayed in the network, so it only reached the database after the two writes.</p>
</li>
<li>
<p>Client B’s read returned 1 before client A received its response from the database, saying that
the write of the value 1 was successful. This is also okay: it doesn’t mean the value was read
before it was written, it just means the <em>ok</em> response from the database to client A was slightly
delayed in the network.</p>
</li>
<li>
<p>This model doesn’t assume any transaction isolation: another client may change a value at any
time. For example, C first reads 1 and then reads 2, because the value was changed by B between
the two reads. An atomic compare-and-set (<em>cas</em>) operation can be used to check the value hasn’t
been concurrently changed by another client: B and C’s <em>cas</em> requests succeed, but D’s <em>cas</em>
request fails (by the time the database processes it, the value of <em>x</em> is no longer 0).</p>
</li>
<li>
<p>The final read by client B (in a shaded bar) is not linearizable. The operation is concurrent with
C’s <em>cas</em> write, which updates <em>x</em> from 2 to 4. In the absence of other requests, it would be okay for
B’s read to return 2. However, client A has already read the new value 4 before B’s read started,
so B is not allowed to read an older value than A. Again, it’s the same situation as with Alice
and Bob in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>.</p>
</li>
</ul>

<p>That is the intuition behind linearizability; the formal definition
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1990jq" class="totri-footnote">6</a>] describes it more precisely. It is
possible (though computationally expensive) to test whether a system’s behavior is linearizable by
recording the timings of all requests and responses, and checking whether they can be arranged into
a valid sequential order [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2014tb-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2014tb">11</a>].
<a data-type="indexterm" data-primary="linearizability" data-secondary="definition" data-startref="ix_lineardef" id="idm140417550798912"></a></p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_consistency_serializability">
<h5>Linearizability Versus Serializability</h5>
<p><a data-type="indexterm" data-primary="linearizability" data-secondary="versus serializability" id="idm140417550795904"></a>
<a data-type="indexterm" data-primary="serializability" data-secondary="linearizability versus" id="idm140417550794928"></a>
Linearizability is easily confused with serializability (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_serializability">“Serializability”</a>),
as both words seem to mean something like “can be arranged in a sequential order.” However, they are
two quite different guarantees, and it is important to distinguish between them:</p>
<dl>
<dt>Serializability</dt>
<dd>
<p>Serializability is an isolation property of <em>transactions</em>, where every transaction may read and
write multiple objects (rows, documents, records)—see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_multi_object">“Single-Object and Multi-Object Operations”</a>. It
guarantees that transactions behave the same as if they had executed in <em>some</em> serial order (each
transaction running to completion before the next transaction starts). It is okay for that serial
order to be different from the order in which transactions were actually run
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014wz-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014wz">12</a>].</p>
</dd>
<dt>Linearizability</dt>
<dd>
<p>Linearizability is a recency guarantee on reads and writes of a register (an <em>individual object</em>).
It doesn’t group operations together into transactions, so it does not prevent problems such as
write skew (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_write_skew">“Write Skew and Phantoms”</a>), unless you take additional measures such as
materializing conflicts (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_materializing_conflicts">“Materializing conflicts”</a>).</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="strict serializability" id="idm140417550782416"></a>
<a data-type="indexterm" data-primary="strong one-copy serializability" id="idm140417550781360"></a>
<a data-type="indexterm" data-primary="two-phase locking (2PL)" id="idm140417550780560"></a>
A database may provide both serializability and linearizability, and this combination is known as
<em>strict serializability</em> or <em>strong one-copy serializability</em> (<em>strong-1SR</em>)
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014vc_ch9" class="totri-footnote">4</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bernstein1987va_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bernstein1987va_ch9">13</a>]. Implementations of serializability
based on two-phase locking (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_2pl">“Two-Phase Locking (2PL)”</a>) or actual serial execution (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_serial">“Actual Serial Execution”</a>) are typically linearizable.</p>

<p><a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="serializable snapshot isolation (SSI)" id="idm140417550773184"></a>
However, serializable snapshot isolation (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_ssi">“Serializable Snapshot Isolation (SSI)”</a>) is not linearizable: by
design, it makes reads from a consistent snapshot, to avoid lock contention between readers and
writers. The whole point of a consistent snapshot is that it does not include writes that are more
recent than the snapshot, and thus reads from the snapshot are not linearizable.</p>
</div></aside>
</div></section>













<section data-type="sect2" data-pdf-bookmark="Relying on Linearizability"><div class="sect2" id="sec_consistency_linearizability_usage">
<h2>Relying on Linearizability</h2>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="relying on" id="ix_linearrely"></a>
In what circumstances is linearizability useful? Viewing the final score of a sporting match is
perhaps a frivolous example: a result that is outdated by a few seconds is unlikely to cause any
real harm in this situation. However, there a few areas in which linearizability is an important
requirement for making a system work correctly.</p>










<section data-type="sect3" data-pdf-bookmark="Locking and leader election"><div class="sect3" id="idm140417550767856">
<h3>Locking and leader election</h3>

<p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="locking and leader election" id="idm140417550766672"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="relying on" data-tertiary="locking and leader election" id="idm140417550765488"></a>
<a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" id="idm140417550764096"></a>
A system that uses single-leader replication needs to ensure that there is indeed only one leader,
not several (split brain). One way of electing a leader is to use a lock: every node that starts up
tries to acquire the lock, and the one that succeeds becomes the leader
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Burrows2006wz-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Burrows2006wz">14</a>]. No matter how this
lock is implemented, it must be linearizable: all nodes must agree which node owns the lock;
otherwise it is useless.</p>

<p><a data-type="indexterm" data-primary="coordination" data-secondary="services" id="idm140417550759952"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="locks and leader election" id="idm140417550758432"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="locks and leader election" id="idm140417550757296"></a>
<a data-type="indexterm" data-primary="Curator (ZooKeeper recipes)" id="idm140417550756160"></a><a data-type="indexterm" data-primary="Apache Curator" data-see="Curator" id="idm140417550755440"></a>
Coordination services like Apache ZooKeeper
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2013wi_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>] and etcd
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Etcd-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Etcd">16</a>] are often used to implement
distributed locks and leader election. They use consensus algorithms to implement linearizable
operations in a fault-tolerant way (we discuss such algorithms later in this chapter, in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus_ft">“Fault-Tolerant Consensus”</a>).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550750080-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550750080">iii</a></sup> There are still
<span class="keep-together">many subtle</span> details to implementing locks and leader
election correctly (see for example the fencing issue in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_lock_fencing">“The leader and the lock”</a>), and
libraries like Apache Curator
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ApacheCurator-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ApacheCurator">17</a>]
help by providing higher-level recipes on top of ZooKeeper. However, a linearizable storage service
is the basic foundation for these coordination tasks.</p>

<p><a data-type="indexterm" data-primary="Oracle (database)" data-secondary="Real Application Clusters (RAC)" id="idm140417550741504"></a>
Distributed locking is also used at a much more granular level in some distributed databases, such as
Oracle Real Application Clusters (RAC) [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Vallath2006ut-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Vallath2006ut">18</a>]. RAC uses a lock per disk page, with multiple nodes sharing access
to the same disk storage system. Since these linearizable locks are on the critical path of
transaction execution, RAC deployments usually have a dedicated cluster interconnect network for
communication between database nodes.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Constraints and uniqueness guarantees"><div class="sect3" id="sec_consistency_uniqueness">
<h3>Constraints and uniqueness guarantees</h3>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="relying on" data-tertiary="constraints and uniqueness" id="idm140417550736576"></a>
<a data-type="indexterm" data-primary="uniqueness constraints" data-secondary="requiring linearizability" id="idm140417550735136"></a>
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="requiring linearizability" id="idm140417550734016"></a>
Uniqueness constraints are common in databases: for example, a username or email address must
uniquely identify one user, and in a file storage service there cannot be two files with the same
path and filename. If you want to enforce this constraint as the data is written (such that if two people
try to concurrently create a user or a file with the same name, one of them will be returned an
error), you need linearizability.</p>

<p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="implementing uniqueness constraints" id="idm140417550732192"></a>
This situation is actually similar to a lock: when a user registers for your service, you can think
of them acquiring a “lock” on their chosen username. The operation is also very similar to an atomic
compare-and-set, setting the username to the ID of the user who claimed it, provided that the
username is not already taken.</p>

<p>Similar issues arise if you want to ensure that a bank account balance never goes negative, or that
you don’t sell more items than you have in stock in the warehouse, or that two people don’t
concurrently book the same seat on a flight or in a theater. These constraints all require there to
be a single up-to-date value (the account balance, the stock level, the seat occupancy) that all
nodes agree on.</p>

<p>In real applications, it is sometimes acceptable to treat such constraints loosely (for example, if
a flight is overbooked, you can move customers to a different flight and offer them compensation for
the inconvenience). In such cases, linearizability may not be needed, and we will discuss such
loosely interpreted constraints in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_integrity">“Timeliness and Integrity”</a>.</p>

<p>However, a hard uniqueness constraint, such as the one you typically find in relational databases,
requires linearizability. Other kinds of constraints, such as foreign key or attribute constraints,
can be implemented without requiring linearizability
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014th_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014th_ch9">19</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Cross-channel timing dependencies"><div class="sect3" id="sec_consistency_multi_channel">
<h3>Cross-channel timing dependencies</h3>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="relying on" data-tertiary="cross-channel timing dependencies" id="idm140417550723520"></a>
<a data-type="indexterm" data-primary="time" data-secondary="cross-channel timing dependencies" id="idm140417550722080"></a>
<a data-type="indexterm" data-primary="staleness (old data)" data-secondary="cross-channel timing dependencies" id="idm140417550720960"></a>
Notice a detail in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>: if Alice hadn’t exclaimed the score, Bob
wouldn’t have known that the result of his query was stale. He would have just refreshed the page
again a few seconds later, and eventually seen the final score. The linearizability violation was
only noticed because there was an additional communication channel in the system (Alice’s voice to
Bob’s ears).</p>

<p><a data-type="indexterm" data-primary="race conditions" data-secondary="avoiding with linearizability" id="idm140417550718368"></a>
Similar situations can arise in computer systems. For example, say you have a website where users
can upload a photo, and a background process resizes the photos to lower resolution for faster
download (thumbnails). The architecture and dataflow of this system is illustrated in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_thumbnailer">Figure&nbsp;9-5</a>.</p>

<p>The image resizer needs to be explicitly instructed to perform a resizing job, and this instruction
is sent from the web server to the resizer via a message queue (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>). The web server
doesn’t place the entire photo on the queue, since most message brokers are designed for small
messages, and a photo may be several megabytes in size. Instead, the photo is first written to a
file storage service, and once the write is complete, the instruction to the resizer is placed on
the queue.</p>

<figure><div id="fig_consistency_thumbnailer" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0905.png" alt="ddia 0905" width="2880" height="922" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0905.png">
<h6><span class="label">Figure 9-5. </span>The web server and image resizer communicate both through file storage and a message queue, opening the potential for race conditions.</h6>
</div></figure>

<p>If the file storage service is linearizable, then this system should work fine. If it is not
linearizable, there is the risk of a race condition: the message queue (steps 3 and 4 in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_thumbnailer">Figure&nbsp;9-5</a>) might be faster than the internal replication inside the storage
service. In this case, when the resizer fetches the image (step 5), it might see an old version of
the image, or nothing at all. If it processes an old version of the image, the full-size and resized
images in the file storage become permanently inconsistent.</p>

<p>This problem arises because there are two different communication channels between the web server
and the resizer: the file storage and the message queue. Without the recency guarantee of
linearizability, race conditions between these two channels are possible. This situation is analogous to
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>, where there was also a race condition between two
communication channels: the database replication and the real-life audio channel between Alice’s
mouth and Bob’s ears.</p>

<p>Linearizability is not the only way of avoiding this race condition, but it’s the simplest to
understand. If you control the additional communication channel (like in the case of the message
queue, but not in the case of Alice and Bob), you can use alternative approaches similar to what we
discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_ryw">“Reading Your Own Writes”</a>, at the cost of additional complexity.
<a data-type="indexterm" data-primary="linearizability" data-secondary="relying on" data-startref="ix_linearrely" id="idm140417550707200"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Implementing Linearizable Systems"><div class="sect2" id="sec_consistency_implementing_linearizable">
<h2>Implementing Linearizable Systems</h2>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="of different replication methods" id="ix_linearimpl"></a>
Now that we’ve looked at a few examples in which linearizability is useful, let’s think about how we
might implement a system that offers linearizable semantics.</p>

<p>Since linearizability essentially means “behave as though there is only a single copy of the data,
and all operations on it are atomic,” the simplest answer would be to really only use a single copy
of the data. However, that approach would not be able to tolerate faults: if the node holding that
one copy failed, the data would be lost, or at least inaccessible until the node was brought up
again.</p>

<p>The most common approach to making a system fault-tolerant is to use replication. Let’s revisit the
replication methods from <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>, and compare whether they can be made linearizable:</p>
<dl>
<dt>Single-leader replication (potentially linearizable)</dt>
<dd>
<p><a data-type="indexterm" data-primary="leader-based replication" data-secondary="linearizability of operations" id="idm140417550698848"></a>
<a data-type="indexterm" data-primary="single-leader replication" data-see="leader-based replication" id="idm140417550697584"></a>
In a system with single-leader replication (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_leader">“Leaders and Followers”</a>), the leader has the
primary copy of the data that is used for writes, and the followers maintain backup copies of the
data on other nodes. If you make reads from the leader, or from synchronously updated followers,
they have the <em>potential</em> to be
linearizable.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550694960-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550694960">iv</a></sup>
However, not every single-leader database is actually linearizable, either by design (e.g.,
because it uses snapshot isolation) or due to concurrency bugs
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uh">10</a>].</p>

<p>Using the leader for reads relies on the assumption that you know for sure who the leader is. As
discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_majority">“The Truth Is Defined by the Majority”</a>, it is quite possible for a node to think that it is the
leader, when in fact it is not—and if the delusional leader continues to serve requests, it is
likely to violate linearizability
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2014vc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2014vc">20</a>]. With asynchronous replication, failover may
even lose committed writes (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>), which violates both durability and
linearizability.</p>
</dd>
<dt>Consensus algorithms (linearizable)</dt>
<dd>
<p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="linearizable operations" id="idm140417550686272"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="linearizable operations" id="idm140417550685104"></a>
<a data-type="indexterm" data-primary="split brain" data-secondary="preventing" id="idm140417550683984"></a>
Some consensus algorithms, which we will discuss later in this chapter, bear a resemblance to
single-leader replication. However, consensus protocols contain measures to prevent split brain
and stale replicas. Thanks to these details, consensus algorithms can implement linearizable
storage safely. This is how ZooKeeper
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Junqueira2011jc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2011jc">21</a>]
and etcd
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ongaro2014wq-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ongaro2014wq">22</a>] work, for example.</p>
</dd>
<dt>Multi-leader replication (not linearizable)</dt>
<dd>
<p><a data-type="indexterm" data-primary="multi-leader replication" data-secondary="linearizability, lack of" id="idm140417550676064"></a>
Systems with multi-leader replication are generally not linearizable, because they concurrently
process writes on multiple nodes and asynchronously replicate them to other nodes. For this
reason, they can produce conflicting writes that require resolution (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_write_conflicts">“Handling Write Conflicts”</a>). Such conflicts are an artifact of the lack of a single copy
of the data.</p>
</dd>
<dt>Leaderless replication (probably not linearizable)</dt>
<dd>
<p>For systems with leaderless replication (Dynamo-style; see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_leaderless">“Leaderless Replication”</a>), people
sometimes claim that you can obtain “strong consistency” by requiring quorum reads and writes
(<em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>). Depending on the exact configuration of the quorums, and
depending on how you define strong consistency, this is not quite true.</p>

<p><a data-type="indexterm" data-primary="last write wins (LWW)" id="idm140417550669712"></a>
<a data-type="indexterm" data-primary="clocks" data-secondary="skew" id="idm140417550668656"></a>
<a data-type="indexterm" data-primary="skew" data-secondary="clock skew" id="idm140417550667552"></a>
<a data-type="indexterm" data-primary="sloppy quorums" data-secondary="lack of linearizability" id="idm140417550666448"></a>
“Last write wins” conflict resolution methods based on time-of-day clocks (e.g., in Cassandra; see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_relying">“Relying on Synchronized Clocks”</a>) are almost certainly nonlinearizable, because clock timestamps
cannot be guaranteed to be consistent with actual event ordering due to clock skew. Sloppy quorums
(<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_sloppy_quorum">“Sloppy Quorums and Hinted Handoff”</a>) also ruin any chance of linearizability. Even with strict
quorums, nonlinearizable behavior is possible, as demonstrated in the next section.</p>
</dd>
</dl>










<section data-type="sect3" data-pdf-bookmark="Linearizability and quorums"><div class="sect3" id="sec_consistency_quorum_linearizable">
<h3>Linearizability and quorums</h3>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="using quorums" id="idm140417550661648"></a>
<a data-type="indexterm" data-primary="leaderless replication" data-secondary="quorums" data-tertiary="consistency limitations" id="idm140417550660416"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="of different replication methods" data-tertiary="using quorums" id="idm140417550659200"></a>
<a data-type="indexterm" data-primary="quorums" data-secondary="limitations of consistency" id="idm140417550657792"></a>
<a data-type="indexterm" data-primary="replication" data-secondary="leaderless" data-tertiary="limitations of quorum consistency" id="idm140417550656592"></a>
Intuitively, it seems as though strict quorum reads and writes should be linearizable in a
Dynamo-style model. However, when we have variable network delays, it is possible to have race
conditions, as demonstrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_leaderless">Figure&nbsp;9-6</a>.</p>

<figure><div id="fig_consistency_leaderless" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0906.png" alt="ddia 0906" width="2880" height="1602" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0906.png">
<h6><span class="label">Figure 9-6. </span>A nonlinearizable execution, despite using a strict quorum.</h6>
</div></figure>

<p>In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_leaderless">Figure&nbsp;9-6</a>, the initial value of <em>x</em> is 0, and a writer client is updating
<em>x</em> to 1 by sending the write to all three replicas (<em>n</em>&nbsp;=&nbsp;3, <em>w</em>&nbsp;=&nbsp;3).
Concurrently, client A reads from a quorum of two nodes (<em>r</em>&nbsp;=&nbsp;2) and sees the new value 1
on one of the nodes. Also concurrently with the write, client B reads from a different quorum of two
nodes, and gets back the old value 0 from both.</p>

<p>The quorum condition is met (<em>w</em>&nbsp;+&nbsp;<em>r</em> &gt; <em>n</em>), but this execution is nevertheless not
linearizable: B’s request begins after A’s request completes, but B returns the old value while A
returns the new value. (It’s once again the Alice and Bob situation from
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>.)</p>

<p><a data-type="indexterm" data-primary="Cassandra (database)" data-secondary="linearizability, lack of" id="idm140417550645216"></a>
<a data-type="indexterm" data-primary="Riak (database)" data-secondary="linearizability, lack of" id="idm140417550643760"></a>
<a data-type="indexterm" data-primary="read repair (leaderless replication)" data-secondary="for linearizability" id="idm140417550642640"></a>
Interestingly, it <em>is</em> possible to make Dynamo-style quorums linearizable at the cost of reduced
performance: a reader must perform read repair (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_read_repair">“Read repair and anti-entropy”</a>) synchronously,
before returning results to the application
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Attiya1995bm-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1995bm">23</a>],
and a writer must read the latest state of a quorum of nodes before sending its writes
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lynch1997gr-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lynch1997gr">24</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cachin2011wt-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>].
However, Riak does not perform synchronous read repair due to the performance penalty
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Elliott2015zg-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Elliott2015zg">26</a>].
Cassandra <em>does</em> wait for read repair to complete on quorum reads
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ekstrom2012ix-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ekstrom2012ix">27</a>],
but it loses linearizability if there are multiple concurrent writes to the same key, due to its use
of last-write-wins conflict resolution.</p>

<p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="relation to consensus" id="idm140417550626224"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to compare-and-set" id="idm140417550625120"></a>
Moreover, only linearizable read and write operations can be implemented in this way; a
linearizable compare-and-set operation cannot, because it requires a consensus algorithm
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Herlihy1991gk-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1991gk">28</a>].</p>

<p>In summary, it is safest to assume that a leaderless system with Dynamo-style replication does not
provide linearizability.
<a data-type="indexterm" data-primary="linearizability" data-secondary="of different replication methods" data-startref="ix_linearimpl" id="idm140417550620448"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="The Cost of Linearizability"><div class="sect2" id="sec_linearizability_cost">
<h2>The Cost of Linearizability</h2>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="cost of" id="ix_linearcost"></a>
As some replication methods can provide linearizability and others cannot, it is interesting to
explore the pros and cons of linearizability in more depth.</p>

<p><a data-type="indexterm" data-primary="replication" data-secondary="multi-leader" data-tertiary="across multiple datacenters" id="idm140417550615488"></a>
<a data-type="indexterm" data-primary="multi-leader replication" data-secondary="use cases" data-tertiary="multi-datacenter replication" id="idm140417550613920"></a>
<a data-type="indexterm" data-primary="datacenters" data-secondary="replication across multiple" data-tertiary="multi-leader replication" id="idm140417550612512"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="multi-datacenter" id="idm140417550611104"></a>
We already discussed some use cases for different replication methods in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>; for
example, we saw that multi-leader replication is often a good choice for multi-datacenter
replication (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_multi_dc">“Multi-datacenter operation”</a>). An example of such a deployment is illustrated in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_cap_availability">Figure&nbsp;9-7</a>.</p>

<figure><div id="fig_consistency_cap_availability" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0907.png" alt="ddia 0907" width="2880" height="1294" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0907.png">
<h6><span class="label">Figure 9-7. </span>A network interruption forcing a choice between linearizability and availability.</h6>
</div></figure>

<p>Consider what happens if there is a network interruption between the two datacenters. Let’s assume
that the network within each datacenter is working, and clients can reach the datacenters, but the
datacenters cannot connect to each other.</p>

<p>With a multi-leader database, each datacenter can continue operating normally: since writes from one
datacenter are asynchronously replicated to the other, the writes are simply queued up and exchanged
when network connectivity is restored.</p>

<p>On the other hand, if single-leader replication is used, then the leader must be in one of the
datacenters. Any writes and any linearizable reads must be sent to the leader—thus, for any
clients connected to a follower datacenter, those read and write requests must be sent synchronously
over the network to the leader datacenter.</p>

<p>If the network between datacenters is interrupted in a single-leader setup, clients connected to
follower datacenters cannot contact the leader, so they cannot make any writes to the database, nor
any linearizable reads. They can still make reads from the follower, but they might be stale
(nonlinearizable). If the application requires linearizable reads and writes, the network
interruption causes the application to become unavailable in the datacenters that cannot contact the
leader.</p>

<p>If clients can connect directly to the leader datacenter, this is not a problem, since the
application continues to work normally there. But clients that can only reach a follower datacenter
will experience an outage until the network link is repaired.</p>










<section data-type="sect3" data-pdf-bookmark="The CAP theorem"><div class="sect3" id="sec_consistency_cap">
<h3>The CAP theorem</h3>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="cost of" data-tertiary="CAP theorem" id="idm140417550600064"></a>
<a data-type="indexterm" data-primary="CAP theorem" id="ix_CAP"></a>
This issue is not just a consequence of single-leader and multi-leader replication: any linearizable
database has this problem, no matter how it is implemented. The issue also isn’t specific to
multi-datacenter deployments, but can occur on any unreliable network, even within one datacenter.
The trade-off is as follows:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550597248-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550597248" class="totri-footnote">v</a></sup></p>

<ul>
<li>
<p>If your application <em>requires</em> linearizability, and some replicas are disconnected from the other
replicas due to a network problem, then some replicas cannot process requests while they are
disconnected: they must either wait until the network problem is fixed, or return an error (either
way, they become <em>unavailable</em>).</p>
</li>
<li>
<p>If your application <em>does not require</em> linearizability, then it can be written in a way that each
replica can process requests independently, even if it is disconnected from other replicas (e.g.,
multi-leader). In this case, the application can remain <em>available</em> in the face of a network
problem, but its behavior is not linearizable.</p>
</li>
</ul>

<p>Thus, applications that don’t require linearizability can be more tolerant of network problems. This
insight is popularly known as the <em>CAP theorem</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fox1999bs-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fox1999bs">29</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gilbert2002il-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2002il">30</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gilbert2012bf-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2012bf">31</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brewer2012ba-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Brewer2012ba">32</a>],
named by Eric Brewer in 2000, although the trade-off has been known to designers of
distributed databases since the 1970s
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Davidson1985hv-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Davidson1985hv">33</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Johnson1975we-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Johnson1975we">34</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lindsay1979wv_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lindsay1979wv_ch9">35</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fischer1982hc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fischer1982hc">36</a>].</p>

<p>CAP was originally proposed as a rule of thumb, without precise definitions, with the goal of
starting a discussion about trade-offs in databases. At the time, many distributed databases
focused on providing linearizable semantics on a cluster of machines with shared storage
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Vallath2006ut">18</a>], and CAP encouraged database engineers
to explore a wider design space of distributed shared-nothing systems, which were more suitable for
implementing large-scale web services [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brewer2012tr-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Brewer2012tr">37</a>].
CAP deserves credit for this culture shift—witness the explosion of new database technologies
since the mid-2000s (known as NoSQL).</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_consistency_cap_theorem">
<h5>The Unhelpful CAP Theorem</h5>
<p>CAP is sometimes presented as <em>Consistency, Availability, Partition tolerance: pick 2 out of 3</em>.
Unfortunately, putting it this way is misleading
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Brewer2012ba">32</a>] because network partitions are a kind of
fault, so they aren’t something about which you have a choice: they will happen whether you like it or not
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Robinson2010tp-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Robinson2010tp">38</a>].</p>

<p>At times when the network is working correctly, a system can provide both consistency
(linearizability) and total availability. When a network fault occurs, you have to choose between
either linearizability or total availability. Thus, a better way of phrasing CAP would be
<em>either Consistent or Available when Partitioned</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cockcroft2014wv-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cockcroft2014wv">39</a>]. A more reliable network needs to
make this choice less often, but at some point the choice is inevitable.</p>

<p>In discussions of CAP there are several contradictory definitions of the term <em>availability</em>, and
the formalization as a theorem [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2002il">30</a>] does not
match its usual meaning [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015vp">40</a>]. Many
so-called “highly available” (fault-tolerant) systems actually do not meet CAP’s idiosyncratic
definition of availability. All in all, there is a lot of misunderstanding and confusion around CAP,
and it does not help us understand systems better, so CAP is best avoided.</p>
</div></aside>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="in CAP theorem" id="idm140417550551984"></a>
<a data-type="indexterm" data-primary="availability" data-secondary="in CAP theorem" id="idm140417550550656"></a>
<a data-type="indexterm" data-primary="networks" data-secondary="network partitions" id="idm140417550549552"></a>
The CAP theorem as formally defined [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2002il">30</a>] is
of very narrow scope: it only considers one consistency model (namely linearizability) and one kind
of fault (<em>network partitions</em>,<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550547136-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550547136">vi</a></sup> or nodes that
are alive but disconnected from each other). It doesn’t say anything about network delays, dead
nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little
practical value for designing systems [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015un" class="totri-footnote">9</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2015vp-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015vp">40</a>].</p>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="impossibility results" id="idm140417550540288"></a>
There are many more interesting impossibility results in distributed systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lynch1989kj-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lynch1989kj">41</a>],
and CAP has now been superseded by more precise results
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mahajan2011wz" class="totri-footnote">2</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Attiya2015dm-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya2015dm">42</a>],
so it is of mostly historical interest today.
<a data-type="indexterm" data-primary="CAP theorem" data-startref="ix_CAP" id="idm140417550532288"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Linearizability and network delays"><div class="sect3" id="idm140417550531248">
<h3>Linearizability and network delays</h3>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="cost of" data-tertiary="memory on multi-core CPUs" id="idm140417550529920"></a>
<a data-type="indexterm" data-primary="networks" data-secondary="linearizability and network delays" id="idm140417550528304"></a>
<a data-type="indexterm" data-primary="threads (concurrency)" data-secondary="memory barriers" id="idm140417550527184"></a>
<a data-type="indexterm" data-primary="memory barrier (CPU instruction)" id="idm140417550526080"></a>
<a data-type="indexterm" data-primary="fence (CPU instruction)" id="idm140417550525312"></a>
<a data-type="indexterm" data-primary="CPUs" data-secondary="cache coherence and memory barriers" id="idm140417550524576"></a>
Although linearizability is a useful guarantee, surprisingly few systems are actually linearizable
in practice. For example, even RAM on a modern multi-core CPU is not linearizable
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sewell2010fj-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Sewell2010fj">43</a>]:
if a thread running on one CPU core writes to a memory address, and a thread on another CPU core
reads the same address shortly afterward, it is not guaranteed to read the value written by the
first thread (unless a <em>memory barrier</em> or <em>fence</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thompson2011tr-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Thompson2011tr">44</a>]
is used).</p>

<p><a data-type="indexterm" data-primary="caches" data-secondary="in CPUs" id="idm140417550516752"></a>
The reason for this behavior is that every CPU core has its own memory cache and store buffer.
Memory access first goes to the cache by default, and any changes are asynchronously written out to
main memory. Since accessing data in the cache is much faster than going to main memory
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Drepper2007wb_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Drepper2007wb_ch9">45</a>], this feature is essential for
good performance on modern CPUs. However, there are now several copies of the data (one in main
memory, and perhaps several more in various caches), and these copies are asynchronously updated, so
linearizability is lost.</p>

<p><a data-type="indexterm" data-primary="performance" data-secondary="of linearizability" id="idm140417550512560"></a>
Why make this trade-off? It makes no sense to use the CAP theorem to justify the multi-core memory
consistency model: within one computer we usually assume reliable communication, and we don’t expect
one CPU core to be able to continue operating normally if it is disconnected from the rest of the
computer. The reason for dropping linearizability is <em>performance</em>, not fault tolerance.</p>

<p>The same is true of many distributed databases that choose not to provide linearizable guarantees:
they do so primarily to increase performance, not so much for fault tolerance
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Abadi2012hb-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Abadi2012hb">46</a>].
Linearizability is slow—and this is true all the time, not only during a network fault.</p>

<p>Can’t we maybe find a more efficient implementation of linearizable storage? It seems the answer is
no: Attiya and Welch [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Attiya1994gw-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1994gw">47</a>]
prove that if you want linearizability, the response time of read and write requests is at least
proportional to the uncertainty of delays in the network. In a network with highly variable delays,
like most computer networks (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_queueing">“Timeouts and Unbounded Delays”</a>), the response time of linearizable
reads and writes is inevitably going to be high. A faster algorithm for linearizability does not
exist, but weaker consistency models can be much faster, so this trade-off is important for
latency-sensitive systems. In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ch_future">Chapter&nbsp;12</a> we will discuss some approaches for avoiding
linearizability without sacrificing correctness.
<a data-type="indexterm" data-primary="linearizability" data-secondary="cost of" data-startref="ix_linearcost" id="idm140417550501216"></a>
<a data-type="indexterm" data-primary="concurrency" data-secondary="in replicated systems" data-startref="ix_concrepllinear" id="idm140417550499872"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="linearizability" data-startref="ix_consislinear" id="idm140417550498496"></a>
<a data-type="indexterm" data-primary="linearizability" data-startref="ix_linear" id="idm140417550497120"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Ordering Guarantees"><div class="sect1" id="sec_consistency_ordering">
<h1>Ordering Guarantees</h1>

<p><a data-type="indexterm" data-primary="ordering" id="ix_order"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="ordering guarantees" id="ix_consisorder"></a>
We said previously that a linearizable register behaves as if there is only a single copy of the
data, and that every operation appears to take effect atomically at one point in time. This
definition implies that operations are executed in some well-defined order. We illustrated the
ordering in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a> by joining up the operations in the order in which
they seem to have executed.</p>

<p>Ordering has been a recurring theme in this book, which suggests that it might be an important
fundamental idea. Let’s briefly recap some of the other contexts in which we have discussed
ordering:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="conflicts" data-secondary="relation to operation ordering" id="idm140417550489776"></a>
In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a> we saw that the main purpose of the leader in single-leader replication is
to determine the <em>order of writes</em> in the replication log—that is, the order in which followers
apply those writes. If there is no single leader, conflicts can occur due to concurrent operations
(see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_write_conflicts">“Handling Write Conflicts”</a>).</p>
</li>
<li>
<p>Serializability, which we discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>, is about ensuring that transactions
behave as if they were executed in <em>some sequential order</em>. It can be achieved by literally
executing transactions in that serial order, or by allowing concurrent execution while preventing
serialization conflicts (by locking or aborting).</p>
</li>
<li>
<p>The use of timestamps and clocks in distributed systems that we discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>
(see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_relying">“Relying on Synchronized Clocks”</a>) is another attempt to introduce order into a disorderly
world, for example to determine which one of two writes happened later.</p>
</li>
</ul>

<p>It turns out that there are deep connections between ordering, linearizability, and consensus.
Although this notion is a bit more theoretical and abstract than the rest of this book, it is very
helpful for clarifying our understanding of what systems can and cannot do. We will explore this
topic in the next few sections.</p>








<section data-type="sect2" data-pdf-bookmark="Ordering and Causality"><div class="sect2" id="sec_consistency_causality">
<h2>Ordering and Causality</h2>

<p><a data-type="indexterm" data-primary="causality" data-secondary="causal ordering" id="ix_causeorder"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="causal" id="ix_conscausal"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="causal ordering" id="ix_ordercause"></a>
There are several reasons why ordering keeps coming up, and one of the reasons is that it helps
preserve <em>causality</em>. We have already seen several examples over the course of this book where
causality has been important:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="causal ordering" id="idm140417550472432"></a>
In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_consistent_prefix">“Consistent Prefix Reads”</a> (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#fig_replication_consistent_prefix">Figure&nbsp;5-5</a>) we saw an example
where the observer of a conversation saw first the answer to a question, and then the question
being answered. This is confusing because it violates our intuition of cause and effect: if a
question is answered, then clearly the question had to be there first, because the person giving
the answer must have seen the question (assuming they are not psychic and cannot see into the
future). We say that there is a <em>causal dependency</em> between the question and the answer.</p>
</li>
<li>
<p>A similar pattern appeared in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#fig_replication_causality">Figure&nbsp;5-9</a>, where we looked at the replication
between three leaders and noticed that some writes could “overtake” others due to network delays.
From the perspective of one of the replicas it would look as though there was an update to a row
that did not exist. Causality here means that a row must first be created before it can be
updated.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="happens-before relationship" id="idm140417550465952"></a>
In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a> we observed that if you have two operations A and B, there are
three possibilities: either A happened before B, or B happened before A, or A and B are
concurrent. This <em>happened before</em> relationship is another expression of causality: if A happened
before B, that means B might have known about A, or built upon A, or depended on A. If A and B are
concurrent, there is no causal link between them; in other words, we are sure that neither knew
about the other.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="causality" data-secondary="consistent snapshots" id="idm140417550462656"></a>
<a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="causal consistency" id="idm140417550461328"></a>
In the context of snapshot isolation for transactions (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_snapshot_isolation">“Snapshot Isolation and Repeatable Read”</a>),
we said that a transaction reads from a consistent snapshot. But what does “consistent” mean in
this context? It means <em>consistent with causality</em>: if the snapshot contains an answer, it must
also contain the question being answered
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ahamad1995gl-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ahamad1995gl">48</a>].
Observing the entire database at a single point in time makes it consistent with causality: the
effects of all operations that happened causally before that point in time are visible, but no
operations that happened causally afterward can be seen.
<a data-type="indexterm" data-primary="read skew (transaction isolation)" data-secondary="as violation of causality" id="idm140417550455488"></a>
<a data-type="indexterm" data-primary="causality" data-secondary="violations of" id="idm140417550454384"></a>
Read skew (non-repeatable reads, as illustrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#fig_transactions_item_many_preceders">Figure&nbsp;7-6</a>) means
reading data in a state that violates causality.</p>
</li>
<li>
<p>Our examples of write skew between transactions (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_write_skew">“Write Skew and Phantoms”</a>) also
demonstrated causal dependencies: in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#fig_transactions_write_skew">Figure&nbsp;7-8</a>, Alice was allowed to go
off call because the transaction thought that Bob was still on call, and vice versa. In this case,
the action of going off call is causally dependent on the observation of who is currently on call.
Serializable snapshot isolation (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_ssi">“Serializable Snapshot Isolation (SSI)”</a>) detects write skew by tracking the
causal dependencies between transactions.</p>
</li>
<li>
<p>In the example of Alice and Bob watching football (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_0">Figure&nbsp;9-1</a>), the
fact that Bob got a stale result from the server after hearing Alice exclaim the result is a
causality violation: Alice’s exclamation is causally dependent on the announcement of the score,
so Bob should also be able to see the score after hearing Alice. The same pattern appeared again
in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_multi_channel">“Cross-channel timing dependencies”</a> in the guise of an image resizing service.</p>
</li>
</ul>

<p>Causality imposes an ordering on events: cause comes before effect; a message is sent before that
message is received; the question comes before the answer. And, like in real life, one thing leads
to another: one node reads some data and then writes something as a result, another node reads the
thing that was written and writes something else in turn, and so on. These chains of causally
dependent operations define the causal order in the system—i.e., what happened before what.</p>

<p>If a system obeys the ordering imposed by causality, we say that it is <em>causally consistent</em>. For
example, snapshot isolation provides causal consistency: when you read from the database, and you
see some piece of data, then you must also be able to see any data that causally precedes it
(assuming it has not been deleted in the meantime).</p>










<section data-type="sect3" data-pdf-bookmark="The causal order is not a total order"><div class="sect3" id="sec_consistency_causal_not_total">
<h3>The causal order is not a total order</h3>

<p><a data-type="indexterm" data-primary="total order" id="idm140417550441952"></a>
A <em>total order</em> allows any two elements to be compared, so if you have two elements, you can always
say which one is greater and which one is smaller. For example, natural numbers are totally ordered:
if I give you any two numbers, say 5 and 13, you can tell me that 13 is greater than 5.</p>

<p><a data-type="indexterm" data-primary="partial order" id="idm140417550440128"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="causal ordering" data-tertiary="partial order" id="idm140417550439072"></a>
However, mathematical sets are not totally ordered: is {<em>a</em>,&nbsp;<em>b</em>} greater than
{<em>b</em>,&nbsp;<em>c</em>}? Well, you can’t really compare them, because neither is a subset of the other.
We say they are <em>incomparable</em>, and therefore mathematical sets are <em>partially ordered</em>: in some
cases one set is greater than another (if one set contains all the elements of another), but in
other cases they are incomparable.</p>

<p>The difference between a total order and a partial order is reflected in different database
consistency models:</p>
<dl>
<dt>Linearizability</dt>
<dd>
<p>In a linearizable system, we have a <em>total order</em> of operations: if the system behaves as if there
is only a single copy of the data, and every operation is atomic, this means that for any two
operations we can always say which one happened first. This total ordering is illustrated as a
timeline in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a>.</p>
</dd>
<dt>Causality</dt>
<dd>
<p>We said that two operations are concurrent if neither happened before the other (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_happens_before">“The “happens-before” relationship and concurrency”</a>). Put another way, two events are ordered if they are causally
related (one happened before the other), but they are incomparable if they are concurrent. This
means that causality defines a <em>partial order</em>, not a total order: some operations are ordered
with respect to each other, but some are incomparable.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="concurrency" data-secondary="ordering of operations" id="idm140417550427584"></a>
Therefore, according to this definition, there are no concurrent operations in a linearizable
datastore: there must be a single timeline along which all operations are totally ordered. There
might be several requests waiting to be handled, but the datastore ensures that every request is
handled atomically at a single point in time, acting on a single copy of the data, along a single
timeline, without any concurrency.</p>

<p>Concurrency would mean that the timeline branches and merges again—and in this case, operations on
different branches are incomparable (i.e., concurrent). We saw this phenomenon in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>: for example, <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#fig_replication_causal_dependencies">Figure&nbsp;5-14</a> is not a straight-line
total order, but rather a jumble of different operations going on concurrently. The arrows in the
diagram indicate causal dependencies—the partial ordering of operations.</p>

<p><a data-type="indexterm" data-primary="Git (version control system)" id="idm140417550423328"></a>
If you are familiar with distributed version control systems such as Git, their version histories
are very much like the graph of causal dependencies. Often one commit happens after another, in a
straight line, but sometimes you get branches (when several people concurrently work on a project),
and merges are created when those concurrently created commits are combined.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Linearizability is stronger than causal consistency"><div class="sect3" id="idm140417550421968">
<h3>Linearizability is stronger than causal consistency</h3>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="stronger than causal consistency" id="idm140417550420624"></a>
<a data-type="indexterm" data-primary="causality" data-secondary="causal ordering" data-tertiary="linearizability and" id="idm140417550419504"></a>
So what is the relationship between the causal order and linearizability? The answer is that
linearizability <em>implies</em> causality: any system that is linearizable will preserve causality
correctly [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1986cg" class="totri-footnote">7</a>]. In particular, if there are
multiple communication channels in a system (such as the message queue and the file storage service
in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_thumbnailer">Figure&nbsp;9-5</a>), linearizability ensures that causality is automatically
preserved without the system having to do anything special (such as passing around timestamps
between different components).</p>

<p>The fact that linearizability ensures causality is what makes linearizable systems simple to
understand and appealing. However, as discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_linearizability_cost">“The Cost of Linearizability”</a>, making a system
linearizable can harm its performance and availability, especially if the system has significant
network delays (for example, if it’s geographically distributed). For this reason, some distributed
data systems have abandoned linearizability, which allows them to achieve better performance but can
make them difficult to work with.</p>

<p>The good news is that a middle ground is possible. Linearizability is not the only way of preserving
causality—there are other ways too. A system can be causally consistent without incurring the
performance hit of making it linearizable (in particular, the CAP theorem does not apply). In fact,
causal consistency is the strongest possible consistency model that does not slow down due to
network delays, and remains available in the face of network failures
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mahajan2011wz" class="totri-footnote">2</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya2015dm">42</a>].</p>

<p>In many cases, systems that appear to require linearizability in fact only really require causal
consistency, which can be implemented more efficiently. Based on this observation, researchers are
exploring new kinds of databases that preserve causality, with performance and availability
characteristics that are similar to those of eventually consistent systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lloyd2013vf-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lloyd2013vf">49</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zawirski2013wc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Zawirski2013wc">50</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2013wl-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2013wl">51</a>].</p>

<p>As this research is quite recent, not much of it has yet made its way into production systems, and
there are still challenges to be overcome
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ajoux2015wh_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ajoux2015wh_ch9">52</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014tn-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014tn">53</a>].
However, it is a promising direction for future systems.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Capturing causal dependencies"><div class="sect3" id="idm140417550398784">
<h3>Capturing causal dependencies</h3>

<p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="capturing" id="idm140417550397360"></a>
<a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="causal dependencies" id="idm140417550396256"></a>
We won’t go into all the nitty-gritty details of how nonlinearizable systems can maintain causal
consistency here, but just briefly explore some of the key ideas.</p>

<p>In order to maintain causality, you need to know which operation <em>happened before</em> which other
operation. This is a partial order: concurrent operations may be processed in any order, but if one
operation happened before another, then they must be processed in that order on every replica. Thus,
when a replica processes an operation, it must ensure that all causally preceding operations (all
operations that happened before) have already been processed; if some preceding operation is
missing, the later operation must wait until the preceding operation has been processed.</p>

<p>In order to determine causal dependencies, we need some way of describing the “knowledge” of a node
in the system. If a node had already seen the value X when it issued the write Y, then X and Y may
be causally related. The analysis uses the kinds of questions you would expect in a
criminal investigation of fraud charges: did the CEO <em>know</em> about X at the time when they made
decision Y?</p>

<p><a data-type="indexterm" data-primary="version vectors" data-secondary="capturing causal dependencies" id="idm140417550391776"></a>
The techniques for determining which operation happened before which other operation are similar to
what we discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>. That section discussed causality in a
leaderless datastore, where we need to detect concurrent writes to the same key in order to prevent
lost updates. Causal consistency goes further: it needs to track causal dependencies across the
entire database, not just for a single key. Version vectors can be generalized to do this
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Goncalves2015ky-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Goncalves2015ky">54</a>].</p>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in nonlinearizable systems" id="idm140417550386000"></a>
In order to determine the causal ordering, the database needs to know which version of the data was
read by the application. This is why, in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#fig_replication_causality_single">Figure&nbsp;5-13</a>, the version number
from the prior operation is passed back to the database on a write. A similar idea appears in the
conflict detection of SSI, as discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_ssi">“Serializable Snapshot Isolation (SSI)”</a>:
when a transaction wants to commit, the database checks whether the version of the data that it read
is still up to date. To this end, the database keeps track of which data has been read by which
transaction.
<a data-type="indexterm" data-primary="causality" data-secondary="causal ordering" data-startref="ix_causeorder" id="idm140417550382384"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="causal ordering" data-startref="ix_ordercause" id="idm140417550381040"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Sequence Number Ordering"><div class="sect2" id="sec_consistency_timestamps">
<h2>Sequence Number Ordering</h2>

<p><a data-type="indexterm" data-primary="ordering" data-secondary="by sequence numbers" id="ix_orderseq"></a>
<a data-type="indexterm" data-primary="sequence number ordering" id="ix_seqnum"></a>
Although causality is an important theoretical concept, actually keeping track of all causal
dependencies can become impractical. In many applications, clients read lots of data before writing
something, and then it is not clear whether the write is causally dependent on all or only some of
those prior reads. Explicitly tracking all the data that has been read would mean a large overhead.</p>

<p><a data-type="indexterm" data-primary="timestamps" id="idm140417550374768"></a>
<a data-type="indexterm" data-primary="logical clocks" id="idm140417550373712"></a>
However, there is a better way: we can use <em>sequence numbers</em> or <em>timestamps</em> to order events. A
timestamp need not come from a time-of-day clock (or physical clock, which have many problems, as
discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks">“Unreliable Clocks”</a>). It can instead come from a <em>logical clock</em>, which
is an algorithm to generate a sequence of numbers to identify operations, typically using counters
that are incremented for every operation.</p>

<p><a data-type="indexterm" data-primary="total order" data-secondary="sequence numbers or timestamps" id="idm140417550370400"></a>
Such sequence numbers or timestamps are compact (only a few bytes in size), and they provide a
<em>total order</em>: that is, every operation has a unique sequence number, and you can always compare two
sequence numbers to determine which is greater (i.e., which operation happened later).</p>

<p><a data-type="indexterm" data-primary="causality" data-secondary="causal ordering" data-tertiary="total order consistent with" id="idm140417550368320"></a>
<a data-type="indexterm" data-primary="causality" data-secondary="consistency with" id="ix_consistcausal"></a>
In particular, we can create sequence numbers in a total order that is
<em>consistent with causality</em>:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550365168-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550365168">vii</a></sup>
we promise that if operation A causally happened before B, then A occurs before B in the total
order (A has a lower sequence number than B). Concurrent operations may be ordered arbitrarily. Such
a total order captures all the causality information, but also imposes more ordering than strictly
required by causality.</p>

<p>In a database with single-leader replication (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_leader">“Leaders and Followers”</a>), the replication log
defines a total order of write operations that is consistent with causality. The leader can simply
increment a counter for each operation, and thus assign a monotonically increasing sequence number
to each operation in the replication log. If a follower applies the writes in the order they appear
in the replication log, the state of the follower is always causally consistent (even if it is
lagging behind the leader).</p>










<section data-type="sect3" data-pdf-bookmark="Noncausal sequence number generators"><div class="sect3" id="idm140417550361088">
<h3>Noncausal sequence number generators</h3>

<p><a data-type="indexterm" data-primary="sequence number ordering" data-secondary="generators" id="idm140417550359728"></a>
If there is not a single leader (perhaps because you are using a multi-leader or leaderless
database, or because the database is partitioned), it is less clear how to generate sequence numbers
for operations. Various methods are used in practice:</p>

<ul>
<li>
<p>Each node can generate its own independent set of sequence numbers. For example, if you have two
nodes, one node can generate only odd numbers and the other only even numbers. In general, you
could reserve some bits in the binary representation of the sequence number to contain a unique
node identifier, and this would ensure that two different nodes can never generate the same
sequence number.</p>
</li>
<li>
<p>You can attach a timestamp from a time-of-day clock (physical clock) to each
operation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Conery2014ti-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Conery2014ti">55</a>]. Such timestamps are
not sequential, but if they have sufficiently high resolution, they might be sufficient to totally
order operations. This fact is used in the last write wins conflict resolution method (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_lww">“Timestamps for ordering events”</a>).</p>
</li>
<li>
<p>You can preallocate blocks of sequence numbers. For example, node A might claim the
block of sequence numbers from 1 to 1,000, and node B might claim the block from 1,001 to 2,000.
Then each node can independently assign sequence numbers from its block, and allocate a new block
when its supply of sequence numbers begins to run low.</p>
</li>
</ul>

<p>These three options all perform better and are more scalable than pushing all operations through a
single leader that increments a counter. They generate a unique, approximately increasing sequence
number for each operation. However, they all have a problem: the sequence numbers they generate are
<em>not consistent with causality</em>.</p>

<p>The causality problems occur because these sequence number generators do not correctly capture the
ordering of operations across different nodes:</p>

<ul>
<li>
<p>Each node may process a different number of operations per second. Thus, if one node generates
even numbers and the other generates odd numbers, the counter for even numbers may lag behind the
counter for odd numbers, or vice versa. If you have an odd-numbered operation and an even-numbered
operation, you cannot accurately tell which one causally happened first.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="timestamps" data-secondary="ordering events" id="idm140417550347072"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-secondary="use of timestamps" id="idm140417550345968"></a>
Timestamps from physical clocks are subject to clock skew, which can make them inconsistent with
causality. For example, see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_timestamps">Figure&nbsp;8-3</a>, which shows a scenario in which an
operation that happened causally later was actually assigned a lower
timestamp.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550343712-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550343712">viii</a></sup></p>
</li>
<li>
<p>In the case of the block allocator, one operation may be given a sequence number in the range from
1,001 to 2,000, and a causally later operation may be given a number in the range from 1 to 1,000.
Here, again, the sequence number is inconsistent with causality.</p>
</li>
</ul>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Lamport timestamps"><div class="sect3" id="idm140417550340304">
<h3>Lamport timestamps</h3>

<p><a data-type="indexterm" data-primary="causality" data-secondary="causal ordering" data-tertiary="total order consistent with" id="idm140417550338928"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-secondary="Lamport timestamps" id="idm140417550337360"></a>
<a data-type="indexterm" data-primary="Lamport timestamps" id="idm140417550336288"></a>
<a data-type="indexterm" data-primary="timestamps" data-secondary="Lamport" id="idm140417550335456"></a>
Although the three sequence number generators just described are inconsistent with causality, there is
actually a simple method for generating sequence numbers that <em>is</em> consistent with causality. It is
called a <em>Lamport timestamp</em>, proposed in 1978 by Leslie Lamport
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1978jq_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1978jq_ch9">56</a>],
in what is now one of the most-cited papers in the field of distributed systems.</p>

<p>The use of Lamport timestamps is illustrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_lamport_ts">Figure&nbsp;9-8</a>. Each node has a
unique identifier, and each node keeps a counter of the number of operations it has processed. The
Lamport timestamp is then simply a pair of (<em>counter</em>, <em>node ID</em>). Two nodes may sometimes have the
same counter value, but by including the node ID in the timestamp, each timestamp is made unique.</p>

<figure><div id="fig_consistency_lamport_ts" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0908.png" alt="ddia 0908" width="2880" height="1210" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0908.png">
<h6><span class="label">Figure 9-8. </span>Lamport timestamps provide a total ordering consistent with causality.</h6>
</div></figure>

<p>A Lamport timestamp bears no relationship to a physical time-of-day clock, but it provides total
ordering: if you have two timestamps, the one with a greater counter value is the greater timestamp;
if the counter values are the same, the one with the greater node ID is the greater timestamp.</p>

<p>So far this description is essentially the same as the even/odd counters described in the last
section. The key idea about Lamport timestamps, which makes them consistent with causality, is the
following: every node and every client keeps track of the <em>maximum</em> counter value it has seen so
far, and includes that maximum on every request. When a node receives a request or response with a
maximum counter value greater than its own counter value, it immediately increases its own counter
to that maximum.</p>

<p>This is shown in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_lamport_ts">Figure&nbsp;9-8</a>, where client A receives a counter value of 5 from
node 2, and then sends that maximum of 5 to node 1. At that time, node 1’s counter was only 1, but
it was immediately moved forward to 5, so the next operation had an incremented counter value of 6.</p>

<p>As long as the maximum counter value is carried along with every operation, this scheme ensures that
the ordering from the Lamport timestamps is consistent with causality, because every causal
dependency results in an increased timestamp.</p>

<p>Lamport timestamps are sometimes confused with version vectors, which we saw in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_concurrent">“Detecting Concurrent Writes”</a>. Although there are some similarities, they have a different purpose:
version vectors can distinguish whether two operations are concurrent or whether one is causally
dependent on the other, whereas Lamport timestamps always enforce a total ordering. From the total
ordering of Lamport timestamps, you cannot tell whether two operations are concurrent or whether
they are causally dependent. The advantage of Lamport timestamps over version vectors is that they
are more compact.
<a data-type="indexterm" data-primary="causality" data-secondary="consistency with" data-startref="ix_consistcausal" id="idm140417550320048"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Timestamp ordering is not sufficient"><div class="sect3" id="sec_consistency_unique_constraint">
<h3>Timestamp ordering is not sufficient</h3>

<p><a data-type="indexterm" data-primary="sequence number ordering" data-secondary="insufficiency for enforcing constraints" id="idm140417550316960"></a>
<a data-type="indexterm" data-primary="timestamps" data-secondary="insufficiency for enforcing constraints" id="idm140417550315776"></a>
Although Lamport timestamps define a total order of operations that is consistent with causality,
they are not quite sufficient to solve many common problems in distributed systems.</p>

<p>For example, consider a system that needs to ensure that a username uniquely identifies a user
account. If two users concurrently try to create an account with the same username, one of the two
should succeed and the other should fail. (We touched on this problem previously in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_lock_fencing">“The leader and the lock”</a>.)</p>

<p>At first glance, it seems as though a total ordering of operations (e.g., using Lamport timestamps)
should be sufficient to solve this problem: if two accounts with the same username are created, pick
the one with the lower timestamp as the winner (the one who grabbed the username first), and let the
one with the greater timestamp fail. Since timestamps are totally ordered, this comparison is always
valid.</p>

<p>This approach works for determining the winner after the fact: once you have collected all the
username creation operations in the system, you can compare their timestamps. However, it is not
sufficient when a node has just received a request from a user to create a username, and needs to
decide <em>right now</em> whether the request should succeed or fail. At that moment, the node does not
know whether another node is concurrently in the process of creating an account with the same
username, and what timestamp that other node may assign to the operation.</p>

<p>In order to be sure that no other node is in the process of concurrently creating an account with
the same username and a lower timestamp, you would have to check with every other node to see what
it is doing [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1978jq_ch9">56</a>].
If one of the other nodes has failed or cannot be reached due to a network problem, this system
would grind to a halt. This is not the kind of fault-tolerant system that we need.</p>

<p>The problem here is that the total order of operations only emerges after you have collected all of
the operations. If another node has generated some operations, but you don’t yet know what they are,
you cannot construct the final ordering of operations: the unknown operations from the other node
may need to be inserted at various positions in the total order.</p>

<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="relation to event ordering" id="idm140417550307872"></a>
To conclude: in order to implement something like a uniqueness constraint for usernames, it’s not
sufficient to have a total ordering of operations—you also need to know when that order is
finalized. If you have an operation to create a username, and you are sure that no other node can
insert a claim for the same username ahead of your operation in the total order, then you can safely
declare the operation successful.</p>

<p>This idea of knowing when your total order is finalized is captured in the topic of <em>total order
broadcast</em>.
<a data-type="indexterm" data-primary="consistency" data-secondary="causal" data-startref="ix_conscausal" id="idm140417550305408"></a>
<a data-type="indexterm" data-primary="sequence number ordering" data-startref="ix_seqnum" id="idm140417550304000"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="by sequence numbers" data-startref="ix_orderseq" id="idm140417550302880"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Total Order Broadcast"><div class="sect2" id="sec_consistency_total_order">
<h2>Total Order Broadcast</h2>

<p><a data-type="indexterm" data-primary="total order broadcast" id="ix_totalorder"></a>
<a data-type="indexterm" data-primary="atomic broadcast" data-see="total order broadcast" id="idm140417550298960"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="total order broadcast" id="ix_ordertotal"></a>
If your program runs only on a single CPU core, it is easy to define a total ordering of operations:
it is simply the order in which they were executed by the CPU. However, in a distributed system,
getting all nodes to agree on the same total ordering of operations is tricky. In the last section
we discussed ordering by timestamps or sequence numbers, but found that it is not as powerful as
single-leader replication (if you use timestamp ordering to implement a uniqueness constraint, you
cannot tolerate any faults).</p>

<p><a data-type="indexterm" data-primary="failover" data-secondary="leader election" id="idm140417550295680"></a>
As discussed, single-leader replication determines a total order of operations by choosing one node
as the leader and sequencing all operations on a single CPU core on the leader. The challenge then
is how to scale the system if the throughput is greater than a single leader can handle, and also
how to handle failover if the leader fails (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>). In the
distributed systems literature, this problem is known as <em>total order broadcast</em> or <em>atomic
broadcast</em> [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Defago2004ji-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Defago2004ji">57</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Attiya2004ke-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya2004ke">58</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550286352-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550286352">ix</a></sup></p>
<div data-type="note" epub:type="note"><h1>Scope of ordering guarantee</h1>
<p><a data-type="indexterm" data-primary="coordination" data-secondary="cross-partition ordering" id="idm140417550282896"></a>
Partitioned databases with a single leader per partition often maintain ordering only per partition,
which means they cannot offer consistency guarantees (e.g., consistent snapshots, foreign key
references) across partitions. Total ordering across all partitions is possible, but requires
additional coordination [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Balakrishnan2012wm-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2012wm">59</a>].</p>
</div>

<p><a data-type="indexterm" data-primary="messages" data-secondary="using total order broadcast" id="idm140417550278576"></a>
Total order broadcast is usually described as a protocol for exchanging messages between nodes.
Informally, it requires that two safety properties always be satisfied:</p>
<dl>
<dt>Reliable delivery</dt>
<dd>
<p>No messages are lost: if a message is delivered to one node, it is delivered to all nodes.</p>
</dd>
<dt>Totally ordered delivery</dt>
<dd>
<p>Messages are delivered to every node in the same order.</p>
</dd>
</dl>

<p>A correct algorithm for total order broadcast must ensure that the reliability and ordering
properties are always satisfied, even if a node or the network is faulty. Of course, messages will
not be delivered while the network is interrupted, but an algorithm can keep retrying so that the
messages get through when the network is eventually repaired (and then they must still be delivered
in the correct order).</p>










<section data-type="sect3" data-pdf-bookmark="Using total order broadcast"><div class="sect3" id="idm140417550273104">
<h3>Using total order broadcast</h3>

<p><a data-type="indexterm" data-primary="total order broadcast" data-secondary="using" id="idm140417550271712"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="use of Zab algorithm" id="idm140417550270608"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="use of Raft algorithm" id="idm140417550269440"></a>
Consensus services such as ZooKeeper and etcd actually implement total order broadcast. This fact is a
hint that there is a strong connection between total order broadcast and consensus, which we will
explore later in this chapter.</p>

<p><a data-type="indexterm" data-primary="replication" data-secondary="state machine replication" id="idm140417550267808"></a>
<a data-type="indexterm" data-primary="state machine replication" id="idm140417550266512"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to replication" id="idm140417550265664"></a>
Total order broadcast is exactly what you need for database replication: if every message represents
a write to the database, and every replica processes the same writes in the same order, then the
replicas will remain consistent with each other (aside from any temporary replication lag). This
principle is known as <em>state machine replication</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schneider1990vy-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Schneider1990vy">60</a>], and we will return to it in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>

<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="in state machine replication" id="idm140417550260672"></a>
<a data-type="indexterm" data-primary="serializability" data-secondary="serial execution" data-tertiary="using stored procedures" id="idm140417550259504"></a>
<a data-type="indexterm" data-primary="stored procedures" data-secondary="and total order broadcast" id="idm140417550258128"></a>
Similarly, total order broadcast can be used to implement serializable transactions: as discussed in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_serial">“Actual Serial Execution”</a>, if every message represents a deterministic transaction to be executed
as a stored procedure, and if every node processes those messages in the same order, then the
partitions and replicas of the database are kept consistent with each other
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thomson2012tx-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Thomson2012tx">61</a>].</p>

<p>An important aspect of total order broadcast is that the order is fixed at the time the messages are
delivered: a node is not allowed to retroactively insert a message into an earlier position in the
order if subsequent messages have already been delivered. This fact makes total order broadcast
stronger than timestamp ordering.</p>

<p><a data-type="indexterm" data-primary="logs (data structure)" data-secondary="creating using total order broadcast" id="idm140417550252432"></a>
Another way of looking at total order broadcast is that it is a way of creating a <em>log</em> (as in a
replication log, transaction log, or write-ahead log): delivering a message is like appending to the
log. Since all nodes must deliver the same messages in the same order, all nodes can read the log
and see the same sequence of messages.</p>

<p><a data-type="indexterm" data-primary="fencing (preventing split brain)" data-secondary="generating fencing tokens" id="idm140417550250384"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="generating fencing tokens" id="idm140417550249248"></a>
Total order broadcast is also useful for implementing a lock service that provides fencing tokens
(see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_fencing_tokens">“Fencing tokens”</a>). Every request to acquire the lock is appended as a message
to the log, and all messages are sequentially numbered in the order they appear in the log. The
sequence number can then serve as a fencing token, because it is monotonically increasing. In
ZooKeeper, this sequence number is called <code>zxid</code>
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Implementing linearizable storage using total order broadcast"><div class="sect3" id="sec_consistency_abcast_to_lin">
<h3>Implementing linearizable storage using total order broadcast</h3>

<p><a data-type="indexterm" data-primary="total order broadcast" data-secondary="using to implement linearizable storage" id="idm140417550243744"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="implementing with total order broadcast" id="idm140417550242368"></a>
As illustrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_linearizability_3">Figure&nbsp;9-4</a>, in a linearizable system there is a total
order of operations. Does that mean linearizability is the same as total order broadcast? Not quite,
but there are close links between the two.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550240192-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550240192" class="totri-footnote">x</a></sup>
<a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="relation to consensus" id="idm140417550233488"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to compare-and-set" id="idm140417550232432"></a></p>

<p>Total order broadcast is asynchronous: messages are guaranteed to be delivered reliably in a fixed
order, but there is no guarantee about <em>when</em> a message will be delivered (so one recipient may lag
behind the others). By contrast, linearizability is a recency guarantee: a read is guaranteed to see
the latest value written.</p>

<p>However, if you have total order broadcast, you can build linearizable storage on top of it. For
example, you can ensure that usernames uniquely identify user accounts.</p>

<p>Imagine that for every possible username, you can have a linearizable register with an atomic
compare-and-set operation. Every register initially has the value <code>null</code> (indicating that the
username is not taken). When a user wants to create a username, you execute a compare-and-set
operation on the register for that username, setting it to the user account ID, under the condition
that the previous register value is <code>null</code>. If multiple users try to concurrently grab the same
username, only one of the compare-and-set operations will succeed, because the others will see a
value other than <code>null</code> (due to linearizability).</p>

<p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="implementing with total order broadcast" id="idm140417550227024"></a>
You can implement such a linearizable compare-and-set operation as follows by using total order
broadcast as an append-only log
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Balakrishnan2013ko-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2013ko">62</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vanRenesse2004td-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2004td">63</a>]:</p>
<ol>
<li>
<p>Append a message to the log, tentatively indicating the username you want to claim.</p>
</li>
<li>
<p>Read the log, and wait for the message you appended to be delivered back to
you.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550218432-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550218432">xi</a></sup></p>
</li>
<li>
<p>Check for any messages claiming the username that you want. If the first message for your desired
username is your own message, then you are successful: you can commit the username claim (perhaps
by appending another message to the log) and acknowledge it to the client. If the first message
for your desired username is from another user, you abort the operation.</p>
</li>

</ol>

<p><a data-type="indexterm" data-primary="concurrency" data-secondary="reducing, through event logs" id="idm140417550214576"></a>
<a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in log-based systems" id="idm140417550213408"></a>
Because log entries are delivered to all nodes in the same order, if there are several concurrent
writes, all nodes will agree on which one came first. Choosing the first of the conflicting
writes as the winner and aborting later ones ensures that all nodes agree on whether a write was
committed or aborted.  A similar approach can be used to implement serializable multi-object
transactions on top of a log [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2013ko">62</a>].</p>

<p><a data-type="indexterm" data-primary="sequential consistency" id="idm140417550210512"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="sequential" id="idm140417550209456"></a>
While this procedure ensures linearizable writes, it doesn’t guarantee linearizable reads—if you
read from a store that is asynchronously updated from the log, it may be stale. (To be precise, the
procedure described here provides <em>sequential consistency</em>
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1994gw">47</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1979ky-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1979ky">64</a>],
sometimes also known as <em>timeline consistency</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Soztutar2015vj-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Soztutar2015vj">65</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cooper2008fn-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cooper2008fn">66</a>],
a slightly weaker guarantee than linearizability.) To make reads linearizable, there are a few
options:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="quorum reads" id="idm140417550197456"></a>
You can sequence reads through the log by appending a message, reading the log, and performing the
actual read when the message is delivered back to you. The message’s position in the log thus
defines the point in time at which the read happens. (Quorum reads in etcd work somewhat like this
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Etcd">16</a>].)</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="linearizable operations" id="idm140417550194480"></a>
If the log allows you to fetch the position of the latest log message in a linearizable way, you
can query that position, wait for all entries up to that position to be delivered to you, and then
perform the read. (This is the idea behind ZooKeeper’s <code>sync()</code> operation
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>].)</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="chain replication" data-secondary="linearizable reads" id="idm140417550191008"></a>
You can make your read from a replica that is synchronously updated on writes, and is thus sure
to be up to date. (This technique is used in chain replication
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2004td">63</a>]; see also <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sidebar_replication_research">“Research on Replication”</a>.)</p>
</li>
</ul>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Implementing total order broadcast using linearizable storage"><div class="sect3" id="idm140417550244896">
<h3>Implementing total order broadcast using linearizable storage</h3>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="algorithms" data-tertiary="using linearizable operations" id="idm140417550186464"></a>
<a data-type="indexterm" data-primary="total order broadcast" data-secondary="implementing with linearizable storage" id="idm140417550185072"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="using to implement total order broadcast" id="idm140417550183952"></a>
The last section showed how to build a linearizable compare-and-set operation from total order
broadcast. We can also turn it around, assume that we have linearizable storage, and show how to
build total order broadcast from it.</p>

<p><a data-type="indexterm" data-primary="atomicity (concurrency)" data-secondary="atomic increment-and-get" id="idm140417550182320"></a>
The easiest way is to assume you have a linearizable register that stores an integer and that has an
atomic increment-and-get operation [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1991gk">28</a>].
Alternatively, an atomic compare-and-set operation would also do the job.</p>

<p>The algorithm is simple: for every message you want to send through total order broadcast, you
increment-and-get the linearizable integer, and then attach the value you got from the register as a
sequence number to the message. You can then send the message to all nodes (resending any lost
messages), and the recipients will deliver the messages consecutively by sequence number.</p>

<p>Note that unlike Lamport timestamps, the numbers you get from incrementing the linearizable register
form a sequence with no gaps. Thus, if a node has delivered message 4 and receives an incoming
message with a sequence number of 6, it knows that it must wait for message 5 before it can deliver
message 6. The same is not the case with Lamport timestamps—in fact, this is the key difference
between total order broadcast and timestamp ordering.</p>

<p>How hard could it be to make a linearizable integer with an atomic increment-and-get operation? As
usual, if things never failed, it would be easy: you could just keep it in a variable on one node.
The problem lies in handling the situation when network connections to that node are interrupted,
and restoring the value when that node fails
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2012wm">59</a>].
In general, if you think hard enough about linearizable sequence number generators, you inevitably
end up with a consensus algorithm.</p>

<p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="relation to consensus" id="idm140417550176560"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to compare-and-set" id="idm140417550175440"></a>
This is no coincidence: it can be proved that a linearizable compare-and-set (or increment-and-get)
register and total order broadcast are both <em>equivalent to consensus</em>
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1991gk">28</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chandra1996cp-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>].
That is, if you can solve one of these problems, you can transform it into a solution for the
others. This is quite a profound and surprising insight!</p>

<p>It is time to finally tackle the consensus problem head-on, which we will do in the rest of this
chapter.
<a data-type="indexterm" data-primary="total order broadcast" data-startref="ix_totalorder" id="idm140417550169392"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="total order broadcast" data-startref="ix_ordertotal" id="idm140417550168320"></a>
<a data-type="indexterm" data-primary="ordering" data-startref="ix_order" id="idm140417550166944"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="ordering guarantees" data-startref="ix_consisorder" id="idm140417550165840"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Distributed Transactions and Consensus"><div class="sect1" id="sec_consistency_consensus">
<h1>Distributed Transactions and Consensus</h1>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" id="ix_consensusdt"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" id="ix_transactdtprac"></a>
<a data-type="indexterm" data-primary="distributed transactions" data-see="transactions" id="idm140417550160128"></a>
Consensus is one of the most important and fundamental problems in distributed computing. On the
surface, it seems simple: informally, the goal is simply to <em>get several nodes to agree on
something</em>. You might think that this shouldn’t be too hard. Unfortunately, many broken systems have
been built in the mistaken belief that this problem is easy to solve.</p>

<p>Although consensus is very important, the section about it appears late in this book because the
topic is quite subtle, and appreciating the subtleties requires some prerequisite knowledge. Even
in the academic research community, the understanding of consensus only gradually crystallized over
the course of decades, with many misunderstandings along the way. Now that we have discussed
replication (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>), transactions (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>), system models
(<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>), linearizability, and total order broadcast (this chapter), we are
finally ready to tackle the consensus problem.</p>

<p>There are a number of situations in which it is important for nodes to agree. For example:</p>
<dl>
<dt>Leader election</dt>
<dd>
<p><a data-type="indexterm" data-primary="split brain" data-secondary="in consensus algorithms" id="idm140417550152976"></a>
<a data-type="indexterm" data-primary="failover" data-secondary="leader election" id="idm140417550151872"></a>
In a database with single-leader replication, all nodes need to agree on which node is the leader.
The leadership position might become contested if some nodes can’t communicate with others due to
a network fault. In this case, consensus is important to avoid a bad failover, resulting in a
split brain situation in which two nodes both believe themselves to be the leader (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>). If there were two leaders, they would both accept writes and their
data would diverge, leading to inconsistency and data loss.</p>
</dd>
<dt>Atomic commit</dt>
<dd>
<p><a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="atomic commit" id="idm140417550148208"></a>
In a database that supports transactions spanning several nodes or partitions, we have the problem
that a transaction may fail on some nodes but succeed on others. If we want to maintain
transaction atomicity (in the sense of ACID; see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_acid_atomicity">“Atomicity”</a>), we have to get
all nodes to agree on the outcome of the transaction: either they all abort/roll back (if anything
goes wrong) or they all commit (if nothing goes wrong). This instance of consensus is known as the
<em>atomic commit</em> problem.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417550145296-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550145296">xii</a></sup></p>
</dd>
</dl>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_consistency_flp">
<h5>The Impossibility of Consensus</h5>
<p><a data-type="indexterm" data-primary="consensus" data-secondary="impossibility of" id="idm140417550138256"></a>
<a data-type="indexterm" data-primary="FLP result (on consensus)" id="idm140417550137152"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="impossibility results" id="idm140417550136256"></a>
You may have heard about the FLP result
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fischer1985ji-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fischer1985ji">68</a>]—named after the
authors Fischer, Lynch, and Paterson—which proves that there is no algorithm that is always able to
reach consensus if there is a risk that a node may crash. In a distributed system, we must assume
that nodes may crash, so reliable consensus is impossible. Yet, here we are, discussing algorithms
for achieving consensus. What is going on here?</p>

<p>The answer is that the FLP result is proved in the asynchronous system model (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_system_model">“System Model and Reality”</a>), a very restrictive model that assumes a deterministic algorithm
that cannot use any clocks or timeouts. If the algorithm is allowed to use timeouts, or some other
way of identifying suspected crashed nodes (even if the suspicion is sometimes wrong), then
consensus becomes solvable [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>]. Even just
allowing the algorithm to use random numbers is sufficient to get around the impossibility result
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="BenOr1983dh-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#BenOr1983dh">69</a>].</p>

<p>Thus, although the FLP result about the impossibility of consensus is of great theoretical
importance, distributed systems can usually achieve consensus in practice.</p>
</div></aside>

<p><a data-type="indexterm" data-primary="two-phase commit (2PC)" id="idm140417550126336"></a>
In this section we will first examine the atomic commit problem in more detail. In particular, we
will discuss the <em>two-phase commit</em> (2PC) algorithm, which is the most common way of solving atomic
commit and which is implemented in various databases, messaging systems, and application servers. It
turns out that 2PC is a kind of consensus algorithm—but not a very good one
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray2006cu-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray2006cu">70</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Guerraoui1995bi-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Guerraoui1995bi">71</a>].</p>

<p><a data-type="indexterm" data-primary="Zab (consensus algorithm)" data-secondary="use in ZooKeeper" id="idm140417550118656"></a>
<a data-type="indexterm" data-primary="Raft (consensus algorithm)" data-secondary="use in etcd" id="idm140417550117536"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="use of Zab algorithm" id="idm140417550116416"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="use of Raft algorithm" id="idm140417550115296"></a>
By learning from 2PC we will then work our way toward better consensus algorithms, such as those
used in ZooKeeper (Zab) and etcd (Raft).</p>








<section data-type="sect2" data-pdf-bookmark="Atomic Commit and Two-Phase Commit (2PC)"><div class="sect2" id="sec_consistency_2pc">
<h2>Atomic Commit and Two-Phase Commit (2PC)</h2>

<p><a data-type="indexterm" data-primary="commits (transactions)" data-secondary="atomic commit" data-seealso="atomicity; transactions" id="ix_commitatomic"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="two-phase commit" id="ix_consensusdtatom"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="transaction atomicity" id="ix_faulttolatomic"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="two-phase commit" id="ix_transactdtcommit"></a>
In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a> we learned that the purpose of transaction atomicity is to provide simple
semantics in the case where something goes wrong in the middle of making several writes. The outcome
of a transaction is either a successful <em>commit</em>, in which case all of the transaction’s writes are
made durable, or an <em>abort</em>, in which case all of the transaction’s writes are rolled back (i.e.,
undone or discarded).</p>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="of secondary indexes" id="idm140417550103600"></a>
Atomicity prevents failed transactions from littering the database with half-finished results and
half-updated state. This is especially important for multi-object transactions (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_multi_object">“Single-Object and Multi-Object Operations”</a>) and databases that maintain secondary indexes. Each secondary
index is a separate data structure from the primary data—thus, if you modify some data, the
corresponding change needs to also be made in the secondary index. Atomicity ensures that the
secondary index stays consistent with the primary data (if the index became inconsistent with the
primary data, it would not be very useful).</p>










<section data-type="sect3" data-pdf-bookmark="From single-node to distributed atomic commit"><div class="sect3" id="idm140417550100960">
<h3>From single-node to distributed atomic commit</h3>

<p>For transactions that execute at a single database node, atomicity is commonly implemented by the
storage engine. When the client asks the database node to commit the transaction, the database makes
the transaction’s writes durable (typically in a write-ahead log; see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_btree_wal">“Making B-trees reliable”</a>) and
then appends a commit record to the log on disk. If the database crashes in the middle of this
process, the transaction is recovered from the log when the node restarts: if the commit record was
successfully written to disk before the crash, the transaction is considered committed; if not, any
writes from that transaction are rolled back.</p>

<p>Thus, on a single node, transaction commitment crucially depends on the <em>order</em> in which data is
durably written to disk: first the data, then the commit record
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pillai2014vx_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Pillai2014vx_ch9">72</a>].
The key deciding moment for whether the transaction commits or aborts is the moment at which the
disk finishes writing the commit record: before that moment, it is still possible to abort (due to a
crash), but after that moment, the transaction is committed (even if the database crashes). Thus, it
is a single device (the controller of one particular disk drive, attached to one particular node)
that makes the commit atomic.</p>

<p>However, what if multiple nodes are involved in a transaction? For example, perhaps you have a
multi-object transaction in a partitioned database, or a term-partitioned secondary index (in which
the index entry may be on a different node from the primary data; see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_secondary_indexes">“Partitioning and Secondary Indexes”</a>). Most “NoSQL” distributed datastores do not support such
distributed transactions, but various clustered relational systems do (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_dist_trans">“Distributed Transactions in Practice”</a>).</p>

<p>In these cases, it is not sufficient to simply send a commit request to all of the nodes and
independently commit the transaction on each one. In doing so, it could easily happen that the
commit succeeds on some nodes and fails on other nodes, which would violate the atomicity guarantee:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="in two-phase commit" id="idm140417550089872"></a>
Some nodes may detect a constraint violation or conflict, making an abort necessary, while other
nodes are successfully able to commit.</p>
</li>
<li>
<p>Some of the commit requests might be lost in the network, eventually aborting due to a timeout,
while other commit requests get through.</p>
</li>
<li>
<p>Some nodes may crash before the commit record is fully written and roll back on recovery, while
others successfully commit.</p>
</li>
</ul>

<p>If some nodes commit the transaction but others abort it, the nodes become inconsistent with each
other (like in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#fig_transactions_atomicity">Figure&nbsp;7-3</a>). And once a transaction has been committed on one
node, it cannot be retracted again if it later turns out that it was aborted on another node. For
this reason, a node must only commit once it is certain that all other nodes in the transaction are
also going to commit.</p>

<p>A transaction commit must be irrevocable—you are not allowed to change your mind and retroactively
abort a transaction after it has been committed. The reason for this rule is that once data has been
committed, it becomes visible to other transactions, and thus other clients may start relying on
that data; this principle forms the basis of <em>read committed</em> isolation, discussed in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_read_committed">“Read Committed”</a>. If a transaction was allowed to abort after committing, any
transactions that read the committed data would be based on data that was retroactively declared not
to have existed—so they would have to be reverted as well.</p>

<p><a data-type="indexterm" data-primary="compensating transactions" id="idm140417550082112"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="of compensating transactions" id="idm140417550081088"></a>
(It is possible for the effects of a committed transaction to later be undone by another,
<em>compensating transaction</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1981wi_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray1981wi_ch9">73</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GarciaMolina1987ca_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#GarciaMolina1987ca_ch9">74</a>].
However, from the database’s point of view this is a separate transaction, and thus any
cross-transaction correctness requirements are the application’s problem.)
<a data-type="indexterm" data-primary="commits (transactions)" data-secondary="atomic commit" data-startref="ix_commitatomic" id="idm140417550074176"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Introduction to two-phase commit"><div class="sect3" id="idm140417550072960">
<h3>Introduction to two-phase commit</h3>

<p><a data-type="indexterm" data-primary="commits (transactions)" data-secondary="two-phase commit (2PC)" id="ix_commit2pc"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" id="ix_twopc"></a>
<a data-type="indexterm" data-primary="XA transactions" id="idm140417550068864"></a>
<a data-type="indexterm" data-primary="Java Transaction API (JTA)" id="idm140417550068032"></a>
<a data-type="indexterm" data-primary="WS-AtomicTransaction (2PC)" id="idm140417550067184"></a>
Two-phase commit is an algorithm for achieving atomic transaction commit across multiple
nodes—i.e., to ensure that either all nodes commit or all nodes abort. It is a classic algorithm
in distributed databases [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bernstein1987va_ch9">13</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lindsay1979wv_ch9">35</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mohan1986hh-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mohan1986hh">75</a>]. 2PC is used
internally in some databases and also made available to applications in the form of <em>XA transactions</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="XASpec1991vk-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#XASpec1991vk">76</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Spille2004vr-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004vr">77</a>] (which are supported by the Java Transaction API, for example)
or via WS-AtomicTransaction for SOAP web services
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Neto2008be-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Neto2008be">78</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Johnson2004hl-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Johnson2004hl">79</a>].</p>

<p>The basic flow of 2PC is illustrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_two_phase_commit">Figure&nbsp;9-9</a>. Instead of a single
commit request, as with a single-node transaction, the commit/abort process in 2PC is split into two
phases (hence the name).</p>

<figure><div id="fig_consistency_two_phase_commit" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0909.png" alt="ddia 0909" width="2880" height="969" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0909.png">
<h6><span class="label">Figure 9-9. </span>A successful execution of two-phase commit (2PC).</h6>
</div></figure>
<div data-type="note" epub:type="note"><h1>Don’t confuse 2PC and 2PL</h1>
<p><a data-type="indexterm" data-primary="two-phase locking (2PL)" data-secondary="confusion with two-phase commit" id="idm140417550046608"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="confusion with two-phase locking" id="idm140417550045440"></a>
Two-phase <em>commit</em> (2PC) and two-phase <em>locking</em> (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_2pl">“Two-Phase Locking (2PL)”</a>) are two very
different things. 2PC provides atomic commit in a distributed database, whereas 2PL provides
serializable isolation. To avoid confusion, it’s best to think of them as entirely separate concepts
and to ignore the unfortunate similarity in the names.</p>
</div>

<p><a data-type="indexterm" data-primary="coordinator (in 2PC)" id="idm140417550041824"></a>
<a data-type="indexterm" data-primary="transaction coordinator" data-see="coordinator" id="idm140417550040768"></a>
<a data-type="indexterm" data-primary="transaction manager" data-see="coordinator" id="idm140417550039664"></a>
<a data-type="indexterm" data-primary="Narayana (transaction coordinator)" id="idm140417550038560"></a>
<a data-type="indexterm" data-primary="JOTM (transaction coordinator)" id="idm140417550037760"></a>
<a data-type="indexterm" data-primary="BTM (transaction coordinator)" id="idm140417550036912"></a>
<a data-type="indexterm" data-primary="Microsoft" data-secondary="MSDTC (transaction coordinator)" id="idm140417550036064"></a>
<a data-type="indexterm" data-primary="Java Enterprise Edition (EE)" id="idm140417550034944"></a>
2PC uses a new component that does not normally appear in single-node transactions: a
<em>coordinator</em> (also known as <em>transaction manager</em>). The coordinator is often implemented as a
library within the same application process that is requesting the transaction (e.g., embedded in a
Java EE container), but it can also be a separate process or service. Examples of such coordinators
include Narayana, JOTM, BTM, or MSDTC.</p>

<p>A 2PC transaction begins with the application reading and writing data on multiple database nodes,
as normal. We call these database nodes <em>participants</em> in the transaction. When the application is
ready to commit, the coordinator begins phase 1: it sends a <em>prepare</em> request to each of the nodes,
asking them whether they are able to commit. The coordinator then tracks the responses from the
participants:</p>

<ul>
<li>
<p>If all participants reply “yes,” indicating they are ready to commit, then the coordinator sends
out a <em>commit</em> request in phase 2, and the commit actually takes place.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="aborts (transactions)" data-secondary="in two-phase commit" id="idm140417550028784"></a>
If any of the participants replies “no,” the coordinator sends an <em>abort</em> request to all nodes in
phase 2.</p>
</li>
</ul>

<p>This process is somewhat like the traditional marriage ceremony in Western cultures: the minister
asks the bride and groom individually whether each wants to marry the other, and typically receives
the answer “I do” from both. After receiving both acknowledgments, the minister pronounces the
couple husband and wife: the transaction is committed, and the happy fact is broadcast to all
attendees. If either bride or groom does not say “yes,” the ceremony is aborted
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray1981wi_ch9">73</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="A system of promises"><div class="sect3" id="idm140417550025088">
<h3>A system of promises</h3>

<p><a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="how it works" id="idm140417550023712"></a>
From this short description it might not be clear why two-phase commit ensures atomicity, while
one-phase commit across several nodes does not. Surely the prepare and commit requests can just
as easily be lost in the two-phase case. What makes 2PC different?</p>

<p>To understand why it works, we have to break down the process in a bit more detail:</p>
<ol>
<li>
<p>When the application wants to begin a distributed transaction, it requests a transaction ID from
the coordinator. This transaction ID is globally unique.</p>
</li>
<li>
<p>The application begins a single-node transaction on each of the participants, and attaches the
globally unique transaction ID to the single-node transaction. All reads and writes are done in
one of these single-node transactions. If anything goes wrong at this stage (for example, a node
crashes or a request times out), the coordinator or any of the participants can abort.</p>
</li>
<li>
<p>When the application is ready to commit, the coordinator sends a prepare request to all
participants, tagged with the global transaction ID. If any of these requests fails or times out,
the coordinator sends an abort request for that transaction ID to all participants.</p>
</li>
<li>
<p>When a participant receives the prepare request, it makes sure that it can definitely commit
the transaction under all circumstances.
<a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in two-phase commit" id="idm140417550017088"></a>
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="in two-phase commit" id="idm140417550015712"></a>
This includes writing all transaction data to disk (a crash, a power failure, or running out of
disk space is not an acceptable excuse for refusing to commit later), and checking for any
conflicts or constraint violations. By replying “yes” to the coordinator, the node promises to
commit the transaction without error if requested. In other words, the participant surrenders the
right to abort the transaction, but without actually committing it.</p>
</li>
<li>
<p>When the coordinator has received responses to all prepare requests, it makes a definitive
decision on whether to commit or abort the transaction (committing only if all participants voted
“yes”). The coordinator must write that decision to its transaction log on disk so that it knows
which way it decided in case it subsequently crashes. This is called the <em>commit point</em>.</p>
</li>
<li>
<p>Once the coordinator’s decision has been written to disk, the commit or abort request is sent
to all participants. If this request fails or times out, the coordinator must retry forever until
it succeeds. There is no more going back: if the decision was to commit, that decision must be
enforced, no matter how many retries it takes. If a participant has crashed in the meantime, the
transaction will be committed when it recovers—since the participant voted “yes,” it cannot
refuse to commit when it recovers.</p>
</li>

</ol>

<p>Thus, the protocol contains two crucial “points of no return”: when a participant votes “yes,” it
promises that it will definitely be able to commit later (although the coordinator may still choose to
abort); and once the coordinator decides, that decision is irrevocable. Those promises ensure the
atomicity of 2PC. (Single-node atomic commit lumps these two events into one: writing the commit
record to the transaction log.)</p>

<p>Returning to the marriage analogy, before saying “I do,” you and your bride/groom have the freedom
to abort the transaction by saying “No way!” (or something to that effect). However, after saying “I
do,” you cannot retract that statement. If you faint after saying “I do” and you don’t hear the
minister speak the words “You are now husband and wife,” that doesn’t change the fact that the
transaction was committed. When you recover consciousness later, you can find out whether you are
married or not by querying the minister for the status of your global transaction ID, or you can
wait for the minister’s next retry of the commit request (since the retries will have continued
throughout your period of unconsciousness).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Coordinator failure"><div class="sect3" id="idm140417550008784">
<h3>Coordinator failure</h3>

<p><a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="coordinator failure" id="idm140417550007408"></a>
<a data-type="indexterm" data-primary="coordinator (in 2PC)" data-secondary="failure" id="idm140417550006304"></a>
We have discussed what happens if one of the participants or the network fails during 2PC: if any of
the prepare requests fail or time out, the coordinator aborts the transaction; if any of the
commit or abort requests fail, the coordinator retries them indefinitely. However, it is less
clear what happens if the coordinator crashes.</p>

<p><a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="in doubt/uncertain status" id="idm140417550004592"></a>
<a data-type="indexterm" data-primary="in doubt (transaction status)" id="idm140417550003280"></a>
<a data-type="indexterm" data-primary="uncertain (transaction status)" data-see="in doubt" id="idm140417550002480"></a>
If the coordinator fails before sending the prepare requests, a participant can safely abort the
transaction. But once the participant has received a prepare request and voted “yes,” it can no
longer abort unilaterally—it must wait to hear back from the coordinator whether the transaction
was committed or aborted. If the coordinator crashes or the network fails at this point, the
participant can do nothing but wait. A participant’s transaction in this state is called <em>in doubt</em>
or <em>uncertain</em>.</p>

<p>The situation is illustrated in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_2pc_crash">Figure&nbsp;9-10</a>. In this particular example, the
coordinator actually decided to commit, and database 2 received the commit request. However, the
coordinator crashed before it could send the commit request to database 1, and so database 1 does
not know whether to commit or abort. Even a timeout does not help here: if database 1 unilaterally
aborts after a timeout, it will end up inconsistent with database 2, which has committed. Similarly,
it is not safe to unilaterally commit, because another participant may have aborted.</p>

<figure><div id="fig_consistency_2pc_crash" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0910.png" alt="ddia 0910" width="2880" height="1006" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_0910.png">
<h6><span class="label">Figure 9-10. </span>The coordinator crashes after participants vote “yes.” Database 1 does not know whether to commit or abort.</h6>
</div></figure>

<p>Without hearing from the coordinator, the participant has no way of knowing whether to commit or
abort. In principle, the participants could communicate among themselves to find out how each
participant voted and come to some agreement, but that is not part of the 2PC protocol.</p>

<p>The only way 2PC can complete is by waiting for the coordinator to recover. This is why the
coordinator must write its commit or abort decision to a transaction log on disk before sending
commit or abort requests to participants: when the coordinator recovers, it determines the status of
all in-doubt transactions by reading its transaction log. Any transactions that don’t have a commit
record in the coordinator’s log are aborted. Thus, the commit point of 2PC comes down to a regular
single-node atomic commit on the coordinator.
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-startref="ix_twopc" id="idm140417549994448"></a>
<a data-type="indexterm" data-primary="commits (transactions)" data-secondary="two-phase commit (2PC)" data-startref="ix_commit2pc" id="idm140417549993344"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Three-phase commit"><div class="sect3" id="sec_consistency_3pc">
<h3>Three-phase commit</h3>

<p><a data-type="indexterm" data-primary="blocking atomic commit" id="idm140417549990384"></a>
<a data-type="indexterm" data-primary="three-phase commit" id="idm140417549989328"></a>
<a data-type="indexterm" data-primary="commits (transactions)" data-secondary="three-phase commit (3PC)" id="idm140417549988496"></a>
<a data-type="indexterm" data-primary="nonblocking atomic commit" id="idm140417549987424"></a>
<a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="atomic commit" data-tertiary="blocking and nonblocking" id="idm140417549986576"></a>
Two-phase commit is called a <em>blocking</em> atomic commit protocol due to the fact that 2PC can become
stuck waiting for the coordinator to recover. In theory, it is possible to make an atomic commit
protocol <em>nonblocking</em>, so that it does not get stuck if a node fails. However, making this work in
practice is not so straightforward.</p>

<p>As an alternative to 2PC, an algorithm called <em>three-phase commit</em> (3PC) has been proposed
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bernstein1987va_ch9">13</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Skeen1981jc-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Skeen1981jc">80</a>].
However, 3PC assumes a network with bounded delay and nodes with bounded response times; in most
practical systems with unbounded network delay and process pauses (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>), it
cannot guarantee atomicity.</p>

<p><a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="perfect failure detectors" id="idm140417549978320"></a>
In general, nonblocking atomic commit requires a <em>perfect failure detector</em>
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Guerraoui1995bi">71</a>]—i.e., a reliable mechanism for telling
whether a node has crashed or not. In a network with unbounded delay a timeout is not a reliable
failure detector, because a request may time out due to a network problem even if no node has
crashed. For this reason, 2PC continues to be used, despite the known problem with coordinator
failure.
<a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="two-phase commit" data-startref="ix_consensusdtatom" id="idm140417549974240"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="two-phase commit" data-startref="ix_transactdtcommit" id="idm140417549972560"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Distributed Transactions in Practice"><div class="sect2" id="sec_consistency_dist_trans">
<h2>Distributed Transactions in Practice</h2>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="in practice" id="ix_consensusdtprac"></a>
<a data-type="indexterm" data-primary="performance" data-secondary="of distributed transactions" id="idm140417549967472"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="use of" id="ix_transactdtuse"></a>
Distributed transactions, especially those implemented with two-phase commit, have a mixed
reputation. On the one hand, they are seen as providing an important safety guarantee that would be
hard to achieve otherwise; on the other hand, they are criticized for causing operational problems,
killing performance, and promising more than they can deliver
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hohpe2005hn-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Hohpe2005hn">81</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Helland2007td_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Helland2007td_ch9">82</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oliver2011wt-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Oliver2011wt">83</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rahien2014uz-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Rahien2014uz">84</a>].
Many cloud services choose not to implement distributed transactions due to the operational
problems they engender [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Vasters2012wa-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Vasters2012wa">85</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="NServiceBus2015tf-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#NServiceBus2015tf">86</a>].</p>

<p><a data-type="indexterm" data-primary="MySQL (database)" data-secondary="performance of XA transactions" id="idm140417549950560"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="performance cost" id="idm140417549949440"></a>
Some implementations of distributed transactions carry a heavy performance penalty—for example,
distributed transactions in MySQL are reported to be over 10 times slower than single-node
transactions [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wigginton2013vk-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Wigginton2013vk">87</a>], so it is
not surprising when people advise against using them. Much of the performance cost inherent in
two-phase commit is due to the additional disk forcing (<code>fsync</code>) that is required for crash recovery
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Spille2004ur-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004ur">88</a>], and the additional network round-trips.</p>

<p>However, rather than dismissing distributed transactions outright, we should examine them in some
more detail, because there are important lessons to be learned from them. To begin, we should be
precise about what we mean by “distributed transactions.” Two quite different types of distributed
transactions are often conflated:</p>
<dl>
<dt>Database-internal distributed transactions</dt>
<dd>
<p><a data-type="indexterm" data-primary="database-internal distributed transactions" id="idm140417549940672"></a>
Some distributed databases (i.e., databases that use replication and partitioning in their standard
configuration) support internal transactions among the nodes of that database. For example,
VoltDB and MySQL Cluster’s NDB storage engine have such internal transaction
support. In this case, all the nodes participating in the transaction are running the same
database software.</p>
</dd>
<dt>Heterogeneous distributed transactions</dt>
<dd>
<p><a data-type="indexterm" data-primary="heterogeneous distributed transactions" id="idm140417549938176"></a>
In a <em>heterogeneous</em> transaction, the participants are two or more different technologies: for
example, two databases from different vendors, or even non-database systems such as message
brokers. A distributed transaction across these systems must ensure atomic commit, even though
the systems may be entirely different under the hood.</p>
</dd>
</dl>

<p>Database-internal transactions do not have to be compatible with any other system, so they can
use any protocol and apply optimizations specific to that particular technology. For that reason,
database-internal distributed transactions can often work quite well. On the other hand,
transactions spanning heterogeneous technologies are a lot more challenging.</p>










<section data-type="sect3" data-pdf-bookmark="Exactly-once message processing"><div class="sect3" id="sec_consistency_exactly_once">
<h3>Exactly-once message processing</h3>

<p><a data-type="indexterm" data-primary="exactly-once semantics" id="idm140417549933632"></a>
<a data-type="indexterm" data-primary="messages" data-secondary="exactly-once semantics" id="idm140417549932800"></a>
<a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="atomic commit" data-tertiary="in stream processing" id="idm140417549931696"></a>
Heterogeneous distributed transactions allow diverse systems to be integrated in powerful ways. For
example, a message from a message queue can be acknowledged as processed if and only if the database
transaction for processing the message was successfully committed. This is implemented by atomically
committing the message acknowledgment and the database writes in a single transaction. With
distributed transaction support, this is possible, even if the message broker and the database are
two unrelated technologies running on different machines.</p>

<p>If either the message delivery or the database transaction fails, both are aborted, and so the
message broker may safely redeliver the message later. Thus, by atomically committing the message
and the side effects of its processing, we can ensure that the message is <em>effectively</em> processed
exactly once, even if it required a few retries before it succeeded. The abort discards any
side effects of the partially completed transaction.</p>

<p>Such a distributed transaction is only possible if all systems affected by the transaction are able
to use the same atomic commit protocol, however. For example, say a side effect of processing a
message is to send an email, and the email server does not support two-phase commit: it could happen
that the email is sent two or more times if message processing fails and is retried. But if all side
effects of processing a message are rolled back on transaction abort, then the processing step can
safely be retried as if nothing had happened.</p>

<p>We will return to the topic of exactly-once message processing in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>. Let’s look first at
the atomic commit protocol that allows such heterogeneous distributed transactions.
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="use of" data-startref="ix_transactdtuse" id="idm140417549926000"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="transaction atomicity" data-startref="ix_faulttolatomic" id="idm140417549924368"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="XA transactions"><div class="sect3" id="idm140417549922992">
<h3>XA transactions</h3>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="XA transactions" id="ix_consdtxa"></a>
<a data-type="indexterm" data-primary="coordinator (in 2PC)" data-secondary="in XA transactions" id="ix_coordxatrans"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="XA transactions" id="ix_transactdtxa"></a>
<a data-type="indexterm" data-primary="XA transactions" id="ix_xatrans"></a>
<a data-type="indexterm" data-primary="eXtended Architecture transactions" data-see="XA transactions" id="idm140417549915536"></a>
<a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="distributed transaction support" id="idm140417549914416"></a>
<a data-type="indexterm" data-primary="MySQL (database)" data-secondary="distributed transaction support" id="idm140417549913296"></a>
<a data-type="indexterm" data-primary="IBM" data-secondary="DB2 (database)" data-tertiary="distributed transaction support" id="idm140417549912176"></a>
<a data-type="indexterm" data-primary="SQL Server (database)" data-secondary="distributed transaction support" id="idm140417549910784"></a>
<a data-type="indexterm" data-primary="Oracle (database)" data-secondary="distributed transaction support" id="idm140417549909664"></a>
<a data-type="indexterm" data-primary="ActiveMQ (messaging)" data-secondary="distributed transaction support" id="idm140417549908544"></a>
<a data-type="indexterm" data-primary="HornetQ (messaging)" data-secondary="distributed transaction support" id="idm140417549907424"></a>
<a data-type="indexterm" data-primary="MSMQ (messaging)" id="idm140417549906304"></a>
<a data-type="indexterm" data-primary="IBM" data-secondary="MQ (messaging)" data-tertiary="distributed transaction support" id="idm140417549905472"></a>
<em>X/Open XA</em> (short for <em>eXtended Architecture</em>) is a standard for implementing two-phase commit
across heterogeneous technologies [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#XASpec1991vk">76</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004vr">77</a>]. It was introduced in 1991 and has been widely
implemented: XA is supported by many traditional relational databases (including PostgreSQL, MySQL,
DB2, SQL Server, and Oracle) and message brokers (including ActiveMQ, HornetQ, MSMQ, and IBM MQ).</p>

<p><a data-type="indexterm" data-primary="Application Programming Interfaces (APIs)" data-secondary="for distributed transactions" id="idm140417549901296"></a>
<a data-type="indexterm" data-primary="Java Enterprise Edition (EE)" id="idm140417549900208"></a>
<a data-type="indexterm" data-primary="Java Transaction API (JTA)" id="idm140417549899360"></a>
<a data-type="indexterm" data-primary="Java Message Service (JMS)" data-secondary="distributed transaction support" id="idm140417549898512"></a>
<a data-type="indexterm" data-primary="Java Database Connectivity (JDBC)" data-secondary="distributed transaction support" id="idm140417549897376"></a>
XA is not a network protocol—it is merely a C API for interfacing with a transaction coordinator.
Bindings for this API exist in other languages; for example, in the world of Java EE applications,
XA transactions are implemented using the Java Transaction API (JTA), which in turn is supported by
many drivers for databases using Java Database Connectivity (JDBC) and drivers for message brokers
using the Java Message Service (JMS) APIs.</p>

<p>XA assumes that your application uses a network driver or client library to communicate with the
participant databases or messaging services. If the driver supports XA, that means it calls the XA
API to find out whether an operation should be part of a distributed transaction—and if so, it
sends the necessary information to the database server. The driver also exposes callbacks through
which the coordinator can ask the participant to prepare, commit, or abort.</p>

<p>The transaction coordinator implements the XA API. The standard does not specify how it should be
implemented, but in practice the coordinator is often simply a library that is loaded into the same
process as the application issuing the transaction (not a separate service). It keeps track of the
participants in a transaction, collects partipants’ responses after asking them to prepare (via a
callback into the driver), and uses a log on the local disk to keep track of the commit/abort
decision for each transaction.</p>

<p><a data-type="indexterm" data-primary="fault tolerance" data-secondary="of distributed transactions" id="ix_faulttolxa"></a>
If the application process crashes, or the machine on which the application is running dies, the
coordinator goes with it. Any participants with prepared but uncommitted transactions are then stuck
in doubt. Since the coordinator’s log is on the application server’s local disk, that server must be
restarted, and the coordinator library must read the log to recover the commit/abort outcome of each
transaction. Only then can the coordinator use the database driver’s XA callbacks to ask
participants to commit or abort, as appropriate. The database server cannot contact the coordinator
directly, since all communication must go via its client library.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Holding locks while in doubt"><div class="sect3" id="sec_consistency_locking">
<h3>Holding locks while in doubt</h3>

<p><a data-type="indexterm" data-primary="locks" data-secondary="in two-phase commit (2PC)" data-tertiary="in-doubt transactions holding locks" id="idm140417549889920"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="transactions holding locks" id="idm140417549888464"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="in doubt/uncertain status" id="idm140417549887344"></a>
<a data-type="indexterm" data-primary="in doubt (transaction status)" data-secondary="holding locks" id="idm140417549885936"></a>
Why do we care so much about a transaction being stuck in doubt? Can’t the rest of the system just
get on with its work, and ignore the in-doubt transaction that will be cleaned up eventually?</p>

<p>The problem is with <em>locking</em>. As discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_read_committed">“Read Committed”</a>, database
transactions usually take a row-level exclusive lock on any rows they modify, to prevent dirty
writes. In addition, if you want serializable isolation, a database using two-phase locking would also
have to take a shared lock on any rows <em>read</em> by the transaction (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_2pl">“Two-Phase Locking (2PL)”</a>).</p>

<p>The database cannot release those locks until the transaction commits or aborts (illustrated as a
shaded area in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#fig_consistency_two_phase_commit">Figure&nbsp;9-9</a>). Therefore, when using two-phase commit, a
transaction must hold onto the locks throughout the time it is in doubt. If the coordinator has
crashed and takes 20 minutes to start up again, those locks will be held for 20 minutes. If the
coordinator’s log is entirely lost for some reason, those locks will be held forever—or at least
until the situation is manually resolved by an administrator.</p>

<p>While those locks are held, no other transaction can modify those rows. Depending on the database,
other transactions may even be blocked from reading those rows. Thus, other transactions cannot
simply continue with their business—if they want to access that same data, they will be blocked.
This can cause large parts of your application to become unavailable until the in-doubt transaction
is resolved.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Recovering from coordinator failure"><div class="sect3" id="idm140417549878624">
<h3>Recovering from coordinator failure</h3>

<p><a data-type="indexterm" data-primary="coordinator (in 2PC)" data-secondary="recovery" id="idm140417549877280"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="coordinator recovery" id="idm140417549875952"></a>
<a data-type="indexterm" data-primary="in doubt (transaction status)" data-secondary="orphaned transactions" id="idm140417549874848"></a>
In theory, if the coordinator crashes and is restarted, it should cleanly recover its state from the
log and resolve any in-doubt transactions. However, in practice, <em>orphaned</em> in-doubt transactions do
occur [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dhariwal2008vq-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Dhariwal2008vq">89</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Randal2013wu-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Randal2013wu">90</a>]—that is,
transactions for which the coordinator cannot decide the outcome for whatever reason (e.g., because
the transaction log has been lost or corrupted due to a software bug). These transactions cannot be
resolved automatically, so they sit forever in the database, holding locks and blocking other
transactions.</p>

<p>Even rebooting your database servers will not fix this problem, since a correct implementation of
2PC must preserve the locks of an in-doubt transaction even across restarts (otherwise it would risk
violating the atomicity guarantee). It’s a sticky <span class="keep-together">situation.</span></p>

<p>The only way out is for an administrator to manually decide whether to commit or roll back the
transactions. The administrator must examine the participants of each in-doubt transaction,
determine whether any participant has committed or aborted already, and then apply the same outcome
to the other participants. Resolving the problem potentially requires a lot of manual effort, and
most likely needs to be done under high stress and time pressure during a serious production outage
(otherwise, why would the coordinator be in such a bad state?).</p>

<p><a data-type="indexterm" data-primary="heuristic decisions (in 2PC)" id="idm140417549866208"></a>
<a data-type="indexterm" data-primary="XA transactions" data-secondary="heuristic decisions" id="idm140417549865184"></a>
Many XA implementations have an emergency escape hatch called <em>heuristic decisions</em>: allowing a
participant to unilaterally decide to abort or commit an in-doubt transaction without a definitive
decision from the coordinator [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#XASpec1991vk">76</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004vr">77</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="SQLServerInDoubt-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#SQLServerInDoubt">91</a>]. To be clear, <em>heuristic</em> here is a euphemism for <em>probably breaking atomicity</em>,
since it violates the system of promises in two-phase commit. Thus, heuristic decisions are intended
only for getting out of catastrophic situations, and not for regular use.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Limitations of distributed transactions"><div class="sect3" id="sec_consistency_trans_limits">
<h3>Limitations of distributed transactions</h3>

<p><a data-type="indexterm" data-primary="distributed systems" data-secondary="limitations of distributed transactions" id="idm140417549857328"></a>
<a data-type="indexterm" data-primary="two-phase commit (2PC)" data-secondary="issues in practice" id="idm140417549856160"></a>
<a data-type="indexterm" data-primary="XA transactions" data-secondary="limitations of" id="idm140417549855056"></a>
XA transactions solve the real and important problem of keeping several participant data systems
consistent with each other, but as we have seen, they also introduce major operational problems. In
particular, the key realization is that the transaction coordinator is itself a kind of database (in
which transaction outcomes are stored), and so it needs to be approached with the same care as any
other important database:</p>

<ul>
<li>
<p>If the coordinator is not replicated but runs only on a single machine, it is a single point of
failure for the entire system (since its failure causes other application servers to block on
locks held by in-doubt transactions). Surprisingly, many coordinator implementations are not
highly available by default, or have only rudimentary replication support.</p>
</li>
<li>
<p>Many server-side applications are developed in a stateless model (as favored by HTTP), with all
persistent state stored in a database, which has the advantage that application servers can be
added and removed at will. However, when the coordinator is part of the application server, it
changes the nature of the deployment. Suddenly, the coordinator’s logs become a crucial part of the
durable system state—as important as the databases themselves, since the coordinator logs are
required in order to recover in-doubt transactions after a crash. Such application servers are no
longer stateless.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="heterogeneous distributed transactions" id="idm140417549849824"></a>
<a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in two-phase commit" id="idm140417549848944"></a>
<a data-type="indexterm" data-primary="locks" data-secondary="in two-phase commit (2PC)" data-tertiary="deadlock detection" id="idm140417549847488"></a>
<a data-type="indexterm" data-primary="deadlocks" data-secondary="detection, in two-phase commit (2PC)" id="idm140417549846096"></a>
Since XA needs to be compatible with a wide range of data systems, it is necessarily a lowest
common denominator. For example, it cannot detect deadlocks across different systems (since that
would require a standardized protocol for systems to exchange information on the locks that each
transaction is waiting for), and it does not work with SSI (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_ssi">“Serializable Snapshot Isolation (SSI)”</a>), since
that would require a protocol for identifying conflicts across different systems.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="database-internal distributed transactions" id="idm140417549842992"></a>
<a data-type="indexterm" data-primary="failures" data-secondary="amplification by distributed transactions" id="idm140417549841840"></a>
<a data-type="indexterm" data-primary="amplification" data-secondary="of failures" id="idm140417549840704"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="failure amplification" id="idm140417549839600"></a>
<a data-type="indexterm" data-primary="FoundationDB (database)" data-secondary="serializable transactions" id="idm140417549838256"></a>
<a data-type="indexterm" data-primary="serializability" data-secondary="serializable snapshot isolation (SSI)" data-tertiary="distributed execution" id="idm140417549837136"></a>
For database-internal distributed transactions (not XA), the limitations are not so great—for
example, a distributed version of SSI is possible. However, there remains the problem that for 2PC
to successfully commit a transaction, <em>all</em> participants must respond. Consequently, if <em>any</em> part
of the system is broken, the transaction also fails. Distributed transactions thus have a tendency
of <em>amplifying failures</em>, which runs counter to our goal of building fault-tolerant systems.</p>
</li>
</ul>

<p>Do these facts mean we should give up all hope of keeping several systems consistent with each
other? Not quite—there are alternative methods that allow us to achieve the same thing without
the pain of heterogeneous distributed transactions. We will return to these in Chapters
<a data-type="xref" data-xrefstyle="select:labelnumber" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">11</a>
and   <a data-type="xref" data-xrefstyle="select:labelnumber" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ch_future">12</a>.
But first, we should wrap up the topic of <span class="keep-together">consensus.</span>
<a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="in practice" data-startref="ix_consensusdtprac" id="idm140417549830064"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-tertiary="XA transactions" data-startref="ix_consdtxa" id="idm140417549828416"></a>
<a data-type="indexterm" data-primary="coordinator (in 2PC)" data-secondary="in XA transactions" data-startref="ix_coordxatrans" id="idm140417549826800"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-startref="ix_transactdtprac" id="idm140417549825424"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="XA transactions" data-startref="ix_transactdtxa" id="idm140417549824032"></a>
<a data-type="indexterm" data-primary="XA transactions" data-startref="ix_xatrans" id="idm140417549822368"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="of distributed transactions" data-startref="ix_faulttolxa" id="idm140417549821264"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Fault-Tolerant Consensus"><div class="sect2" id="sec_consistency_consensus_ft">
<h2>Fault-Tolerant Consensus</h2>

<p><a data-type="indexterm" data-primary="consensus" id="ix_consensusdtfault"></a>
Informally, consensus means getting several nodes to agree on something. For example, if several
people concurrently try to book the last seat on an airplane, or the same seat in a theater, or try
to register an account with the same username, then a consensus algorithm could be used to determine
which one of these mutually incompatible operations should be the winner.</p>

<p>The consensus problem is normally formalized as follows: one or more nodes may <em>propose</em> values, and
the consensus algorithm <em>decides</em> on one of those values. In the seat-booking example, when several
customers are concurrently trying to buy the last seat, each node handling a customer request may
propose the ID of the customer it is serving, and the decision indicates which one of those
customers got the seat.</p>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="algorithms" data-tertiary="safety and liveness properties" id="idm140417549815152"></a>
<a data-type="indexterm" data-primary="distributed systems" data-secondary="formalization of consensus" id="idm140417549813760"></a>
In this formalism, a consensus algorithm must satisfy the following properties
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>]:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417549811840-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417549811840">xiii</a></sup><a data-type="indexterm" data-primary="uniform consensus" data-seealso="consensus" id="idm140417549811504"></a></p>
<dl>
<dt>Uniform agreement</dt>
<dd>
<p><a data-type="indexterm" data-primary="agreement" data-seealso="consensus" id="idm140417549805648"></a>
No two nodes decide differently.</p>
</dd>
<dt>Integrity</dt>
<dd>
<p><a data-type="indexterm" data-primary="integrity" data-secondary="in consensus formalization" id="idm140417549803392"></a>
No node decides twice.</p>
</dd>
<dt>Validity</dt>
<dd>
<p><a data-type="indexterm" data-primary="validity (consensus)" id="idm140417549801088"></a>
If a node decides value <em>v</em>, then <em>v</em> was proposed by some node.</p>
</dd>
<dt>Termination</dt>
<dd>
<p><a data-type="indexterm" data-primary="termination (consensus)" id="idm140417549798096"></a>
Every node that does not crash eventually decides some value.</p>
</dd>
</dl>

<p>The uniform agreement and integrity properties define the core idea of consensus: everyone decides
on the same outcome, and once you have decided, you cannot change your mind. The validity property
exists mostly to rule out trivial solutions: for example, you could have an algorithm that always
decides <code>null</code>, no matter what was proposed; this algorithm would satisfy the agreement and
integrity properties, but not the validity property.</p>

<p><a data-type="indexterm" data-primary="fault tolerance" data-secondary="formalization in consensus" id="ix_faulttolconsensus"></a>
If you don’t care about fault tolerance, then satisfying the first three properties is easy: you can
just hardcode one node to be the “dictator,” and let that node make all of the decisions. However,
if that one node fails, then the system can no longer make any decisions. This is, in fact, what we
saw in the case of two-phase commit: if the coordinator fails, in-doubt participants cannot decide
whether to commit or abort.</p>

<p>The termination property formalizes the idea of fault tolerance. It essentially says that a
consensus algorithm cannot simply sit around and do nothing forever—in other words, it must make
progress. Even if some nodes fail, the other nodes must still reach a decision. (Termination is a
liveness property, whereas the other three are safety properties—see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_safety_liveness">“Safety and liveness”</a>.)</p>

<p>The system model of consensus assumes that when a node “crashes,” it suddenly disappears and never
comes back. (Instead of a software crash, imagine that there is an earthquake, and the datacenter
containing your node is destroyed by a landslide. You must assume that your node is buried under 30
feet of mud and is never going to come back online.) In this system model, any algorithm that has to
wait for a node to recover is not going to be able to satisfy the termination property. In
particular, 2PC does not meet the requirements for termination.</p>

<p>Of course, if <em>all</em> nodes crash and none of them are running, then it is not possible for any
algorithm to decide anything. There is a limit to the number of failures that an algorithm can
tolerate: in fact, it can be proved that any consensus algorithm requires at least a majority of
nodes to be functioning correctly in order to assure termination
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>]. That majority can safely form a quorum
(see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>).</p>

<p><a data-type="indexterm" data-primary="safety and liveness properties" data-secondary="in consensus algorithms" id="idm140417549787840"></a>
Thus, the termination property is subject to the assumption that fewer than half of the nodes are
crashed or unreachable. However, most implementations of consensus ensure that the safety
properties—agreement, integrity, and validity—are always met, even if a majority of nodes fail or
there is a severe network problem
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dwork1988dr_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Dwork1988dr_ch9">92</a>].
Thus, a large-scale outage can stop the system from being able to process requests, but it cannot
corrupt the consensus system by causing it to make invalid decisions.</p>

<p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="consensus algorithms and" id="idm140417549783088"></a>
Most consensus algorithms assume that there are no Byzantine faults, as discussed in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_byzantine">“Byzantine Faults”</a>. That is, if a node does not correctly follow the protocol (for
example, if it sends contradictory messages to different nodes), it may break the safety properties
of the protocol. It is possible to make consensus robust against Byzantine faults as long as fewer
than one-third of the nodes are Byzantine-faulty [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Castro2002ej-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Castro2002ej">93</a>],
but we don’t have space to discuss those algorithms in detail in this book.</p>










<section data-type="sect3" data-pdf-bookmark="Consensus algorithms and total order broadcast"><div class="sect3" id="sec_consistency_consensus_abcast">
<h3>Consensus algorithms and total order broadcast</h3>

<p><a data-type="indexterm" data-primary="Viewstamped Replication (consensus algorithm)" id="idm140417549775248"></a>
<a data-type="indexterm" data-primary="Paxos (consensus algorithm)" id="idm140417549773616"></a>
<a data-type="indexterm" data-primary="Raft (consensus algorithm)" id="idm140417549772768"></a>
<a data-type="indexterm" data-primary="Zab (consensus algorithm)" id="idm140417549771920"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="algorithms" id="ix_consalgobcast"></a>
<a data-type="indexterm" data-primary="total order broadcast" data-secondary="consensus algorithms and" id="ix_tobcastcons"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="use of Zab algorithm" id="idm140417549768304"></a>
The best-known fault-tolerant consensus algorithms are Viewstamped Replication (VSR)
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Oki1988ci-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Oki1988ci">94</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Liskov2012ut-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Liskov2012ut">95</a>],
Paxos [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport1998ea-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1998ea">96</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lamport2001ud-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport2001ud">97</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chandra2007vp-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra2007vp">98</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vanRenesse2011wu-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2011wu">99</a>],
Raft [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ongaro2014wq">22</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ongaro2014wk-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ongaro2014wk">100</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Howard2015ko-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2015ko">101</a>],
and Zab [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2011jc">21</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Medeiros2012ur-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Medeiros2012ur">102</a>].
There are quite a few similarities between these algorithms, but they are not the same
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="vanRenesse2014dj-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2014dj">103</a>].
In this book we won’t go into full details of the different algorithms: it’s sufficient to be aware
of some of the high-level ideas that they have in common, unless you’re implementing a consensus
system yourself (which is probably not advisable—it’s hard
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra2007vp">98</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Portnoy2012vs-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Portnoy2012vs">104</a>]).</p>

<p>Most of these algorithms actually don’t directly use the formal model described here (proposing and
deciding on a single value, while satisfying the agreement, integrity, validity, and termination
properties). Instead, they decide on a <em>sequence</em> of values, which makes them <em>total order
broadcast</em> algorithms, as discussed previously in this chapter (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_total_order">“Total Order Broadcast”</a>).</p>

<p>Remember that total order broadcast requires messages to be delivered exactly once, in the same
order, to all nodes. If you think about it, this is equivalent to performing several rounds of
consensus: in each round, nodes propose the message that they want to send next, and then decide on
the next message to be delivered in the total order
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>].</p>

<p><a data-type="indexterm" data-primary="corruption of data" data-secondary="formalization in consensus" id="idm140417549733408"></a>
So, total order broadcast is equivalent to repeated rounds of consensus (each consensus decision
corresponding to one message delivery):</p>

<ul>
<li>
<p>Due to the agreement property of consensus, all nodes decide to deliver the same messages in the
same order.</p>
</li>
<li>
<p>Due to the integrity property, messages are not duplicated.</p>
</li>
<li>
<p>Due to the validity property, messages are not corrupted and not fabricated out of thin air.</p>
</li>
<li>
<p>Due to the termination property, messages are not lost.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="Multi-Paxos (total order broadcast)" id="idm140417549727552"></a>
<a data-type="indexterm" data-primary="Paxos (consensus algorithm)" data-secondary="Multi-Paxos (total order broadcast)" id="idm140417549726704"></a>
Viewstamped Replication, Raft, and Zab implement total order broadcast directly, because that is more
efficient than doing repeated rounds of one-value-at-a-time consensus. In the case of Paxos, this
optimization is known as Multi-Paxos.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Single-leader replication and consensus"><div class="sect3" id="sec_consistency_consensus_leader">
<h3>Single-leader replication and consensus</h3>

<p><a data-type="indexterm" data-primary="fault tolerance" data-secondary="formalization in consensus" data-tertiary="use of replication" id="idm140417549723600"></a>
<a data-type="indexterm" data-primary="replication" data-secondary="single-leader" data-tertiary="relation to consensus" id="idm140417549721984"></a>
<a data-type="indexterm" data-primary="leader-based replication" data-secondary="relation to consensus" id="idm140417549720608"></a>
In <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a> we discussed single-leader replication (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_leader">“Leaders and Followers”</a>), which
takes all the writes to the leader and applies them to the followers in the same order, thus keeping
replicas up to date. Isn’t this essentially total order broadcast? How come we didn’t have to worry
about consensus in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>?</p>

<p>The answer comes down to how the leader is chosen. If the leader is manually chosen and configured
by the humans in your operations team, you essentially have a “consensus algorithm” of the
dictatorial variety: only one node is allowed to accept writes (i.e., make decisions about the order
of writes in the replication log), and if that node goes down, the system becomes unavailable for
writes until the operators manually configure a different node to be the leader. Such a system can
work well in practice, but it does not satisfy the termination property of consensus because it
requires human intervention in order to make progress.</p>

<p>Some databases perform automatic leader election and failover, promoting a follower to be the new
leader if the old leader fails (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_failover">“Handling Node Outages”</a>). This brings us closer to
fault-tolerant total order broadcast, and thus to solving consensus.</p>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="algorithms" data-tertiary="preventing split brain" id="idm140417549714096"></a>
<a data-type="indexterm" data-primary="split brain" data-secondary="in consensus algorithms" id="idm140417549712848"></a>
However, there is a problem. We previously discussed the problem of split brain, and said that all
nodes need to agree who the leader is—otherwise two different nodes could each believe themselves to
be the leader, and consequently get the database into an inconsistent state. Thus, we need consensus
in order to elect a leader. But if the consensus algorithms described here are actually total order
broadcast algorithms, and total order broadcast is like single-leader replication, and single-leader
replication requires a leader, then…</p>

<p>It seems that in order to elect a leader, we first need a leader. In order to solve consensus, we
must first solve consensus. How do we break out of this conundrum?</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Epoch numbering and quorums"><div class="sect3" id="idm140417549710416">
<h3>Epoch numbering and quorums</h3>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of consensus" id="idm140417549709072"></a>
<a data-type="indexterm" data-primary="quorums" data-secondary="in consensus algorithms" id="idm140417549707744"></a>
<a data-type="indexterm" data-primary="epoch (consensus algorithms)" id="idm140417549706640"></a>
<a data-type="indexterm" data-primary="Paxos (consensus algorithm)" data-secondary="ballot number" id="idm140417549705840"></a>
<a data-type="indexterm" data-primary="Viewstamped Replication (consensus algorithm)" data-secondary="view number" id="idm140417549704720"></a>
<a data-type="indexterm" data-primary="Raft (consensus algorithm)" data-secondary="term number" id="idm140417549703520"></a>
All of the consensus protocols discussed so far internally use a leader in some form or another, but
they don’t guarantee that the leader is unique. Instead, they can make a weaker guarantee: the
protocols define an <em>epoch number</em> (called the <em>ballot number</em> in Paxos, <em>view number</em> in
Viewstamped Replication, and <em>term number</em> in Raft) and guarantee that within each epoch, the leader
is unique.</p>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in consensus algorithms" id="idm140417549700256"></a>
Every time the current leader is thought to be dead, a vote is started among the nodes to elect a
new leader. This election is given an incremented epoch number, and thus epoch numbers are totally
ordered and monotonically increasing. If there is a conflict between two different leaders in two
different epochs (perhaps because the previous leader actually wasn’t dead after all), then the
leader with the higher epoch number prevails.</p>

<p>Before a leader is allowed to decide anything, it must first check that there isn’t some other
leader with a higher epoch number which might take a conflicting decision. How does a leader know
that it hasn’t been ousted by another node? Recall <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_majority">“The Truth Is Defined by the Majority”</a>: a node cannot
necessarily trust its own judgment—just because a node thinks that it is the leader, that does not
necessarily mean the other nodes accept it as their leader.</p>

<p>Instead, it must collect votes from a <em>quorum</em> of nodes (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_quorum_condition">“Quorums for reading and writing”</a>).
For every decision that a leader wants to make, it must send the proposed value to the other nodes
and wait for a quorum of nodes to respond in favor of the proposal. The quorum typically, but not
always, consists of a majority of nodes
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Howard2016tz_ch9-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2016tz_ch9">105</a>]. A node votes in favor of a proposal only if it
is not aware of any other leader with a higher epoch.</p>

<p>Thus, we have two rounds of voting: once to choose a leader, and a second time to vote on a leader’s
proposal. The key insight is that the quorums for those two votes must overlap: if a vote on a
proposal succeeds, at least one of the nodes that voted for it must have also participated in the
most recent leader election [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2016tz_ch9">105</a>]. Thus, if
the vote on a proposal does not reveal any higher-numbered epoch, the current leader can conclude
that no leader election with a higher epoch number has happened, and therefore be sure that it still
holds the leadership. It can then safely decide the proposed value.</p>

<p>This voting process looks superficially similar to two-phase commit. The biggest differences are
that in 2PC the coordinator is not elected, and that fault-tolerant consensus algorithms only
require votes from a majority of nodes, whereas 2PC requires a “yes” vote from <em>every</em> participant.
Moreover, consensus algorithms define a recovery process by which nodes can get into a consistent
state after a new leader is elected, ensuring that the safety properties are always met. These
differences are key to the correctness and fault tolerance of a consensus algorithm.
<a data-type="indexterm" data-primary="consensus" data-secondary="algorithms" data-startref="ix_consalgobcast" id="idm140417549689120"></a>
<a data-type="indexterm" data-primary="total order broadcast" data-secondary="consensus algorithms and" data-startref="ix_tobcastcons" id="idm140417549687744"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Limitations of consensus"><div class="sect3" id="idm140417549686352">
<h3>Limitations of consensus</h3>

<p>Consensus algorithms are a huge breakthrough for distributed systems: they bring concrete safety
properties (agreement, integrity, and validity) to systems where everything else is uncertain, and
they nevertheless remain fault-tolerant (able to make progress as long as a majority of nodes are
working and reachable). They provide total order broadcast, and therefore they can also implement
linearizable atomic operations in a fault-tolerant way (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_abcast_to_lin">“Implementing linearizable storage using total order broadcast”</a>).</p>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="cost of" id="idm140417549683296"></a>
Nevertheless, they are not used everywhere, because the benefits come at a cost.</p>

<p>The process by which nodes vote on proposals before they are decided is a kind of synchronous
replication. As discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_sync_async">“Synchronous Versus Asynchronous Replication”</a>, databases are often configured to use
asynchronous replication. In this configuration, some committed data can potentially be lost on
failover—but many people choose to accept this risk for the sake of better performance.</p>

<p>Consensus systems always require a strict majority to operate. This means you need a minimum of
three nodes in order to tolerate one failure (the remaining two out of three form a majority), or a
minimum of five nodes to tolerate two failures (the remaining three out of five form a majority). If
a network failure cuts off some nodes from the rest, only the majority portion of the network can
make progress, and the rest is blocked (see also <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_linearizability_cost">“The Cost of Linearizability”</a>).</p>

<p>Most consensus algorithms assume a fixed set of nodes that participate in voting, which means that
you can’t just add or remove nodes in the cluster. <em>Dynamic membership</em> extensions to consensus
algorithms allow the set of nodes in the cluster to change over time, but they are much less well
understood than static membership algorithms.</p>

<p>Consensus systems generally rely on timeouts to detect failed nodes. In environments with highly
variable network delays, especially geographically distributed systems, it often happens that a node
falsely believes the leader to have failed due to a transient network issue. Although this error does not
harm the safety properties, frequent leader elections result in terrible performance because the
system can end up spending more time choosing a leader than doing any useful work.</p>

<p><a data-type="indexterm" data-primary="Raft (consensus algorithm)" data-secondary="sensitivity to network problems" id="idm140417549676480"></a>
Sometimes, consensus algorithms are particularly sensitive to network problems. For example, Raft
has been shown to have unpleasant edge cases
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Howard2015cw-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2015cw">106</a>]:
if the entire network is working correctly except for one particular network link that is
consistently unreliable, Raft can get into situations where leadership continually bounces between
two nodes, or the current leader is continually forced to resign, so the system effectively never
makes progress. Other consensus algorithms have similar problems, and designing algorithms that are
more robust to unreliable networks is still an open research problem.
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="formalization in consensus" data-startref="ix_faulttolconsensus" id="idm140417549671632"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Membership and Coordination Services"><div class="sect2" id="sec_consistency_membership">
<h2>Membership and Coordination Services</h2>

<p><a data-type="indexterm" data-primary="consensus" data-secondary="membership and coordination services" id="ix_consensusdtmem"></a>
<a data-type="indexterm" data-primary="coordination" data-secondary="services" id="ix_coordserv"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" id="ix_zkmembership"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" id="ix_etcdmembership"></a>
Projects like ZooKeeper or etcd are often described as “distributed key-value stores” or
“coordination and configuration services.” The API of such a service looks pretty much like that of
a database: you can read and write the value for a given key, and iterate over keys. So if they’re
basically databases, why do they go to all the effort of implementing a consensus algorithm? What
makes them different from any other kind of database?</p>

<p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="use of ZooKeeper" id="idm140417549662976"></a>
<a data-type="indexterm" data-primary="OpenStack" data-secondary="Nova (cloud infrastructure)" data-tertiary="use of ZooKeeper" id="idm140417549661872"></a>
<a data-type="indexterm" data-primary="YARN (job scheduler)" data-secondary="use of ZooKeeper" id="idm140417549660480"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="YARN" data-see="YARN" id="idm140417549659376"></a>
<a data-type="indexterm" data-primary="Apache Hadoop" data-see="Hadoop" id="idm140417549657984"></a>
To understand this, it is helpful to briefly explore how a service like ZooKeeper is used. As an
application developer, you will rarely need to use ZooKeeper directly, because it is actually not well
suited as a general-purpose database. It is more likely that you will end up relying on it indirectly via
some other project: for example, HBase, Hadoop YARN, OpenStack Nova, and Kafka all rely on ZooKeeper
running in the background. What is it that these projects get from it?</p>

<p><a data-type="indexterm" data-primary="total order broadcast" data-secondary="implementation in ZooKeeper and etcd" id="idm140417549656128"></a>
ZooKeeper and etcd are designed to hold small amounts of data that can fit entirely in memory
(although they still write to disk for durability)—so you wouldn’t want to store all of your
application’s data here. That small amount of data is replicated across all the nodes using a
fault-tolerant total order broadcast algorithm. As discussed previously, total order broadcast is
just what you need for database replication: if each message represents a write to the database,
applying the same writes in the same order keeps replicas consistent with each other.</p>

<p><a data-type="indexterm" data-primary="Google" data-secondary="Chubby (lock service)" id="idm140417549654160"></a>
ZooKeeper is modeled after Google’s Chubby lock service
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Burrows2006wz">14</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra2007vp">98</a>],
implementing not only total order broadcast (and hence consensus), but also an interesting set of
other features that turn out to be particularly useful when building distributed systems:</p>
<dl>
<dt>Linearizable atomic operations</dt>
<dd>
<p><a data-type="indexterm" data-primary="linearizability" data-secondary="in ZooKeeper" id="idm140417549649600"></a>
<a data-type="indexterm" data-primary="leases" data-secondary="implementation with ZooKeeper" id="idm140417549648272"></a>
<a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-tertiary="implementation with ZooKeeper" id="idm140417549647200"></a>
<a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="implementing locks" id="idm140417549645808"></a>
Using an atomic compare-and-set operation, you can implement a lock: if several nodes concurrently
try to perform the same operation, only one of them will succeed. The consensus protocol
guarantees that the operation will be atomic and linearizable, even if a node fails or the network
is interrupted at any point. A distributed lock is usually implemented as a <em>lease</em>, which has an
expiry time so that it is eventually released in case the client fails (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_pauses">“Process Pauses”</a>).</p>
</dd>
<dt>Total ordering of operations</dt>
<dd>
<p><a data-type="indexterm" data-primary="fencing (preventing split brain)" data-secondary="generating fencing tokens" id="idm140417549641936"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="generating fencing tokens" id="idm140417549640624"></a>
As discussed in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_lock_fencing">“The leader and the lock”</a>, when some resource is protected by a lock or
lease, you need a <em>fencing token</em> to prevent clients from conflicting with each other in the case
of a process pause. The fencing token is some number that monotonically increases every time the
lock is acquired. ZooKeeper provides this by totally ordering all operations and giving each
operation a monotonically increasing transaction ID (<code>zxid</code>) and version number (<code>cversion</code>)
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>].</p>
</dd>
<dt>Failure detection</dt>
<dd>
<p><a data-type="indexterm" data-primary="failures" data-secondary="failure detection" data-tertiary="using ZooKeeper" id="idm140417549635072"></a>
Clients maintain a long-lived session on ZooKeeper servers, and the client and server periodically
exchange heartbeats to check that the other node is still alive. Even if the connection is
temporarily interrupted, or a ZooKeeper node fails, the session remains active. However, if the
heartbeats cease for a duration that is longer than the session timeout, ZooKeeper declares the
session to be dead. Any locks held by a session can be configured to be automatically released
when the session times out (ZooKeeper calls these <em>ephemeral nodes</em>).</p>
</dd>
<dt>Change notifications</dt>
<dd>
<p>Not only can one client read locks and values that were created by another client, but it can also
watch them for changes. Thus, a client can find out when another client joins the cluster (based
on the value it writes to ZooKeeper), or if another client fails (because its session times out
and its ephemeral nodes disappear). By subscribing to notifications, a client avoids having to
frequently poll to find out about changes.</p>
</dd>
</dl>

<p>Of these features, only the linearizable atomic operations really require consensus. However, it is
the combination of these features that makes systems like ZooKeeper so useful for distributed
coordination.</p>










<section data-type="sect3" data-pdf-bookmark="Allocating work to nodes"><div class="sect3" id="idm140417549630112">
<h3>Allocating work to nodes</h3>

<p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="use for partition assignment" id="idm140417549628736"></a>
One example in which the ZooKeeper/Chubby model works well is if you have several instances of a
process or service, and one of them needs to be chosen as leader or primary. If the leader fails,
one of the other nodes should take over. This is of course useful for single-leader databases, but
it’s also useful for job schedulers and similar stateful systems.</p>

<p>Another example arises when you have some partitioned resource (database, message streams, file
storage, distributed actor system, etc.) and need to decide which partition to assign to which node.
As new nodes join the cluster, some of the partitions need to be moved from existing nodes to the
new nodes in order to rebalance the load (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_rebalancing">“Rebalancing Partitions”</a>). As nodes are
removed or fail, other nodes need to take over the failed nodes’ work.</p>

<p><a data-type="indexterm" data-primary="Curator (ZooKeeper recipes)" id="idm140417549625312"></a>
These kinds of tasks can be achieved by judicious use of atomic operations, ephemeral nodes, and
notifications in ZooKeeper. If done correctly, this approach allows the application to automatically
recover from faults without human intervention. It’s not easy, despite the appearance of libraries
such as Apache Curator [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ApacheCurator">17</a>] that have sprung
up to provide higher-level tools on top of the ZooKeeper client API—but it is still much better than
attempting to implement the necessary consensus algorithms from scratch, which has a poor success
record [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kingsbury2015uk-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uk">107</a>].</p>

<p>An application may initially run only on a single node, but eventually may grow to thousands of
nodes. Trying to perform majority votes over so many nodes would be terribly inefficient. Instead,
ZooKeeper runs on a fixed number of nodes (usually three or five) and performs its majority votes
among those nodes while supporting a potentially large number of clients. Thus, ZooKeeper provides a
way of “outsourcing” some of the work of coordinating nodes (consensus, operation ordering, and
failure detection) to an external service.</p>

<p><a data-type="indexterm" data-primary="BookKeeper (replicated log)" id="idm140417549619680"></a>
<a data-type="indexterm" data-primary="Apache BookKeeper" data-see="BookKeeper" id="idm140417549618784"></a>
Normally, the kind of data managed by ZooKeeper is quite slow-changing: it represents information
like “the node running on 10.1.1.23 is the leader for partition 7,” which may change on a timescale
of minutes or hours. It is not intended for storing the runtime state of the application, which may
change thousands or even millions of times per second. If application state needs to be replicated
from one node to another, other tools (such as Apache BookKeeper
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kelly2014lq-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kelly2014lq">108</a>]) can be used.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Service discovery"><div class="sect3" id="idm140417549614944">
<h3>Service discovery</h3>

<p><a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-secondary="service discovery" id="idm140417549613744"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-secondary="service discovery" id="idm140417549612448"></a>
<a data-type="indexterm" data-primary="Consul (service discovery)" id="idm140417549611328"></a>
<a data-type="indexterm" data-primary="service discovery" id="idm140417549610480"></a>
<a data-type="indexterm" data-primary="cloud computing" data-secondary="need for service discovery" id="idm140417549609648"></a>
ZooKeeper, etcd, and Consul are also often used for <em>service discovery</em>—that is, to find out which
IP address you need to connect to in order to reach a particular service. In cloud datacenter
environments, where it is common for virtual machines to continually come and go, you often don’t
know the IP addresses of your services ahead of time. Instead, you can configure your services such
that when they start up they register their network endpoints in a service registry, where they can
then be found by other services.</p>

<p><a data-type="indexterm" data-primary="service discovery" data-secondary="using DNS" id="idm140417549607536"></a>
<a data-type="indexterm" data-primary="DNS (Domain Name System)" id="idm140417549606400"></a>
However, it is less clear whether service discovery actually requires consensus. DNS is the
traditional way of looking up the IP address for a service name, and it uses multiple layers of
caching to achieve good performance and availability. Reads from DNS are absolutely not
linearizable, and it is usually not considered problematic if the results from a DNS query are a
little stale [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fournier2015wt-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fournier2015wt">109</a>].
It is more important that DNS is reliably available and robust to network interruptions.</p>

<p>Although service discovery does not require consensus, leader election does. Thus, if your consensus
system already knows who the leader is, then it can make sense to also use that information to help
other services discover who the leader is. For this purpose, some consensus systems support
read-only caching replicas. These replicas asynchronously receive the log of all decisions of the
consensus algorithm, but do not actively participate in voting. They are therefore able to serve
read requests that do not need to be linearizable.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Membership services"><div class="sect3" id="idm140417549601648">
<h3>Membership services</h3>

<p><a data-type="indexterm" data-primary="membership services" id="idm140417549600416"></a>
<a data-type="indexterm" data-primary="aerospace systems" id="idm140417549599584"></a>
ZooKeeper and friends can be seen as part of a long history of research into <em>membership services</em>,
which goes back to the 1980s and has been important for building highly reliable systems, e.g., for
air traffic control [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Birman2010ct-marker" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Birman2010ct">110</a>].</p>

<p>A membership service determines which nodes are currently active and live members of a cluster. As
we saw throughout <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>, due to unbounded network delays it’s not possible to reliably
detect whether another node has failed. However, if you couple failure detection with consensus,
nodes can come to an agreement about which nodes should be considered alive or not.</p>

<p>It could still happen that a node is incorrectly declared dead by consensus, even though it is
actually alive. But it is nevertheless very useful for a system to have agreement on which nodes
constitute the current membership. For example, choosing a leader could mean simply choosing the
lowest-numbered among the current members, but this approach would not work if different nodes have
divergent opinions on who the current members are.
<a data-type="indexterm" data-primary="consensus" data-secondary="membership and coordination services" data-startref="ix_consensusdtmem" id="idm140417549592736"></a>
<a data-type="indexterm" data-primary="coordination" data-secondary="services" data-startref="ix_coordserv" id="idm140417549591296"></a>
<a data-type="indexterm" data-primary="ZooKeeper (coordination service)" data-startref="ix_zkmembership" id="idm140417549589920"></a>
<a data-type="indexterm" data-primary="etcd (coordination service)" data-startref="ix_etcdmembership" id="idm140417549588800"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm140417550164128">
<h1>Summary</h1>

<p>In this chapter we examined the topics of consistency and consensus from several different angles.
We looked in depth at linearizability, a popular consistency model: its goal is to make replicated
data appear as though there were only a single copy, and to make all operations act on it atomically.
Although linearizability is appealing because it is easy to understand—it makes a database behave
like a variable in a single-threaded program—it has the downside of being slow, especially in
environments with large network delays.</p>

<p>We also explored causality, which imposes an ordering on events in a system (what happened before
what, based on cause and effect). Unlike linearizability, which puts all operations in a single,
totally ordered timeline, causality provides us with a weaker consistency model: some things can be
concurrent, so the version history is like a timeline with branching and merging. Causal consistency
does not have the coordination overhead of linearizability and is much less sensitive to network
problems.</p>

<p>However, even if we capture the causal ordering (for example using Lamport timestamps), we saw that
some things cannot be implemented this way: in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_unique_constraint">“Timestamp ordering is not sufficient”</a> we considered
the example of ensuring that a username is unique and rejecting concurrent registrations for the
same username. If one node is going to accept a registration, it needs to somehow know that another
node isn’t concurrently in the process of registering the same name. This problem led us toward
<em>consensus</em>.</p>

<p>We saw that achieving consensus means deciding something in such a way that all nodes agree on what was
decided, and such that the decision is irrevocable. With some digging, it turns out that a wide
range of problems are actually reducible to consensus and are equivalent to each other (in the sense
that if you have a solution for one of them, you can easily transform it into a solution for one of
the others). Such equivalent problems include:</p>
<dl>
<dt>Linearizable compare-and-set registers</dt>
<dd>
<p><a data-type="indexterm" data-primary="compare-and-set operations" data-secondary="relation to consensus" id="idm140417549580080"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to compare-and-set" id="idm140417549579200"></a>
The register needs to atomically <em>decide</em> whether to set its value, based on whether its current
value equals the parameter given in the operation.</p>
</dd>
<dt>Atomic transaction commit</dt>
<dd>
<p>A database must <em>decide</em> whether to commit or abort a distributed transaction.</p>
</dd>
<dt>Total order broadcast</dt>
<dd>
<p>The messaging system must <em>decide</em> on the order in which to deliver messages.</p>
</dd>
<dt>Locks and leases</dt>
<dd>
<p><a data-type="indexterm" data-primary="locks" data-secondary="distributed locking" data-tertiary="relation to consensus" id="idm140417549572624"></a>
When several clients are racing to grab a lock or lease, the lock <em>decides</em> which one successfully
acquired it.</p>
</dd>
<dt>Membership/coordination service</dt>
<dd>
<p>Given a failure detector (e.g., timeouts), the system must <em>decide</em> which nodes are alive, and
which should be considered dead because their sessions timed out.</p>
</dd>
<dt>Uniqueness constraint</dt>
<dd>
<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="relation to consensus" id="idm140417549567616"></a>
When several transactions concurrently try to create conflicting records with the same key, the
constraint must <em>decide</em> which one to allow and which should fail with a constraint violation.</p>
</dd>
</dl>

<p>All of these are straightforward if you only have a single node, or if you are willing to assign the
decision-making capability to a single node. This is what happens in a single-leader database: all
the power to make decisions is vested in the leader, which is why such databases are able to provide
linearizable operations, uniqueness constraints, a totally ordered replication log, and more.</p>

<p>However, if that single leader fails, or if a network interruption makes the leader unreachable,
such a system becomes unable to make any progress. There are three ways of handling that situation:</p>
<ol>
<li>
<p>Wait for the leader to recover, and accept that the system will be blocked in the meantime. Many
XA/JTA transaction coordinators choose this option. This approach does not fully solve consensus
because it does not satisfy the termination property: if the leader does not recover, the system
can be blocked forever.</p>
</li>
<li>
<p>Manually fail over by getting humans to choose a new leader node and reconfigure the system to use
it. Many relational databases take this approach. It is a kind of consensus by “act of God”—the
human operator, outside of the computer system, makes the decision. The speed of failover is
limited by the speed at which humans can act, which is generally slower than computers.</p>
</li>
<li>
<p>Use an algorithm to automatically choose a new leader. This approach requires a consensus
algorithm, and it is advisable to use a proven algorithm that correctly handles adverse network
conditions [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uk">107</a>].</p>
</li>

</ol>

<p>Although a single-leader database can provide linearizability without executing a consensus
algorithm on every write, it still requires consensus to maintain its leadership and for leadership
changes. Thus, in some sense, having a leader only “kicks the can down the road”: consensus is still
required, only in a different place, and less frequently. The good news is that fault-tolerant
algorithms and systems for consensus exist, and we briefly discussed them in this chapter.</p>

<p>Tools like ZooKeeper play an important role in providing an “outsourced” consensus, failure
detection, and membership service that applications can use. It’s not easy to use, but it is much
better than trying to develop your own algorithms that can withstand all the problems discussed in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>. If you find yourself wanting to do one of those things that is reducible to
consensus, and you want it to be fault-tolerant, then it is advisable to use something like
ZooKeeper.</p>

<p>Nevertheless, not every system necessarily requires consensus: for example, leaderless and
multi-leader replication systems typically do not use global consensus. The conflicts that occur in
these systems (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_write_conflicts">“Handling Write Conflicts”</a>) are a consequence of not having consensus
across different leaders, but maybe that’s okay: maybe we simply need to cope without
linearizability and learn to work better with data that has branching and merging version histories.</p>

<p>This chapter referenced a large body of research on the theory of distributed systems. Although the
theoretical papers and proofs are not always easy to understand, and sometimes make unrealistic
assumptions, they are incredibly valuable for informing practical work in this field: they help us
reason about what can and cannot be done, and help us find the counterintuitive ways in which
distributed systems are often flawed. If you have the time, the references are well worth exploring.
<a data-type="indexterm" data-primary="consensus" data-secondary="distributed transactions" data-startref="ix_consensusdt" id="idm140417549554416"></a>
<a data-type="indexterm" data-primary="consensus" data-startref="ix_consensusdtfault" id="idm140417549553024"></a></p>

<p>This brings us to the end of <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/part02.html#part_distributed_data">Part&nbsp;II</a> of this book, in which we covered replication
(<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>), partitioning (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#ch_partitioning">Chapter&nbsp;6</a>), transactions (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>),
distributed system failure models (<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#ch_distributed">Chapter&nbsp;8</a>), and finally consistency and consensus
(<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>). Now that we have laid a firm foundation of theory, in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/part03.html#part_systems">Part&nbsp;III</a> we will
turn once again to more practical systems, and discuss how to build powerful applications from
heterogeneous building blocks.</p>
</div></section>







<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550867248"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550867248-marker" class="totri-footnote">i</a></sup> A
subtle detail of this diagram is that it assumes the existence of a global clock, represented by the
horizontal axis. Even though real systems typically don’t have accurate clocks (see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks">“Unreliable Clocks”</a>), this assumption is okay: for the purposes of
analyzing a distributed algorithm, we may pretend that an accurate global clock exists, as long as
the algorithm doesn’t have access to it [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1994gw">47</a>].
Instead, the algorithm can only see a mangled approximation of real time, as produced by a quartz
oscillator and NTP.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550847280"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550847280-marker">ii</a></sup> A register in which reads may return
either the old or the new value if they are concurrent with a write is known as a <em>regular
register</em> [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1986cg" class="totri-footnote">7</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550750080"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550750080-marker">iii</a></sup> Strictly
speaking, ZooKeeper and etcd provide linearizable writes, but reads may be stale, since by default
they can be served by any one of the replicas. You can optionally request a linearizable read: etcd
calls this a <em>quorum read</em> [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Etcd">16</a>], and in
ZooKeeper you need to call <code>sync()</code> before the read
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9">15</a>]; see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_abcast_to_lin">“Implementing linearizable storage using total order broadcast”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550694960"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550694960-marker">iv</a></sup> Partitioning (sharding) a
single-leader database, so that there is a separate leader per partition, does not affect
linearizability, since it is only a single-object guarantee. Cross-partition transactions are a
different matter (see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus">“Distributed Transactions and Consensus”</a>).</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550597248"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550597248-marker" class="totri-footnote">v</a></sup> These two
choices are sometimes known as CP (consistent but not available under network partitions) and AP
(available but not consistent under network partitions), respectively. However, this classification
scheme has several flaws [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015un" class="totri-footnote">9</a>], so it is
best avoided.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550547136"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550547136-marker">vi</a></sup> As discussed in
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_network_faults">“Network Faults in Practice”</a>, this book uses <em>partitioning</em>
to refer to deliberately breaking down a large dataset into smaller ones (<em>sharding</em>; see
<a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#ch_partitioning">Chapter&nbsp;6</a>). By contrast, a network partition is a particular
type of network fault, which we normally don’t consider separately from other kinds of faults.
However, since it’s the P in CAP, we can’t avoid the confusion in this case.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550365168"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550365168-marker">vii</a></sup> A total order that
is <em>inconsistent</em> with causality is easy to create, but not very useful. For example, you can
generate a random UUID for each operation, and compare UUIDs lexicographically to define the total
ordering of operations. This is a valid total order, but the random UUIDs tell you nothing about
which operation actually happened first, or whether the operations were concurrent.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550343712"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550343712-marker">viii</a></sup> It is possible to make physical
clock timestamps consistent with causality: in <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_spanner">“Synchronized clocks for global snapshots”</a>
we discussed Google’s Spanner, which estimates the expected clock skew and waits out the
uncertainty interval before committing a write. This method ensures that a causally later transaction is
given a greater timestamp. However, most clocks cannot provide the required uncertainty
metric.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550286352"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550286352-marker">ix</a></sup> The
term <em>atomic broadcast</em> is traditional, but it is very confusing as it’s inconsistent with
other uses of the word <em>atomic</em>: it has nothing to do with atomicity in ACID transactions
and is only indirectly related to atomic operations (in the sense of multi-threaded programming) or
atomic registers (linearizable storage). The term <em>total order multicast</em> is another
synonym.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550240192"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550240192-marker" class="totri-footnote">x</a></sup> In a
formal sense, a linearizable read-write register is an “easier” problem. Total order broadcast is
equivalent to consensus [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp">67</a>], which has no
deterministic solution in the asynchronous crash-stop model
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fischer1985ji">68</a>], whereas a linearizable read-write
register <em>can</em> be implemented in the same system model
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1995bm">23</a>, <a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lynch1997gr">24</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt">25</a>]. However, supporting atomic operations such as
compare-and-set or increment-and-get in a register makes it equivalent to consensus
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1991gk">28</a>]. Thus,
the problems of consensus and a linearizable register are closely related.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550218432"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550218432-marker">xi</a></sup> If you don’t wait, but acknowledge the
write immediately after it has been enqueued, you get something similar to the memory consistency model
of multi-core x86 processors [<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Sewell2010fj">43</a>]. That
model is neither linearizable nor sequentially consistent.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417550145296"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417550145296-marker">xii</a></sup> Atomic commit is
formalized slightly differently from consensus: an atomic transaction can commit only if
<em>all</em> participants vote to commit, and must abort if any participant needs to abort.
Consensus is allowed to decide on <em>any</em> value that is proposed by one of the participants.
However, atomic commit and consensus are reducible to each other
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray2006cu">70</a>,
<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Guerraoui1995bi">71</a>]. <em>Nonblocking</em> atomic commit is
harder than consensus—see <a data-type="xref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_3pc">“Three-phase commit”</a>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417549811840"><sup><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#idm140417549811840-marker">xiii</a></sup> This
particular variant of consensus is called <em>uniform consensus</em>, which is equivalent to regular
consensus in asynchronous systems with unreliable failure detectors
[<a data-type="noteref" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Guerraoui1995bi">71</a>]. The academic literature usually
refers to <em>processes</em> rather than <em>nodes</em>, but we use <em>nodes</em> here for
consistency with the rest of this book.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2013jc_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2013jc_ch9-marker" class="totri-footnote">1</a>] Peter Bailis and Ali Ghodsi:
“<a href="http://queue.acm.org/detail.cfm?id=2462076">Eventual Consistency Today: Limitations,
Extensions, and Beyond</a>,” <em>ACM Queue</em>, volume 11, number 3, pages 55-63, March 2013.
<a href="http://dx.doi.org/10.1145/2460276.2462076">doi:10.1145/2460276.2462076</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mahajan2011wz">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mahajan2011wz-marker" class="totri-footnote">2</a>] Prince Mahajan, Lorenzo Alvisi, and Mike Dahlin:
“<a href="http://apps.cs.utexas.edu/tech_reports/reports/tr/TR-2036.pdf">Consistency,
Availability, and Convergence</a>,” University of Texas at Austin, Department of Computer
Science, Tech Report UTCS TR-11-22, May 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Scotti2015uc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Scotti2015uc-marker" class="totri-footnote">3</a>] Alex Scotti:
“<a href="http://www.slideshare.net/AlexScotti1/allyourbase-55212398">Adventures in Building Your
Own Database</a>,” at <em>All Your Base</em>, November 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014vc_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014vc_ch9-marker" class="totri-footnote">4</a>] Peter Bailis, Aaron Davidson, Alan Fekete, et al.:
“<a href="http://arxiv.org/pdf/1302.0309.pdf">Highly Available Transactions: Virtues and
Limitations</a>,” at <em>40th International Conference on Very Large Data Bases</em> (VLDB),
September 2014. Extended version published as pre-print arXiv:1302.0309 [cs.DB].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Viotti2016wr">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Viotti2016wr-marker" class="totri-footnote">5</a>] Paolo Viotti and Marko Vukolić:
“<a href="http://arxiv.org/abs/1512.00168">Consistency in Non-Transactional Distributed Storage
Systems</a>,” arXiv:1512.00168, 12 April 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Herlihy1990jq">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1990jq-marker" class="totri-footnote">6</a>] Maurice P. Herlihy and Jeannette M. Wing:
“<a href="http://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf">Linearizability:
A Correctness Condition for Concurrent Objects</a>,” <em>ACM Transactions on Programming
Languages and Systems</em> (TOPLAS), volume 12, number 3, pages 463–492, July 1990.
<a href="http://dx.doi.org/10.1145/78969.78972">doi:10.1145/78969.78972</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1986cg">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1986cg-marker" class="totri-footnote">7</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/interprocess.pdf">On
interprocess communication</a>,” <em>Distributed Computing</em>, volume 1, number 2, pages 77–101,
June 1986. <a href="http://dx.doi.org/10.1007/BF01786228">doi:10.1007/BF01786228</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gifford1981tu">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gifford1981tu-marker" class="totri-footnote">8</a>] David K. Gifford:
“<a href="http://www.mirrorservice.org/sites/www.bitsavers.org/pdf/xerox/parc/techReports/CSL-81-8_Information_Storage_in_a_Decentralized_Computer_System.pdf">Information
Storage in a Decentralized Computer System</a>,” Xerox Palo Alto Research Centers, CSL-81-8, June 1981.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2015un">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015un-marker" class="totri-footnote">9</a>] Martin Kleppmann:
“<a href="http://martin.kleppmann.com/2015/05/11/please-stop-calling-databases-cp-or-ap.html">Please
Stop Calling Databases CP or AP</a>,” <em>martin.kleppmann.com</em>, May 11, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2015uh">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uh-marker">10</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/322-call-me-maybe-mongodb-stale-reads">Call Me Maybe: MongoDB
Stale Reads</a>,” <em>aphyr.com</em>, April 20, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2014tb">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2014tb-marker">11</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/314-computational-techniques-in-knossos">Computational
Techniques in Knossos</a>,” <em>aphyr.com</em>, May 17, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014wz">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014wz-marker">12</a>] Peter Bailis:
“<a href="http://www.bailis.org/blog/linearizability-versus-serializability/">Linearizability
Versus Serializability</a>,” <em>bailis.org</em>, September 24, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bernstein1987va_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bernstein1987va_ch9-marker">13</a>] Philip A. Bernstein, Vassos Hadzilacos, and Nathan Goodman:
<a href="http://research.microsoft.com/en-us/people/philbe/ccontrol.aspx"><em>Concurrency
Control and Recovery in Database Systems</em></a>. Addison-Wesley, 1987. ISBN: 978-0-201-10715-9,
available online at <em>research.microsoft.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Burrows2006wz">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Burrows2006wz-marker">14</a>] Mike Burrows:
“<a href="http://research.google.com/archive/chubby.html">The Chubby Lock Service for
Loosely-Coupled Distributed Systems</a>,” at <em>7th USENIX Symposium on Operating System
Design and Implementation</em> (OSDI), November 2006.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2013wi_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2013wi_ch9-marker">15</a>] Flavio P. Junqueira and Benjamin Reed:
<em>ZooKeeper: Distributed Process Coordination</em>. O’Reilly Media, 2013.
ISBN: 978-1-449-36130-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Etcd">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Etcd-marker">16</a>] “<a href="https://coreos.com/etcd/docs/2.0.12/">etcd 2.0.12
Documentation</a>,” CoreOS, Inc., 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ApacheCurator">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ApacheCurator-marker">17</a>] “<a href="http://curator.apache.org/">Apache
Curator</a>,” Apache Software Foundation, <em>curator.apache.org</em>, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Vallath2006ut">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Vallath2006ut-marker">18</a>] Morali Vallath:
<em>Oracle 10g RAC Grid, Services &amp; Clustering</em>. Elsevier Digital Press, 2006.
ISBN: 978-1-555-58321-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014th_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014th_ch9-marker">19</a>] Peter Bailis, Alan Fekete, Michael J Franklin, et al.:
“<a href="http://arxiv.org/pdf/1402.2237.pdf">Coordination-Avoiding Database Systems</a>,”
<em>Proceedings of the VLDB Endowment</em>, volume 8, number 3, pages 185–196, November 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2014vc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2014vc-marker">20</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/316-call-me-maybe-etcd-and-consul">Call Me Maybe: etcd and
Consul</a>,” <em>aphyr.com</em>, June 9, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Junqueira2011jc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Junqueira2011jc-marker">21</a>] Flavio P. Junqueira, Benjamin C. Reed, and Marco Serafini:
“<a href="https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf">Zab: High-Performance Broadcast for
Primary-Backup Systems</a>,” at <em>41st IEEE International Conference on Dependable
Systems and Networks</em> (DSN), June 2011.
<a href="http://dx.doi.org/10.1109/DSN.2011.5958223">doi:10.1109/DSN.2011.5958223</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ongaro2014wq">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ongaro2014wq-marker">22</a>] Diego Ongaro and John K. Ousterhout:
“<a href="http://ramcloud.stanford.edu/raft.pdf">In Search of an Understandable Consensus
Algorithm (Extended Version)</a>,” at <em>USENIX Annual Technical Conference</em>
(ATC), June 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Attiya1995bm">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1995bm-marker">23</a>] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev:
“<a href="http://www.cse.huji.ac.il/course/2004/dist/p124-attiya.pdf">Sharing Memory Robustly in
Message-Passing Systems</a>,” <em>Journal of the ACM</em>, volume 42, number 1, pages 124–142, January 1995.
<a href="http://dx.doi.org/10.1145/200836.200869">doi:10.1145/200836.200869</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lynch1997gr">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lynch1997gr-marker">24</a>] Nancy Lynch and Alex Shvartsman:
“<a href="http://groups.csail.mit.edu/tds/papers/Lynch/FTCS97.pdf">Robust Emulation of Shared Memory
Using Dynamic Quorum-Acknowledged Broadcasts</a>,” at <em>27th Annual International Symposium on
Fault-Tolerant Computing</em> (FTCS), June 1997.
<a href="http://dx.doi.org/10.1109/FTCS.1997.614100">doi:10.1109/FTCS.1997.614100</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cachin2011wt">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cachin2011wt-marker">25</a>] Christian Cachin, Rachid Guerraoui, and Luís Rodrigues:
<a href="http://www.distributedprogramming.net/"><em>Introduction to Reliable and Secure Distributed
Programming</em></a>, 2nd edition. Springer, 2011. ISBN: 978-3-642-15259-7,
<a href="http://dx.doi.org/10.1007/978-3-642-15260-3">doi:10.1007/978-3-642-15260-3</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Elliott2015zg">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Elliott2015zg-marker">26</a>] Sam Elliott, Mark Allen, and Martin Kleppmann:
<a href="https://twitter.com/lenary/status/654761711933648896">personal communication</a>,
thread on <em>twitter.com</em>, October 15, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ekstrom2012ix">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ekstrom2012ix-marker">27</a>] Niklas Ekström, Mikhail Panchenko, and Jonathan Ellis:
“<a href="http://mail-archives.apache.org/mod_mbox/cassandra-dev/201210.mbox/%3CFA480D1DC3964E2C8B0A14E0880094C9%40Robotech%3E">Possible
Issue with Read Repair?</a>,” email thread on <em>cassandra-dev</em> mailing list, October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Herlihy1991gk">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Herlihy1991gk-marker">28</a>] Maurice P. Herlihy:
“<a href="https://cs.brown.edu/~mph/Herlihy91/p124-herlihy.pdf">Wait-Free Synchronization</a>,”
<em>ACM Transactions on Programming Languages and Systems</em> (TOPLAS), volume 13, number 1,
pages 124–149, January 1991.
<a href="http://dx.doi.org/10.1145/114005.102808">doi:10.1145/114005.102808</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fox1999bs">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fox1999bs-marker">29</a>] Armando Fox and Eric A. Brewer:
“<a href="http://radlab.cs.berkeley.edu/people/fox/static/pubs/pdf/c18.pdf">Harvest,
Yield, and Scalable Tolerant Systems</a>,” at <em>7th Workshop on Hot Topics in Operating
Systems</em> (HotOS), March 1999.
<a href="http://dx.doi.org/10.1109/HOTOS.1999.798396">doi:10.1109/HOTOS.1999.798396</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gilbert2002il">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2002il-marker">30</a>] Seth Gilbert and Nancy Lynch:
“<a href="http://www.comp.nus.edu.sg/~gilbert/pubs/BrewersConjecture-SigAct.pdf">Brewer’s Conjecture and
the Feasibility of Consistent, Available, Partition-Tolerant Web Services</a>,”
<em>ACM SIGACT News</em>, volume 33, number 2, pages 51–59, June 2002.
<a href="http://dx.doi.org/10.1145/564585.564601">doi:10.1145/564585.564601</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gilbert2012bf">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gilbert2012bf-marker">31</a>] Seth Gilbert and Nancy Lynch:
“<a href="http://groups.csail.mit.edu/tds/papers/Gilbert/Brewer2.pdf">Perspectives on the CAP
Theorem</a>,” <em>IEEE Computer Magazine</em>, volume 45, number 2, pages 30–36, February 2012.
<a href="http://dx.doi.org/10.1109/MC.2011.389">doi:10.1109/MC.2011.389</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brewer2012ba">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Brewer2012ba-marker">32</a>] Eric A. Brewer:
“<a href="http://cs609.cs.ua.edu/CAP12.pdf">CAP Twelve Years Later: How the ‘Rules’ Have
Changed</a>,” <em>IEEE Computer Magazine</em>, volume 45, number 2, pages 23–29, February 2012.
<a href="http://dx.doi.org/10.1109/MC.2012.37">doi:10.1109/MC.2012.37</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Davidson1985hv">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Davidson1985hv-marker">33</a>] Susan B. Davidson, Hector Garcia-Molina, and Dale Skeen:
“<a href="http://delab.csd.auth.gr/~dimitris/courses/mpc_fall05/papers/invalidation/acm_csur85_partitioned_network_consistency.pdf">Consistency
in Partitioned Networks</a>,” <em>ACM Computing Surveys</em>, volume 17, number 3, pages 341–370, September 1985.
<a href="http://dx.doi.org/10.1145/5505.5508">doi:10.1145/5505.5508</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Johnson1975we">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Johnson1975we-marker">34</a>] Paul R. Johnson and Robert H. Thomas:
“<a href="https://tools.ietf.org/html/rfc677">RFC 677: The Maintenance of Duplicate
Databases</a>,” Network Working Group, January 27, 1975.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lindsay1979wv_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lindsay1979wv_ch9-marker">35</a>] Bruce G. Lindsay, Patricia Griffiths Selinger, C. Galtieri, et al.:
“<a href="http://domino.research.ibm.com/library/cyberdig.nsf/papers/A776EC17FC2FCE73852579F100578964/%24File/RJ2571.pdf">Notes
on Distributed Databases</a>,” IBM Research, Research Report RJ2571(33471), July 1979.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fischer1982hc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fischer1982hc-marker">36</a>] Michael J. Fischer and Alan Michael:
“<a href="http://www.cs.ucsb.edu/~agrawal/spring2011/ugrad/p70-fischer.pdf">Sacrificing
Serializability to Attain High Availability of Data in an Unreliable Network</a>,” at
<em>1st ACM Symposium on Principles of Database Systems</em> (PODS), March 1982.
<a href="http://dx.doi.org/10.1145/588111.588124">doi:10.1145/588111.588124</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brewer2012tr">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Brewer2012tr-marker">37</a>] Eric A. Brewer:
“<a href="http://www.infoq.com/presentations/NoSQL-History">NoSQL: Past, Present, Future</a>,”
at <em>QCon San Francisco</em>, November 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Robinson2010tp">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Robinson2010tp-marker">38</a>] Henry Robinson:
“<a href="http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/">CAP
Confusion: Problems with ‘Partition Tolerance,’</a>” <em>blog.cloudera.com</em>, April 26, 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cockcroft2014wv">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cockcroft2014wv-marker">39</a>] Adrian Cockcroft:
“<a href="http://www.infoq.com/presentations/migration-cloud-native">Migrating to
Microservices</a>,” at <em>QCon London</em>, March 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2015vp">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kleppmann2015vp-marker">40</a>] Martin Kleppmann:
“<a href="http://arxiv.org/abs/1509.05393">A Critique of the CAP Theorem</a>,” arXiv:1509.05393,
September 17, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lynch1989kj">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lynch1989kj-marker">41</a>] Nancy A. Lynch:
“<a href="http://groups.csail.mit.edu/tds/papers/Lynch/podc89.pdf">A Hundred Impossibility Proofs
for Distributed Computing</a>,” at <em>8th ACM Symposium on Principles of Distributed
Computing</em> (PODC), August 1989.
<a href="http://dx.doi.org/10.1145/72981.72982">doi:10.1145/72981.72982</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Attiya2015dm">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya2015dm-marker">42</a>] Hagit Attiya, Faith Ellen, and Adam Morrison:
“<a href="http://www.cs.technion.ac.il/people/mad/online-publications/podc2015-replds.pdf">Limitations
of Highly-Available Eventually-Consistent Data Stores</a>,” at <em>ACM Symposium on Principles of
Distributed Computing</em> (PODC), July 2015.
<a href="http://dx.doi.org/10.1145/2767386.2767419">doi:10.1145/2767386.2767419</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sewell2010fj">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Sewell2010fj-marker">43</a>] Peter Sewell, Susmit Sarkar,
Scott Owens, et al.:
“<a href="http://www.cl.cam.ac.uk/~pes20/weakmemory/cacm.pdf">x86-TSO: A Rigorous and Usable
Programmer’s Model for x86 Multiprocessors</a>,” <em>Communications of the ACM</em>,
volume 53, number 7, pages 89–97, July 2010.
<a href="http://dx.doi.org/10.1145/1785414.1785443">doi:10.1145/1785414.1785443</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thompson2011tr">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Thompson2011tr-marker">44</a>] Martin Thompson:
“<a href="http://mechanical-sympathy.blogspot.co.uk/2011/07/memory-barriersfences.html">Memory
Barriers/Fences</a>,” <em>mechanical-sympathy.blogspot.co.uk</em>, July 24, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Drepper2007wb_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Drepper2007wb_ch9-marker">45</a>] Ulrich Drepper:
“<a href="http://www.akkadia.org/drepper/cpumemory.pdf">What Every Programmer Should Know About
Memory</a>,” <em>akkadia.org</em>, November 21, 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Abadi2012hb">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Abadi2012hb-marker">46</a>] Daniel J. Abadi:
“<a href="http://cs-www.cs.yale.edu/homes/dna/papers/abadi-pacelc.pdf">Consistency Tradeoffs in
Modern Distributed Database System Design</a>,” <em>IEEE Computer Magazine</em>,
volume 45, number 2, pages 37–42, February 2012.
<a href="http://dx.doi.org/10.1109/MC.2012.33">doi:10.1109/MC.2012.33</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Attiya1994gw">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya1994gw-marker">47</a>] Hagit Attiya and Jennifer L. Welch:
“<a href="http://courses.csail.mit.edu/6.852/01/papers/p91-attiya.pdf">Sequential Consistency
Versus Linearizability</a>,” <em>ACM Transactions on Computer Systems</em> (TOCS),
volume 12, number 2, pages 91–122, May 1994.
<a href="http://dx.doi.org/10.1145/176575.176576">doi:10.1145/176575.176576</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ahamad1995gl">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ahamad1995gl-marker">48</a>] Mustaque Ahamad, Gil Neiger, James E. Burns, et al.:
“<a href="http://www-i2.informatik.rwth-aachen.de/i2/fileadmin/user_upload/documents/Seminar_MCMM11/Causal_memory_1996.pdf">Causal
Memory: Definitions, Implementation, and Programming</a>,” <em>Distributed
Computing</em>, volume 9, number 1, pages 37–49, March 1995.
<a href="http://dx.doi.org/10.1007/BF01784241">doi:10.1007/BF01784241</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lloyd2013vf">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lloyd2013vf-marker">49</a>] Wyatt Lloyd, Michael J. Freedman,
Michael Kaminsky, and David G. Andersen:
“<a href="https://www.usenix.org/system/files/conference/nsdi13/nsdi13-final149.pdf">Stronger
Semantics for Low-Latency Geo-Replicated Storage</a>,” at <em>10th USENIX Symposium on Networked
Systems Design and Implementation</em> (NSDI), April 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zawirski2013wc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Zawirski2013wc-marker">50</a>] Marek Zawirski, Annette Bieniusa, Valter Balegas, et al.:
“<a href="http://arxiv.org/abs/1310.3107">SwiftCloud: Fault-Tolerant Geo-Replication Integrated All
the Way to the Client Machine</a>,” INRIA Research Report 8347, August 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2013wl">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2013wl-marker">51</a>] Peter Bailis, Ali Ghodsi, Joseph M Hellerstein, and Ion Stoica:
“<a href="http://db.cs.berkeley.edu/papers/sigmod13-bolton.pdf">Bolt-on Causal Consistency</a>,” at
<em>ACM International Conference on Management of Data</em> (SIGMOD), June 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ajoux2015wh_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ajoux2015wh_ch9-marker">52</a>] Philippe Ajoux, Nathan Bronson, Sanjeev
Kumar, et al.:
“<a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf">Challenges
to Adopting Stronger Consistency at Scale</a>,” at <em>15th USENIX Workshop on Hot Topics in
Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014tn">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Bailis2014tn-marker">53</a>] Peter Bailis:
“<a href="http://www.bailis.org/blog/causality-is-expensive-and-what-to-do-about-it/">Causality
Is Expensive (and What to Do About It)</a>,” <em>bailis.org</em>, February 5, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Goncalves2015ky">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Goncalves2015ky-marker">54</a>] Ricardo Gonçalves, Paulo Sérgio Almeida,
Carlos Baquero, and Victor Fonte:
“<a href="http://haslab.uminho.pt/tome/files/global_logical_clocks.pdf">Concise Server-Wide
Causality Management for Eventually Consistent Data Stores</a>,” at <em>15th IFIP International
Conference on Distributed Applications and Interoperable Systems</em> (DAIS), June 2015.
<a href="http://dx.doi.org/10.1007/978-3-319-19129-4_6">doi:10.1007/978-3-319-19129-4_6</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Conery2014ti">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Conery2014ti-marker">55</a>] Rob Conery:
“<a href="http://rob.conery.io/2014/05/29/a-better-id-generator-for-postgresql/">A Better ID
Generator for PostgreSQL</a>,” <em>rob.conery.io</em>, May 29, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1978jq_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1978jq_ch9-marker">56</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-US/um/people/Lamport/pubs/time-clocks.pdf">Time, Clocks,
and the Ordering of Events in a Distributed System</a>,” <em>Communications of the ACM</em>,
volume 21, number 7, pages 558–565, July 1978.
<a href="http://dx.doi.org/10.1145/359545.359563">doi:10.1145/359545.359563</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Defago2004ji">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Defago2004ji-marker">57</a>] Xavier Défago, André Schiper, and Péter Urbán:
“<a href="https://dspace.jaist.ac.jp/dspace/bitstream/10119/4883/1/defago_et_al.pdf">Total
Order Broadcast and Multicast Algorithms: Taxonomy and Survey</a>,” <em>ACM Computing
Surveys</em>, volume 36, number 4, pages 372–421, December 2004.
<a href="http://dx.doi.org/10.1145/1041680.1041682">doi:10.1145/1041680.1041682</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Attiya2004ke">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Attiya2004ke-marker">58</a>] Hagit Attiya and Jennifer Welch: <em>Distributed
Computing: Fundamentals, Simulations and Advanced Topics</em>, 2nd edition.
John Wiley &amp; Sons, 2004. ISBN: 978-0-471-45324-6,
<a href="http://dx.doi.org/10.1002/0471478210">doi:10.1002/0471478210</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Balakrishnan2012wm">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2012wm-marker">59</a>] Mahesh
Balakrishnan, Dahlia Malkhi, Vijayan Prabhakaran, et al.:
“<a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final30.pdf">CORFU: A
Shared Log Design for Flash Clusters</a>,” at <em>9th USENIX Symposium on Networked
Systems Design and Implementation</em> (NSDI), April 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schneider1990vy">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Schneider1990vy-marker">60</a>] Fred B. Schneider:
“<a href="http://www.cs.cornell.edu/fbs/publications/smsurvey.pdf">Implementing Fault-Tolerant
Services Using the State Machine Approach: A Tutorial</a>,” <em>ACM Computing Surveys</em>, volume
22, number 4, pages 299–319, December 1990.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thomson2012tx">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Thomson2012tx-marker">61</a>] Alexander Thomson, Thaddeus Diamond, Shu-Chun Weng, et al.:
“<a href="http://cs.yale.edu/homes/thomson/publications/calvin-sigmod12.pdf">Calvin: Fast
Distributed Transactions for Partitioned Database Systems</a>,” at <em>ACM International Conference
on Management of Data</em> (SIGMOD), May 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Balakrishnan2013ko">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Balakrishnan2013ko-marker">62</a>] Mahesh Balakrishnan, Dahlia Malkhi, Ted Wobber, et al.:
“<a href="http://research.microsoft.com/pubs/199947/Tango.pdf">Tango: Distributed Data Structures
over a Shared Log</a>,” at <em>24th ACM Symposium on Operating Systems
Principles</em> (SOSP), November 2013.
<a href="http://dx.doi.org/10.1145/2517349.2522732">doi:10.1145/2517349.2522732</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vanRenesse2004td">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2004td-marker">63</a>] Robbert van Renesse and Fred B. Schneider:
“<a href="http://static.usenix.org/legacy/events/osdi04/tech/full_papers/renesse/renesse.pdf">Chain
Replication for Supporting High Throughput and Availability</a>,” at <em>6th USENIX
Symposium on Operating System Design and Implementation</em> (OSDI), December 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1979ky">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1979ky-marker">64</a>] Leslie Lamport:
“<a href="http://research-srv.microsoft.com/en-us/um/people/lamport/pubs/multi.pdf">How to Make a
Multiprocessor Computer That Correctly Executes Multiprocess Programs</a>,” <em>IEEE
Transactions on Computers</em>, volume 28, number 9, pages 690–691, September 1979.
<a href="http://dx.doi.org/10.1109/TC.1979.1675439">doi:10.1109/TC.1979.1675439</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Soztutar2015vj">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Soztutar2015vj-marker">65</a>] Enis Söztutar, Devaraj Das, and Carter Shanklin:
“<a href="http://hortonworks.com/blog/apache-hbase-high-availability-next-level/">Apache HBase
High Availability at the Next Level</a>,” <em>hortonworks.com</em>, January 22, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cooper2008fn">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Cooper2008fn-marker">66</a>] Brian F Cooper, Raghu Ramakrishnan, Utkarsh Srivastava, et al.:
“<a href="http://www.mpi-sws.org/~druschel/courses/ds/papers/cooper-pnuts.pdf">PNUTS: Yahoo!’s
Hosted Data Serving Platform</a>,” at <em>34th International Conference on Very Large Data
Bases</em> (VLDB), August 2008.
<a href="http://dx.doi.org/10.14778/1454159.1454167">doi:10.14778/1454159.1454167</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chandra1996cp">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra1996cp-marker">67</a>] Tushar Deepak Chandra and Sam Toueg:
“<a href="http://courses.csail.mit.edu/6.852/08/papers/CT96-JACM.pdf">Unreliable Failure
Detectors for Reliable Distributed Systems</a>,” <em>Journal of the ACM</em>,
volume 43, number 2, pages 225–267, March 1996.
<a href="http://dx.doi.org/10.1145/226643.226647">doi:10.1145/226643.226647</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fischer1985ji">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fischer1985ji-marker">68</a>] Michael J. Fischer, Nancy Lynch, and Michael S. Paterson:
“<a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">Impossibility of Distributed Consensus with
One Faulty Process</a>,” <em>Journal of the ACM</em>, volume 32, number 2, pages 374–382, April 1985.
<a href="http://dx.doi.org/10.1145/3149.214121">doi:10.1145/3149.214121</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="BenOr1983dh">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#BenOr1983dh-marker">69</a>] Michael Ben-Or: “Another Advantage of Free
Choice: Completely Asynchronous Agreement Protocols,” at <em>2nd ACM Symposium on Principles of
Distributed Computing</em> (PODC), August 1983.
<a href="http://dl.acm.org/citation.cfm?id=806707">doi:10.1145/800221.806707</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray2006cu">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray2006cu-marker">70</a>] Jim N. Gray and Leslie Lamport:
“<a href="http://db.cs.berkeley.edu/cs286/papers/paxoscommit-tods2006.pdf">Consensus on
Transaction Commit</a>,” <em>ACM Transactions on Database Systems</em> (TODS), volume 31,
number 1, pages 133–160, March 2006.
<a href="http://dx.doi.org/10.1145/1132863.1132867">doi:10.1145/1132863.1132867</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Guerraoui1995bi">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Guerraoui1995bi-marker">71</a>] Rachid Guerraoui:
“<a href="https://pdfs.semanticscholar.org/5d06/489503b6f791aa56d2d7942359c2592e44b0.pdf">Revisiting the
Relationship Between Non-Blocking Atomic Commitment and Consensus</a>,” at <em>9th International
Workshop on Distributed Algorithms</em> (WDAG), September 1995.
<a href="http://dx.doi.org/10.1007/BFb0022140">doi:10.1007/BFb0022140</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pillai2014vx_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Pillai2014vx_ch9-marker">72</a>] Thanumalayan Sankaranarayana Pillai, Vijay Chidambaram,
Ramnatthan Alagappan, et al.: “<a href="http://research.cs.wisc.edu/wind/Publications/alice-osdi14.pdf">All
File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications</a>,”
at <em>11th USENIX Symposium on Operating Systems Design and Implementation</em> (OSDI),
October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1981wi_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Gray1981wi_ch9-marker">73</a>] Jim Gray:
“<a href="http://research.microsoft.com/en-us/um/people/gray/papers/theTransactionConcept.pdf">The
Transaction Concept: Virtues and Limitations</a>,” at <em>7th International Conference on
Very Large Data Bases</em> (VLDB), September 1981.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GarciaMolina1987ca_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#GarciaMolina1987ca_ch9-marker">74</a>] Hector Garcia-Molina and Kenneth Salem:
“<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">Sagas</a>,” at
<em>ACM International Conference on Management of Data</em> (SIGMOD), May 1987.
<a href="http://dx.doi.org/10.1145/38713.38742">doi:10.1145/38713.38742</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mohan1986hh">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Mohan1986hh-marker">75</a>] C. Mohan, Bruce G. Lindsay, and Ron Obermarck:
“<a href="https://cs.brown.edu/courses/csci2270/archives/2012/papers/dtxn/p378-mohan.pdf">Transaction
Management in the R* Distributed Database Management System</a>,”
<em>ACM Transactions on Database Systems</em>, volume 11, number 4, pages 378–396, December 1986.
<a href="http://dx.doi.org/10.1145/7239.7266">doi:10.1145/7239.7266</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="XASpec1991vk">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#XASpec1991vk-marker">76</a>] “<a href="http://pubs.opengroup.org/onlinepubs/009680699/toc.pdf">Distributed
Transaction Processing: The XA Specification</a>,” X/Open Company Ltd., Technical Standard
XO/CAE/91/300, December 1991. ISBN: 978-1-872-63024-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Spille2004vr">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004vr-marker">77</a>] Mike Spille:
“<a href="http://www.jroller.com/pyrasun/entry/xa_exposed_part_ii_schwartz">XA Exposed, Part II</a>,”
<em>jroller.com</em>, April 3, 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Neto2008be">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Neto2008be-marker">78</a>] Ivan Silva Neto and Francisco Reverbel:
“<a href="http://www.ime.usp.br/~reverbel/papers/icis2008.pdf">Lessons Learned from Implementing
WS-Coordination and WS-AtomicTransaction</a>,” at <em>7th IEEE/ACIS International Conference on
Computer and Information Science</em> (ICIS), May 2008.
<a href="http://dx.doi.org/10.1109/ICIS.2008.75">doi:10.1109/ICIS.2008.75</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Johnson2004hl">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Johnson2004hl-marker">79</a>] James E. Johnson, David E. Langworthy, Leslie Lamport,
and Friedrich H. Vogt:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/wsfm-web.pdf">Formal
Specification of a Web Services Protocol</a>,” at <em>1st International Workshop on Web Services and
Formal Methods</em> (WS-FM), February 2004.
<a href="http://dx.doi.org/10.1016/j.entcs.2004.02.022">doi:10.1016/j.entcs.2004.02.022</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Skeen1981jc">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Skeen1981jc-marker">80</a>] Dale Skeen:
“<a href="http://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/Ske81.pdf">Nonblocking Commit
Protocols</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), April 1981.
<a href="http://dx.doi.org/10.1145/582318.582339">doi:10.1145/582318.582339</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hohpe2005hn">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Hohpe2005hn-marker">81</a>] Gregor Hohpe:
“<a href="http://www.martinfowler.com/ieeeSoftware/coffeeShop.pdf">Your Coffee Shop Doesn’t Use
Two-Phase Commit</a>,” <em>IEEE Software</em>, volume 22, number 2, pages 64–66, March 2005.
<a href="http://dx.doi.org/10.1109/MS.2005.52">doi:10.1109/MS.2005.52</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Helland2007td_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Helland2007td_ch9-marker">82</a>] Pat Helland:
“<a href="http://www-db.cs.wisc.edu/cidr/cidr2007/papers/cidr07p15.pdf">Life Beyond Distributed
Transactions: An Apostate’s Opinion</a>,” at <em>3rd Biennial Conference on Innovative Data Systems
Research</em> (CIDR), January 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oliver2011wt">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Oliver2011wt-marker">83</a>] Jonathan Oliver:
“<a href="http://blog.jonathanoliver.com/my-beef-with-msdtc-and-two-phase-commits/">My Beef with
MSDTC and Two-Phase Commits</a>,” <em>blog.jonathanoliver.com</em>, April 4, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rahien2014uz">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Rahien2014uz-marker">84</a>] Oren Eini (Ahende Rahien):
“<a href="http://ayende.com/blog/167362/the-fallacy-of-distributed-transactions">The Fallacy of
Distributed Transactions</a>,” <em>ayende.com</em>, July 17, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Vasters2012wa">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Vasters2012wa-marker">85</a>] Clemens Vasters:
“<a href="https://blogs.msdn.microsoft.com/clemensv/2012/07/30/transactions-in-windows-azure-with-service-bus-an-email-discussion/">Transactions
in Windows Azure (with Service Bus) – An Email Discussion</a>,” <em>vasters.com</em>, July 30, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="NServiceBus2015tf">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#NServiceBus2015tf-marker">86</a>] “<a href="https://docs.particular.net/nservicebus/azure/understanding-transactionality-in-azure">Understanding Transactionality in Azure</a>,” NServiceBus Documentation, Particular Software, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wigginton2013vk">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Wigginton2013vk-marker">87</a>] Randy Wigginton, Ryan Lowe,
Marcos Albe, and Fernando Ipar:
“<a href="https://www.percona.com/live/mysql-conference-2013/sites/default/files/slides/XA_final.pdf">Distributed
Transactions in MySQL</a>,” at <em>MySQL Conference and Expo</em>, April 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Spille2004ur">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Spille2004ur-marker">88</a>] Mike Spille:
“<a href="http://www.jroller.com/pyrasun/entry/xa_exposed">XA Exposed, Part I</a>,”
<em>jroller.com</em>, April 3, 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dhariwal2008vq">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Dhariwal2008vq-marker">89</a>] Ajmer Dhariwal:
“<a href="http://www.eraofdata.com/orphaned-msdtc-transactions-2-spids/">Orphaned MSDTC Transactions
(-2 spids)</a>,” <em>eraofdata.com</em>, December 12, 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Randal2013wu">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Randal2013wu-marker">90</a>] Paul Randal:
“<a href="http://www.sqlskills.com/blogs/paul/real-world-story-of-dbcc-page-saving-the-day/">Real
World Story of DBCC PAGE Saving the Day</a>,” <em>sqlskills.com</em>, June 19, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="SQLServerInDoubt">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#SQLServerInDoubt-marker">91</a>] “<a href="https://msdn.microsoft.com/en-us/library/ms179586.aspx">in-doubt
xact resolution Server Configuration Option</a>,” SQL Server 2016 documentation, Microsoft, Inc.,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dwork1988dr_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Dwork1988dr_ch9-marker">92</a>] Cynthia Dwork, Nancy Lynch, and Larry Stockmeyer:
“<a href="http://www.net.t-labs.tu-berlin.de/~petr/ADC-07/papers/DLS88.pdf">Consensus in the
Presence of Partial Synchrony</a>,” <em>Journal of the ACM</em>, volume 35, number 2, pages 288–323,
April 1988. <a href="http://dx.doi.org/10.1145/42282.42283">doi:10.1145/42282.42283</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Castro2002ej">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Castro2002ej-marker">93</a>] Miguel Castro and Barbara H. Liskov:
“<a href="http://zoo.cs.yale.edu/classes/cs426/2012/bib/castro02practical.pdf">Practical Byzantine
Fault Tolerance and Proactive Recovery</a>,” <em>ACM Transactions on Computer Systems</em>,
volume 20, number 4, pages 396–461, November 2002.
<a href="http://dx.doi.org/10.1145/571637.571640">doi:10.1145/571637.571640</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Oki1988ci">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Oki1988ci-marker">94</a>] Brian M. Oki and Barbara H. Liskov:
“<a href="http://www.cs.princeton.edu/courses/archive/fall11/cos518/papers/viewstamped.pdf">Viewstamped
Replication: A New Primary Copy Method to Support Highly-Available Distributed Systems</a>,” at
<em>7th ACM Symposium on Principles of Distributed Computing</em> (PODC), August 1988.
<a href="http://dx.doi.org/10.1145/62546.62549">doi:10.1145/62546.62549</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Liskov2012ut">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Liskov2012ut-marker">95</a>] Barbara H. Liskov and James Cowling:
“<a href="http://pmg.csail.mit.edu/papers/vr-revisited.pdf">Viewstamped Replication Revisited</a>,”
Massachusetts Institute of Technology, Tech Report MIT-CSAIL-TR-2012-021, July 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport1998ea">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport1998ea-marker">96</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/lamport-paxos.pdf">The
Part-Time Parliament</a>,” <em>ACM Transactions on Computer Systems</em>, volume 16, number 2,
pages 133–169, May 1998.
<a href="http://dx.doi.org/10.1145/279227.279229">doi:10.1145/279227.279229</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lamport2001ud">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Lamport2001ud-marker">97</a>] Leslie Lamport:
“<a href="http://research.microsoft.com/en-us/um/people/lamport/pubs/paxos-simple.pdf">Paxos Made
Simple</a>,” <em>ACM SIGACT News</em>, volume 32, number 4, pages 51–58, December 2001.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chandra2007vp">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Chandra2007vp-marker">98</a>] Tushar Deepak Chandra, Robert Griesemer, and Joshua
Redstone: “<a href="http://www.read.seas.harvard.edu/~kohler/class/08w-dsi/chandra07paxos.pdf">Paxos
Made Live – An Engineering Perspective</a>,” at <em>26th ACM Symposium on Principles of Distributed
Computing</em> (PODC), June 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vanRenesse2011wu">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2011wu-marker">99</a>] Robbert
van Renesse: “<a href="http://www.cs.cornell.edu/home/rvr/Paxos/paxos.pdf">Paxos Made Moderately
Complex</a>,” <em>cs.cornell.edu</em>, March 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ongaro2014wk">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Ongaro2014wk-marker">100</a>] Diego Ongaro:
“<a href="https://github.com/ongardie/dissertation">Consensus: Bridging Theory and Practice</a>,”
PhD Thesis, Stanford University, August 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Howard2015ko">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2015ko-marker">101</a>] Heidi Howard, Malte Schwarzkopf, Anil Madhavapeddy,
and Jon Crowcroft: “<a href="http://www.cl.cam.ac.uk/~ms705/pub/papers/2015-osr-raft.pdf">Raft
Refloated: Do We Have Consensus?</a>,” <em>ACM SIGOPS Operating Systems Review</em>, volume 49,
number 1, pages 12–21, January 2015.
<a href="http://dx.doi.org/10.1145/2723872.2723876">doi:10.1145/2723872.2723876</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Medeiros2012ur">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Medeiros2012ur-marker">102</a>] André Medeiros:
“<a href="http://www.tcs.hut.fi/Studies/T-79.5001/reports/2012-deSouzaMedeiros.pdf">ZooKeeper’s Atomic
Broadcast Protocol: Theory and Practice</a>,” Aalto University School of Science, March 20, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="vanRenesse2014dj">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#vanRenesse2014dj-marker">103</a>] Robbert van Renesse, Nicolas Schiper, and
Fred B. Schneider: “<a href="http://arxiv.org/abs/1309.5671">Vive La Différence: Paxos vs.
Viewstamped Replication vs. Zab</a>,” <em>IEEE Transactions on Dependable and Secure Computing</em>,
volume 12, number 4, pages 472–484, September 2014.
<a href="http://dx.doi.org/10.1109/TDSC.2014.2355848">doi:10.1109/TDSC.2014.2355848</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Portnoy2012vs">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Portnoy2012vs-marker">104</a>] Will
Portnoy: “<a href="http://blog.willportnoy.com/2012/06/lessons-learned-from-paxos.html">Lessons
Learned from Implementing Paxos</a>,” <em>blog.willportnoy.com</em>, June 14, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Howard2016tz_ch9">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2016tz_ch9-marker">105</a>] Heidi Howard, Dahlia Malkhi, and Alexander Spiegelman:
“<a href="https://arxiv.org/abs/1608.06696">Flexible Paxos: Quorum Intersection Revisited</a>,”
<em>arXiv:1608.06696</em>, August 24, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Howard2015cw">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Howard2015cw-marker">106</a>] Heidi Howard and Jon Crowcroft:
“<a href="http://www.sigcomm.org/sites/default/files/ccr/papers/2015/August/2829988-2790010.pdf">Coracle:
Evaluating Consensus at the Internet Edge</a>,” at <em>Annual Conference of the ACM Special Interest
Group on Data Communication</em> (SIGCOMM), August 2015.
<a href="http://dx.doi.org/10.1145/2829988.2790010">doi:10.1145/2829988.2790010</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kingsbury2015uk">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kingsbury2015uk-marker">107</a>] Kyle Kingsbury:
“<a href="https://aphyr.com/posts/323-call-me-maybe-elasticsearch-1-5-0">Call Me Maybe:
Elasticsearch 1.5.0</a>,” <em>aphyr.com</em>, April 27, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kelly2014lq">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Kelly2014lq-marker">108</a>] Ivan Kelly:
“<a href="https://github.com/ivankelly/bookkeeper-tutorial">BookKeeper Tutorial</a>,”
<em>github.com</em>, October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fournier2015wt">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Fournier2015wt-marker">109</a>] Camille Fournier:
“<a href="http://www.ustream.tv/recorded/61483409">Consensus Systems for the Skeptical
Architect</a>,” at <em>Craft Conference</em>, Budapest, Hungary, April 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Birman2010ct">[<a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#Birman2010ct-marker">110</a>] Kenneth P. Birman:
“<a href="https://www.truststc.org/pubs/713.html">A History of the Virtual Synchrony Replication
Model</a>,” in <em>Replication: Theory and Practice</em>, Springer LNCS volume 5959, chapter 6,
pages 91–120, 2010. ISBN: 978-3-642-11293-5,
<a href="http://dx.doi.org/10.1007/978-3-642-11294-2_6">doi:10.1007/978-3-642-11294-2_6</a></p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. The Trouble with Distributed Systems</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/part03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">III. Derived Data</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 69px;">
    <a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/preferences/">Settings</a></li>
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    
    
      <img src="https://www.oreilly.com/library/view/oreilly_set_cookie/" alt="" style="display:none;">
    
    
    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.006px; left: 1356px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/safari-books-archive/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#">Reset</a>
</div>
</div>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5084659072603079"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.44105579625524416" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=3cddeeec-61c8-7fc6-2474-72ca6b3bc237&amp;pi=1200101525&amp;lg=en-US&amp;sw=1440&amp;sh=900&amp;sc=24&amp;tl=9.%20Consistency%20and%20Consensus%20-%20Designing%20Data-Intensive%20Applications&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdesigning-data-intensive-applications%2F9781491903063%2Fch09.html&amp;r=&amp;lt=31795&amp;evt=pageLoad&amp;msclkid=N&amp;rn=234820"></div></body></html>