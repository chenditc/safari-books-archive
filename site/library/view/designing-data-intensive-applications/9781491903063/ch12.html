<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/designing-data-intensive-applications/9781491903063/ch12.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3905629"
  data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99"
  data-username="safaribooksonline122"
  data-account-type="Trial"
  
  data-activated-trial-date="12/09/2018"


  data-archive="9781491903063"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch12.html"
  data-epub-title="Designing Data-Intensive Applications" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/designing-data-intensive-applications/9781491903063/ch12.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3905629" data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99" data-username="safaribooksonline122" data-account-type="Trial" data-activated-trial-date="12/09/2018" data-archive="9781491903063" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch12.html" data-epub-title="Designing Data-Intensive Applications" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491903063"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>12. The Future of Data Systems - Designing Data-Intensive Applications</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:1.5em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.map-ebook{page-break-after:always}
    </style><link rel="canonical" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html"><meta name="description" content=" Chapter 12. The Future of Data Systems If a thing be ordained to another as to its end, its last end cannot consist in the preservation of its being. Hence ... "><meta property="og:title" content="12. The Future of Data Systems"><meta itemprop="isPartOf" content="/library/view/designing-data-intensive-applications/9781491903063/"><meta itemprop="name" content="12. The Future of Data Systems"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch12.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491903063/"><meta property="og:description" itemprop="description" content=" Chapter 12. The Future of Data Systems If a thing be ordained to another as to its end, its last end cannot consist in the preservation of its being. Hence ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449373320"><meta property="og:book:author" itemprop="author" content="Martin Kleppmann"><meta property="og:book:tag" itemprop="about" content="Core Programming"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/f04af719-1c84-4fc3-9be3-1f1b4622ab99/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Designing Data-Intensive Applications
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781491903063/chapter/ch12.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch12.html&amp;text=Designing%20Data-Intensive%20Applications&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch12.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch12.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%2012.%20The%20Future%20of%20Data%20Systems&amp;body=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch12.html%0D%0Afrom%20Designing%20Data-Intensive%20Applications%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">11. Stream Processing</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/glossary01.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Glossary</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 12. The Future of Data Systems"><div class="chapter" id="ch_future">
<h1><span class="label">Chapter 12. </span>The Future of Data Systems</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
  <p><em>If a thing be ordained to another as to its end, its last end cannot consist in the preservation of
its being. Hence a captain does not intend as a last end, the preservation of the ship entrusted to
him, since a ship is ordained to something else as its end, viz. to navigation.</em></p>
  <p><em>(Often quoted as: If the highest aim of a captain was the preserve his ship, he would keep it in
port forever.)</em></p>
  <p data-type="attribution">St. Thomas Aquinas, <em>Summa Theologica</em> (1265–1274)</p>
</blockquote>

<div class="map-ebook">
 <img id="c278" src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ch12-map-ebook.png" width="2756" height="2100" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ch12-map-ebook.png">
</div>

<p><a data-type="indexterm" data-primary="data systems" data-secondary="future of" id="ix_datasysfut"></a>
So far, this book has been mostly about describing things as they <em>are</em> at present. In this final
chapter, we will shift our perspective toward the future and discuss how things <em>should be</em>: I will
propose some ideas and approaches that, I believe, may fundamentally improve the ways we design and
build applications.</p>

<p>Opinions and speculation about the future are of course subjective, and so I will use the first
person in this chapter when writing about my personal opinions. You are welcome to disagree with
them and form your own opinions, but I hope that the ideas in this chapter will at least be a
starting point for a productive discussion and bring some clarity to concepts that are often
confused.</p>

<p><a data-type="indexterm" data-primary="reliability" id="idm140417546928080"></a><a data-type="indexterm" data-primary="scalability" id="idm140417546927152"></a><a data-type="indexterm" data-primary="maintainability" id="idm140417546926480"></a>
The goal of this book was outlined in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch01.html#ch_introduction">Chapter&nbsp;1</a>: to explore how to create applications and
systems that are <em>reliable</em>, <em>scalable</em>, and <em>maintainable</em>. These themes have run through all of
the chapters: for example, we discussed many fault-tolerance algorithms that help improve
reliability, partitioning to improve scalability, and mechanisms for evolution and abstraction that
improve maintainability. In this chapter we will bring all of these ideas together, and build on
them to envisage the future. Our goal is to discover how to design applications that are better than
the ones of today—robust, correct, evolvable, and ultimately beneficial to humanity.</p>






<section data-type="sect1" data-pdf-bookmark="Data Integration"><div class="sect1" id="sec_future_integration">
<h1>Data Integration</h1>

<p><a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="data integration" id="ix_datasysfutdi"></a>
<a data-type="indexterm" data-primary="data integration" id="ix_dataint"></a>
A recurring theme in this book has been that for any given problem, there are several solutions, all
of which have different pros, cons, and trade-offs. For example, when discussing storage engines in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#ch_storage">Chapter&nbsp;3</a>, we saw log-structured storage, B-trees, and column-oriented storage. When discussing
replication in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>, we saw single-leader, multi-leader, and leaderless approaches.</p>

<p>If you have a problem such as “I want to store some data and look it up again later,” there is no
one right solution, but many different approaches that are each appropriate in different
circumstances. A software implementation typically has to pick one particular approach. It’s hard
enough to get one code path robust and performing well—trying to do everything in one piece of
software almost guarantees that the implementation will be poor.</p>

<p>Thus, the most appropriate choice of software tool also depends on the circumstances. Every piece of
software, even a so-called “general-purpose” database, is designed for a particular usage pattern.</p>

<p>Faced with this profusion of alternatives, the first challenge is then to figure out the mapping
between the software products and the circumstances in which they are a good fit. Vendors are
understandably reluctant to tell you about the kinds of workloads for which their software is poorly
suited, but hopefully the previous chapters have equipped you with some questions to ask in order to
read between the lines and better understand the trade-offs.</p>

<p>However, even if you perfectly understand the mapping between tools and circumstances for their use,
there is another challenge: in complex applications, data is often used in several different ways.
There is unlikely to be one piece of software that is suitable for <em>all</em> the
different circumstances in which the data is used, so you inevitably end up having to cobble
together several different pieces of software in order to provide your application’s functionality.</p>








<section data-type="sect2" data-pdf-bookmark="Combining Specialized Tools by Deriving Data"><div class="sect2" id="idm140417546912752">
<h2>Combining Specialized Tools by Deriving Data</h2>

<p><a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" id="ix_dataintcombine"></a>
<a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="full text search support" id="idm140417546910000"></a>
For example, it is common to need to integrate an OLTP database with a full-text search index in
order to handle queries for arbitrary keywords. Although some databases (such as PostgreSQL) include
a full-text indexing feature, which can be sufficient for simple applications
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Belaid2015tl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Belaid2015tl" class="totri-footnote">1</a>], more sophisticated search
facilities require specialist information retrieval tools. Conversely, search indexes are generally
not very suitable as a durable system of record, and so many applications need to combine two
different tools in order to satisfy all of the requirements.</p>

<p><a data-type="indexterm" data-primary="denormalization (data representation)" data-secondary="updating derived data" id="idm140417546905872"></a>
We touched on the issue of integrating data systems in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_sync">“Keeping Systems in Sync”</a>. As the number of
different representations of the data increases, the integration problem becomes harder. Besides the
database and the search index, perhaps you need to keep copies of the data in analytics systems
(data warehouses, or batch and stream processing systems); maintain caches or denormalized versions
of objects that were derived from the original data; pass the data through machine learning,
classification, ranking, or recommendation systems; or send notifications based on changes to the
data.</p>

<p>Surprisingly often I see software engineers make statements like, “In my experience, 99% of people
only need X” or “…don’t need X” (for various values of X). I think that such statements say more
about the experience of the speaker than about the actual usefulness of a technology. The range of
different things you might want to do with data is dizzyingly wide. What one person considers to be
an obscure and pointless feature may well be a central requirement for someone else. The need for
data integration often only becomes apparent if you zoom out and consider the dataflows across an
entire organization.</p>










<section data-type="sect3" data-pdf-bookmark="Reasoning about dataflows"><div class="sect3" id="idm140417546902192">
<h3>Reasoning about dataflows</h3>

<p><a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" data-tertiary="reasoning about dataflows" id="idm140417546900848"></a>
<a data-type="indexterm" data-primary="dataflow" data-secondary="reasoning about" id="idm140417546899392"></a>
When copies of the same data need to be maintained in several storage systems in order to satisfy
different access patterns, you need to be very clear about the inputs and outputs: where is data
written first, and which representations are derived from which sources? How do you get data into
all the right places, in the right formats?</p>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="of secondary indexes" id="idm140417546897680"></a>
<a data-type="indexterm" data-primary="systems of record" data-secondary="change data capture" id="idm140417546896576"></a>
For example, you might arrange for data to first be written to a system of record database,
capturing the changes made to that database (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_cdc">“Change Data Capture”</a>) and then applying the changes
to the search index in the same order. If change data capture (CDC) is the only way of updating the
index, you can be confident that the index is entirely derived from the system of record, and
therefore consistent with it (barring bugs in the software). Writing to the database is the only way
of supplying new input into this system.</p>

<p><a data-type="indexterm" data-primary="indexes" data-secondary="secondary" data-tertiary="problems with dual writes" id="idm140417546893872"></a>
<a data-type="indexterm" data-primary="secondary indexes" data-secondary="problems with dual writes" id="idm140417546892480"></a>
Allowing the application to directly write to both the search index and the database introduces the
problem shown in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#fig_stream_write_order">Figure&nbsp;11-4</a>, in which two clients concurrently send conflicting
writes, and the two storage systems process them in a different order. In this case, neither the
database nor the search index is “in charge” of determining the order of writes, and so they may
make contradictory decisions and become permanently inconsistent with each other.</p>

<p>If it is possible for you to funnel all user input through a single system that decides on an
ordering for all writes, it becomes much easier to derive other representations of the data by
processing the writes in the same order. This is an application of the state machine replication
approach that we saw in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_total_order">“Total Order Broadcast”</a>. Whether you use change data capture or an
event sourcing log is less important than simply the principle of deciding on a total order.</p>

<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="and idempotence" id="idm140417546888112"></a>
Updating a derived data system based on an event log can often be made deterministic and idempotent
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_idempotence">“Idempotence”</a>), making it quite easy to recover from faults.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Derived data versus distributed transactions"><div class="sect3" id="sec_future_derived_vs_transactions">
<h3>Derived data versus distributed transactions</h3>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="across different databases" id="idm140417546884224"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" data-tertiary="derived data versus distributed transactions" id="idm140417546883104"></a>
<a data-type="indexterm" data-primary="derived data" data-secondary="versus distributed transactions" id="idm140417546881616"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="avoiding" id="idm140417546880496"></a>
The classic approach for keeping different data systems consistent with each other involves
distributed transactions, as discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_2pc">“Atomic Commit and Two-Phase Commit (2PC)”</a>. How does the approach of using
derived data systems fare in comparison to distributed transactions?</p>

<p>At an abstract level, they achieve a similar goal by different means. Distributed transactions
decide on an ordering of writes by using locks for mutual exclusion (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_2pl">“Two-Phase Locking (2PL)”</a>),
while CDC and event sourcing use a log for ordering.  Distributed transactions use atomic commit to
ensure that changes take effect exactly once, while log-based systems are often based on
deterministic retry and idempotence.</p>

<p><a data-type="indexterm" data-primary="linearizability" data-secondary="of derived data systems" id="idm140417546876144"></a>
The biggest difference is that transaction systems usually provide linearizability (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_linearizability">“Linearizability”</a>), which implies useful guarantees such as reading your own
writes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_ryw">“Reading Your Own Writes”</a>). On the other hand, derived data systems are often updated
asynchronously, and so they do not by default offer the same timing guarantees.</p>

<p>Within limited environments that are willing to pay the cost of distributed transactions, they have
been used successfully. However, I think that XA has poor fault tolerance and performance
characteristics (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_dist_trans">“Distributed Transactions in Practice”</a>), which severely limit its usefulness. I believe
that it might be possible to create a better protocol for distributed transactions, but getting such
a protocol widely adopted and integrated with existing tools would be challenging, and unlikely to
happen soon.</p>

<p>In the absence of widespread support for a good distributed transaction protocol, I believe that
log-based derived data is the most promising approach for integrating different data systems.
However, guarantees such as reading your own writes are useful, and I don’t think that it is
productive to tell everyone “eventual consistency is inevitable—suck it up and learn to deal with
it” (at least not without good guidance on <em>how</em> to deal with it).</p>

<p>In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_correctness">“Aiming for Correctness”</a> we will discuss some approaches for implementing stronger guarantees
on top of asynchronously derived systems, and work toward a middle ground between distributed
transactions and asynchronous log-based systems.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The limits of total ordering"><div class="sect3" id="idm140417546868560">
<h3>The limits of total ordering</h3>

<p><a data-type="indexterm" data-primary="total order" data-secondary="limits of" id="idm140417546867456"></a>
<a data-type="indexterm" data-primary="ordering" data-secondary="limits of total ordering" id="idm140417546866352"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" data-tertiary="limits of total ordering" id="idm140417546865184"></a>
<a data-type="indexterm" data-primary="causal dependencies" data-secondary="capturing" data-tertiary="by total ordering" id="idm140417546863776"></a>
<a data-type="indexterm" data-primary="logs (data structure)" data-secondary="scalability limits" id="idm140417546862400"></a>
With systems that are small enough, constructing a totally ordered event log is entirely feasible
(as demonstrated by the popularity of databases with single-leader replication, which construct
precisely such a log). However, as systems are scaled toward bigger and more complex workloads,
limitations begin to emerge:</p>

<ul>
<li>
<p>In most cases, constructing a totally ordered log requires all events to pass through a <em>single
leader node</em> that decides on the ordering. If the throughput of events is greater than a single
machine can handle, you need to partition it across multiple machines (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_log">“Partitioned Logs”</a>).
The order of events in two different partitions is then ambiguous.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="coordination" data-secondary="cross-datacenter" id="idm140417546857328"></a>
<a data-type="indexterm" data-primary="datacenters" data-secondary="geographically distributed" id="idm140417546856224"></a>
<a data-type="indexterm" data-primary="geographically distributed datacenters" id="idm140417546855104"></a>
If the servers are spread across multiple <em>geographically distributed</em> datacenters, for example in
order to tolerate an entire datacenter going offline, you typically have a separate leader in each
datacenter, because network delays make synchronous cross-datacenter coordination inefficient (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_multi_leader">“Multi-Leader Replication”</a>). This implies an undefined ordering of events that originate in
two different datacenters.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="microservices" data-secondary="causal dependencies across services" id="idm140417546851888"></a>
<a data-type="indexterm" data-primary="services" data-secondary="microservices" data-tertiary="causal dependencies across services" id="idm140417546850768"></a>
When applications are deployed as <em>microservices</em> (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_dataflow_rpc">“Dataflow Through Services: REST and RPC”</a>), a common
design choice is to deploy each service and its durable state as an independent unit, with no
durable state shared between services. When two events originate in different services, there is
no defined order for those events.</p>
</li>
<li>
<p>Some applications maintain client-side state that is updated immediately on user input (without
waiting for confirmation from a server), and even continue to work offline (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_offline_clients">“Clients with offline operation”</a>). With such applications, clients and servers are very likely
to see events in different orders.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="events" data-secondary="deciding on total order of" id="idm140417546845312"></a>
<a data-type="indexterm" data-primary="total order broadcast" id="idm140417546844192"></a>
In formal terms, deciding on a total order of events is known as <em>total order broadcast</em>, which is
equivalent to consensus (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus_abcast">“Consensus algorithms and total order broadcast”</a>). Most consensus algorithms are
designed for situations in which the throughput of a single node is sufficient to process the entire
stream of events, and these algorithms do not provide a mechanism for multiple nodes to share the
work of ordering the events. It is still an open research problem to design consensus algorithms
that can scale beyond the throughput of a single node and that work well in a geographically
distributed setting.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Ordering events to capture causality"><div class="sect3" id="sec_future_capture_causality">
<h3>Ordering events to capture causality</h3>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="causal" id="idm140417546839664"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" data-tertiary="ordering events to capture causality" id="idm140417546838560"></a>
<a data-type="indexterm" data-primary="causality" data-secondary="ordering events to capture" id="idm140417546837104"></a>
<a data-type="indexterm" data-primary="events" data-secondary="ordering to capture causality" id="idm140417546835984"></a>
In cases where there is no causal link between events, the lack of a total order is not a big
problem, since concurrent events can be ordered arbitrarily. Some other cases are easy to handle:
for example, when there are multiple updates of the same object, they can be totally ordered by
routing all updates for a particular object ID to the same log partition. However, causal
dependencies sometimes arise in more subtle ways (see also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_causality">“Ordering and Causality”</a>).</p>

<p><a data-type="indexterm" data-primary="incidents" data-secondary="sending message to ex-partner" id="idm140417546833312"></a>
For example, consider a social networking service, and two users who were in a relationship but have
just broken up. One of the users removes the other as a friend, and then sends a message to their
remaining friends complaining about their ex-partner. The user’s intention is that their ex-partner
should not see the rude message, since the message was sent after the friend status was revoked.</p>

<p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="sending message to friends (example)" id="idm140417546831520"></a>
However, in a system that stores friendship status in one place and messages in another place, that
ordering dependency between the <em>unfriend</em> event and the <em>message-send</em> event may be lost. If the
causal dependency is not captured, a service that sends notifications about new messages may process
the <em>message-send</em> event before the <em>unfriend</em> event, and thus incorrectly send a notification to
the ex-partner.</p>

<p>In this example, the notifications are effectively a join between the messages and the friend list,
making it related to the timing issues of joins that we discussed previously (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_join_time">“Time-dependence of joins”</a>). Unfortunately, there does not seem to be a simple answer to this problem
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ajoux2015wh_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ajoux2015wh_ch12" class="totri-footnote">2</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Helland2009vd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd" class="totri-footnote">3</a>].
Starting points include:
<a data-type="indexterm" data-primary="causal dependencies" data-secondary="capturing" id="idm140417546822496"></a></p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="logical clocks" id="idm140417546820576"></a><a data-type="indexterm" data-primary="timestamps" data-secondary="logical" id="idm140417546819872"></a>
Logical timestamps can provide total ordering without coordination (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_timestamps">“Sequence Number Ordering”</a>), so they may help in cases where total order broadcast is not
feasible. However, they still require recipients to handle events that are delivered out of order,
and they require additional metadata to be passed around.</p>
</li>
<li>
<p>If you can log an event to record the state of the system that the user saw before making a
decision, and give that event a unique identifier, then any later events can reference that event
identifier in order to record the causal dependency
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kerr2016va-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kerr2016va" class="totri-footnote">4</a>].
We will return to this idea in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_read_events">“Reads are events too”</a>.</p>
</li>
<li>
<p>Conflict resolution algorithms (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sidebar_conflict_resolution">“Automatic Conflict Resolution”</a>) help with processing events
that are delivered in an unexpected order. They are useful for maintaining state, but they do not
help if actions have external side effects (such as sending a notification to a user).</p>
</li>
</ul>

<p>Perhaps, over time, patterns for application development will emerge that allow causal dependencies
to be captured efficiently, and derived state to be maintained correctly, without forcing all events
to go through the bottleneck of total order <span class="keep-together">broadcast.</span>
<a data-type="indexterm" data-primary="data integration" data-secondary="combining tools by deriving data" data-startref="ix_dataintcombine" id="idm140417546809648"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Batch and Stream Processing"><div class="sect2" id="sec_future_batch_streaming">
<h2>Batch and Stream Processing</h2>

<p><a data-type="indexterm" data-primary="data integration" data-secondary="batch and stream processing" id="ix_dataintbatch"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="for data integration" id="ix_batchfut"></a>
<a data-type="indexterm" data-primary="stream processing" data-secondary="for data integration" id="ix_strmprocfut"></a>
I would say that the goal of data integration is to make sure that data ends up in the right form in
all the right places. Doing so requires consuming inputs, transforming, joining, filtering,
aggregating, training models, evaluating, and eventually writing to the appropriate outputs. Batch
and stream processors are the tools for achieving this goal.</p>

<p><a data-type="indexterm" data-primary="derived data" data-secondary="outputs of batch and stream processing" id="idm140417546801728"></a>
The outputs of batch and stream processes are derived datasets such as search indexes, materialized
views, recommendations to show to users, aggregate metrics, and so on (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_output">“The Output of Batch Workflows”</a> and
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_uses">“Uses of Stream Processing”</a>).</p>

<p>As we saw in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ch_batch">Chapter&nbsp;10</a> and <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>, batch and stream processing have a lot of principles in
common, and the main fundamental difference is that stream processors operate on unbounded datasets
whereas batch process inputs are of a known, finite size. There are also many detailed differences
in the ways the processing engines are implemented, but these distinctions are beginning to blur.</p>

<p><a data-type="indexterm" data-primary="microbatching" id="idm140417546796048"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="stream processing on top of batch processing" id="idm140417546794992"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="integration of batch and stream processing" id="idm140417546793824"></a>
Spark performs stream processing on top of a batch processing engine by breaking the stream into
<em>microbatches</em>, whereas Apache Flink performs batch processing on top of a stream processing engine
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Tzoumas2015tn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Tzoumas2015tn" class="totri-footnote">5</a>].
In principle, one type of processing can be emulated on top of the other, although the performance
characteristics vary: for example, microbatching may perform poorly on hopping or sliding windows
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kim2016uw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kim2016uw" class="totri-footnote">6</a>].</p>










<section data-type="sect3" data-pdf-bookmark="Maintaining derived state"><div class="sect3" id="idm140417546787648">
<h3>Maintaining derived state</h3>

<p><a data-type="indexterm" data-primary="data integration" data-secondary="batch and stream processing" data-tertiary="maintaining derived state" id="idm140417546786256"></a>
<a data-type="indexterm" data-primary="state" data-secondary="maintaining derived state" id="idm140417546784624"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="maintaining derived state" id="idm140417546783504"></a>
<a data-type="indexterm" data-primary="stream processing" data-secondary="maintaining derived state" id="idm140417546782384"></a>
<a data-type="indexterm" data-primary="deterministic operations" data-secondary="computing derived data" id="idm140417546781264"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in stream processing" data-tertiary="maintaining derived state" id="idm140417546780144"></a>
Batch processing has a quite strong functional flavor (even if the code is not written in a
functional programming language): it encourages deterministic, pure functions whose output depends
only on the input and which have no side effects other than the explicit outputs, treating inputs as
immutable and outputs as append-only. Stream processing is similar, but it extends operators to
allow managed, fault-tolerant state (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_state_fault_tolerance">“Rebuilding state after a failure”</a>).</p>

<p>The principle of deterministic functions with well-defined inputs and outputs is not only good for
fault tolerance (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_idempotence">“Idempotence”</a>), but also simplifies reasoning about the dataflows
in an organization [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2013vs_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12" class="totri-footnote">7</a>]. No matter whether the derived data
is a search index, a statistical model, or a cache, it is helpful to think in terms of data
pipelines that derive one thing from another, pushing state changes in one system through functional
application code and applying the effects to derived systems.</p>

<p><a data-type="indexterm" data-primary="amplification" data-secondary="of failures" id="idm140417546773264"></a>
<a data-type="indexterm" data-primary="failures" data-secondary="amplification by distributed transactions" id="idm140417546772160"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="failure amplification" id="idm140417546771024"></a>
In principle, derived data systems could be maintained synchronously, just like a relational
database updates secondary indexes synchronously within the same transaction as writes to the table
being indexed. However, asynchrony is what makes systems based on event logs robust: it allows a
fault in one part of the system to be contained locally, whereas distributed transactions abort if
any one participant fails, so they tend to amplify failures by spreading them to the rest of the
system (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_trans_limits">“Limitations of distributed transactions”</a>).</p>

<p><a data-type="indexterm" data-primary="partitioning" data-secondary="multi-partition operations" data-tertiary="secondary index maintenance" id="idm140417546768016"></a>
<a data-type="indexterm" data-primary="secondary indexes" data-secondary="partitioning" data-tertiary="index maintenance" id="idm140417546766608"></a>
We saw in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_secondary_indexes">“Partitioning and Secondary Indexes”</a> that secondary indexes often cross partition
boundaries. A partitioned system with secondary indexes either needs to send writes to multiple
partitions (if the index is term-partitioned) or send reads to all partitions (if the index is
document-partitioned). Such cross-partition communication is also most reliable and scalable if the
index is maintained asynchronously [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Helland2007td_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2007td_ch12" class="totri-footnote">8</a>]
(see also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_unbundled_multi_partition">“Multi-partition data processing”</a>).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Reprocessing data for application evolution"><div class="sect3" id="sec_future_reprocessing">
<h3>Reprocessing data for application evolution</h3>

<p><a data-type="indexterm" data-primary="evolvability" data-secondary="reprocessing data" id="idm140417546759136"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="batch and stream processing" data-tertiary="reprocessing data" id="idm140417546758032"></a>
<a data-type="indexterm" data-primary="reprocessing data" data-seealso="evolvability" id="idm140417546756688"></a>
<a data-type="indexterm" data-primary="logs (data structure)" data-secondary="log-based messaging" data-tertiary="replaying old messages" id="idm140417546755584"></a>
<a data-type="indexterm" data-primary="messaging systems" data-secondary="event logs" data-tertiary="replaying old messages" id="idm140417546754208"></a>
When maintaining derived data, batch and stream processing are both useful. Stream processing allows
changes in the input to be reflected in derived views with low delay, whereas batch processing
allows large amounts of accumulated historical data to be reprocessed in order to derive new views
onto an existing dataset.</p>

<p><a data-type="indexterm" data-primary="schemas" data-secondary="evolution of" id="idm140417546752240"></a>
In particular, reprocessing existing data provides a good mechanism for maintaining a system,
evolving it to support new features and changed requirements (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#ch_encoding">Chapter&nbsp;4</a>). Without
reprocessing, schema evolution is limited to simple changes like adding a new optional field to a
record, or adding a new type of record. This is the case both in a schema-on-write and in a
schema-on-read context (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_schema_flexibility">“Schema flexibility in the document model”</a>). On the other hand, with
reprocessing it is possible to restructure a dataset into a completely different model in order to
better serve new requirements.</p>
<aside data-type="sidebar" epub:type="sidebar"><div class="sidebar" id="sidebar_railway_gauges">
<h5>Schema Migrations on Railways</h5>
<p><a data-type="indexterm" data-primary="schemas" data-secondary="schema migration on railways" id="idm140417546747216"></a>
<a data-type="indexterm" data-primary="railways, schema migration on" id="idm140417546746048"></a>
Large-scale “schema migrations” occur in noncomputer systems as well. For example, in the early
days of railway building in 19th-century England there were various competing standards for the
gauge (the distance between the two rails). Trains built for one gauge couldn’t run on tracks of
another gauge, which restricted the possible interconnections in the train network
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="NetworkRail-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#NetworkRail" class="totri-footnote">9</a>].</p>

<p>After a single standard gauge was finally decided upon in 1846, tracks with other gauges had to be
converted—but how do you do this without shutting down the train line for months or years? The
solution is to first convert the track to <em>dual gauge</em> or <em>mixed gauge</em> by adding a third rail. This
conversion can be done gradually, and when it is done, trains of both gauges can run on the line,
using two of the three rails. Eventually, once all trains have been converted to the standard gauge, the
rail providing the nonstandard gauge can be removed.</p>

<p>“Reprocessing” the existing tracks in this way, and allowing the old and new versions to exist side by side,
makes it possible to change the gauge gradually over the course of years. Nevertheless, it is an
expensive undertaking, which is why nonstandard gauges still exist today. For example, the BART
system in the San Francisco Bay Area uses a different gauge from the majority of the US.</p>
</div></aside>

<p><a data-type="indexterm" data-primary="evolvability" data-secondary="of databases" id="idm140417546739744"></a>
<a data-type="indexterm" data-primary="migrating (rewriting) data" id="idm140417546738656"></a>
Derived views allow <em>gradual</em> evolution. If you want to restructure a dataset, you do not
need to perform the migration as a sudden switch. Instead, you can maintain the old schema and the
new schema side by side as two independently derived views onto the same underlying data. You can
then start shifting a small number of users to the new view in order to test its performance and
find any bugs, while most users continue to be routed to the old view. Gradually, you can increase
the proportion of users accessing the new view, and eventually you can drop the old view
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Xu2017bl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Xu2017bl">10</a>].</p>

<p><a data-type="indexterm" data-primary="Agile" data-secondary="minimizing irreversibility" id="idm140417546734240"></a>
The beauty of such a gradual migration is that every stage of the process is easily reversible if
something goes wrong: you always have a working system to go back to. By reducing the risk of
irreversible damage, you can be more confident about going ahead, and thus move faster to improve
your system
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bartlett2015wv_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bartlett2015wv_ch12">11</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The lambda architecture"><div class="sect3" id="idm140417546730304">
<h3>The lambda architecture</h3>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="combining with stream processing" data-tertiary="lambda architecture" id="idm140417546729024"></a>
<a data-type="indexterm" data-primary="stream processing" data-secondary="combining with batch processing" data-tertiary="lambda architecture" id="idm140417546727456"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="batch and stream processing" data-tertiary="lambda architecture" id="idm140417546726112"></a>
<a data-type="indexterm" data-primary="lambda architecture" id="idm140417546724720"></a>
If batch processing is used to reprocess historical data, and stream processing is used to process
recent updates, then how do you combine the two? The <em>lambda architecture</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Marz2015th-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Marz2015th">12</a>]
is a proposal in this area that has gained a lot of attention.</p>

<p><a data-type="indexterm" data-primary="event sourcing" data-secondary="comparison to lambda architecture" id="idm140417546721152"></a>
The core idea of the lambda architecture is that incoming data should be recorded by appending
immutable events to an always-growing dataset, similarly to event sourcing (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_event_sourcing">“Event Sourcing”</a>). From these events, read-optimized views are derived. The lambda
architecture proposes running two different systems in parallel: a batch processing system such as
Hadoop MapReduce, and a separate stream-processing system such as Storm.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of derived data" id="idm140417546718608"></a>
In the lambda approach, the stream processor consumes the events and quickly produces an approximate
update to the view; the batch processor later consumes the <em>same</em> set of events and produces a
corrected version of the derived view. The reasoning behind this design is that batch processing is
simpler and thus less prone to bugs, while stream processors are thought to be less reliable and
harder to make fault-tolerant (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_fault_tolerance">“Fault Tolerance”</a>). Moreover, the stream process can
use fast approximate algorithms while the batch process uses slower exact algorithms.</p>

<p>The lambda architecture was an influential idea that shaped the design of data systems for the
better, particularly by popularizing the principle of deriving views onto streams of immutable
events and reprocessing events when needed. However, I also think that it has a number of
practical problems:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="Twitter" data-secondary="Summingbird (processing library)" id="idm140417546713952"></a>
Having to maintain the same logic to run both in a batch and in a stream processing framework is
significant additional effort. Although libraries such as Summingbird
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Boykin2014vf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Boykin2014vf">13</a>] provide an abstraction for
computations that can be run in either a batch or a streaming context, the operational complexity
of debugging, tuning, and maintaining two different systems remains
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2014wv_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2014wv_ch12">14</a>].</p>
</li>
<li>
<p>Since the stream pipeline and the batch pipeline produce separate outputs, they need to be merged
in order to respond to user requests. This merge is fairly easy if the computation is a simple
aggregation over a tumbling window, but it becomes significantly harder if the view is derived
using more complex operations such as joins and sessionization, or if the output is not a time
series.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="straggler events" id="idm140417546705776"></a><a data-type="indexterm" data-primary="events" data-secondary="stragglers" id="idm140417546705072"></a>
Although it is great to have the ability to reprocess the entire historical dataset, doing so
frequently is expensive on large datasets. Thus, the batch pipeline often needs to be set up to
process incremental batches (e.g., an hour’s worth of data at the end of every hour) rather than
reprocessing everything. This raises the problems discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_time">“Reasoning About Time”</a>, such as
handling stragglers and handling windows that cross boundaries between batches. Incrementalizing
a batch computation adds complexity, making it more akin to the streaming layer, which runs
counter to the goal of keeping the batch layer as simple as possible.</p>
</li>
</ul>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Unifying batch and stream processing"><div class="sect3" id="idm140417546702160">
<h3>Unifying batch and stream processing</h3>

<p><a data-type="indexterm" data-primary="data integration" data-secondary="batch and stream processing" data-tertiary="unifying" id="idm140417546700992"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="combining with stream processing" data-tertiary="unifying technologies" id="idm140417546699600"></a>
<a data-type="indexterm" data-primary="stream processing" data-secondary="combining with batch processing" data-tertiary="unifying technologies" id="idm140417546698208"></a>
More recent work has enabled the benefits of the lambda architecture to be enjoyed without its
downsides, by allowing both batch computations (reprocessing historical data) and stream
computations (processing events as they arrive) to be implemented in the same system
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CastroFernandez2015uz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#CastroFernandez2015uz">15</a>].</p>

<p>Unifying batch and stream processing in one system requires the following features, which are
becoming increasingly widely available:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="evolvability" data-secondary="reprocessing data" id="idm140417546692720"></a>
<a data-type="indexterm" data-primary="reprocessing data" id="idm140417546691616"></a>
<a data-type="indexterm" data-primary="logs (data structure)" data-secondary="log-based messaging" data-tertiary="replaying old messages" id="idm140417546690784"></a>
<a data-type="indexterm" data-primary="messaging systems" data-secondary="event logs" data-tertiary="replaying old messages" id="idm140417546689408"></a>
The ability to replay historical events through the same processing engine that handles the stream
of recent events. For example, log-based message brokers have the ability to replay messages (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_replay">“Replaying old messages”</a>), and some stream processors can read input from a distributed filesystem
like HDFS.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="exactly-once semantics" data-secondary="parity with batch processors" id="idm140417546686320"></a>
Exactly-once semantics for stream processors—that is, ensuring that the output is the same as
if no faults had occurred, even if faults did in fact occur (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_fault_tolerance">“Fault Tolerance”</a>).
Like with batch processing, this requires discarding the partial output of any failed tasks.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="time" data-secondary="reasoning about, in stream processors" data-tertiary="event time versus processing time" id="idm140417546683392"></a>
  <a data-type="indexterm" data-primary="events" data-secondary="event time versus processing time" id="idm140417546681648"></a>
  <a data-type="indexterm" data-primary="stream processing" data-secondary="reasoning about time" data-tertiary="event time versus processing time" id="idm140417546680528"></a>
  <a data-type="indexterm" data-primary="Beam (dataflow library)" id="idm140417546679136"></a><a data-type="indexterm" data-primary="Apache Beam" data-see="Beam" id="idm140417546678432"></a>
  <a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="integration of batch and stream processing" id="idm140417546677360"></a>
  <a data-type="indexterm" data-primary="Google" data-secondary="Cloud Dataflow (stream processor)" id="idm140417546676144"></a>
  Tools for windowing by event time, not by processing time, since processing time is meaningless
  when reprocessing historical events (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_time">“Reasoning About Time”</a>). For example, Apache Beam provides
  an API for expressing such computations, which can then be run using Apache Flink or Google Cloud
  Dataflow.
<a data-type="indexterm" data-primary="stream processing" data-secondary="for data integration" data-startref="ix_strmprocfut" id="idm140417546673904"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="for data integration" data-startref="ix_batchfut" id="idm140417546672560"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="future of" data-tertiary="batch and stream processing" data-startref="ix_dataintbatch" id="idm140417546671184"></a>
<a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="data integration" data-startref="ix_datasysfutdi" id="idm140417546669520"></a>
<a data-type="indexterm" data-primary="data integration" data-startref="ix_dataint" id="idm140417546667872"></a></p>
</li>
</ul>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Unbundling Databases"><div class="sect1" id="sec_future_unbundling">
<h1>Unbundling Databases</h1>

<p><a data-type="indexterm" data-primary="databases" data-secondary="unbundling" id="ix_dbunbun"></a>
<a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="unbundling databases" id="ix_datsysfutunbun"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="by unbundling databases" id="ix_dataintunbundl"></a>
<a data-type="indexterm" data-primary="unbundling databases" id="ix_unbunddb"></a>
<a data-type="indexterm" data-primary="composing data systems" data-see="unbundling databases" id="idm140417546659280"></a>
<a data-type="indexterm" data-primary="operating systems versus databases" id="idm140417546658176"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to Unix" id="idm140417546657376"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="relation to Hadoop" id="idm140417546656256"></a>
At a most abstract level, databases, Hadoop, and operating systems all perform the same functions:
they store some data, and they allow you to process and query that data
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ritchie1974gg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ritchie1974gg">16</a>].
A database stores data in records of some data model (rows in tables, documents, vertices in a
graph, etc.) while an operating system’s filesystem stores data in files—but at their core, both
are “information management” systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brewer2011uh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Brewer2011uh">17</a>]. As we saw in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ch_batch">Chapter&nbsp;10</a>, the Hadoop ecosystem is somewhat like a
distributed version of Unix.</p>

<p>Of course, there are many practical differences. For example, many filesystems do not cope very well
with a directory containing 10 million small files, whereas a database containing 10 million small
records is completely normal and unremarkable. Nevertheless, the similarities and differences
between operating systems and databases are worth exploring.</p>

<p><a data-type="indexterm" data-primary="relational databases" data-secondary="philosophy compared to Unix" id="idm140417546648064"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to relational databases" id="idm140417546646896"></a>
Unix and relational databases have approached the information management problem with very different
philosophies. Unix viewed its purpose as presenting programmers with a logical but fairly low-level
hardware abstraction, whereas relational databases wanted to give application programmers a
high-level abstraction that would hide the complexities of data structures on disk, concurrency,
crash recovery, and so on. Unix developed pipes and files that are just sequences of bytes, whereas
databases developed SQL and transactions.</p>

<p>Which approach is better? Of course, it depends what you want. Unix is “simpler” in the sense that
it is a fairly thin wrapper around hardware resources; relational databases are “simpler” in the
sense that a short declarative query can draw on a lot of powerful infrastructure (query
optimization, indexes, join methods, concurrency control, replication, etc.) without the author of
the query needing to understand the implementation details.</p>

<p><a data-type="indexterm" data-primary="NoSQL" id="idm140417546644096"></a>
The tension between these philosophies has lasted for decades (both Unix and the relational model
emerged in the early 1970s) and still isn’t resolved. For example, I would interpret the NoSQL
movement as wanting to apply a Unix-esque approach of low-level abstractions to the domain of
distributed OLTP data storage.</p>

<p>In this section I will attempt to reconcile the two philosophies, in the hope that we can combine
the best of both worlds.</p>








<section data-type="sect2" data-pdf-bookmark="Composing Data Storage Technologies"><div class="sect2" id="idm140417546642272">
<h2>Composing Data Storage Technologies</h2>

<p><a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="composing data storage technologies" id="ix_dbunbunstore"></a>
<a data-type="indexterm" data-primary="storage" data-secondary="composing data storage technologies" id="ix_storecompose"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="composing data storage technologies" id="ix_unbunddbstore"></a>
<a data-type="indexterm" data-primary="caches" data-secondary="as derived data" id="ix_cacheprecomp"></a>
<a data-type="indexterm" data-primary="indexes" data-secondary="as derived data" id="ix_indexderived"></a>
<a data-type="indexterm" data-primary="materialization" data-secondary="materialized views" data-tertiary="as derived data" id="ix_matviewunbund"></a>
Over the course of this book we have discussed various features provided by databases and how they
work, including:</p>

<ul>
<li>
<p>Secondary indexes, which allow you to efficiently search for records based on the value of a field
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_other_indexing">“Other Indexing Structures”</a>)</p>
</li>
<li>
<p>Materialized views, which are a kind of precomputed cache of query results (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_materialized_views">“Aggregation: Data Cubes and Materialized Views”</a>)</p>
</li>
<li>
<p>Replication logs, which keep copies of the data on other nodes up to date (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_implementation">“Implementation of Replication Logs”</a>)</p>
</li>
<li>
<p>Full-text search indexes, which allow keyword search in text (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_full_text">“Full-text search and fuzzy indexes”</a>) and
which are built into some relational databases
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Belaid2015tl" class="totri-footnote">1</a>]</p>
</li>
</ul>

<p>In Chapters <a data-type="xref" data-xrefstyle="select:labelnumber" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ch_batch">10</a>
and <a data-type="xref" data-xrefstyle="select:labelnumber" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">11</a>,
similar themes emerged. We talked about building full-text search indexes (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_output">“The Output of Batch Workflows”</a>), about materialized view maintenance (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_mat_view">“Maintaining materialized views”</a>), and about
replicating changes from a database to derived data systems (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_cdc">“Change Data Capture”</a>).</p>

<p>It seems that there are parallels between the features that are built into databases and the
derived data systems that people are building with batch and stream processors.</p>










<section data-type="sect3" data-pdf-bookmark="Creating an index"><div class="sect3" id="idm140417546617088">
<h3>Creating an index</h3>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="consistent snapshots" id="idm140417546615760"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="of secondary indexes" id="idm140417546614432"></a>
<a data-type="indexterm" data-primary="CREATE INDEX statement (SQL)" id="idm140417546613328"></a>
<a data-type="indexterm" data-primary="indexes" data-secondary="creating" id="idm140417546612528"></a>
<a data-type="indexterm" data-primary="snapshots (databases)" data-secondary="computing derived data" id="idm140417546611424"></a>
Think about what happens when you run <code>CREATE INDEX</code> to create a new index in a relational database.
The database has to scan over a consistent snapshot of a table, pick out all of the field values
being indexed, sort them, and write out the index. Then it must process the backlog of writes that
have been made since the consistent snapshot was taken (assuming the table was not locked while
creating the index, so writes could continue). Once that is done, the database must continue to keep
the index up to date whenever a transaction writes to the table.</p>

<p>This process is remarkably similar to setting up a new follower replica (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_new_replica">“Setting Up New Followers”</a>), and also very similar to bootstrapping change data capture in a
streaming system (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_cdc_snapshot">“Initial snapshot”</a>).</p>

<p>Whenever you run <code>CREATE INDEX</code>, the database essentially reprocesses the existing dataset (as
discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_reprocessing">“Reprocessing data for application evolution”</a>) and derives the index as a new view onto the existing
data. The existing data may be a snapshot of the state rather than a log of all changes that ever
happened, but the two are closely related (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_immutability">“State, Streams, and Immutability”</a>).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The meta-database of everything"><div class="sect3" id="idm140417546604224">
<h3>The meta-database of everything</h3>

<p>In this light, I think that the dataflow across an entire organization starts looking like one huge
database [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12" class="totri-footnote">7</a>]. Whenever a batch, stream,
or ETL process transports data from one place and form to another place and form, it is acting like
the database subsystem that keeps indexes or materialized views up to date.</p>

<p>Viewed like this, batch and stream processors are like elaborate implementations of triggers, stored
procedures, and materialized view maintenance routines. The derived data systems they maintain are
like different index types. For example, a relational database may support B-tree indexes, hash
indexes, spatial indexes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_index_multicolumn">“Multi-column indexes”</a>), and other types of indexes. In the
emerging architecture of derived data systems, instead of implementing those facilities as features
of a single integrated database product, they are provided by various different pieces of software,
running on different machines, administered by different teams.</p>

<p><a data-type="indexterm" data-primary="unbundling databases" data-secondary="composing data storage technologies" data-tertiary="federation versus unbundling" id="idm140417546599376"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="by unbundling databases" data-tertiary="comparison to federated databases" id="idm140417546597968"></a>
Where will these developments take us in the future? If we start from the premise that there is no
single data model or storage format that is suitable for all access patterns, I speculate that there
are two avenues by which different storage and processing tools can nevertheless be composed into a
cohesive system:</p>
<dl>
<dt>Federated databases: unifying reads</dt>
<dd>
<p><a data-type="indexterm" data-primary="federated databases" id="idm140417546594928"></a><a data-type="indexterm" data-primary="polystores" id="idm140417546594000"></a>
<a data-type="indexterm" data-primary="PostgreSQL (database)" data-secondary="foreign data wrappers" id="idm140417546593200"></a>
It is possible to provide a unified query interface to a wide variety of underlying storage
engines and processing methods—an approach known as a <em>federated database</em> or <em>polystore</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stonebraker2015wu-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stonebraker2015wu">18</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Duggan2015de-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Duggan2015de">19</a>].
For example, PostgreSQL’s <em>foreign data wrapper</em> feature fits this pattern
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dybka2015bn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Dybka2015bn">20</a>].
Applications that need a specialized data model or query interface can still access the underlying
storage engines directly, while users who want to combine data from disparate places can do so
easily through the federated interface.</p>

<p><a data-type="indexterm" data-primary="relational databases" data-secondary="philosophy compared to Unix" id="idm140417546582688"></a>
A federated query interface follows the relational tradition of a single integrated system with a
high-level query language and elegant semantics, but a complicated implementation.</p>
</dd>
<dt>Unbundled databases: unifying writes</dt>
<dd>
<p>While federation addresses read-only querying across several different systems, it does not have a
good answer to synchronizing writes across those systems. We said that within a single database,
creating a consistent index is a built-in feature. When we compose several storage systems, we
similarly need to ensure that all data changes end up in all the right places, even in the face of
faults. Making it easier to reliably plug together storage systems (e.g., through change data
capture and event logs) is like <em>unbundling</em> a database’s index-maintenance features in a way that
can synchronize writes across disparate technologies
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12" class="totri-footnote">7</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lomet2009tc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Lomet2009tc">21</a>].</p>

<p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to relational databases" id="idm140417546575392"></a>
The unbundled approach follows the Unix tradition of small tools that do one thing well
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2015tz_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2015tz_ch12">22</a>], that communicate through a uniform low-level API (pipes), and that can be
composed using a higher-level language (the shell)
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ritchie1974gg">16</a>].</p>
</dd>
</dl>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Making unbundling work"><div class="sect3" id="sec_future_unbundling_favor">
<h3>Making unbundling work</h3>

<p>Federation and unbundling are two sides of the same coin: composing a reliable, scalable, and
maintainable system out of diverse components. Federated read-only querying requires mapping one
data model into another, which takes some thought but is ultimately quite a manageable problem. I
think that keeping the writes to several storage systems in sync is the harder engineering problem,
and so I will focus <span class="keep-together">on it.</span></p>

<p><a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="avoiding" id="idm140417546567600"></a>
The traditional approach to synchronizing writes requires distributed transactions across
heterogeneous storage systems [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stonebraker2015wu">18</a>], which
I think is the wrong solution (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_derived_vs_transactions">“Derived data versus distributed transactions”</a>). Transactions within a
single storage or stream processing system are feasible, but when data crosses the boundary between
different technologies, I believe that an asynchronous event log with idempotent writes is a much
more robust and practical approach.</p>

<p>For example, distributed transactions are used within some stream processors to achieve exactly-once
semantics (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_atomic_commit">“Atomic commit revisited”</a>), and this can work quite well. However, when a
transaction would need to involve systems written by different groups of people (e.g., when data is
written from a stream processor to a distributed key-value store or search index), the lack of a
standardized transaction protocol makes integration much harder. An ordered log of events with
idempotent consumers (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_idempotence">“Idempotence”</a>) is a much simpler abstraction, and thus much
more feasible to implement across heterogeneous systems
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12" class="totri-footnote">7</a>].</p>

<p><a data-type="indexterm" data-primary="loose coupling" id="idm140417546560352"></a>
The big advantage of log-based integration is <em>loose coupling</em> between the various components, which
manifests itself in two ways:</p>
<ol>
<li>
<p><a data-type="indexterm" data-primary="consumers (message streams)" data-secondary="not keeping up with producers" id="idm140417546558064"></a>
At a system level, asynchronous event streams make the system as a whole more robust to outages
or performance degradation of individual components. If a consumer runs slow or fails, the event
log can buffer messages (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_disk_usage">“Disk space usage”</a>), allowing the producer and any other
consumers to continue running unaffected. The faulty consumer can catch up when it is fixed, so
it doesn’t miss any data, and the fault is contained. By contrast, the synchronous interaction of
distributed transactions tends to escalate local faults into large-scale failures (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_trans_limits">“Limitations of distributed transactions”</a>).</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="microservices" data-secondary="loose coupling" id="idm140417546554000"></a>
<a data-type="indexterm" data-primary="services" data-secondary="microservices" data-tertiary="loose coupling" id="idm140417546552896"></a>
At a human level, unbundling data systems allows different software components and services to be
developed, improved, and maintained independently from each other by different teams.
Specialization allows each team to focus on doing one thing well, with well-defined interfaces to
other teams’ systems. Event logs provide an interface that is powerful enough to capture fairly
strong consistency properties (due to durability and ordering of events), but also general enough
to be applicable to almost any kind of data.</p>
</li>

</ol>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Unbundled versus integrated systems"><div class="sect3" id="idm140417546550592">
<h3>Unbundled versus integrated systems</h3>

<p><a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to composing storage technologies" id="idm140417546549312"></a>
If unbundling does indeed become the way of the future, it will not replace databases in their
current form—they will still be needed as much as ever. Databases are still required for
maintaining state in stream processors, and in order to serve queries for the output of batch and
stream processors (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_output">“The Output of Batch Workflows”</a> and <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_processing">“Processing Streams”</a>). Specialized query
engines will continue to be important for particular workloads: for example, query engines in MPP
data warehouses are optimized for exploratory analytic queries and handle this kind of workload
very well (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>).</p>

<p>The complexity of running several different pieces of infrastructure can be a problem: each piece of
software has a learning curve, configuration issues, and operational quirks, and so it is worth
deploying as few moving parts as possible. A single integrated software product may also be able to
achieve better and more predictable performance on the kinds of workloads for which it is designed,
compared to a system consisting of several tools that you have composed with application code
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hugg2016tq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Hugg2016tq">23</a>].
As I said in the <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/preface01.html#preface">Preface</a>, building for scale that you don’t need is wasted effort and may lock you
into an inflexible design. In effect, it is a form of premature optimization.</p>

<p>The goal of unbundling is not to compete with individual databases on performance for particular
workloads; the goal is to allow you to combine several different databases in order to achieve good
performance for a much wider range of workloads than is possible with a single piece of software. It’s
about breadth, not depth—in the same vein as the diversity of storage and processing models that
we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>.</p>

<p>Thus, if there is a single technology that does everything you need, you’re most likely best off
simply using that product rather than trying to reimplement it yourself from lower-level components.
The advantages of unbundling and composition only come into the picture when there is no single
piece of software that satisfies all your requirements.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="What’s missing?"><div class="sect3" id="idm140417546538384">
<h3>What’s missing?</h3>

<p><a data-type="indexterm" data-primary="bash shell (Unix)" id="idm140417546537184"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="composing data storage technologies" data-tertiary="need for high-level language" id="idm140417546536352"></a>
The tools for composing data systems are getting better, but I think one major part is missing: we
don’t yet have the unbundled-database equivalent of the Unix shell (i.e., a high-level language for
composing storage and processing systems in a simple and declarative way).</p>

<p>For example, I would love it if we could simply declare <code>mysql | elasticsearch</code>, by analogy to Unix
pipes [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2015tz_ch12">22</a>], which would be the
unbundled equivalent of <code>CREATE INDEX</code>: it would take all the documents in a MySQL database and
index them in an Elasticsearch cluster. It would then continually capture all the changes made to
the database and automatically apply them to the search index, without us having to write custom
application code. This kind of integration should be possible with almost any kind of storage or
indexing system.</p>

<p><a data-type="indexterm" data-primary="differential dataflow" id="idm140417546531872"></a>
<a data-type="indexterm" data-primary="dataflow" data-secondary="differential" id="idm140417546530704"></a>
Similarly, it would be great to be able to precompute and update caches more easily. Recall that a
materialized view is essentially a precomputed cache, so you could imagine creating a cache by
declaratively specifying materialized views for complex queries, including recursive queries on
graphs (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_graph">“Graph-Like Data Models”</a>) and application logic. There is interesting early-stage
research in this area, such as <em>differential dataflow</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2013tt-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McSherry2013tt">24</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Murray2013jg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Murray2013jg">25</a>],
and I hope that these ideas will find their way into production systems.
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="composing data storage technologies" data-startref="ix_unbunddbstore" id="idm140417546522624"></a>
<a data-type="indexterm" data-primary="storage" data-secondary="composing data storage technologies" data-startref="ix_storecompose" id="idm140417546521248"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="composing data storage technologies" data-startref="ix_dbunbunstore" id="idm140417546519856"></a>
<a data-type="indexterm" data-primary="caches" data-secondary="as derived data" data-startref="ix_cacheprecomp" id="idm140417546518192"></a>
<a data-type="indexterm" data-primary="indexes" data-secondary="as derived data" data-startref="ix_indexderived" id="idm140417546516816"></a>
<a data-type="indexterm" data-primary="materialization" data-secondary="materialized views" data-tertiary="as derived data" data-startref="ix_matviewunbund" id="idm140417546515440"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Designing Applications Around Dataflow"><div class="sect2" id="sec_future_dataflow">
<h2>Designing Applications Around Dataflow</h2>

<p><a data-type="indexterm" data-primary="dataflow" id="ix_dflowapp"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="designing applications around dataflow" id="ix_dbunbdflow"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="designing applications around dataflow" id="ix_unbdbdesapp"></a>
<a data-type="indexterm" data-primary="inside-out databases" data-seealso="unbundling databases" id="idm140417546508064"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="inside-out" data-seealso="unbundling databases" id="idm140417546506960"></a>
The approach of unbundling databases by composing specialized storage and processing systems with
application code is also becoming known as the “database inside-out” approach
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Shapira2016ej-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Shapira2016ej">26</a>], after the title of a conference talk I
gave in 2014 [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2014ht-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2014ht">27</a>].
However, calling it a “new architecture” is too grandiose. I see it more as a design pattern, a
starting point for discussion, and we give it a name simply so that we can better talk about it.</p>

<p><a data-type="indexterm" data-primary="programming languages" data-secondary="dataflow languages" id="idm140417546500656"></a>
<a data-type="indexterm" data-primary="programming languages" data-secondary="functional reactive programming (FRP)" id="idm140417546498848"></a>
<a data-type="indexterm" data-primary="functional reactive programming (FRP)" id="idm140417546497776"></a>
<a data-type="indexterm" data-primary="Oz (programming language)" id="idm140417546496928"></a>
<a data-type="indexterm" data-primary="Juttle (query language)" id="idm140417546496080"></a>
<a data-type="indexterm" data-primary="Elm (programming language)" id="idm140417546495248"></a>
<a data-type="indexterm" data-primary="Bloom (programming language)" id="idm140417546494400"></a>
<a data-type="indexterm" data-primary="declarative languages" data-secondary="Bloom" id="idm140417546493552"></a>
<a data-type="indexterm" data-primary="query languages" data-secondary="Juttle" id="idm140417546492448"></a>
These ideas are not mine; they are simply an amalgamation of other people’s ideas from which I think
we should learn. In particular, there is a lot of overlap with <em>dataflow</em> languages such as Oz
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="VanRoy2004th-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#VanRoy2004th">28</a>] and Juttle
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Juttle2016-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Juttle2016">29</a>],
<em>functional reactive programming</em> (FRP) languages such as Elm
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Czaplicki2013ig-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Czaplicki2013ig">30</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bainomugisha2013bh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bainomugisha2013bh">31</a>],
<a data-type="indexterm" data-primary="logic programming languages" id="idm140417546480256"></a><a data-type="indexterm" data-primary="programming languages" data-secondary="logic programming" id="idm140417546479520"></a>
and <em>logic programming</em> languages such as Bloom [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alvaro2011wn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alvaro2011wn">32</a>]. The term <em>unbundling</em> in this context was proposed by Jay Kreps
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12" class="totri-footnote">7</a>].</p>

<p><a data-type="indexterm" data-primary="spreadsheets, dataflow programming capabilities" id="idm140417546499712"></a>
Even spreadsheets have dataflow programming capabilities that are miles ahead of most mainstream
programming languages [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hermans2015ws-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Hermans2015ws">33</a>]. In a spreadsheet, you can put a formula in one cell (for example, the sum of cells
in another column), and whenever any input to the formula changes, the result of the formula is
automatically recalculated. This is exactly what we want at a data system level: when a record in a
database changes, we want any index for that record to be automatically updated, and any cached
views or aggregations that depend on the record to be automatically refreshed. You should not have
to worry about the technical details of how this refresh happens, but be able to simply trust that
it works correctly.</p>

<p><a data-type="indexterm" data-primary="VisiCalc (spreadsheets)" id="idm140417546470736"></a>
Thus, I think that most data systems still have something to learn from the features that VisiCalc
already had in 1979 [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="BricklinVisiCalc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#BricklinVisiCalc">34</a>]. The difference from spreadsheets is that today’s
data systems need to be fault-tolerant, scalable, and store data durably. They also need to be able
to integrate disparate technologies written by different groups of people over time, and reuse
existing libraries and services: it is unrealistic to expect all software to be developed using one
particular language, framework, or tool.</p>

<p>In this section I will expand on these ideas and explore some ways of building applications around
the ideas of unbundled databases and dataflow.</p>










<section data-type="sect3" data-pdf-bookmark="Application code as a derivation function"><div class="sect3" id="sec_future_dataflow_derivation">
<h3>Application code as a derivation function</h3>

<p><a data-type="indexterm" data-primary="derived data" data-secondary="through application code" id="idm140417546464704"></a>
When one dataset is derived from another, it goes through some kind of transformation function. For
example:</p>

<ul>
<li>
<p>A secondary index is a kind of derived dataset with a straightforward transformation function: for
each row or document in the base table, it picks out the values in the columns or fields being
indexed, and sorts by those values (assuming a B-tree or SSTable index, which are sorted by key,
as discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#ch_storage">Chapter&nbsp;3</a>).</p>
</li>
<li>
<p>A full-text search index is created by applying various natural language processing functions such
as language detection, word segmentation, stemming or lemmatization, spelling correction, and
synonym identification, followed by building a data structure for efficient lookups (such as an
inverted index).</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="machine learning" data-secondary="models derived from training data" id="idm140417546459360"></a>
In a machine learning system, we can consider the model as being derived from the training data by
applying various feature extraction and statistical analysis functions. When the model is applied
to new input data, the output of the model is derived from the input and the model (and hence,
indirectly, from the training data).</p>
</li>
<li>
<p>A cache often contains an aggregation of data in the form in which it is going to be displayed in
a user interface (UI). Populating the cache thus requires knowledge of what fields are referenced
in the UI; changes in the UI may require updating the definition of how the cache is populated and
rebuilding the cache.</p>
</li>
</ul>

<p>The derivation function for a secondary index is so commonly required that it is built into many
databases as a core feature, and you can invoke it by merely saying <code>CREATE INDEX</code>. For full-text
indexing, basic linguistic features for common languages may be built into a database, but the more
sophisticated features often require domain-specific tuning. In machine learning, feature
engineering is notoriously application-specific, and often has to incorporate detailed knowledge
about the user interaction and deployment of an application
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sculley2014un-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Sculley2014un">35</a>].</p>

<p><a data-type="indexterm" data-primary="stored procedures" data-secondary="similarity to stream processors" id="idm140417546452432"></a>
When the function that creates a derived dataset is not a standard cookie-cutter function like
creating a secondary index, custom code is required to handle the application-specific aspects. And
this custom code is where many databases struggle. Although relational databases commonly support
triggers, stored procedures, and user-defined functions, which can be used to execute application
code within the database, they have been somewhat of an afterthought in database design (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_transmit">“Transmitting Event Streams”</a>).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Separation of application code and state"><div class="sect3" id="idm140417546449840">
<h3>Separation of application code and state</h3>

<p><a data-type="indexterm" data-primary="state" data-secondary="separation of application code and" id="idm140417546448496"></a>
<a data-type="indexterm" data-primary="package managers" id="idm140417546447424"></a>
In theory, databases could be deployment environments for arbitrary application code, like an
operating system. However, in practice they have turned out to be poorly suited for this purpose.
They do not fit well with the requirements of modern application development, such as dependency and
package management, version control, rolling upgrades, evolvability, monitoring, metrics, calls to
network services, and integration with external systems.</p>

<p><a data-type="indexterm" data-primary="Mesos (cluster manager)" id="idm140417546445872"></a>
<a data-type="indexterm" data-primary="YARN (job scheduler)" id="idm140417546445040"></a>
<a data-type="indexterm" data-primary="Docker (container manager)" id="idm140417546444208"></a>
<a data-type="indexterm" data-primary="Kubernetes (cluster manager)" id="idm140417546443360"></a>
On the other hand, deployment and cluster management tools such as Mesos, YARN, Docker, Kubernetes,
and others are designed specifically for the purpose of running application code. By focusing on
doing one thing well, they are able to do it much better than a database that provides execution of
user-defined functions as one of its many features.</p>

<p>I think it makes sense to have some parts of a system that specialize in durable data storage, and
other parts that specialize in running application code. The two can interact while still remaining
independent.</p>

<p>Most web applications today are deployed as stateless services, in which any user request can be
routed to any application server, and the server forgets everything about the request once it has
sent the response. This style of deployment is convenient, as servers can be added or removed at
will, but the state has to go somewhere: typically, a database. The trend has been to keep stateless
application logic separate from state management (databases): not putting application logic in the
database and not putting persistent state in the application
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2015dn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2015dn">36</a>]. As
people in the functional programming community like to joke, “We believe in the separation of Church
and state” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Steele2001ts-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Steele2001ts">37</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417546434816-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546434816" class="totri-footnote">i</a></sup></p>

<p>In this typical web application model, the database acts as a kind of mutable shared variable that
can be accessed synchronously over the network. The application can read and update the variable,
and the database takes care of making it durable, providing some concurrency control and fault
tolerance.</p>

<p>However, in most programming languages you cannot subscribe to changes in a mutable variable—you can
only read it periodically. Unlike in a spreadsheet, readers of the variable don’t get notified if
the value of the variable changes. <a data-type="indexterm" data-primary="observer pattern" id="idm140417546431696"></a> (You can implement such notifications in
your own code—this is known as the <em>observer pattern</em>—but most languages do not have this pattern as
a built-in feature.)</p>

<p><a data-type="indexterm" data-primary="databases" data-secondary="relation to event streams" data-tertiary="API support for change streams" id="idm140417546430112"></a>
Databases have inherited this passive approach to mutable data: if you want to find out whether the
content of the database has changed, often your only option is to poll (i.e., to repeat your query
periodically). Subscribing to changes is only just beginning to emerge as a feature (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_change_api">“API support for change streams”</a>).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Dataflow: Interplay between state changes and application code"><div class="sect3" id="idm140417546427424">
<h3>Dataflow: Interplay between state changes and application code</h3>

<p><a data-type="indexterm" data-primary="state" data-secondary="interplay between state changes and application code" id="idm140417546426000"></a>
Thinking about applications in terms of dataflow implies renegotiating the relationship between
application code and state management. Instead of treating a database as a passive variable that is
manipulated by the application, we think much more about the interplay and collaboration between
state, state changes, and code that processes them. Application code responds to state changes in
one place by triggering state changes in another place.</p>

<p><a data-type="indexterm" data-primary="tuple spaces (programming model)" id="idm140417546424144"></a>
We saw this line of thinking in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_databases">“Databases and Streams”</a>, where we discussed treating the log of
changes to a database as a stream of events that we can subscribe to. Message-passing systems such
as actors (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_dataflow_msg">“Message-Passing Dataflow”</a>) also have this concept of responding to events.
Already in the 1980s, the <em>tuple spaces</em> model explored expressing distributed computations in terms
of processes that observe state changes and react to them
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gelernter1985df-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gelernter1985df">38</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Eugster2003ih_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Eugster2003ih_ch12">39</a>].</p>

<p>As discussed, similar things happen inside a database when a trigger fires due to a data change, or
when a secondary index is updated to reflect a change in the table being indexed. Unbundling the
database means taking this idea and applying it to the creation of derived datasets outside of the
primary database: caches, full-text search indexes, machine learning, or analytics systems. We can
use stream processing and messaging systems for this purpose.</p>

<p>The important thing to keep in mind is that maintaining derived data is not the same as asynchronous
job execution, for which messaging systems are traditionally designed (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_logs_vs_messaging">“Logs compared to traditional messaging”</a>):</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="dual writes, problems with" id="idm140417546411776"></a>
When maintaining derived data, the order of state changes is often important (if several views are
derived from an event log, they need to process the events in the same order so that they remain
consistent with each other).  As discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_reordering">“Acknowledgments and redelivery”</a>, many message brokers do
not have this property when redelivering unacknowledged messages. Dual writes are also ruled out
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_sync">“Keeping Systems in Sync”</a>).</p>
</li>
<li>
<p>Fault tolerance is key for derived data: losing just a single message causes the derived dataset
to go permanently out of sync with its data source. Both message delivery and derived state
updates must be reliable. For example, many actor systems by default maintain actor state and
messages in memory, so they are lost if the machine running the actor crashes.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="concurrency" data-secondary="reducing, through event logs" id="idm140417546407136"></a>
<a data-type="indexterm" data-primary="race conditions" data-secondary="preventing with event logs" id="idm140417546406016"></a>
Stable message ordering and fault-tolerant message processing are quite stringent demands, but they
are much less expensive and more operationally robust than distributed transactions. Modern stream
processors can provide these ordering and reliability guarantees at scale, and they allow
application code to be run as stream operators.</p>

<p>This application code can do the arbitrary processing that built-in derivation functions in
databases generally don’t provide. Like Unix tools chained by pipes, stream operators can be
composed to build large systems around dataflow. Each operator takes streams of state changes as
input, and produces other streams of state changes as output.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Stream processors and services"><div class="sect3" id="idm140417546403664">
<h3>Stream processors and services</h3>

<p><a data-type="indexterm" data-primary="stream processing" data-secondary="relation to services" id="idm140417546402320"></a>
<a data-type="indexterm" data-primary="services" data-secondary="relation to batch/stream processors" id="idm140417546401216"></a>
<a data-type="indexterm" data-primary="microservices" data-secondary="relation to batch/stream processors" id="idm140417546400096"></a>
The currently trendy style of application development involves breaking down functionality into a
set of <em>services</em> that communicate via synchronous network requests such as REST APIs (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_dataflow_rpc">“Dataflow Through Services: REST and RPC”</a>). The advantage of such a service-oriented architecture over a single
monolithic application is primarily organizational scalability through loose coupling: different
teams can work on different services, which reduces coordination effort between teams (as long as
the services can be deployed and updated independently).</p>

<p>Composing stream operators into dataflow systems has a lot of similar characteristics to the
microservices approach [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stopford2016tk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stopford2016tk">40</a>]. However, the underlying communication
mechanism is very different: one-directional, asynchronous message streams rather than synchronous
request/response interactions.</p>

<p>Besides the advantages listed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_dataflow_msg">“Message-Passing Dataflow”</a>, such as better fault tolerance,
dataflow systems can also achieve better performance. For example, say a customer is purchasing an
item that is priced in one currency but paid for in another currency. In order to perform the
currency conversion, you need to know the current exchange rate. This operation could be implemented
in two ways [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stopford2016tk">40</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Posta2016uo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Posta2016uo">41</a>]:</p>
<ol>
<li>
<p>In the microservices approach, the code that processes the purchase would probably query an
exchange-rate service or database in order to obtain the current rate for a particular currency.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in stream processing" id="idm140417546387520"></a>
In the dataflow approach, the code that processes purchases would subscribe to a stream of
exchange rate updates ahead of time, and record the current rate in a local database whenever it
changes. When it comes to processing the purchase, it only needs to query the local database.</p>
</li>

</ol>

<p>The second approach has replaced a synchronous network request to another service with a query to a
local database (which may be on the same machine, even in the same
process).<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417546385280-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546385280">ii</a></sup> Not only is the dataflow approach faster, but it is also more
robust to the failure of another service. The fastest and most reliable network request is no
network request at all! Instead of RPC, we now have a stream join between purchase events and
exchange rate update events (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_table_joins">“Stream-table join (stream enrichment)”</a>).</p>

<p>The join is time-dependent: if the purchase events are reprocessed at a later point in time, the
exchange rate will have changed. If you want to reconstruct the original output, you will need to
obtain the historical exchange rate at the original time of purchase. No matter whether you query a
service or subscribe to a stream of exchange rate updates, you will need to handle this time
dependence (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_join_time">“Time-dependence of joins”</a>).</p>

<p>Subscribing to a stream of changes, rather than querying the current state when needed, brings us
closer to a spreadsheet-like model of computation: when some piece of data changes, any derived data
that depends on it can swiftly be updated. There are still many open questions, for example around
issues like time-dependent joins, but I believe that building applications around dataflow ideas is
a very promising direction to go in.
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="designing applications around dataflow" data-startref="ix_unbdbdesapp" id="idm140417546380304"></a>
<a data-type="indexterm" data-primary="dataflow" data-startref="ix_dflowapp" id="idm140417546378912"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="designing applications around dataflow" data-startref="ix_dbunbdflow" id="idm140417546377808"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Observing Derived State"><div class="sect2" id="sec_future_observing">
<h2>Observing Derived State</h2>

<p><a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="observing derived state" id="ix_dbunbderstate"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="observing derived state" id="ix_unbdbderstate"></a>
<a data-type="indexterm" data-primary="state" data-secondary="observing derived state" id="ix_statederobs"></a>
<a data-type="indexterm" data-primary="write path (derived data)" id="idm140417546369936"></a>
At an abstract level, the dataflow systems discussed in the last section give you a process for
creating derived datasets (such as search indexes, materialized views, and predictive models) and
keeping them up to date. Let’s call that process the <em>write path</em>: whenever some piece of
information is written to the system, it may go through multiple stages of batch and stream
processing, and eventually every derived dataset is updated to incorporate the data that was
written. <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_write_read_paths">Figure&nbsp;12-1</a> shows an example of updating a search index.</p>

<figure><div id="fig_future_write_read_paths" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1201.png" alt="ddia 1201" width="2880" height="1101" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1201.png">
<h6><span class="label">Figure 12-1. </span>In a search index, writes (document updates) meet reads (queries).</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="read path (derived data)" id="idm140417546365232"></a>
But why do you create the derived dataset in the first place? Most likely because you want to query
it again at a later time. This is the <em>read path</em>: when serving a user request you read from the
derived dataset, perhaps perform some more processing on the results, and construct the response to
the user.</p>

<p>Taken together, the write path and the read path encompass the whole journey of the data, from the
point where it is collected to the point where it is consumed (probably by another human). The write
path is the portion of the journey that is precomputed—i.e., that is done eagerly as soon as the
data comes in, regardless of whether anyone has asked to see it. The read path is the portion of the
journey that only happens when someone asks for it. If you are familiar with functional programming
languages, you might notice that the write path is similar to eager evaluation, and the read path is
similar to lazy evaluation.</p>

<p>The derived dataset is the place where the write path and the read path meet, as illustrated in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_write_read_paths">Figure&nbsp;12-1</a>. It represents a trade-off between the amount of work that needs to
be done at write time and the amount that needs to be done at read time.</p>










<section data-type="sect3" data-pdf-bookmark="Materialized views and caching"><div class="sect3" id="idm140417546360944">
<h3>Materialized views and caching</h3>

<p><a data-type="indexterm" data-primary="unbundling databases" data-secondary="observing derived state" data-tertiary="materialized views and caching" id="idm140417546359632"></a>
A full-text search index is a good example: the write path updates the index, and the read path
searches the index for keywords. Both reads and writes need to do some work. Writes need to update
the index entries for all terms that appear in the document. Reads need to search for each of the
words in the query, and apply Boolean logic to find documents that contain <em>all</em> of the words in the
query (an <code>AND</code> operator), or <em>any</em> synonym of each of the words (an <code>OR</code> operator).</p>

<p>If you didn’t have an index, a search query would have to scan over all documents (like <code>grep</code>),
which would get very expensive if you had a large number of documents. No index means less work on
the write path (no index to update), but a lot more work on the read path.</p>

<p>On the other hand, you could imagine precomputing the search results for all possible queries. In
that case, you would have less work to do on the read path: no Boolean logic, just find the results
for your query and return them. However, the write path would be a lot more expensive: the set of
possible search queries that could be asked is infinite, and thus precomputing all possible search
results would require infinite time and storage space. That wouldn’t work so
well.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417546354016-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546354016">iii</a></sup></p>

<p>Another option would be to precompute the search results for only a fixed set of the most common
queries, so that they can be served quickly without having to go to the index. The uncommon queries
can still be served from the index. This would generally be called a <em>cache</em> of common queries,
although we could also call it a materialized view, as it would need to be updated when new
documents appear that should be included in the results of one of the common queries.</p>

<p>From this example we can see that an index is not the only possible boundary between the write path
and the read path. Caching of common search results is possible, and <code>grep</code>-like scanning without
the index is also possible on a small number of documents. Viewed like this, the role of caches,
indexes, and materialized views is simple: they shift the boundary between the read path and the
write path.  They allow us to do more work on the write path, by precomputing results, in order to
save effort on the read path.</p>

<p><a data-type="indexterm" data-primary="Twitter" data-secondary="constructing home timelines (example)" id="idm140417546349968"></a>
Shifting the boundary between work done on the write path and the read path was in fact the topic of
the Twitter example at the beginning of this book, in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch01.html#sec_introduction_scalability_load">“Describing Load”</a>. In that
example, we also saw how the boundary between write path and read path might be drawn differently
for celebrities compared to ordinary users. After 500 pages we have come full circle!</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Stateful, offline-capable clients"><div class="sect3" id="idm140417546347472">
<h3>Stateful, offline-capable clients</h3>

<p><a data-type="indexterm" data-primary="clients" data-secondary="stateful and offline-capable" id="idm140417546346192"></a>
<a data-type="indexterm" data-primary="offline systems" data-secondary="stateful, offline-capable clients" id="idm140417546345024"></a>
I find the idea of a boundary between write and read paths interesting because we can discuss
shifting that boundary and explore what that shift means in practical terms. Let’s look at the idea
in a different context.</p>

<p>The huge popularity of web applications in the last two decades has led us to certain assumptions
about application development that are easy to take for granted. In particular, the client/server
model—in which clients are largely stateless and servers have the authority over data—is so
common that we almost forget that anything else exists. However, technology keeps moving on, and I
think it is important to question the status quo from time to time.</p>

<p>Traditionally, web browsers have been stateless clients that can only do useful things when you have
an internet connection (just about the only thing you could do offline was to scroll up and down in
a page that you had previously loaded while online). However, recent “single-page” JavaScript web apps
have gained a lot of stateful capabilities, including client-side user interface interaction and
persistent local storage in the web browser. Mobile apps can similarly store a lot of state on the
device and don’t require a round-trip to the server for most user interactions.</p>

<p><a data-type="indexterm" data-primary="offline-first applications" id="idm140417546341488"></a>
<a data-type="indexterm" data-primary="locality (data access)" data-secondary="in stateful clients" id="idm140417546340640"></a>
These changing capabilities have led to a renewed interest in <em>offline-first</em> applications that do
as much as possible using a local database on the same device, without requiring an internet
connection, and sync with remote servers in the background when a network connection is available
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Feyerke2013wd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Feyerke2013wd">42</a>]. Since mobile devices often have slow and unreliable
cellular internet connections, it’s a big advantage for users if their user interface does not have
to wait for synchronous network requests, and if apps mostly work offline (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_offline_clients">“Clients with offline operation”</a>).</p>

<p>When we move away from the assumption of stateless clients talking to a central database and
toward state that is maintained on end-user devices, a world of new opportunities opens up. In
particular, we can think of the on-device state as a <em>cache of state on the server</em>. The pixels on
the screen are a materialized view onto model objects in the client app; the model objects are a local
replica of state in a remote datacenter [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2014ht">27</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Pushing state changes to clients"><div class="sect3" id="idm140417546333456">
<h3>Pushing state changes to clients</h3>

<p><a data-type="indexterm" data-primary="clients" data-secondary="pushing state changes to" id="idm140417546332112"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="observing derived state" data-tertiary="pushing state changes to clients" id="idm140417546330944"></a>
<a data-type="indexterm" data-primary="staleness (old data)" data-secondary="of client state" id="idm140417546329552"></a>
In a typical web page, if you load the page in a web browser and the data subsequently changes on
the server, the browser does not find out about the change until you reload the page. The browser
only reads the data at one point in time, assuming that it is static—it does not subscribe to
updates from the server. Thus, the state on the device is a stale cache that is not updated unless
you explicitly poll for changes. (HTTP-based feed subscription protocols like RSS are really just a
basic form of polling.)</p>

<p><a data-type="indexterm" data-primary="WebSocket (protocol)" id="idm140417546327664"></a>
<a data-type="indexterm" data-primary="EventSource (browser API)" id="idm140417546326832"></a>
More recent protocols have moved beyond the basic request/response pattern of HTTP: server-sent
events (the EventSource API) and WebSockets provide communication channels by which a web browser can
keep an open TCP connection to a server, and the server can actively push messages to the browser as
long as it remains connected. This provides an opportunity for the server to actively inform the
end-user client about any changes to the state it has stored locally, reducing the staleness of the
client-side state.</p>

<p>In terms of our model of write path and read path, actively pushing state changes all the way to
client devices means extending the write path all the way to the end user. When a client is first
initialized, it would still need to use a read path to get its initial state, but thereafter it
could rely on a stream of state changes sent by the server. The ideas we discussed around stream
processing and messaging are not restricted to running only in a datacenter: we can take the ideas
further, and extend them all the way to end-user devices
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Burckhardt2015hv-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Burckhardt2015hv">43</a>].</p>

<p>The devices will be offline some of the time, and unable to receive any notifications of state
changes from the server during that time. But we already solved that problem: in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_log_offsets">“Consumer offsets”</a> we discussed how a consumer of a log-based message broker can reconnect
after failing or becoming disconnected, and ensure that it doesn’t miss any messages that arrived
while it was disconnected. The same technique works for individual users, where each device is a
small subscriber to a small stream of events.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="End-to-end event streams"><div class="sect3" id="idm140417546319536">
<h3>End-to-end event streams</h3>

<p><a data-type="indexterm" data-primary="end-to-end argument" data-secondary="publish/subscribe streams" id="idm140417546318208"></a>
<a data-type="indexterm" data-primary="streams" data-secondary="end-to-end, pushing events to clients" id="idm140417546316864"></a>
<a data-type="indexterm" data-primary="derived data" data-secondary="observing, by subscribing to streams" id="idm140417546315744"></a>
<a data-type="indexterm" data-primary="Elm (programming language)" id="idm140417546314624"></a>
<a data-type="indexterm" data-primary="Facebook" data-secondary="React, Flux, and Redux (user interface libraries)" id="idm140417546313776"></a>
Recent tools for developing stateful clients and user interfaces, such as the Elm language
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Czaplicki2013ig">30</a>] and Facebook’s toolchain of
React, Flux, and Redux [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Soper2015ue-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Soper2015ue">44</a>],
already manage internal client-side state by subscribing to a stream of events representing user
input or responses from a server, structured similarly to event sourcing (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_event_sourcing">“Event Sourcing”</a>).</p>

<p>It would be very natural to extend this programming model to also allow a server to push
state-change events into this client-side event pipeline. Thus, state changes could flow through an
end-to-end write path: from the interaction on one device that triggers a state change, via event
logs and through several derived data systems and stream processors, all the way to the user
interface of a person observing the state on another device. These state changes could be propagated
with fairly low delay—say, under one second end to end.</p>

<p><a data-type="indexterm" data-primary="real-time" data-secondary="publish/subscribe dataflow" id="idm140417546307232"></a>
Some applications, such as instant messaging and online games, already have such a “real-time”
architecture (in the sense of interactions with low delay, not in the sense of
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_clocks_realtime">“Response time guarantees”</a>). But why don’t we build all applications this way?</p>

<p>The challenge is that the assumption of stateless clients and request/response interactions is very
deeply ingrained in our databases, libraries, frameworks, and protocols. Many datastores support
read and write operations where a request returns one response, but much fewer provide an ability to
subscribe to changes—i.e., a request that returns a stream of responses over time (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_change_api">“API support for change streams”</a>).</p>

<p>In order to extend the write path all the way to the end user, we would need to fundamentally
rethink the way we build many of these systems: moving away from request/response interaction and
toward publish/subscribe dataflow [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2014ht">27</a>]. I
think that the advantages of more responsive user interfaces and better offline support would make
it worth the effort. If you are designing data systems, I hope that you will keep in mind the option
of subscribing to changes, not just querying the current state.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Reads are events too"><div class="sect3" id="sec_future_read_events">
<h3>Reads are events too</h3>

<p><a data-type="indexterm" data-primary="reads as events" id="idm140417546300048"></a>
<a data-type="indexterm" data-primary="events" data-secondary="reads as" id="idm140417546299216"></a>
We discussed that when a stream processor writes derived data to a store (database, cache, or index),
and when user requests query that store, the store acts as the boundary between the write path
and the read path. The store allows random-access read queries to the data that would otherwise
require scanning the whole event log.</p>

<p>In many cases, the data storage is separate from the streaming system. But recall that stream
processors also need to maintain state to perform aggregations and joins (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_joins">“Stream Joins”</a>).
This state is normally hidden inside the stream processor, but some frameworks allow it to also be
queried by outside clients
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thereska2016ul-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Thereska2016ul">45</a>],
turning the stream processor itself into a kind of simple database.</p>

<p>I would like to take that idea further. As discussed so far, the writes to the store go through an
event log, while reads are transient network requests that go directly to the nodes that store the
data being queried. This is a reasonable design, but not the only possible one. It is also possible
to represent read requests as streams of events, and send both the read events and the write events
through a stream processor; the processor responds to read events by emitting the result of the read
to an output stream [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2016vk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McSherry2016vk">46</a>].</p>

<p>When both the writes and the reads are represented as events, and routed to the same stream operator
in order to be handled, we are in fact performing a stream-table join between the stream of read
queries and the database. The read event needs to be sent to the database partition holding the data
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_routing">“Request Routing”</a>), just like batch and stream processors need to copartition inputs
on the same key when joining (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_reduce_joins">“Reduce-Side Joins and Grouping”</a>).</p>

<p>This correspondence between serving requests and performing joins is quite fundamental
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alvaro2015vs-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alvaro2015vs">47</a>]. A one-off read request just passes the request through the join
operator and then immediately forgets it; a subscribe request is a persistent join with past and
future events on the other side of the join.</p>

<p><a data-type="indexterm" data-primary="causal dependencies" data-secondary="capturing" id="idm140417546284928"></a>
Recording a log of read events potentially also has benefits with regard to tracking causal
dependencies and data provenance across a system: it would allow you to reconstruct what the user
saw before they made a particular decision. For example, in an online shop, it is likely that the
predicted shipping date and the inventory status shown to a customer affect whether they choose to
buy an item [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kerr2016va" class="totri-footnote">4</a>]. To analyze this connection,
you need to record the result of the user’s query of the shipping and inventory status.</p>

<p>Writing read events to durable storage thus enables better tracking of causal dependencies (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_capture_causality">“Ordering events to capture causality”</a>), but it incurs additional storage and I/O cost. Optimizing such
systems to reduce the overhead is still an open research problem
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ajoux2015wh_ch12" class="totri-footnote">2</a>]. But if you already log read requests
for operational purposes, as a side effect of request processing, it is not such a great change to
make the log the source of the requests instead.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Multi-partition data processing"><div class="sect3" id="sec_future_unbundled_multi_partition">
<h3>Multi-partition data processing</h3>

<p><a data-type="indexterm" data-primary="partitioning" data-secondary="multi-partition operations" id="idm140417546278256"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="observing derived state" data-tertiary="multi-partition data processing" id="idm140417546277088"></a>
For queries that only touch a single partition, the effort of sending queries through a stream and
collecting a stream of responses is perhaps overkill. However, this idea opens the possibility of
distributed execution of complex queries that need to combine data from several partitions, taking
advantage of the infrastructure for message routing, partitioning, and joining that is already
provided by stream processors.</p>

<p><a data-type="indexterm" data-primary="Storm (stream processor)" data-secondary="distributed RPC" id="idm140417546275008"></a>
Storm’s distributed RPC feature supports this usage pattern (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_actors_drpc">“Message passing and RPC”</a>). For
example, it has been used to compute the number of people who have seen a URL on Twitter—i.e., the
union of the follower sets of everyone who has tweeted that URL
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Marz2012wd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Marz2012wd">48</a>].
As the set of Twitter users is partitioned, this computation requires combining results from many
partitions.</p>

<p>Another example of this pattern occurs in fraud prevention: in order to assess the risk of whether a
particular purchase event is fraudulent, you can examine the reputation scores of the user’s IP
address, email address, billing address, shipping address, and so on. Each of these reputation
databases is itself partitioned, and so collecting the scores for a particular purchase event
requires a sequence of joins with differently partitioned datasets
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bice2016vl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bice2016vl">49</a>].</p>

<p>The internal query execution graphs of MPP databases have similar characteristics (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>). If you need to perform this kind of multi-partition join, it is probably
simpler to use a database that provides this feature than to implement it using a stream processor.
However, treating queries as streams provides an option for implementing large-scale applications
that run against the limits of conventional off-the-shelf solutions.
<a data-type="indexterm" data-primary="state" data-secondary="observing derived state" data-startref="ix_statederobs" id="idm140417546265472"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-secondary="observing derived state" data-startref="ix_unbdbderstate" id="idm140417546264128"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-tertiary="observing derived state" data-startref="ix_dbunbderstate" id="idm140417546262752"></a>
<a data-type="indexterm" data-primary="data integration" data-secondary="by unbundling databases" data-startref="ix_dataintunbundl" id="idm140417546261104"></a>
<a data-type="indexterm" data-primary="unbundling databases" data-startref="ix_unbunddb" id="idm140417546259728"></a>
<a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="unbundling databases" data-startref="ix_datsysfutunbun" id="idm140417546258624"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="unbundling" data-startref="ix_dbunbun" id="idm140417546256976"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Aiming for Correctness"><div class="sect1" id="sec_future_correctness">
<h1>Aiming for Correctness</h1>

<p><a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="correctness, constraints, and integrity" id="ix_datasysfutcorrect"></a>
With stateless services that only read data, it is not a big deal if something goes wrong: you can
fix the bug and restart the service, and everything returns to normal. Stateful systems such as
databases are not so simple: they are designed to remember things forever (more or less), so if
something goes wrong, the effects also potentially last forever—which means they require more
careful thought [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Majors2016wo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Majors2016wo">50</a>].</p>

<p>We want to build applications that are reliable and <em>correct</em> (i.e., programs whose semantics are
well defined and understood, even in the face of various faults). For approximately four decades,
the transaction properties of atomicity, isolation, and durability (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>) have been
the tools of choice for building correct applications. However, those foundations are weaker than
they seem: witness for example the confusion of weak isolation levels (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_isolation_levels">“Weak Isolation Levels”</a>).</p>

<p>In some areas, transactions are being abandoned entirely and replaced with models that offer
better performance and scalability, but much messier semantics (see for example
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_leaderless">“Leaderless Replication”</a>). <em>Consistency</em> is often talked about, but poorly defined (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_acid_consistency">“Consistency”</a> and <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a>). Some people assert that we should
“embrace weak consistency” for the sake of better availability, while lacking a clear idea of what
that actually means in practice.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of transactions" id="idm140417546243120"></a>
<a data-type="indexterm" data-primary="isolation (in transactions)" data-secondary="correctness and" id="idm140417546242016"></a>
For a topic that is so important, our understanding and our engineering methods are surprisingly
flaky.&nbsp;For example, it is very difficult to determine whether it is safe to run a particular
application at a particular transaction isolation level or replication configuration
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bernstein2000jk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bernstein2000jk">51</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jorwekar2007uq_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Jorwekar2007uq_ch12">52</a>].
Often simple solutions appear to work correctly when concurrency is low and there are no faults, but
turn out to have many subtle bugs in more demanding circumstances.</p>

<p><a data-type="indexterm" data-primary="Jepsen (fault tolerance testing)" id="idm140417546234896"></a>
For example, Kyle Kingsbury’s Jepsen experiments
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="KingsburyJepsen-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#KingsburyJepsen">53</a>]
have highlighted the stark discrepancies between some products’ claimed safety guarantees and their
actual behavior in the presence of network problems and crashes. Even if infrastructure products
like databases were free from problems, application code would still need to correctly use the
features they provide, which is error-prone if the configuration is hard to understand (which is the
case with weak isolation levels, quorum configurations, and so on).</p>

<p>If your application can tolerate occasionally corrupting or losing data in unpredictable ways, life
is a lot simpler, and you might be able to get away with simply crossing your fingers and hoping for
the best. On the other hand, if you need stronger assurances of correctness, then serializability and
atomic commit are established approaches, but they come at a cost: they typically only work in a
single datacenter (ruling out geographically distributed architectures), and they limit the scale
and fault-tolerance properties you can achieve.</p>

<p>While the traditional transaction approach is not going away, I also believe it is not the last word
in making applications correct and resilient to faults. In this section I will suggest some ways of
thinking about correctness in the context of dataflow architectures.</p>








<section data-type="sect2" data-pdf-bookmark="The End-to-End Argument for Databases"><div class="sect2" id="sec_future_end_to_end">
<h2>The End-to-End Argument for Databases</h2>

<p>Just because an application uses a data system that provides comparatively strong safety properties,
such as serializable transactions, that does not mean the application is guaranteed to be free from
data loss or corruption. For example, if an application has a bug that causes it to write incorrect
data, or delete data from a database, serializable transactions aren’t going to save you.</p>

<p>This example may seem frivolous, but it is worth taking seriously: application bugs occur, and
people make mistakes. I used this example in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_immutability">“State, Streams, and Immutability”</a> to argue in favor of
immutable and append-only data, because it is easier to recover from such mistakes if you remove the
ability of faulty code to destroy good data.</p>

<p>Although immutability is useful, it is not a cure-all by itself. Let’s look at a more subtle example
of data corruption that can occur.</p>










<section data-type="sect3" data-pdf-bookmark="Exactly-once execution of an operation"><div class="sect3" id="idm140417546224928">
<h3>Exactly-once execution of an operation</h3>

<p><a data-type="indexterm" data-primary="exactly-once semantics" id="idm140417546223760"></a>
<a data-type="indexterm" data-primary="effectively-once semantics" id="idm140417546222928"></a>
In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_fault_tolerance">“Fault Tolerance”</a> we encountered an idea called <em>exactly-once</em> (or
<em>effectively-once</em>) semantics. If something goes wrong while processing a message, you can either
give up (drop the message—i.e., incur data loss) or try again. If you try again, there is the risk
that it actually succeeded the first time, but you just didn’t find out about the success, and so
the message ends up being processed twice.</p>

<p>Processing twice is a form of data corruption: it is undesirable to charge a customer twice for the
same service (billing them too much) or increment a counter twice (overstating some metric). In this
context, <em>exactly-once</em> means arranging the computation such that the final effect is the same as if
no faults had occurred, even if the operation actually was retried due to some fault. We previously
discussed a few approaches for achieving this goal.</p>

<p><a data-type="indexterm" data-primary="idempotence" data-secondary="idempotent operations" id="idm140417546218432"></a>
<a data-type="indexterm" data-primary="fencing (preventing split brain)" data-secondary="stream processors writing to databases" id="idm140417546217104"></a>
<a data-type="indexterm" data-primary="stream processing" data-secondary="accessing external services within job" id="idm140417546215968"></a>
One of the most effective approaches is to make the operation <em>idempotent</em> (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_idempotence">“Idempotence”</a>); that is, to ensure that it has the same effect, no matter whether it is
executed once or multiple times. However, taking an operation that is not naturally idempotent and
making it idempotent requires some effort and care: you may need to maintain some additional
metadata (such as the set of operation IDs that have updated a value), and ensure fencing when
failing over from one node to another (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_lock_fencing">“The leader and the lock”</a>).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Duplicate suppression"><div class="sect3" id="idm140417546212160">
<h3>Duplicate suppression</h3>

<p><a data-type="indexterm" data-primary="duplicates, suppression of" data-seealso="idempotence" id="idm140417546210784"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="reliability and duplicate suppression" id="idm140417546209712"></a>
The same pattern of needing to suppress duplicates occurs in many other places besides stream
processing. For example, TCP uses sequence numbers on packets to put them in the correct order at
the recipient, and to determine whether any packets were lost or duplicated on the network. Any lost
packets are retransmitted and any duplicates are removed by the TCP stack before it hands the data
to an application.</p>

<p>However, this duplicate suppression only works within the context of a single TCP connection.
Imagine the TCP connection is a client’s connection to a database, and it is currently executing the
transaction in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_non_idempotent">Example&nbsp;12-1</a>. In many databases, a transaction is tied to a client
connection (if the client sends several queries, the database knows that they belong to the same
transaction because they are sent on the same TCP connection). If the client suffers a network
interruption and connection timeout after sending the <code>COMMIT</code>, but before hearing back from the
database server, it does not know whether the transaction has been committed or aborted
(<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#fig_distributed_network">Figure&nbsp;8-1</a>).</p>
<div id="fig_future_non_idempotent" data-type="example">
<h5><span class="label">Example 12-1. </span>A nonidempotent transfer of money from one account to another</h5>

<pre data-type="programlisting" data-code-language="sql"><code class="k">BEGIN</code> <code class="n">TRANSACTION</code><code class="p">;</code>
<code class="k">UPDATE</code> <code class="n">accounts</code> <code class="k">SET</code> <code class="n">balance</code> <code class="o">=</code> <code class="n">balance</code> <code class="o">+</code> <code class="mi">11</code><code class="p">.</code><code class="mi">00</code> <code class="k">WHERE</code> <code class="n">account_id</code> <code class="o">=</code> <code class="mi">1234</code><code class="p">;</code>
<code class="k">UPDATE</code> <code class="n">accounts</code> <code class="k">SET</code> <code class="n">balance</code> <code class="o">=</code> <code class="n">balance</code> <code class="o">-</code> <code class="mi">11</code><code class="p">.</code><code class="mi">00</code> <code class="k">WHERE</code> <code class="n">account_id</code> <code class="o">=</code> <code class="mi">4321</code><code class="p">;</code>
<code class="k">COMMIT</code><code class="p">;</code></pre></div>

<p>The client can reconnect to the database and retry the transaction, but now it is outside of the
scope of TCP duplicate suppression. Since the transaction in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_non_idempotent">Example&nbsp;12-1</a> is not
idempotent, it could happen that $22 is transferred instead of the desired $11. Thus, even though
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_non_idempotent">Example&nbsp;12-1</a> is a standard example for transaction atomicity, it is actually not
correct, and real banks do not work like this
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd" class="totri-footnote">3</a>].</p>

<p>Two-phase commit (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_2pc">“Atomic Commit and Two-Phase Commit (2PC)”</a>) protocols break the 1:1 mapping between a TCP
connection and a transaction, since they must allow a transaction coordinator to reconnect to a
database after a network fault, and tell it whether to commit or abort an in-doubt transaction. Is
this sufficient to ensure that the transaction will only be executed once? Unfortunately not.</p>

<p>Even if we can suppress duplicate transactions between the database client and server, we still need
to worry about the network between the end-user device and the application server. For example, if
the end-user client is a web browser, it probably uses an HTTP POST request to submit an instruction
to the server. Perhaps the user is on a weak cellular data connection, and they succeed in sending
the POST, but the signal becomes too weak before they are able to receive the response from the
server.</p>

<p>In this case, the user will probably be shown an error message, and they may retry manually. Web
browsers warn, “Are you sure you want to submit this form again?”—and the user says yes, because
they wanted the operation to happen. (The Post/Redirect/Get pattern
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Jouravlev2004wh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Jouravlev2004wh">54</a>] avoids this warning message in normal
operation, but it doesn’t help if the POST request times out.) From the web server’s point of view
the retry is a separate request, and from the database’s point of view it is a separate transaction.
The usual deduplication mechanisms don’t help.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Operation identifiers"><div class="sect3" id="idm140417546154032">
<h3>Operation identifiers</h3>

<p><a data-type="indexterm" data-primary="operation identifiers" id="idm140417546152592"></a>
<a data-type="indexterm" data-primary="duplicates, suppression of" data-secondary="using a unique ID" id="idm140417546151760"></a>
<a data-type="indexterm" data-primary="idempotence" data-secondary="by giving operations unique IDs" id="idm140417546150688"></a>
To make the operation idempotent through several hops of network communication, it is not sufficient
to rely just on a transaction mechanism provided by a database—you need to consider the <em>end-to-end</em>
flow of the request.</p>

<p>For example, you could generate a unique identifier for an operation (such as a UUID) and include it
as a hidden form field in the client application, or calculate a hash of all the relevant form
fields to derive the operation ID [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd" class="totri-footnote">3</a>]. If
the web browser submits the POST request twice, the two requests will have the same operation ID.
You can then pass that operation ID all the way through to the database and check that you only ever
execute one operation with a given ID, as shown in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a>.</p>
<div id="fig_future_request_id" data-type="example">
<h5><span class="label">Example 12-2. </span>Suppressing duplicate requests using a unique ID</h5>

<pre data-type="programlisting" data-code-language="sql"><code class="k">ALTER</code> <code class="k">TABLE</code> <code class="n">requests</code> <code class="k">ADD</code> <code class="k">UNIQUE</code> <code class="p">(</code><code class="n">request_id</code><code class="p">);</code>

<code class="k">BEGIN</code> <code class="n">TRANSACTION</code><code class="p">;</code>

<code class="k">INSERT</code> <code class="k">INTO</code> <code class="n">requests</code>
  <code class="p">(</code><code class="n">request_id</code><code class="p">,</code> <code class="n">from_account</code><code class="p">,</code> <code class="n">to_account</code><code class="p">,</code> <code class="n">amount</code><code class="p">)</code>
  <code class="k">VALUES</code><code class="p">(</code><code class="s1">'0286FDB8-D7E1-423F-B40B-792B3608036C'</code><code class="p">,</code> <code class="mi">4321</code><code class="p">,</code> <code class="mi">1234</code><code class="p">,</code> <code class="mi">11</code><code class="p">.</code><code class="mi">00</code><code class="p">);</code>

<code class="k">UPDATE</code> <code class="n">accounts</code> <code class="k">SET</code> <code class="n">balance</code> <code class="o">=</code> <code class="n">balance</code> <code class="o">+</code> <code class="mi">11</code><code class="p">.</code><code class="mi">00</code> <code class="k">WHERE</code> <code class="n">account_id</code> <code class="o">=</code> <code class="mi">1234</code><code class="p">;</code>
<code class="k">UPDATE</code> <code class="n">accounts</code> <code class="k">SET</code> <code class="n">balance</code> <code class="o">=</code> <code class="n">balance</code> <code class="o">-</code> <code class="mi">11</code><code class="p">.</code><code class="mi">00</code> <code class="k">WHERE</code> <code class="n">account_id</code> <code class="o">=</code> <code class="mi">4321</code><code class="p">;</code>

<code class="k">COMMIT</code><code class="p">;</code></pre></div>

<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="ensuring idempotence" id="idm140417546143616"></a>
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a> relies on a uniqueness constraint on the <code>request_id</code> column. If a
transaction attempts to insert an ID that already exists, the <code>INSERT</code> fails and the transaction is
aborted, preventing it from taking effect twice. Relational databases can generally maintain a
uniqueness constraint correctly, even at weak isolation levels (whereas an application-level
check-then-insert may fail under nonserializable isolation, as discussed in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_write_skew">“Write Skew and Phantoms”</a>).</p>

<p><a data-type="indexterm" data-primary="event sourcing" data-secondary="large, reliable data systems" id="idm140417546027808"></a>
Besides suppressing duplicate requests, the <code>requests</code> table in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a> acts as a
kind of event log, hinting in the direction of event sourcing (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_event_sourcing">“Event Sourcing”</a>).
The updates to the account balances don’t actually have to happen in the same transaction as the
insertion of the event, since they are redundant and could be derived from the request event in a
downstream consumer—as long as the event is processed exactly once, which can again be enforced
using the request ID.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The end-to-end argument"><div class="sect3" id="sec_future_e2e_argument">
<h3>The end-to-end argument</h3>

<p><a data-type="indexterm" data-primary="databases" data-secondary="end-to-end argument for" id="ix_dbendtoend"></a>
<a data-type="indexterm" data-primary="end-to-end argument" id="ix_enddb"></a>
This scenario of suppressing duplicate transactions is just one example of a more general principle
called the <em>end-to-end argument</em>, which was articulated by Saltzer, Reed, and Clark in 1984
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saltzer1984do_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Saltzer1984do_ch12">55</a>]:</p>
<blockquote>
<p>The function in question can completely and correctly be implemented only with the knowledge and
help of the application standing at the endpoints of the communication system. Therefore, providing
that questioned function as a feature of the communication system itself is not possible. (Sometimes
an incomplete version of the function provided by the communication system may be useful as a
performance enhancement.)</p></blockquote>

<p>In our example, the <em>function in question</em> was duplicate suppression. We saw that TCP suppresses
duplicate packets at the TCP connection level, and some stream processors provide so-called
exactly-once semantics at the message processing level, but that is not enough to prevent a user
from submitting a duplicate request if the first one times out.  By themselves, TCP, database
transactions, and stream processors cannot entirely rule out these duplicates. Solving the problem
requires an end-to-end solution: a transaction identifier that is passed all the way from the
end-user client to the database.</p>

<p><a data-type="indexterm" data-primary="integrity" data-secondary="integrity checks" data-tertiary="end-to-end" id="idm140417546014176"></a>
<a data-type="indexterm" data-primary="hard disks" data-secondary="detecting corruption" id="idm140417546012640"></a>
<a data-type="indexterm" data-primary="solid state drives (SSDs)" data-secondary="detecting corruption" id="idm140417546011536"></a>
<a data-type="indexterm" data-primary="Ethernet (networks)" data-secondary="packet checksums" id="idm140417546010416"></a>
<a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="packet checksums" id="idm140417546009312"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="detecting" id="idm140417546008192"></a>
The end-to-end argument also applies to checking the integrity of data: checksums built into
Ethernet, TCP, and TLS can detect corruption of packets in the network, but they cannot detect
corruption due to bugs in the software at the sending and receiving ends of the network connection,
or corruption on the disks where the data is stored. If you want to catch all possible sources of
data corruption, you also need end-to-end checksums.</p>

<p><a data-type="indexterm" data-primary="cryptography" data-secondary="end-to-end encryption and authentication" id="idm140417546006512"></a>
A similar argument applies with encryption
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Saltzer1984do_ch12">55</a>]:
the password on your home WiFi network protects against people snooping your WiFi traffic, but not
against attackers elsewhere on the internet; TLS/SSL between your client and the server protects
against network attackers, but not against compromises of the server. Only end-to-end encryption and
authentication can protect against all of these things.</p>

<p>Although the low-level features (TCP duplicate suppression, Ethernet checksums, WiFi encryption)
cannot provide the desired end-to-end features by themselves, they are still useful, since they
reduce the probability of problems at the higher levels. For example, HTTP requests would often get
mangled if we didn’t have TCP putting the packets back in the right order. We just need to remember
that the low-level reliability features are not by themselves sufficient to ensure end-to-end
correctness.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Applying end-to-end thinking in data systems"><div class="sect3" id="idm140417546003120">
<h3>Applying end-to-end thinking in data systems</h3>

<p><a data-type="indexterm" data-primary="fault tolerance" data-secondary="in log-based systems" id="idm140417546001712"></a>
This brings me back to my original thesis: just because an application uses a data system that
provides comparatively strong safety properties, such as serializable transactions, that does not
mean the application is guaranteed to be free from data loss or corruption. The application itself
needs to take end-to-end measures, such as duplicate suppression, as well.</p>

<p>That is a shame, because fault-tolerance mechanisms are hard to get right. Low-level reliability
mechanisms, such as those in TCP, work quite well, and so the remaining higher-level faults occur
fairly rarely. It would be really nice to wrap up the remaining high-level fault-tolerance machinery
in an abstraction so that application code needn’t worry about it—but I fear that we have not yet
found the right abstraction.</p>

<p>Transactions have long been seen as a good abstraction, and I do believe that they are useful. As
discussed in the introduction to <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#ch_transactions">Chapter&nbsp;7</a>, they take a wide range of possible issues
(concurrent writes, constraint violations, crashes, network interruptions, disk failures) and
collapse them down to two possible outcomes: commit or abort. That is a huge simplification of the
programming model, but I fear that it is not enough.</p>

<p>Transactions are expensive, especially when they involve heterogeneous storage technologies (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_dist_trans">“Distributed Transactions in Practice”</a>). When we refuse to use distributed transactions because they are too
expensive, we end up having to reimplement fault-tolerance mechanisms in application code. As
numerous examples throughout this book have shown, reasoning about concurrency and partial failure
is difficult and counterintuitive, and so I suspect that most application-level mechanisms do not
work correctly. The consequence is lost or corrupted data.</p>

<p>For these reasons, I think it is worth exploring fault-tolerance abstractions that make it easy to
provide application-specific end-to-end correctness properties, but also maintain good performance
and good operational characteristics in a large-scale distributed environment.
<a data-type="indexterm" data-primary="end-to-end argument" data-startref="ix_enddb" id="idm140417545995296"></a>
<a data-type="indexterm" data-primary="databases" data-secondary="end-to-end argument for" data-startref="ix_dbendtoend" id="idm140417545994192"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Enforcing Constraints"><div class="sect2" id="sec_future_constraints">
<h2>Enforcing Constraints</h2>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="in log-based systems" id="ix_correctconstr"></a>
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="in log-based systems" id="ix_constrain"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="enforcing constraints" data-see="constraints" id="idm140417545988512"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="avoiding" id="ix_disttransavoid"></a>
Let’s think about correctness in the context of the ideas around unbundling databases
(<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_unbundling">“Unbundling Databases”</a>). We saw that end-to-end duplicate suppression can be achieved with a
request ID that is passed all the way from the client to the database that records the write. What
about other kinds of constraints?</p>

<p>In particular, let’s focus on uniqueness constraints—such as the one we relied on in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a>. In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_uniqueness">“Constraints and uniqueness guarantees”</a> we saw several other examples of
application features that need to enforce uniqueness: a username or email address must uniquely
identify a user, a file storage service cannot have more than one file with the same name, and two
people cannot book the same seat on a flight or in a theater.</p>

<p><a data-type="indexterm" data-primary="meeting room booking (example)" id="idm140417545981696"></a>
Other kinds of constraints are very similar: for example, ensuring that an account balance never goes
negative, that you don’t sell more items than you have in stock in the warehouse, or that a meeting
room does not have overlapping bookings. Techniques that enforce uniqueness can often be used for
these kinds of constraints as well.</p>










<section data-type="sect3" data-pdf-bookmark="Uniqueness constraints require consensus"><div class="sect3" id="idm140417545980368">
<h3>Uniqueness constraints require consensus</h3>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict detection" data-tertiary="in log-based systems" id="idm140417545978992"></a>
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="relation to consensus" id="idm140417545977616"></a>
<a data-type="indexterm" data-primary="consensus" data-secondary="relation to uniqueness constraints" id="idm140417545976512"></a>
<a data-type="indexterm" data-primary="uniqueness constraints" data-secondary="requiring consensus" id="idm140417545975392"></a>
In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#ch_consistency">Chapter&nbsp;9</a> we saw that in a distributed setting, enforcing a uniqueness constraint
requires consensus: if there are several concurrent requests with the same value, the system somehow
needs to decide which one of the conflicting operations is accepted, and reject the others as
violations of the constraint.</p>

<p>The most common way of achieving this consensus is to make a single node the leader, and put it in
charge of making all the decisions. That works fine as long as you don’t mind funneling all requests
through a single node (even if the client is on the other side of the world), and as long as that
node doesn’t fail. If you need to tolerate the leader failing, you’re back at the consensus problem
again (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_consensus_leader">“Single-leader replication and consensus”</a>).</p>

<p>Uniqueness checking can be scaled out by partitioning based on the value that needs to be unique.
For example, if you need to ensure uniqueness by request ID, as in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a>, you
can ensure all requests with the same request ID are routed to the same partition (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#ch_partitioning">Chapter&nbsp;6</a>). If you need usernames to be unique, you can partition by hash of username.</p>

<p>However, asynchronous multi-master replication is ruled out, because it could happen that different
masters concurrently accept conflicting writes, and thus the values are no longer unique (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_implementing_linearizable">“Implementing Linearizable Systems”</a>). If you want to be able to immediately reject any
writes that would violate the constraint, synchronous coordination is unavoidable
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bailis2014th_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2014th_ch12">56</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Uniqueness in log-based messaging"><div class="sect3" id="sec_future_uniqueness_log">
<h3>Uniqueness in log-based messaging</h3>

<p><a data-type="indexterm" data-primary="messaging systems" data-secondary="uniqueness in log-based messaging" id="idm140417545963648"></a>
<a data-type="indexterm" data-primary="logs (data structure)" data-secondary="implementing uniqueness constraints" id="idm140417545962368"></a>
<a data-type="indexterm" data-primary="uniqueness constraints" data-secondary="uniqueness in log-based messaging" id="idm140417545961248"></a>
<a data-type="indexterm" data-primary="total order broadcast" id="idm140417545960128"></a>
The log ensures that all consumers see messages in the same order—a guarantee that is formally known
as <em>total order broadcast</em> and is equivalent to consensus (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_total_order">“Total Order Broadcast”</a>). In
the unbundled database approach with log-based messaging, we can use a very similar approach to
enforce uniqueness constraints.</p>

<p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in stream processing" id="idm140417545957616"></a>
A stream processor consumes all the messages in a log partition sequentially on a single thread (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_logs_vs_messaging">“Logs compared to traditional messaging”</a>). Thus, if the log is partitioned based on the value that needs to
be unique, a stream processor can unambiguously and deterministically decide which one of several
conflicting operations came first. For example, in the case of several users trying to claim the
same username [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Yarmula2016wv-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Yarmula2016wv">57</a>]:</p>
<ol>
<li>
<p>Every request for a username is encoded as a message, and appended to a partition determined by
the hash of the username.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in stream processing" id="idm140417545951248"></a>
A stream processor sequentially reads the requests in the log, using a local database to keep
track of which usernames are taken. For every request for a username that is available, it
records the name as taken and emits a success message to an output stream. For every request for
a username that is already taken, it emits a rejection message to an output stream.</p>
</li>
<li>
<p>The client that requested the username watches the output stream and waits for a success or
rejection message corresponding to its request.</p>
</li>

</ol>

<p>This algorithm is basically the same as in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_abcast_to_lin">“Implementing linearizable storage using total order broadcast”</a>. It scales easily to a
large request throughput by increasing the number of partitions, as each partition can be processed
independently.</p>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="determining what is a conflict" id="idm140417545946720"></a>
<a data-type="indexterm" data-primary="Bayou (database)" id="idm140417545945600"></a>
The approach works not only for uniqueness constraints, but also for many other kinds of constraints.
Its fundamental principle is that any writes that may conflict are routed to the same partition and
processed sequentially. As discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_what_is_conflict">“What is a conflict?”</a> and
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_write_skew">“Write Skew and Phantoms”</a>, the definition of a conflict may depend on the application, but the
stream processor can use arbitrary logic to validate a request. This idea is similar to the approach
pioneered by Bayou in the 1990s
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terry1995dn_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Terry1995dn_ch12">58</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Multi-partition request processing"><div class="sect3" id="idm140417545939664">
<h3>Multi-partition request processing</h3>

<p><a data-type="indexterm" data-primary="partitioning" data-secondary="multi-partition operations" data-tertiary="enforcing constraints" id="idm140417545938288"></a>
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="in log-based systems" data-tertiary="across multiple partitions" id="idm140417545936736"></a>
<a data-type="indexterm" data-primary="operation identifiers" id="idm140417545935344"></a>
<a data-type="indexterm" data-primary="duplicates, suppression of" data-secondary="using a unique ID" id="idm140417545934512"></a>
<a data-type="indexterm" data-primary="idempotence" data-secondary="by giving operations unique IDs" id="idm140417545933392"></a>
Ensuring that an operation is executed atomically, while satisfying constraints, becomes more
interesting when several partitions are involved. In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#fig_future_request_id">Example&nbsp;12-2</a>, there are
potentially three partitions: the one containing the request ID, the one containing the payee
account, and the one containing the payer account. There is no reason why those three things should
be in the same partition, since they are all independent from each other.</p>

<p><a data-type="indexterm" data-primary="coordination" data-secondary="cross-partition ordering" id="idm140417545930896"></a>
In the traditional approach to databases, executing this transaction would require an atomic commit
across all three partitions, which essentially forces it into a total order with respect to all
other transactions on any of those partitions. Since there is now cross-partition coordination,
different partitions can no longer be processed independently, so throughput is likely to suffer.</p>

<p><a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="atomic commit" data-tertiary="avoiding" id="idm140417545929168"></a>
However, it turns out that equivalent correctness can be achieved with partitioned logs, and without
an atomic commit:</p>
<ol>
<li>
<p>The request to transfer money from account A to account B is given a unique request ID by the
client, and appended to a log partition based on the request ID.</p>
</li>
<li>
<p>A stream processor reads the log of requests. For each request message it emits two messages to
output streams: a debit instruction to the payer account A (partitioned by A), and a credit
instruction to the payee account B (partitioned by B). The original request ID is included in
those emitted messages.</p>
</li>
<li>
<p>Further processors consume the streams of credit and debit instructions, deduplicate by request ID,
and apply the changes to the account balances.</p>
</li>

</ol>

<p>Steps 1 and 2 are necessary because if the client directly sent the credit and debit instructions,
it would require an atomic commit across those two partitions to ensure that either both or neither
happen. To avoid the need for a distributed transaction, we first durably log the request as a
single message, and then derive the credit and debit instructions from that first message.
Single-object writes are atomic in almost all data systems (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_single_object">“Single-object writes”</a>),
and so the request either appears in the log or it doesn’t, without any need for a multi-partition
atomic commit.</p>

<p><a data-type="indexterm" data-primary="checkpointing" data-secondary="in stream processors" id="idm140417545921872"></a>
If the stream processor in step 2 crashes, it resumes processing from its last checkpoint. In doing
so, it does not skip any request messages, but it may process requests multiple times and produce
duplicate credit and debit instructions. However, since it is deterministic, it will just produce
the same instructions again, and the processors in step 3 can easily deduplicate them using the
end-to-end request ID.</p>

<p>If you want to ensure that the payer account is not overdrawn by this transfer, you can additionally
have a stream processor (partitioned by payer account number) that maintains account balances and
validates transactions. Only valid transactions would then be placed in the request log in step 1.</p>

<p>By breaking down the multi-partition transaction into two differently partitioned stages and using
the end-to-end request ID, we have achieved the same correctness property (every request is applied
exactly once to both the payer and payee accounts), even in the presence of faults, and without using
an atomic commit protocol. The idea of using multiple differently partitioned stages is similar to
what we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_unbundled_multi_partition">“Multi-partition data processing”</a> (see also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_concurrency">“Concurrency control”</a>).
<a data-type="indexterm" data-primary="constraints (databases)" data-secondary="in log-based systems" data-startref="ix_constrain" id="idm140417545917104"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="in log-based systems" data-startref="ix_correctconstr" id="idm140417545915728"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Timeliness and Integrity"><div class="sect2" id="sec_future_integrity">
<h2>Timeliness and Integrity</h2>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="timeliness and integrity" id="ix_correcttimely"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in log-based systems" id="ix_faulttollog"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="of derived data systems" id="idm140417545909824"></a>
A convenient property of transactions is that they are typically linearizable (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_linearizability">“Linearizability”</a>): that is, a writer waits until a transaction is committed, and
thereafter its writes are immediately visible to all readers.</p>

<p>This is not the case when unbundling an operation across multiple stages of stream processors:
consumers of a log are asynchronous by design, so a sender does not wait until its message has been
processed by consumers. However, it is possible for a client to wait for a message to appear on an
output stream. This is what we did in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_uniqueness_log">“Uniqueness in log-based messaging”</a> when checking whether a
uniqueness constraint was satisfied.</p>

<p>In this example, the correctness of the uniqueness check does not depend on whether the sender of
the message waits for the outcome. The waiting only has the purpose of synchronously informing the
sender whether or not the uniqueness check succeeded, but this notification can be decoupled from
the effects of processing the message.</p>

<p><a data-type="indexterm" data-primary="consistency" id="idm140417545905072"></a>
<a data-type="indexterm" data-primary="consistency" data-secondary="timeliness and integrity" id="idm140417545904240"></a>
More generally, I think the term <em>consistency</em> conflates two different requirements that are worth
considering separately:</p>
<dl>
<dt>Timeliness</dt>
<dd>
<p><a data-type="indexterm" data-primary="timeliness" id="idm140417545901360"></a>
<a data-type="indexterm" data-primary="staleness (old data)" data-secondary="versus timeliness" id="idm140417545900528"></a>
Timeliness means ensuring that users observe the system in an up-to-date state. We saw previously
that if a user reads from a stale copy of the data, they may observe it in an inconsistent state
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_lag">“Problems with Replication Lag”</a>). However, that inconsistency is temporary, and will eventually be
resolved simply by waiting and trying again.</p>

<p><a data-type="indexterm" data-primary="read-after-write consistency" id="idm140417545898016"></a>
The CAP theorem (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_linearizability_cost">“The Cost of Linearizability”</a>) uses consistency in the sense of linearizability,
which is a strong way of achieving timeliness. Weaker timeliness properties like <em>read-after-write</em>
consistency (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_ryw">“Reading Your Own Writes”</a>) can also be useful.</p>
</dd>
<dt>Integrity</dt>
<dd>
<p><a data-type="indexterm" data-primary="integrity" id="idm140417545870336"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="integrity as absence of" id="idm140417545869504"></a>
Integrity means absence of corruption; i.e., no data loss, and no contradictory or false data. In
particular, if some derived dataset is maintained as a view onto some underlying data (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_state_from_log">“Deriving current state from the event log”</a>), the derivation must be correct. For example, a database index must
correctly reflect the contents of the database—an index in which some records are missing is
not very useful.</p>

<p>If integrity is violated, the inconsistency is permanent: waiting and trying again is not going to
fix database corruption in most cases. Instead, explicit checking and repair is needed. In the
context of ACID transactions (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_acid">“The Meaning of ACID”</a>), consistency is usually understood as
some kind of application-specific notion of integrity. Atomicity and durability are important tools
for preserving integrity.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="eventual consistency" data-secondary="and perpetual inconsistency" id="idm140417545865040"></a>
<a data-type="indexterm" data-primary="perpetual inconsistency" id="idm140417545863920"></a>
In slogan form: violations of timeliness are “eventual consistency,” whereas violations of integrity
are “perpetual inconsistency.”</p>

<p>I am going to assert that in most applications, integrity is much more important than timeliness.
Violations of timeliness can be annoying and confusing, but violations of integrity can be
catastrophic.</p>

<p>For example, on your credit card statement, it is not surprising if a transaction that you made
within the last 24 hours does not yet appear—it is normal that these systems have a certain lag.
We know that banks reconcile and settle transactions asynchronously, and timeliness is not very
important here [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd" class="totri-footnote">3</a>]. However, it would be
very bad if the statement balance was not equal to the sum of the transactions plus the previous
statement balance (an error in the sums), or if a transaction was charged to you but not paid to the
merchant (disappearing money). Such problems would be violations of the integrity of the system.</p>










<section data-type="sect3" data-pdf-bookmark="Correctness of dataflow systems"><div class="sect3" id="idm140417545860288">
<h3>Correctness of dataflow systems</h3>

<p><a data-type="indexterm" data-primary="integrity" data-secondary="correctness of dataflow systems" id="idm140417545858976"></a>
<a data-type="indexterm" data-primary="dataflow" data-secondary="correctness of dataflow systems" id="idm140417545857808"></a>
<a data-type="indexterm" data-primary="timeliness" data-secondary="correctness of dataflow systems" id="idm140417545856688"></a>
ACID transactions usually provide both timeliness (e.g., linearizability) and integrity (e.g.,
atomic commit) guarantees. Thus, if you approach application correctness from the point of view of
ACID transactions, the distinction between timeliness and integrity is fairly inconsequential.</p>

<p>On the other hand, an interesting property of the event-based dataflow systems that we have
discussed in this chapter is that they decouple timeliness and integrity. When processing event
streams asynchronously, there is no guarantee of timeliness, unless you explicitly build consumers
that wait for a message to arrive before returning. But integrity is in fact central to streaming
systems.</p>

<p><a data-type="indexterm" data-primary="exactly-once semantics" data-secondary="preservation of integrity" id="idm140417545854176"></a>
<a data-type="indexterm" data-primary="effectively-once semantics" data-secondary="preservation of integrity" id="idm140417545853056"></a>
<em>Exactly-once</em> or <em>effectively-once</em> semantics (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_fault_tolerance">“Fault Tolerance”</a>) is a mechanism
for preserving integrity. If an event is lost, or if an event takes effect twice, the integrity of a
data system could be violated. Thus, fault-tolerant message delivery and duplicate suppression
(e.g., idempotent operations) are important for maintaining the integrity of a data system in the
face of faults.</p>

<p>As we saw in the last section, reliable stream processing systems can preserve integrity without
requiring distributed transactions and an atomic commit protocol, which means they can potentially
achieve comparable correctness with much better performance and operational robustness. We achieved
this integrity through a combination of mechanisms:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="event sourcing" data-secondary="large, reliable data systems" id="idm140417545848160"></a>
Representing the content of the write operation as a single message, which can easily be written
atomically—an approach that fits very well with event sourcing (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_event_sourcing">“Event Sourcing”</a>)</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="computing derived data" id="idm140417545845296"></a>
Deriving all other state updates from that single message using deterministic derivation
functions, similarly to stored procedures (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_serial">“Actual Serial Execution”</a> and
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_dataflow_derivation">“Application code as a derivation function”</a>)</p>
</li>
<li>
<p>Passing a client-generated request ID through all these levels of processing, enabling end-to-end
duplicate suppression and idempotence</p>
</li>
<li>
<p>Making messages immutable and allowing derived data to be reprocessed from time to time,
which makes it easier to recover from bugs (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_immutability_pros">“Advantages of immutable events”</a>)</p>
</li>
</ul>

<p>This combination of mechanisms seems to me a very promising direction for building fault-tolerant
applications in the future.
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in log-based systems" data-startref="ix_faulttollog" id="idm140417545838656"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Loosely interpreted constraints"><div class="sect3" id="idm140417545837280">
<h3>Loosely interpreted constraints</h3>

<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="asynchronously checked" id="idm140417545836000"></a>
<a data-type="indexterm" data-primary="uniqueness constraints" data-secondary="asynchronously checked" id="idm140417545834896"></a>
As discussed previously, enforcing a uniqueness constraint requires consensus, typically implemented by
funneling all events in a particular partition through a single node. This limitation is unavoidable
if we want the traditional form of uniqueness constraint, and stream processing cannot avoid it.</p>

<p>However, another thing to realize is that many real applications can actually get away with much
weaker notions of uniqueness:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="compensating transactions" id="idm140417545832016"></a>
<a data-type="indexterm" data-primary="sagas" data-see="compensating transactions" id="idm140417545830944"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="compensating" data-see="compensating transactions" id="idm140417545829824"></a>
If two people concurrently register the same username or book the same seat, you can send one of
them a message to apologize, and ask them to choose a different one. This kind of change to correct
a mistake is called a <em>compensating transaction</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray1981wi_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gray1981wi_ch12">59</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GarciaMolina1987ca_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#GarciaMolina1987ca_ch12">60</a>].</p>
</li>
<li>
<p>If customers order more items than you have in your warehouse, you can order in more stock,
apologize to customers for the delay, and offer them a discount. This is actually the same as what
you’d have to do if, say, a forklift truck ran over some of the items in your warehouse, leaving you
with fewer items in stock than you thought you had [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Helland2007tp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2007tp">61</a>]. Thus, the apology
workflow already needs to be part of your business processes anyway, and so it might be
unnecessary to require a linearizable constraint on the number of items in stock.</p>
</li>
<li>
<p>Similarly, many airlines overbook airplanes in the expectation that some passengers will miss
their flight, and many hotels overbook rooms, expecting that some guests will cancel. In these
cases, the constraint of “one person per seat” is deliberately violated for business reasons, and
compensation processes (refunds, upgrades, providing a complimentary room at a neighboring hotel)
are put in place to handle situations in which demand exceeds supply. Even if there was no
overbooking, apology and compensation processes would be needed in order to deal with flights
being cancelled due to bad weather or staff on strike—recovering from such issues is just a normal
part of business [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd" class="totri-footnote">3</a>].</p>
</li>
<li>
<p>If someone withdraws more money than they have in their account, the bank can charge them an
overdraft fee and ask them to pay back what they owe. By limiting the total withdrawals per day,
the risk to the bank is bounded.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="conflicts" data-secondary="conflict resolution" data-tertiary="by apologizing" id="idm140417545815040"></a>
In many business contexts, it is actually acceptable to temporarily violate a constraint and fix
it up later by apologizing. The cost of the apology (in terms of money or reputation) varies, but it
is often quite low: you can’t unsend an email, but you can send a follow-up email with a
correction. If you accidentally charge a credit card twice, you can refund one of the
charges, and the cost to you is just the processing fees and perhaps a customer complaint. Once
money has been paid out of an ATM, you can’t directly get it back, although in principle you can
send debt collectors to recover the money if the account was overdrawn and the customer won’t pay
it back.</p>

<p>Whether the cost of the apology is acceptable is a business decision. If it is acceptable, the
traditional model of checking all constraints before even writing the data is unnecessarily
restrictive, and a linearizable constraint is not needed. It may well be a reasonable choice to go
ahead with a write optimistically, and to check the constraint after the fact. You can still ensure
that the validation occurs before doing things that would be expensive to recover from, but that
doesn’t imply you must do the validation before you even write the data.</p>

<p>These applications <em>do</em> require integrity: you would not want to lose a reservation, or have money
disappear due to mismatched credits and debits. But they <em>don’t</em> require timeliness on the
enforcement of the constraint: if you have sold more items than you have in the warehouse, you can
patch up the problem after the fact by apologizing. Doing so is similar to the conflict resolution
approaches we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#sec_replication_write_conflicts">“Handling Write Conflicts”</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Coordination-avoiding data systems"><div class="sect3" id="idm140417545809344">
<h3>Coordination-avoiding data systems</h3>

<p><a data-type="indexterm" data-primary="constraints (databases)" data-secondary="coordination avoidance" id="idm140417545808000"></a>
<a data-type="indexterm" data-primary="coordination" data-secondary="avoidance" id="idm140417545806896"></a>
<a data-type="indexterm" data-primary="linearizability" data-secondary="of derived data systems" data-tertiary="avoiding coordination" id="idm140417545805792"></a>
We have now made two interesting observations:</p>
<ol>
<li>
<p>Dataflow systems can maintain integrity guarantees on derived data without atomic commit,
linearizability, or synchronous cross-partition coordination.</p>
</li>
<li>
<p>Although strict uniqueness constraints require timeliness and coordination, many applications are
actually fine with loose constraints that may be temporarily violated and fixed up later, as long
as integrity is preserved throughout.</p>
</li>

</ol>

<p><a data-type="indexterm" data-primary="timeliness" data-secondary="coordination-avoiding data systems" id="idm140417545801504"></a>
<a data-type="indexterm" data-primary="integrity" data-secondary="coordination-avoiding data systems" id="idm140417545800384"></a>
Taken together, these observations mean that dataflow systems can provide the data management
services for many applications without requiring coordination, while still giving strong integrity
guarantees. Such <em>coordination-avoiding</em> data systems have a lot of appeal: they can achieve better
performance and fault tolerance than systems that need to perform synchronous coordination
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2014th_ch12">56</a>].</p>

<p>For example, such a system could operate distributed across multiple datacenters in a multi-leader
configuration, asynchronously replicating between regions. Any one datacenter can continue operating
independently from the others, because no synchronous cross-region coordination is required. Such a
system would have weak timeliness guarantees—it could not be linearizable without introducing
coordination—but it can still have strong integrity guarantees.</p>

<p><a data-type="indexterm" data-primary="atomicity (transactions)" data-secondary="atomic commit" data-tertiary="avoiding" id="idm140417545796464"></a>
In this context, serializable transactions are still useful as part of maintaining derived state,
but they can be run at a small scope where they work well
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2007td_ch12" class="totri-footnote">8</a>].
Heterogeneous distributed transactions such as XA transactions (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch09.html#sec_consistency_dist_trans">“Distributed Transactions in Practice”</a>)
are not required. Synchronous coordination can still be introduced in places where it is needed (for
example, to enforce strict constraints before an operation from which recovery is not possible), but
there is no need for everything to pay the cost of coordination if only a small part of an
application needs it [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Burckhardt2015hv">43</a>].</p>

<p>Another way of looking at coordination and constraints: they reduce the number of apologies you have
to make due to inconsistencies, but potentially also reduce the performance and availability of your
system, and thus potentially increase the number of apologies you have to make due to outages. You
cannot reduce the number of apologies to zero, but you can aim to find the best trade-off for your
needs—the sweet spot where there are neither too many inconsistencies nor too many availability
problems.
<a data-type="indexterm" data-primary="correctness" data-secondary="timeliness and integrity" data-startref="ix_correcttimely" id="idm140417545791120"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="distributed transactions" data-tertiary="avoiding" data-startref="ix_disttransavoid" id="idm140417545789728"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Trust, but Verify"><div class="sect2" id="sec_future_verification">
<h2>Trust, but Verify</h2>

<p><a data-type="indexterm" data-primary="auditability" id="ix_auditable"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="auditability" id="ix_correcttrust"></a>
<a data-type="indexterm" data-primary="verification" id="ix_verify"></a>
<a data-type="indexterm" data-primary="system models" data-secondary="assumptions in" id="idm140417545782672"></a>
All of our discussion of correctness, integrity, and fault-tolerance has been under the assumption
that certain things might go wrong, but other things won’t. We call these assumptions our <em>system
model</em> (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_model_real_world">“Mapping system models to the real world”</a>): for example, we should assume that processes can
crash, machines can suddenly lose power, and the network can arbitrarily delay or drop messages. But
we might also assume that data written to disk is not lost after <code>fsync</code>, that data in memory is not
corrupted, and that the multiplication instruction of our CPU always returns the correct result.</p>

<p>These assumptions are quite reasonable, as they are true most of the time, and it would be difficult
to get anything done if we had to constantly worry about our computers making mistakes.
Traditionally, system models take a binary approach toward faults: we assume that some things can
happen, and other things can never happen. In reality, it is more a question of probabilities: some
things are more likely, other things less likely. The question is whether violations of our
assumptions happen often enough that we may encounter them in practice.</p>

<p><a data-type="indexterm" data-primary="TCP (Transmission Control Protocol)" data-secondary="packet checksums" id="idm140417545778240"></a>
We have seen that data can become corrupted while it is sitting untouched on disks (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sidebar_transactions_durability">“Replication and Durability”</a>), and data corruption on the network can sometimes evade the TCP
checksums (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_weak_lying">“Weak forms of lying”</a>). Maybe this is something we should be paying more
attention to?</p>

<p><a data-type="indexterm" data-primary="memory" data-secondary="random bit-flips in" id="idm140417545775216"></a>
One application that I worked on in the past collected crash reports from clients, and some of the
reports we received could only be explained by random bit-flips in the memory of those devices. It
seems unlikely, but if you have enough devices running your software, even very unlikely things do
happen.
<a data-type="indexterm" data-primary="corruption of data" data-secondary="due to pathological memory access" id="idm140417545773488"></a>
<a data-type="indexterm" data-primary="rowhammer (memory corruption)" id="idm140417545772448"></a>
Besides random memory corruption due to hardware faults or radiation, certain pathological memory
access patterns can flip bits even in memory that has no faults
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kim2014bn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kim2014bn">62</a>]—an
effect that can be used to break security mechanisms in operating systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Seaborn2015ve-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Seaborn2015ve">63</a>] (this technique is known as <em>rowhammer</em>). Once you look closely, hardware isn’t quite
the perfect abstraction that it may seem.</p>

<p>To be clear, random bit-flips are still very rare on modern hardware
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray2005vg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gray2005vg">64</a>]. I just want to point out that they are not beyond the realm of possibility,
and so they deserve some attention.</p>










<section data-type="sect3" data-pdf-bookmark="Maintaining integrity in the face of software bugs"><div class="sect3" id="idm140417545763168">
<h3>Maintaining integrity in the face of software bugs</h3>

<p><a data-type="indexterm" data-primary="integrity" data-secondary="maintaining despite software bugs" id="idm140417545761744"></a>
<a data-type="indexterm" data-primary="software bugs" data-secondary="maintaining integrity" id="idm140417545760448"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="violation of uniqueness constraint" id="idm140417545759344"></a>
<a data-type="indexterm" data-primary="incidents" data-secondary="errors in transaction serializability" id="idm140417545758224"></a>
<a data-type="indexterm" data-primary="write skew (transaction isolation)" data-secondary="occurrence in practice" id="idm140417545757104"></a>
Besides such hardware issues, there is always the risk of software bugs, which would not be caught
by lower-level network, memory, or filesystem checksums. Even widely used database software has
bugs: I have personally seen cases of MySQL failing to correctly maintain a uniqueness constraint
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="MySQL73170-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#MySQL73170">65</a>]
and PostgreSQL’s serializable isolation level exhibiting write skew anomalies
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fredericks2015pg_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Fredericks2015pg_ch12">66</a>], even though MySQL and PostgreSQL are robust and
well-regarded databases that have been battle-tested by many people for many years. In less mature
software, the situation is likely to be much worse.</p>

<p>Despite considerable efforts in careful design, testing, and review, bugs still creep in. Although
they are rare, and they eventually get found and fixed, there is still a period during which such
bugs can corrupt data.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of transactions" id="idm140417545750240"></a>
When it comes to application code, we have to assume many more bugs, since most applications don’t
receive anywhere near the amount of review and testing that database code does. Many applications
don’t even correctly use the features that databases offer for preserving integrity, such as foreign
key or uniqueness constraints [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2015dn">36</a>].</p>

<p><a data-type="indexterm" data-primary="consistency" data-secondary="in ACID transactions" id="idm140417545747696"></a>
<a data-type="indexterm" data-primary="ACID properties (transactions)" data-secondary="consistency" id="idm140417545746592"></a>
Consistency in the sense of ACID (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch07.html#sec_transactions_acid_consistency">“Consistency”</a>) is based on the idea
that the database starts off in a consistent state, and a transaction transforms it from
one consistent state to another consistent state. Thus, we expect the database to always be in a
consistent state. However, this notion only makes sense if you assume that the transaction is free
from bugs. If the application uses the database incorrectly in some way, for example using a weak
isolation level unsafely, the integrity of the database cannot be guaranteed.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Don’t just blindly trust what they promise"><div class="sect3" id="idm140417545743984">
<h3>Don’t just blindly trust what they promise</h3>

<p><a data-type="indexterm" data-primary="verification" data-secondary="avoiding blind trust" id="idm140417545742704"></a>
<a data-type="indexterm" data-primary="integrity" data-secondary="integrity checks" data-seealso="auditing" id="idm140417545741600"></a>
<a data-type="indexterm" data-primary="corruption of data" data-secondary="detecting" id="ix_corrupdetect"></a>
With both hardware and software not always living up to the ideal that we would like them to be, it
seems that data corruption is inevitable sooner or later. Thus, we should at least have a way of
finding out if data has been corrupted so that we can fix it and try to track down the source of the
error. Checking the integrity of data is known as <em>auditing</em>.</p>

<p>As discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_immutability_pros">“Advantages of immutable events”</a>, auditing is not just for financial applications.
However, auditability is highly important in finance precisely because everyone knows that mistakes
happen, and we all recognize the need to be able to detect and fix problems.</p>

<p><a data-type="indexterm" data-primary="hard disks" data-secondary="detecting corruption" id="idm140417545736272"></a>
<a data-type="indexterm" data-primary="solid state drives (SSDs)" data-secondary="detecting corruption" id="idm140417545735168"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="checking data integrity" id="idm140417545734048"></a>
<a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-secondary="S3 (object storage)" data-tertiary="checking data integrity" id="idm140417545732928"></a>
Mature systems similarly tend to consider the possibility of unlikely things going wrong, and manage
that risk. For example, large-scale storage systems such as HDFS and Amazon S3 do not fully trust
disks: they run background processes that continually read back files, compare them to other
replicas, and move files from one disk to another, in order to mitigate the risk of silent
corruption [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chen2016rq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Chen2016rq">67</a>].</p>

<p><a data-type="indexterm" data-primary="backups" data-secondary="integrity of" id="idm140417545728384"></a>
If you want to be sure that your data is still there, you have to actually read it and check. Most
of the time it will still be there, but if it isn’t, you really want to find out sooner rather than
later. By the same argument, it is important to try restoring from your backups from time to
time—otherwise you may only find out that your backup is broken when it is too late and you have
already lost data. Don’t just blindly trust that it is all working.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="A culture of verification"><div class="sect3" id="idm140417545726784">
<h3>A culture of verification</h3>

<p><a data-type="indexterm" data-primary="verification" data-secondary="culture of" id="idm140417545725680"></a>
<a data-type="indexterm" data-primary="self-validating systems" id="idm140417545724352"></a>
<a data-type="indexterm" data-primary="auditability" data-secondary="self-auditing systems" id="idm140417545723520"></a>
Systems like HDFS and S3 still have to assume that disks work correctly most of the time—which is a
reasonable assumption, but not the same as assuming that they <em>always</em> work correctly.  However, not
many systems currently have this kind of “trust, but verify” approach of continually auditing
themselves. Many assume that correctness guarantees are absolute and make no provision for the
possibility of rare data corruption. I hope that in the future we will see more <em>self-validating</em> or
<em>self-auditing</em> systems that continually check their own integrity, rather than relying on blind
trust [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2012td_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2012td_ch12">68</a>].</p>

<p>I fear that the culture of ACID databases has led us toward developing applications on the basis of
blindly trusting technology (such as a transaction mechanism), and neglecting any sort of
auditability in the process. Since the technology we trusted worked well enough most of the time,
auditing mechanisms were not deemed worth the investment.</p>

<p>But then the database landscape changed: weaker consistency guarantees became the norm under the
banner of NoSQL, and less mature storage technologies became widely used. Yet, because the audit
mechanisms had not been developed, we continued building applications on the basis of blind trust,
even though this approach had now become more dangerous. Let’s think for a moment about designing
for auditability.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Designing for auditability"><div class="sect3" id="idm140417545716416">
<h3>Designing for auditability</h3>

<p><a data-type="indexterm" data-primary="verification" data-secondary="designing for auditability" id="idm140417545715168"></a>
<a data-type="indexterm" data-primary="auditability" data-secondary="designing for" id="idm140417545714048"></a>
If a transaction mutates several objects in a database, it is difficult to tell after the fact what
that transaction means. Even if you capture the transaction logs (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_cdc">“Change Data Capture”</a>), the
insertions, updates, and deletions in various tables do not necessarily give a clear picture of
<em>why</em> those mutations were performed. The invocation of the application logic that decided on those
mutations is transient and cannot be reproduced.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of derived data" id="idm140417545711008"></a>
<a data-type="indexterm" data-primary="deterministic operations" data-secondary="computing derived data" id="idm140417545709904"></a>
<a data-type="indexterm" data-primary="immutability" data-secondary="advantages of" id="idm140417545708784"></a>
<a data-type="indexterm" data-primary="events" data-secondary="immutable, advantages of" id="idm140417545707680"></a>
<a data-type="indexterm" data-primary="event sourcing" data-secondary="immutability and auditability" id="idm140417545706560"></a>
By contrast, event-based systems can provide better auditability. In the event sourcing approach,
user input to the system is represented as a single immutable event, and any resulting state updates
are derived from that event. The derivation can be made deterministic and repeatable, so that
running the same log of events through the same version of the derivation code will result in the
same state updates.</p>

<p><a data-type="indexterm" data-primary="provenance of data" id="idm140417545704752"></a>
Being explicit about dataflow (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_philosophy">“Philosophy of batch process outputs”</a>) makes the <em>provenance</em> of data much
clearer, which makes integrity checking much more feasible. For the event log, we can use hashes to
check that the event storage has not been corrupted. For any derived state, we can rerun the batch
and stream processors that derived it from the event log in order to check whether we get the same
result, or even run a redundant derivation in parallel.</p>

<p>A deterministic and well-defined dataflow also makes it easier to debug and trace the execution of a
system in order to determine why it did something
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kerr2016va" class="totri-footnote">4</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fowler2011wp_ch12-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Fowler2011wp_ch12">69</a>]. If something unexpected occurred, it is valuable
to have the diagnostic capability to reproduce the exact circumstances that led to the unexpected
event—a kind of time-travel debugging capability.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="The end-to-end argument again"><div class="sect3" id="idm140417545698320">
<h3>The end-to-end argument again</h3>

<p><a data-type="indexterm" data-primary="databases" data-secondary="end-to-end argument for" data-tertiary="checking integrity" id="idm140417545697152"></a>
<a data-type="indexterm" data-primary="end-to-end argument" data-secondary="checking integrity" id="idm140417545695776"></a>
<a data-type="indexterm" data-primary="integrity" data-secondary="integrity checks" data-tertiary="end-to-end" id="idm140417545694672"></a>
<a data-type="indexterm" data-primary="verification" data-secondary="end-to-end integrity checks" id="idm140417545693296"></a>
If we cannot fully trust that every individual component of the system will be free from
corruption—that every piece of hardware is fault-free and that every piece of software is
bug-free—then we must at least periodically check the integrity of our data. If we don’t check, we
won’t find out about corruption until it is too late and it has caused some downstream damage, at
which point it will be much harder and more expensive to track down the problem.</p>

<p>Checking the integrity of data systems is best done in an end-to-end fashion (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#sec_future_end_to_end">“The End-to-End Argument for Databases”</a>): the more systems we can include in an integrity check, the fewer
opportunities there are for corruption to go unnoticed at some stage of the process. If we can check
that an entire derived data pipeline is correct end to end, then any disks, networks, services, and
algorithms along the path are implicitly included in the check.</p>

<p><a data-type="indexterm" data-primary="Agile" data-secondary="moving faster with confidence" id="idm140417545689824"></a>
Having continuous end-to-end integrity checks gives you increased confidence about the correctness
of your systems, which in turn allows you to move faster
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Stokes2016ni-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stokes2016ni">70</a>]. Like automated testing, auditing
increases the chances that bugs will be found quickly, and thus reduces the risk that a change to
the system or a new storage technology will cause damage. If you are not afraid of making changes,
you can much better evolve an application to meet changing requirements.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Tools for auditable data systems"><div class="sect3" id="idm140417545685648">
<h3>Tools for auditable data systems</h3>

<p><a data-type="indexterm" data-primary="auditability" data-secondary="tools for auditable data systems" id="idm140417545684256"></a>
<a data-type="indexterm" data-primary="verification" data-secondary="tools for auditable data systems" id="idm140417545683088"></a>
At present, not many data systems make auditability a top-level concern. Some applications implement
their own audit mechanisms, for example by logging all changes to a separate audit table, but
guaranteeing the integrity of the audit log and the database state is still difficult. A transaction
log can be made tamper-proof by periodically signing it with a hardware security module, but that
does not guarantee that the right transactions went into the log in the first place.</p>

<p><a data-type="indexterm" data-primary="cryptography" data-secondary="proving integrity of data" id="idm140417545681216"></a>
<a data-type="indexterm" data-primary="ledgers" data-secondary="distributed ledger technologies" id="idm140417545679920"></a>
<a data-type="indexterm" data-primary="blockchains" id="idm140417545678800"></a>
<a data-type="indexterm" data-primary="Bitcoin (cryptocurrency)" id="idm140417545677968"></a>
<a data-type="indexterm" data-primary="Ethereum (blockchain)" id="idm140417545677120"></a>
<a data-type="indexterm" data-primary="Ripple (cryptocurrency)" id="idm140417545676288"></a>
<a data-type="indexterm" data-primary="Stellar (cryptocurrency)" id="idm140417545675456"></a>
It would be interesting to use cryptographic tools to prove the integrity of a system in a way that
is robust to a wide range of hardware and software issues, and even potentially malicious actions.
Cryptocurrencies, blockchains, and distributed ledger technologies such as Bitcoin, Ethereum,
Ripple, Stellar, and various others
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="SawtoothLake-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#SawtoothLake">71</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Brown2016uo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Brown2016uo">72</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McConaghy2016vw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McConaghy2016vw">73</a>]
have sprung up to explore this area.</p>

<p>I am not qualified to comment on the merits of these technologies as currencies or mechanisms for
agreeing contracts. However, from a data systems point of view they contain some interesting ideas.
Essentially, they are distributed databases, with a data model and transaction mechanism, in which
different replicas can be hosted by mutually untrusting organizations. The replicas continually
check each other’s integrity and use a consensus protocol to agree on the transactions that should
be executed.</p>

<p><a data-type="indexterm" data-primary="Byzantine faults" data-secondary="Byzantine fault-tolerant systems" id="idm140417545666544"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="Byzantine fault tolerance" id="idm140417545665424"></a>
I am somewhat skeptical about the Byzantine fault tolerance aspects of these technologies (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_byzantine">“Byzantine Faults”</a>), and I find the technique of <em>proof of work</em> (e.g., Bitcoin mining)
extraordinarily wasteful. The transaction throughput of Bitcoin is rather low, albeit for political
and economic reasons more than for technical ones. However, the integrity checking aspects are
interesting.</p>

<p><a data-type="indexterm" data-primary="Merkle trees" id="idm140417545662464"></a>
<a data-type="indexterm" data-primary="certificate transparency" id="idm140417545661408"></a>
Cryptographic auditing and integrity checking often relies on <em>Merkle trees</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Merkle1987jk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Merkle1987jk">74</a>],
which are trees of hashes that can be used to efficiently prove that a record appears in some
dataset (and a few other things). Outside of the hype of cryptocurrencies, <em>certificate
transparency</em> is a security technology that relies on Merkle trees to check the validity of TLS/SSL
certificates [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Laurie2014kr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Laurie2014kr">75</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ryan2014iz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ryan2014iz">76</a>].</p>

<p>I could imagine integrity-checking and auditing algorithms, like those of certificate transparency
and distributed ledgers, becoming more widely used in data systems in general. Some work will be
needed to make them equally scalable as systems without cryptographic auditing, and to keep the
performance penalty as low as possible. But I think this is an interesting area to watch in the future.
<a data-type="indexterm" data-primary="corruption of data" data-secondary="detecting" data-startref="ix_corrupdetect" id="idm140417545649984"></a>
<a data-type="indexterm" data-primary="auditability" data-startref="ix_auditable" id="idm140417545648608"></a>
<a data-type="indexterm" data-primary="correctness" data-secondary="auditability" data-startref="ix_correcttrust" id="idm140417545647504"></a>
<a data-type="indexterm" data-primary="verification" data-startref="ix_verify" id="idm140417545646128"></a>
<a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-tertiary="correctness, constraints, and integrity" data-startref="ix_datasysfutcorrect" id="idm140417545645024"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Doing the Right Thing"><div class="sect1" id="sec_future_ethics">
<h1>Doing the Right Thing</h1>

<p><a data-type="indexterm" data-primary="ethics" id="ix_ethics"></a>
In the final section of this book, I would like to take a step back. Throughout this book we have
examined a wide range of different architectures for data systems, evaluated their pros and cons,
and explored techniques for building reliable, scalable, and maintainable applications. However, we
have left out an important and fundamental part of the discussion, which I would now like to fill
in.</p>

<p><a data-type="indexterm" data-primary="ethics" data-secondary="unintended consequences" id="idm140417545640368"></a>
Every system is built for a purpose; every action we take has both intended and unintended
consequences. The purpose may be as simple as making money, but the consequences for the world may
reach far beyond that original purpose. We, the engineers building these systems, have a
responsibility to carefully consider those consequences and to consciously decide what kind of world
we want to live in.</p>

<p>We talk about data as an abstract thing, but remember that many datasets are about people: their
behavior, their interests, their identity. We must treat such data with humanity and respect. Users
are humans too, and human dignity is paramount.</p>

<p><a data-type="indexterm" data-primary="ethics" data-secondary="code of ethics and professional practice" id="idm140417545637920"></a>
Software development increasingly involves making important ethical choices. There are guidelines to
help software engineers navigate these issues, such as the ACM’s Software Engineering Code of Ethics
and Professional Practice [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ACMEthics-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ACMEthics">77</a>], but they are
rarely discussed, applied, and enforced in practice. As a result, engineers and product managers
sometimes take a very cavalier attitude to privacy and potential negative consequences of their
products [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chollet2016bo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Chollet2016bo">78</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Perisic2016zo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Perisic2016zo">79</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Naughton2016qk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Naughton2016qk">80</a>].</p>

<p>A technology is not good or bad in itself—what matters is how it is used and how it affects people.
This is true for a software system like a search engine in much the same way as it is for a weapon
like a gun. I think it is not sufficient for software engineers to focus exclusively on the
technology and ignore its consequences: the ethical responsibility is ours to bear also. Reasoning
about ethics is difficult, but it is too important to ignore.</p>








<section data-type="sect2" data-pdf-bookmark="Predictive Analytics"><div class="sect2" id="idm140417545626304">
<h2>Predictive Analytics</h2>

<p><a data-type="indexterm" data-primary="predictive analytics" id="ix_predictanal"></a>
<a data-type="indexterm" data-primary="predictive analytics" data-secondary="ethics of" data-see="ethics" id="idm140417545623920"></a>
<a data-type="indexterm" data-primary="analytics" data-secondary="predictive" data-see="predictive analytics" id="idm140417545622544"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="predictive analytics" id="ix_ethicsanalytics"></a>
For example, predictive analytics is a major part of the “Big Data” hype. Using data analysis to
predict the weather, or the spread of diseases, is one thing
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kugler2016hn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kugler2016hn">81</a>];
it is another matter to predict whether a convict is likely to reoffend, whether an applicant for a
loan is likely to default, or whether an insurance customer is likely to make expensive claims. The
latter have a direct effect on individual people’s lives.</p>

<p>Naturally, payment networks want to prevent fraudulent transactions, banks want to avoid bad loans,
airlines want to avoid hijackings, and companies want to avoid hiring ineffective or untrustworthy
people. From their point of view, the cost of a missed business opportunity is low, but the cost of
a bad loan or a problematic employee is much higher, so it is natural for organizations to want to
be cautious. If in doubt, they are better off saying no.</p>

<p>However, as algorithmic decision-making becomes more widespread, someone who has (accurately or
falsely) been labeled as risky by some algorithm may suffer a large number of those “no” decisions.
Systematically being excluded from jobs, air travel, insurance coverage, property rental, financial
services, and other key aspects of society is such a large constraint of the individual’s freedom
that it has been called “algorithmic prison”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Davidow2014ve-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Davidow2014ve">82</a>].
In countries that respect human rights, the criminal justice system presumes innocence until proven
guilty; on the other hand, automated systems can systematically and arbitrarily exclude a person
from participating in society without any proof of guilt, and with little chance of appeal.</p>










<section data-type="sect3" data-pdf-bookmark="Bias and discrimination"><div class="sect3" id="idm140417545612000">
<h3>Bias and discrimination</h3>

<p>Decisions made by an algorithm are not necessarily any better or any worse than those made by a
human. Every person is likely to have biases, even if they actively try to counteract them, and
discriminatory practices can become culturally institutionalized. There is hope that basing
decisions on data, rather than subjective and instinctive assessments by people, could be more fair
and give a better chance to people who are often overlooked in the traditional system
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Peck2013tr-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Peck2013tr">83</a>].</p>

<p><a data-type="indexterm" data-primary="amplification" data-secondary="of bias" id="idm140417545607584"></a>
<a data-type="indexterm" data-primary="bias" id="idm140417545606480"></a>
<a data-type="indexterm" data-primary="predictive analytics" data-secondary="amplifying bias" id="idm140417545605648"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="predictive analytics" data-tertiary="amplifying bias" id="idm140417545604544"></a>
When we develop predictive analytics systems, we are not merely automating a human’s decision by
using software to specify the rules for when to say yes or no; we are even leaving the rules
themselves to be inferred from data. However, the patterns learned by these systems are opaque: even
if there is some correlation in the data, we may not know why. If there is a systematic bias in the
input to an algorithm, the system will most likely learn and amplify that bias in its output
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alexander2016xa-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alexander2016xa">84</a>].</p>

<p><a data-type="indexterm" data-primary="discrimination" id="idm140417545600016"></a>
<a data-type="indexterm" data-primary="machine learning" data-secondary="ethical considerations" data-seealso="ethics" id="idm140417545599296"></a>
In many countries, anti-discrimination laws prohibit treating people differently depending on
protected traits such as ethnicity, age, gender, sexuality, disability, or beliefs. Other features
of a person’s data may be analyzed, but what happens if they are correlated with protected traits?
For example, in racially segregated neighborhoods, a person’s postal code or even their IP address
is a strong predictor of race. Put like this, it seems ridiculous to believe that an algorithm could
somehow take biased data as input and produce fair and impartial output from it
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Emspak2016no-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Emspak2016no">85</a>]. Yet this belief
often seems to be implied by proponents of data-driven decision making, an attitude that has been
satirized as “machine learning is like money laundering for bias”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ceglowski2016uw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ceglowski2016uw">86</a>].</p>

<p>Predictive analytics systems merely extrapolate from the past; if the past is discriminatory, they
codify that discrimination. If we want the future to be better than the past, moral imagination is
required, and that’s something only humans can provide
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ONeil2016vh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>]. Data and models should be our tools, not our masters.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Responsibility and accountability"><div class="sect3" id="idm140417545589888">
<h3>Responsibility and accountability</h3>

<p><a data-type="indexterm" data-primary="responsibility and accountability" id="idm140417545588560"></a>
<a data-type="indexterm" data-primary="accountability" id="idm140417545587664"></a>
Automated decision making opens the question of responsibility and accountability
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>]. If a human makes a mistake, they can be
held accountable, and the person affected by the decision can appeal. Algorithms make mistakes too,
but who is accountable if they go wrong
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Angwin2016qn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Angwin2016qn">88</a>]? When a self-driving car
causes an accident, who is responsible? If an automated credit scoring algorithm systematically
discriminates against people of a particular race or religion, is there any recourse? If a decision
by your machine learning system comes under judicial review, can you explain to the judge how the
algorithm made its decision?</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of personal data" id="idm140417545582944"></a>
<a data-type="indexterm" data-primary="credit rating agencies" id="idm140417545581840"></a>
Credit rating agencies are an old example of collecting data to make decisions about people. A bad
credit score makes life difficult, but at least a credit score is normally based on relevant facts
about a person’s actual borrowing history, and any errors in the record can be corrected (although
the agencies normally do not make this easy). However, scoring algorithms based on machine learning
typically use a much wider range of inputs and are much more opaque, making it harder to understand
how a particular decision has come about and whether someone is being treated in an unfair or
discriminatory way [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Goodman2016vd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Goodman2016vd">89</a>].</p>

<p>A credit score summarizes “How did you behave in the past?” whereas predictive analytics usually
work on the basis of “Who is similar to you, and how did people like you behave in the past?”
Drawing parallels to others’ behavior implies stereotyping people, for example based on where they
live (a close proxy for race and socioeconomic class). What about people who get put in the wrong
bucket? Furthermore, if a decision is incorrect due to erroneous data, recourse is almost impossible
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>].</p>

<p>Much data is statistical in nature, which means that even if the probability distribution on the
whole is correct, individual cases may well be wrong. For example, if the average life expectancy in
your country is 80 years, that doesn’t mean you’re expected to drop dead on your 80th birthday.
From the average and the probability distribution, you can’t say much about the age to which one
particular person will live. Similarly, the output of a prediction system is probabilistic and may
well be wrong in individual cases.</p>

<p>A blind belief in the supremacy of data for making decisions is not only delusional, it is
positively dangerous. As data-driven decision making becomes more widespread, we will need to figure
out how to make algorithms accountable and transparent, how to avoid reinforcing existing biases,
and how to fix them when they inevitably make mistakes.</p>

<p>We will also need to figure out how to prevent data being used to harm people, and realize its
positive potential instead. For example, analytics can reveal financial and social characteristics
of people’s lives. On the one hand, this power could be used to focus aid and support to help those
people who most need it. On the other hand, it is sometimes used by predatory business seeking to
identify vulnerable people and sell them risky products such as high-cost loans and worthless
college degrees [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="USSenate2013um-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#USSenate2013um">90</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Feedback loops"><div class="sect3" id="idm140417545569904">
<h3>Feedback loops</h3>

<p><a data-type="indexterm" data-primary="predictive analytics" data-secondary="feedback loops" id="idm140417545568704"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="predictive analytics" data-tertiary="feedback loops" id="idm140417545567600"></a>
Even with predictive applications that have less immediately far-reaching effects on people, such as
recommendation systems, there are difficult issues that we must confront. When services become good
at predicting what content users want to see, they may end up showing people only opinions they
already agree with, leading to echo chambers in which stereotypes, misinformation, and polarization
can breed. We are already seeing the impact of social media echo chambers on election campaigns
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Solon2016vt-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Solon2016vt">91</a>].</p>

<p>When predictive analytics affect people’s lives, particularly pernicious problems arise due to
self-reinforcing feedback loops. For example, consider the case of employers using credit scores to
evaluate potential hires. You may be a good worker with a good credit score, but suddenly find
yourself in financial difficulties due to a misfortune outside of your control. As you miss payments
on your bills, your credit score suffers, and you will be less likely to find work. Joblessness
pushes you toward poverty, which further worsens your scores, making it even harder to find
employment [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>]. It’s a downward spiral due to
poisonous assumptions, hidden behind a camouflage of mathematical rigor and data.</p>

<p><a data-type="indexterm" data-primary="systems thinking" id="idm140417545561104"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="unintended consequences" id="idm140417545560048"></a>
We can’t always predict when such feedback loops happen. However, many consequences can be predicted
by thinking about the entire system (not just the computerized parts, but also the people
interacting with it)—an approach known as <em>systems thinking</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Meadows2008wq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Meadows2008wq">92</a>].
We can try to understand how a data analysis system responds to different behaviors, structures, or
characteristics. Does the system reinforce and amplify existing differences between people (e.g.,
making the rich richer or the poor poorer), or does it try to combat injustice? And even with the
best intentions, we must beware of unintended consequences.
<a data-type="indexterm" data-primary="predictive analytics" data-startref="ix_predictanal" id="idm140417545556528"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="predictive analytics" data-startref="ix_ethicsanalytics" id="idm140417545555408"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Privacy and Tracking"><div class="sect2" id="idm140417545554032">
<h2>Privacy and Tracking</h2>

<p><a data-type="indexterm" data-primary="privacy" id="ix_privacytrack"></a>
<a data-type="indexterm" data-primary="privacy" data-secondary="ethical considerations" data-see="ethics" id="idm140417545551392"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" id="ix_ethicsprivacy"></a>
Besides the problems of predictive analytics—i.e., using data to make automated decisions about
people—there are ethical problems with data collection itself. What is the relationship between the
organizations collecting data and the people whose data is being collected?</p>

<p><a data-type="indexterm" data-primary="tracking behavioral data" data-seealso="privacy" id="idm140417545548096"></a>
<a data-type="indexterm" data-primary="privacy" data-secondary="tracking behavioral data" id="idm140417545547024"></a>
When a system only stores data that a user has explicitly entered, because they want the system to
store and process it in a certain way, the system is performing a service for the user: the user is
the customer. But when a user’s activity is tracked and logged as a side effect of other things they
are doing, the relationship is less clear. The service no longer just does what the user tells it to
do, but it takes on interests of its own, which may conflict with the user’s interests.</p>

<p>Tracking behavioral data has become increasingly important for user-facing features of many online
services: tracking which search results are clicked helps improve the ranking of search results;
recommending “people who liked X also liked Y” helps users discover interesting and useful things;
A/B tests and user flow analysis can help indicate how a user interface might be improved. Those
features require some amount of tracking of user behavior, and users benefit from them.</p>

<p>However, depending on a company’s business model, tracking often doesn’t stop there. If the service
is funded through advertising, the advertisers are the actual customers, and the users’ interests
take second place. Tracking data becomes more detailed, analyses become further-reaching, and data
is retained for a long time in order to build up detailed profiles of each person for marketing
purposes.</p>

<p>Now the relationship between the company and the user whose data is being collected starts looking
quite different. The user is given a free service and is coaxed into engaging with it as much as
possible. The tracking of the user serves not primarily that individual, but rather the needs of
the advertisers who are funding the service. I think this relationship can be appropriately
described with a word that has more sinister connotations: <em>surveillance</em>.</p>










<section data-type="sect3" data-pdf-bookmark="Surveillance"><div class="sect3" id="idm140417545542288">
<h3>Surveillance</h3>

<p><a data-type="indexterm" data-primary="privacy" data-secondary="surveillance" id="idm140417545540880"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" data-tertiary="surveillance" id="idm140417545539552"></a>
<a data-type="indexterm" data-primary="surveillance" data-seealso="privacy" id="idm140417545538176"></a>
As a thought experiment, try replacing the word <em>data</em> with <em>surveillance</em>, and observe if common
phrases still sound so good
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bernstein2015bp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bernstein2015bp">93</a>].
How about this: “In our surveillance-driven organization we collect real-time surveillance streams
and store them in our surveillance warehouse. Our surveillance scientists use advanced analytics and
surveillance processing in order to derive new insights.”</p>

<p>This thought experiment is unusually polemic for this book, <em>Designing Surveillance-Intensive
Applications</em>, but I think that strong words are needed to emphasize this point. In our attempts to
make software “eat the world”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Andreessen2011xa-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Andreessen2011xa">94</a>], we have built
the greatest mass surveillance infrastructure the world has ever seen. Rushing toward an Internet
of Things, we are rapidly approaching a world in which every inhabited space contains at least one
internet-connected microphone, in the form of smartphones, smart TVs, voice-controlled assistant
devices, baby monitors, and even children’s toys that use cloud-based speech recognition. Many of
these devices have a terrible security record [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Porup2016cx-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Porup2016cx">95</a>].</p>

<p>Even the most totalitarian and repressive regimes could only dream of putting a microphone in every
room and forcing every person to constantly carry a device capable of tracking their location and
movements.  Yet we apparently voluntarily, even enthusiastically, throw ourselves into this world of
total surveillance. The difference is just that the data is being collected by corporations rather
than government agencies [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schneier2015vf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2015vf">96</a>].</p>

<p>Not all data collection necessarily qualifies as surveillance, but examining it as such can help us
understand our relationship with the data collector. Why are we seemingly happy to accept
surveillance by corporations? Perhaps you feel you have nothing to hide—in other words, you are
totally in line with existing power structures, you are not a marginalized minority, and you needn’t
fear persecution [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Grugq2016nw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Grugq2016nw">97</a>]. Not everyone is so fortunate. Or perhaps it’s
because the purpose seems benign—it’s not overt coercion and conformance, but merely better
recommendations and more personalized marketing. However, combined with the discussion of predictive
analytics from the last section, that distinction seems less clear.</p>

<p>We are already seeing car insurance premiums linked to tracking devices in cars, and health
insurance coverage that depends on people wearing a fitness tracking device. When surveillance is
used to determine things that hold sway over important aspects of life, such as insurance coverage
or employment, it starts to appear less benign. Moreover, data analysis can reveal surprisingly
intrusive things: for example, the movement sensor in a smartwatch or fitness tracker can be used to
work out what you are typing (for example, passwords) with fairly good accuracy
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Beltramelli2015wk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Beltramelli2015wk">98</a>]. And algorithms for analysis are only going to get better.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Consent and freedom of choice"><div class="sect3" id="idm140417545518400">
<h3>Consent and freedom of choice</h3>

<p><a data-type="indexterm" data-primary="privacy" data-secondary="consent and freedom of choice" id="idm140417545516992"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" data-tertiary="consent and freedom of choice" id="idm140417545515872"></a>
We might assert that users voluntarily choose to use a service that tracks their activity, and they
have agreed to the terms of service and privacy policy, so they consent to data collection. We might
even claim that users are receiving a valuable service in return for the data they provide, and that
the tracking is necessary in order to provide the service. Undoubtedly, social networks, search
engines, and various other free online services are valuable to users—but there are problems with
this argument.</p>

<p>Users have little knowledge of what data they are feeding into our databases, or how it is retained
and processed—and most privacy policies do more to obscure than to illuminate. Without understanding
what happens to their data, users cannot give any meaningful consent. Often, data from one user also
says things about other people who are not users of the service and who have not agreed to any
terms. The derived datasets that we discussed in this part of the book—in which data from the entire
user base may have been combined with behavioral tracking and external data sources—are precisely
the kinds of data of which users cannot have any meaningful understanding.</p>

<p>Moreover, data is extracted from users through a one-way process, not a relationship with true
reciprocity, and not a fair value exchange. There is no dialog, no option for users to negotiate how
much data they provide and what service they receive in return: the relationship between the service
and the user is very asymmetric and one-sided. The terms are set by the service, not by the user
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zuboff2015jd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd">99</a>].</p>

<p>For a user who does not consent to surveillance, the only real alternative is simply not to use a
service. But this choice is not free either: if a service is so popular that it is “regarded by most
people as essential for basic social participation”
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd">99</a>], then it is not reasonable to expect
people to opt out of this service—using it is <em>de facto</em> mandatory. For example, in most Western
social communities, it has become the norm to carry a smartphone, to use Facebook for socializing,
and to use Google for finding information. Especially when a service has network effects, there is a
social cost to people choosing <em>not</em> to use it.</p>

<p>Declining to use a service due to its tracking of users is only an option for the small number of
people who are privileged enough to have the time and knowledge to understand its privacy policy,
and who can afford to potentially miss out on social participation or professional opportunities
that may have arisen if they had participated in the service. For people in a less privileged
position, there is no meaningful freedom of choice: surveillance becomes inescapable.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Privacy and use of data"><div class="sect3" id="idm140417545505472">
<h3>Privacy and use of data</h3>

<p><a data-type="indexterm" data-primary="privacy" data-secondary="meaning of" id="idm140417545504096"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" data-tertiary="meaning of privacy" id="idm140417545502992"></a>
Sometimes people claim that “privacy is dead” on the grounds that some users are willing to post all
sorts of things about their lives to social media, sometimes mundane and sometimes deeply personal.
However, this claim is false and rests on a misunderstanding of the word <em>privacy</em>.</p>

<p>Having privacy does not mean keeping everything secret; it means having the freedom to choose which
things to reveal to whom, what to make public, and what to keep secret. The right to privacy is a
decision right: it enables each person to decide where they want to be on the spectrum between
secrecy and transparency in each situation [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd">99</a>].
It is an important aspect of a person’s freedom and autonomy.</p>

<p>When data is extracted from people through surveillance infrastructure, privacy rights are not
necessarily eroded, but rather transferred to the data collector. Companies that acquire data
essentially say “trust us to do the right thing with your data,” which means that the right to
decide what to reveal and what to keep secret is transferred from the individual to the company.</p>

<p>The companies in turn choose to keep much of the outcome of this surveillance secret, because to
reveal it would be perceived as creepy, and would harm their business model (which relies on knowing
more about people than other companies do). Intimate information about users is only revealed
indirectly, for example in the form of tools for targeting advertisements to specific groups of
people (such as those suffering from a particular illness).</p>

<p>Even if particular users cannot be personally reidentified from the bucket of people targeted by a
particular ad, they have lost their agency about the disclosure of some intimate information, such
as whether they suffer from some illness. It is not the user who decides what is revealed to whom on
the basis of their personal preferences—it is the company that exercises the privacy right with
the goal of maximizing its profit.</p>

<p><a data-type="indexterm" data-primary="correctness" data-secondary="of personal data" id="idm140417545496416"></a>
Many companies have a goal of not being <em>perceived</em> as creepy—avoiding the question of how
intrusive their data collection actually is, and instead focusing on managing user perceptions. And
even these perceptions are often managed poorly: for example, something may be factually correct,
but if it triggers painful memories, the user may not want to be reminded about it
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zona2016nj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zona2016nj">100</a>].
With any kind of data we should expect the possibility that it is wrong, undesirable, or
inappropriate in some way, and we need to build mechanisms for handling those failures. Whether
something is “undesirable” or “inappropriate” is of course down to human judgment; algorithms are
oblivious to such notions unless we explicitly program them to respect human needs. As engineers of
these systems we must be humble, accepting and planning for such failings.</p>

<p>Privacy settings that allow a user of an online service to control which aspects of their data other
users can see are a starting point for handing back some control to users. However, regardless of
the setting, the service itself still has unfettered access to the data, and is free to use it in
any way permitted by the privacy policy. Even if the service promises not to sell the data to third
parties, it usually grants itself unrestricted rights to process and analyze the data internally,
often going much further than what is overtly visible to users.</p>

<p>This kind of large-scale transfer of privacy rights from individuals to corporations is historically
unprecedented [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd">99</a>]. Surveillance has always
existed, but it used to be expensive and manual, not scalable and automated. Trust relationships
have always existed, for example between a patient and their doctor, or between a defendant and
their attorney—but in these cases the use of data has been strictly governed by ethical, legal,
and regulatory constraints. Internet services have made it much easier to amass huge amounts of
sensitive information without meaningful consent, and to use it at
massive scale without users understanding what is happening to their private data.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Data as assets and power"><div class="sect3" id="idm140417545488688">
<h3>Data as assets and power</h3>

<p><a data-type="indexterm" data-primary="privacy" data-secondary="data as assets and power" id="idm140417545487360"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" data-tertiary="data as assets and power" id="idm140417545486192"></a>
Since behavioral data is a byproduct of users interacting with a service, it is sometimes called
“data exhaust”—suggesting that the data is worthless waste material. Viewed this way, behavioral
and predictive analytics can be seen as a form of recycling that extracts value from data that would
have otherwise been thrown away.</p>

<p>More correct would be to view it the other way round: from an economic point of view, if targeted
advertising is what pays for a service, then behavioral data about people is the service’s core
asset. In this case, the application with which the user interacts is merely a
means to lure users into feeding more and more personal information into the surveillance
infrastructure [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd">99</a>]. The delightful human
creativity and social relationships that often find expression in online services are cynically
exploited by the data extraction machine.</p>

<p>The assertion that personal data is a valuable asset is supported by the existence of data brokers,
a shady industry operating in secrecy, purchasing, aggregating, analyzing, inferring, and reselling
intrusive personal data about people, mostly for marketing purposes
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#USSenate2013um">90</a>].
Startups are valued by their user numbers, by “eyeballs”—i.e., by their surveillance capabilities.</p>

<p><a data-type="indexterm" data-primary="government use of data" id="idm140417545480768"></a>
Because the data is valuable, many people want it. Of course companies want it—that’s why they
collect it in the first place. But governments want to obtain it too: by means of secret deals,
coercion, legal compulsion, or simply stealing it
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schneier2016tf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2016tf">101</a>].
When a company goes bankrupt, the personal data it has collected is one of the assets that get sold.
Moreover, the data is difficult to secure, so breaches happen disconcertingly often
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dunn2016gy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Dunn2016gy">102</a>].</p>

<p>These observations have led critics to saying that data is not just an asset, but a “toxic asset”
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2016tf">101</a>], or at least “hazardous material”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Scott2016cq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Scott2016cq">103</a>]. Even if we think that we are capable of preventing
abuse of data, whenever we collect data, we need to balance the benefits with the risk of it falling
into the wrong hands: computer systems may be compromised by criminals or hostile foreign
intelligence services, data may be leaked by insiders, the company may fall into the hands of
unscrupulous management that does not share our values, or the country may be taken over by a regime
that has no qualms about compelling us to hand over the data.</p>

<p>When collecting data, we need to consider not just today’s political environment, but all possible
future governments. There is no guarantee that every government elected in future will respect human
rights and civil liberties, so “it is poor civic hygiene to install technologies that could someday
facilitate a police state” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schneier2013qj-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2013qj">104</a>].</p>

<p>“Knowledge is power,” as the old adage goes. And furthermore, “to scrutinize others while avoiding
scrutiny oneself is one of the most important forms of power”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ulbricht2016gh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ulbricht2016gh">105</a>].
This is why totalitarian governments want surveillance: it gives them the power to control the
population. Although today’s technology companies are not overtly seeking political power, the data
and knowledge they have accumulated nevertheless gives them a lot of power over our lives, much of
which is surreptitious, outside of public oversight
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Goodman2016um-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Goodman2016um">106</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Remembering the Industrial Revolution"><div class="sect3" id="idm140417545461712">
<h3>Remembering the Industrial Revolution</h3>

<p><a data-type="indexterm" data-primary="Industrial Revolution" id="idm140417545460320"></a>
Data is the defining feature of the information age. The internet, data storage, processing, and
software-driven automation are having a major impact on the global economy and human society. As our daily lives
and social organization have changed in the past decade, and will probably continue to radically
change in the coming decades, comparisons to the Industrial Revolution come to mind
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>,
<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2015vf">96</a>].</p>

<p>The Industrial Revolution came about through major technological and agricultural advances, and it
brought sustained economic growth and significantly improved living standards in the long run. Yet
it also came with major problems: pollution of the air (due to smoke and chemical processes) and the
water (from industrial and human waste) was dreadful. Factory owners lived in splendor, while urban
workers often lived in very poor housing and worked long hours in harsh conditions. Child labor was
common, including dangerous and poorly paid work in mines.</p>

<p>It took a long time before safeguards were established, such as environmental protection
regulations, safety protocols for workplaces, outlawing child labor, and health inspections for
food. Undoubtedly the cost of doing business increased when factories could no longer dump their
waste into rivers, sell tainted foods, or exploit workers. But society as a whole benefited hugely,
and few of us would want to return to a time before those regulations
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh">87</a>].</p>

<p>Just as the Industrial Revolution had a dark side that needed to be managed, our transition to the
information age has major problems that we need to confront and solve. I believe that the collection
and use of data is one of those problems. In the words of Bruce Schneier
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2015vf">96</a>]:</p>
<blockquote>
<p>Data is the pollution problem of the information age, and protecting privacy is the environmental
challenge. Almost all computers produce information. It stays around, festering. How we deal with
it—how we contain it and how we dispose of it—is central to the health of our information
economy. Just as we look back today at the early decades of the industrial age and wonder how our
ancestors could have ignored pollution in their rush to build an industrial world, our grandchildren
will look back at us during these early decades of the information age and judge us on how we
addressed the challenge of data collection and misuse.</p>

<p>We should try to make them proud.</p></blockquote>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Legislation and self-regulation"><div class="sect3" id="sec_future_legislation">
<h3>Legislation and self-regulation</h3>

<p><a data-type="indexterm" data-primary="privacy" data-secondary="legislation and self-regulation" id="idm140417545449600"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="legislation and self-regulation" id="idm140417545448480"></a>
<a data-type="indexterm" data-primary="data protection regulations" id="idm140417545447360"></a>
Data protection laws might be able to help preserve individuals’ rights. For example, the 1995
European Data Protection Directive states that personal data must be “collected for specified,
explicit and legitimate purposes and not further processed in a way incompatible with those
purposes,” and furthermore that data must be “adequate, relevant and not excessive in relation to
the purposes for which they are collected”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DataProtectionDirective1995-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#DataProtectionDirective1995">107</a>].</p>

<p>However, it is doubtful whether this legislation is effective in today’s internet context
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="VanAlsenoy2016wb-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#VanAlsenoy2016wb">108</a>]. These rules run directly counter
to the philosophy of Big Data, which is to maximize data collection, to combine it with other
datasets, to experiment and to explore in order to generate new insights. Exploration means using
data for unforeseen purposes, which is the opposite of the “specified and explicit” purposes for
which the user gave their consent (if we can meaningfully speak of consent at all
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rhoen2016ff-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Rhoen2016ff">109</a>]). Updated
regulations are now being developed [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Goodman2016vd">89</a>].</p>

<p>Companies that collect lots of data about people oppose regulation as being a burden and a hindrance
to innovation. To some extent that opposition is justified. For example, when sharing medical data,
there are clear risks to privacy, but there are also potential opportunities: how many deaths could
be prevented if data analysis was able to help us achieve better diagnostics or find better
treatments [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Leber2016xy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Leber2016xy">110</a>]? Over-regulation may prevent such breakthroughs. It is difficult to balance such
potential opportunities with the risks [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ulbricht2016gh">105</a>].</p>

<p><a data-type="indexterm" data-primary="ethics" data-secondary="respect, dignity, and agency" id="idm140417545432864"></a>
Fundamentally, I think we need a culture shift in the tech industry with regard to personal data. We
should stop regarding users as metrics to be optimized, and remember that they are humans who
deserve respect, dignity, and agency. We should self-regulate our data collection and processing
practices in order to establish and maintain the trust of the people who depend on our software
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ceglowski2015vw-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ceglowski2015vw">111</a>]. And we should take it upon ourselves to educate end users about how their
data is used, rather than keeping them in the dark.</p>

<p>We should allow each individual to maintain their privacy—i.e., their control over own data—and not
steal that control from them through surveillance. Our individual right to control our data is like
the natural environment of a national park: if we don’t explicitly protect and care for it, it will
be destroyed. It will be the tragedy of the commons, and we will all be worse off for it. Ubiquitous
surveillance is not inevitable—we are still able to stop it.</p>

<p><a data-type="indexterm" data-primary="cryptography" data-secondary="end-to-end encryption and authentication" id="idm140417545427824"></a>
How exactly we might achieve this is an open question. To begin with, we should not retain data
forever, but purge it as soon as it is no longer needed
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ceglowski2015vw">111</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thielman2016bt-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Thielman2016bt">112</a>]. Purging data runs counter to the idea of immutability (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#sec_stream_immutability_limitations">“Limitations of immutability”</a>), but that issue can be solved. A promising approach I see
is to enforce access control through cryptographic protocols, rather than merely by policy
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Friedersdorf2014yc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Friedersdorf2014yc">113</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Rogaway2015uc-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Rogaway2015uc">114</a>].
Overall, culture and attitude changes will be necessary.
<a data-type="indexterm" data-primary="privacy" data-startref="ix_privacytrack" id="idm140417545418256"></a>
<a data-type="indexterm" data-primary="ethics" data-secondary="privacy and tracking" data-startref="ix_ethicsprivacy" id="idm140417545417136"></a>
<a data-type="indexterm" data-primary="ethics" data-startref="ix_ethics" id="idm140417545415760"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm140417545553440">
<h1>Summary</h1>

<p><a data-type="indexterm" data-primary="data integration" id="idm140417545413472"></a>
In this chapter we discussed new approaches to designing data systems, and I included my personal
opinions and speculations about the future. We started with the observation that there is no one
single tool that can efficiently serve all possible use cases, and so applications necessarily need
to compose several different pieces of software to accomplish their goals. We discussed how to solve
this <em>data integration</em> problem by using batch processing and event streams to let data changes flow
between different systems.</p>

<p>In this approach, certain systems are designated as systems of record, and other data is derived
from them through transformations. In this way we can maintain indexes, materialized views, machine
learning models, statistical summaries, and more. By making these derivations and transformations
asynchronous and loosely coupled, a problem in one area is prevented from spreading to unrelated
parts of the system, increasing the robustness and fault-tolerance of the system as a whole.</p>

<p>Expressing dataflows as transformations from one dataset to another also helps evolve applications:
if you want to change one of the processing steps, for example to change the structure of an index
or cache, you can just rerun the new transformation code on the whole input dataset in order to
rederive the output. Similarly, if something goes wrong, you can fix the code and reprocess the
data in order to recover.</p>

<p>These processes are quite similar to what databases already do internally, so we recast the idea of
dataflow applications as <em>unbundling</em> the components of a database, and building an application by
composing these loosely coupled components.</p>

<p>Derived state can be updated by observing changes in the underlying data. Moreover, the derived
state itself can further be observed by downstream consumers. We can even take this dataflow all the
way through to the end-user device that is displaying the data, and thus build user interfaces that
dynamically update to reflect data changes and continue to work offline.</p>

<p>Next, we discussed how to ensure that all of this processing remains correct in the presence of
faults. We saw that strong integrity guarantees can be implemented scalably with asynchronous event
processing, by using end-to-end operation identifiers to make operations idempotent and by checking
constraints asynchronously. Clients can either wait until the check has passed, or go ahead without
waiting but risk having to apologize about a constraint violation. This approach is much more
scalable and robust than the traditional approach of using distributed transactions, and fits with
how many business processes work in practice.</p>

<p>By structuring applications around dataflow and checking constraints asynchronously, we can avoid
most coordination and create systems that maintain integrity but still perform well, even in
geographically distributed scenarios and in the presence of faults. We then talked a little about
using audits to verify the integrity of data and detect corruption.</p>

<p>Finally, we took a step back and examined some ethical aspects of building data-intensive
applications. We saw that although data can be used to do good, it can also do significant harm:
making justifying decisions that seriously affect people’s lives and are difficult to appeal
against, leading to discrimination and exploitation, normalizing surveillance, and exposing intimate
information. We also run the risk of data breaches, and we may find that a well-intentioned use of
data has unintended consequences.</p>

<p><a data-type="indexterm" data-primary="ethics" data-secondary="respect, dignity, and agency" id="idm140417545404960"></a>
As software and data are having such a large impact on the world, we engineers must remember that we
carry a responsibility to work toward the kind of world that we want to live in: a world that treats
people with humanity and respect. I hope that we can work together toward that goal.
<a data-type="indexterm" data-primary="data systems" data-secondary="future of" data-startref="ix_datasysfut" id="idm140417545403584"></a></p>
</div></section>







<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417546434816"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546434816-marker" class="totri-footnote">i</a></sup> Explaining a joke rarely improves
it, but I don’t want anyone to feel left out. Here, <em>Church</em> is a reference to the
mathematician Alonzo Church, who created the lambda calculus, an early form of computation that is
the basis for most functional programming languages. The lambda calculus has no mutable state (i.e.,
no variables that can be overwritten), so one could say that mutable state is separate from Church’s
work.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417546385280"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546385280-marker">ii</a></sup> In the microservices approach, you
could avoid the synchronous network request by caching the exchange rate locally in the service that
processes the purchase. However, in order to keep that cache fresh, you would need to periodically
poll for updated exchange rates, or subscribe to a stream of changes—which is exactly what happens
in the dataflow approach.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417546354016"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#idm140417546354016-marker">iii</a></sup> Less facetiously, the set of distinct
search queries with nonempty search results is finite, assuming a finite corpus. However, it would
be exponential in the number of terms in the corpus, which is still pretty bad news.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Belaid2015tl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Belaid2015tl-marker" class="totri-footnote">1</a>] Rachid Belaid:
“<a href="http://rachbelaid.com/postgres-full-text-search-is-good-enough/">Postgres Full-Text Search
is Good Enough!</a>,” <em>rachbelaid.com</em>, July 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ajoux2015wh_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ajoux2015wh_ch12-marker" class="totri-footnote">2</a>] Philippe Ajoux, Nathan Bronson, Sanjeev Kumar, et al.:
“<a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf">Challenges
to Adopting Stronger Consistency at Scale</a>,” at <em>15th USENIX Workshop on Hot Topics
in Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Helland2009vd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2009vd-marker" class="totri-footnote">3</a>] Pat Helland and Dave Campbell:
“<a href="https://database.cs.wisc.edu/cidr/cidr2009/Paper_133.pdf">Building on Quicksand</a>,” at
<em>4th Biennial Conference on Innovative Data Systems Research</em> (CIDR), January 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kerr2016va">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kerr2016va-marker" class="totri-footnote">4</a>] Jessica Kerr:
“<a href="http://blog.jessitron.com/2016/09/provenance-and-causality-in-distributed.html">Provenance
and Causality in Distributed Systems</a>,” <em>blog.jessitron.com</em>, September 25, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Tzoumas2015tn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Tzoumas2015tn-marker" class="totri-footnote">5</a>] Kostas Tzoumas:
“<a href="http://data-artisans.com/batch-is-a-special-case-of-streaming/">Batch Is a Special Case of
Streaming</a>,” <em>data-artisans.com</em>, September 15, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kim2016uw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kim2016uw-marker" class="totri-footnote">6</a>] Shinji Kim and Robert Blafford:
“<a href="http://concord.io/posts/windowing_performance_analysis_w_spark_streaming">Stream Windowing
Performance Analysis: Concord and Spark Streaming</a>,” <em>concord.io</em>, July 6, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2013vs_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2013vs_ch12-marker" class="totri-footnote">7</a>] Jay Kreps:
“<a href="http://engineering.linkedin.com/distributed-systems/log-what-every-software-engineer-should-know-about-real-time-datas-unifying">The
Log: What Every Software Engineer Should Know About Real-Time Data’s Unifying Abstraction</a>,”
<em>engineering.linkedin.com</em>, December 16, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Helland2007td_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2007td_ch12-marker" class="totri-footnote">8</a>] Pat Helland:
“<a href="http://www-db.cs.wisc.edu/cidr/cidr2007/papers/cidr07p15.pdf">Life Beyond Distributed
Transactions: An Apostate’s Opinion</a>,” at <em>3rd Biennial Conference on Innovative Data
Systems Research</em> (CIDR), January 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="NetworkRail">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#NetworkRail-marker" class="totri-footnote">9</a>] “<a href="https://www.networkrail.co.uk/VirtualArchive/great-western/">Great
Western Railway (1835–1948)</a>,” Network Rail Virtual Archive, <em>networkrail.co.uk</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Xu2017bl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Xu2017bl-marker">10</a>] Jacqueline Xu:
“<a href="https://stripe.com/blog/online-migrations">Online Migrations at Scale</a>,”
<em>stripe.com</em>, February 2, 2017.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bartlett2015wv_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bartlett2015wv_ch12-marker">11</a>] Molly Bartlett Dishman and Martin Fowler:
“<a href="http://conferences.oreilly.com/software-architecture/sa2015/public/schedule/detail/40388">Agile
Architecture</a>,” at <em>O’Reilly Software Architecture Conference</em>, March 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Marz2015th">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Marz2015th-marker">12</a>] Nathan Marz and James Warren:
<a href="https://www.manning.com/books/big-data"><em>Big Data: Principles and Best Practices of
Scalable Real-Time Data Systems</em></a>. Manning, 2015. ISBN: 978-1-617-29034-3</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Boykin2014vf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Boykin2014vf-marker">13</a>] Oscar Boykin, Sam Ritchie, Ian O’Connell, and
Jimmy Lin: “<a href="http://www.vldb.org/pvldb/vol7/p1441-boykin.pdf">Summingbird: A Framework for
Integrating Batch and Online MapReduce Computations</a>,” at <em>40th International Conference on
Very Large Data Bases</em> (VLDB), September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2014wv_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2014wv_ch12-marker">14</a>] Jay Kreps:
“<a href="https://www.oreilly.com/ideas/questioning-the-lambda-architecture">Questioning the
Lambda Architecture</a>,” <em>oreilly.com</em>, July 2, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CastroFernandez2015uz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#CastroFernandez2015uz-marker">15</a>] Raul Castro Fernandez, Peter Pietzuch,
Jay Kreps, et al.: “<a href="http://www.cidrdb.org/cidr2015/Papers/CIDR15_Paper25u.pdf">Liquid:
Unifying Nearline and Offline Big Data Integration</a>,” at <em>7th Biennial Conference on
Innovative Data Systems Research</em> (CIDR), January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ritchie1974gg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ritchie1974gg-marker">16</a>] Dennis M. Ritchie and Ken Thompson:
“<a href="http://www.cs.virginia.edu/~zaher/classes/CS656/p365-ritchie.pdf">The UNIX Time-Sharing
System</a>,” <em>Communications of the ACM</em>, volume 17, number 7, pages 365–375, July 1974.
<a href="http://dx.doi.org/10.1145/361011.361061">doi:10.1145/361011.361061</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brewer2011uh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Brewer2011uh-marker">17</a>] Eric A. Brewer and Joseph M. Hellerstein:
“<a href="http://people.eecs.berkeley.edu/~brewer/cs262/systemr.html">CS262a: Advanced Topics in
Computer Systems</a>,” lecture notes, University of California, Berkeley, <em>cs.berkeley.edu</em>,
August 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stonebraker2015wu">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stonebraker2015wu-marker">18</a>] Michael Stonebraker:
“<a href="http://wp.sigmod.org/?p=1629">The Case for Polystores</a>,” <em>wp.sigmod.org</em>,
July 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Duggan2015de">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Duggan2015de-marker">19</a>] Jennie Duggan,
Aaron J. Elmore, Michael Stonebraker, et al.:
“<a href="http://dspace.mit.edu/openaccess-disseminate/1721.1/100936">The BigDAWG Polystore
System</a>,” <em>ACM SIGMOD Record</em>, volume 44, number 2, pages 11–16, June 2015.
<a href="http://dx.doi.org/10.1145/2814710.2814713">doi:10.1145/2814710.2814713</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dybka2015bn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Dybka2015bn-marker">20</a>] Patrycja Dybka:
“<a href="http://www.vertabelo.com/blog/technical-articles/foreign-data-wrappers-for-postgresql">Foreign
Data Wrappers for PostgreSQL</a>,” <em>vertabelo.com</em>, March 24, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lomet2009tc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Lomet2009tc-marker">21</a>] David B. Lomet, Alan Fekete, Gerhard Weikum, and Mike Zwilling:
“<a href="https://www.microsoft.com/en-us/research/publication/unbundling-transaction-services-in-the-cloud/">Unbundling
Transaction Services in the Cloud</a>,” at <em>4th Biennial Conference on Innovative Data Systems
Research</em> (CIDR), January 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2015tz_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2015tz_ch12-marker">22</a>] Martin Kleppmann and Jay Kreps:
“<a href="http://martin.kleppmann.com/papers/kafka-debull15.pdf">Kafka, Samza and the Unix
Philosophy of Distributed Data</a>,” <em>IEEE Data Engineering Bulletin</em>, volume 38, number 4, pages 4–14,
December 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hugg2016tq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Hugg2016tq-marker">23</a>] John Hugg:
“<a href="https://voltdb.com/blog/winning-now-and-future-where-voltdb-shines">Winning Now and in the
Future: Where VoltDB Shines</a>,” <em>voltdb.com</em>, March 23, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2013tt">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McSherry2013tt-marker">24</a>] Frank McSherry, Derek G. Murray, Rebecca Isaacs, and Michael Isard:
“<a href="http://cidrdb.org/cidr2013/Papers/CIDR13_Paper111.pdf">Differential Dataflow</a>,”
at <em>6th Biennial Conference on Innovative Data Systems Research</em> (CIDR), January 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Murray2013jg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Murray2013jg-marker">25</a>] Derek G Murray, Frank McSherry, Rebecca Isaacs, et al.:
“<a href="http://research.microsoft.com/pubs/201100/naiad_sosp2013.pdf">Naiad: A Timely Dataflow System</a>,”
at <em>24th ACM Symposium on Operating Systems Principles</em> (SOSP), pages 439–455, November 2013.
<a href="http://dx.doi.org/10.1145/2517349.2522738">doi:10.1145/2517349.2522738</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Shapira2016ej">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Shapira2016ej-marker">26</a>] Gwen Shapira:
“<a href="https://twitter.com/gwenshap/status/758800071110430720">We have a bunch of customers who
are implementing ‘database inside-out’ concept and they all ask ‘is anyone else doing it? are we
crazy?’</a>” <em>twitter.com</em>, July 28, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2014ht">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kleppmann2014ht-marker">27</a>] Martin Kleppmann:
“<a href="http://martin.kleppmann.com/2015/03/04/turning-the-database-inside-out.html">Turning the
Database Inside-out with Apache Samza,</a>” at <em>Strange Loop</em>, September 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="VanRoy2004th">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#VanRoy2004th-marker">28</a>] Peter Van Roy and Seif Haridi:
<a href="http://www.epsa.org/forms/uploadFiles/3B6300000000.filename.booksingle.pdf"><em>Concepts,
Techniques, and Models of Computer Programming</em></a>. MIT Press, 2004.
ISBN: 978-0-262-22069-9</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Juttle2016">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Juttle2016-marker">29</a>] “<a href="http://juttle.github.io/juttle/">Juttle
Documentation</a>,” <em>juttle.github.io</em>, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Czaplicki2013ig">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Czaplicki2013ig-marker">30</a>] Evan Czaplicki and Stephen Chong:
“<a href="http://people.seas.harvard.edu/~chong/pubs/pldi13-elm.pdf">Asynchronous Functional
Reactive Programming for GUIs</a>,” at <em>34th ACM SIGPLAN Conference on Programming Language
Design and Implementation</em> (PLDI), June 2013.
<a href="http://dx.doi.org/10.1145/2491956.2462161">doi:10.1145/2491956.2462161</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bainomugisha2013bh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bainomugisha2013bh-marker">31</a>] Engineer Bainomugisha, Andoni Lombide Carreton,
Tom van Cutsem, Stijn Mostinckx, and Wolfgang de Meuter:
“<a href="http://soft.vub.ac.be/Publications/2012/vub-soft-tr-12-13.pdf">A Survey on Reactive
Programming</a>,” <em>ACM Computing Surveys</em>, volume 45, number 4, pages 1–34, August 2013.
<a href="http://dx.doi.org/10.1145/2501654.2501666">doi:10.1145/2501654.2501666</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alvaro2011wn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alvaro2011wn-marker">32</a>] Peter
Alvaro, Neil Conway, Joseph M. Hellerstein, and William R. Marczak:
“<a href="http://www.eecs.berkeley.edu/~palvaro/cidr11.pdf">Consistency Analysis in Bloom: A CALM
and Collected Approach</a>,” at <em>5th Biennial Conference on Innovative Data Systems Research</em>
(CIDR), January 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hermans2015ws">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Hermans2015ws-marker">33</a>] Felienne Hermans:
“<a href="https://vimeo.com/145492419">Spreadsheets Are Code</a>,” at <em>Code Mesh</em>, November
2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="BricklinVisiCalc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#BricklinVisiCalc-marker">34</a>] Dan Bricklin and Bob
Frankston: “<a href="http://danbricklin.com/visicalc.htm">VisiCalc: Information from Its
Creators</a>,” <em>danbricklin.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sculley2014un">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Sculley2014un-marker">35</a>] D. Sculley, Gary Holt, Daniel Golovin, et al.:
“<a href="http://research.google.com/pubs/pub43146.html">Machine Learning: The High-Interest Credit
Card of Technical Debt</a>,” at <em>NIPS Workshop on Software Engineering for Machine Learning</em>
(SE4ML), December 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2015dn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2015dn-marker">36</a>] Peter Bailis, Alan Fekete, Michael J Franklin,
et al.: “<a href="http://www.bailis.org/papers/feral-sigmod2015.pdf">Feral Concurrency Control: An
Empirical Investigation of Modern Application Integrity</a>,” at <em>ACM International Conference on
Management of Data</em> (SIGMOD), June 2015.
<a href="http://dx.doi.org/10.1145/2723372.2737784">doi:10.1145/2723372.2737784</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Steele2001ts">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Steele2001ts-marker">37</a>] Guy Steele:
“<a href="https://people.csail.mit.edu/gregs/ll1-discuss-archive-html/msg01134.html">Re: Need for
Macros (Was Re: Icon)</a>,” email to <em>ll1-discuss</em> mailing list, <em>people.csail.mit.edu</em>, December 24,
2001.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gelernter1985df">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gelernter1985df-marker">38</a>] David Gelernter:
“<a href="http://cseweb.ucsd.edu/groups/csag/html/teaching/cse291s03/Readings/p80-gelernter.pdf">Generative
Communication in Linda</a>,” <em>ACM Transactions on Programming Languages and Systems</em>
(TOPLAS), volume 7, number 1, pages 80–112, January 1985.
<a href="http://dx.doi.org/10.1145/2363.2433">doi:10.1145/2363.2433</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Eugster2003ih_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Eugster2003ih_ch12-marker">39</a>] Patrick Th. Eugster, Pascal A. Felber,
Rachid Guerraoui, and Anne-Marie Kermarrec:
“<a href="http://www.cs.ru.nl/~pieter/oss/manyfaces.pdf">The Many Faces of Publish/Subscribe</a>,”
<em>ACM Computing Surveys</em>, volume 35, number 2, pages 114–131, June 2003.
<a href="http://dx.doi.org/10.1145/857076.857078">doi:10.1145/857076.857078</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stopford2016tk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stopford2016tk-marker">40</a>] Ben Stopford:
“<a href="https://www.infoq.com/presentations/microservices-streaming">Microservices in a Streaming
World</a>,” at <em>QCon London</em>, March 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Posta2016uo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Posta2016uo-marker">41</a>] Christian Posta:
“<a href="http://blog.christianposta.com/microservices/why-microservices-should-be-event-driven-autonomy-vs-authority/">Why
Microservices Should Be Event Driven: Autonomy vs Authority</a>,” <em>blog.christianposta.com</em>, May 27, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Feyerke2013wd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Feyerke2013wd-marker">42</a>] Alex Feyerke:
“<a href="http://hood.ie/blog/say-hello-to-offline-first.html">Say Hello to Offline First</a>,”
<em>hood.ie</em>, November 5, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Burckhardt2015hv">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Burckhardt2015hv-marker">43</a>] Sebastian Burckhardt, Daan Leijen, Jonathan
Protzenko, and Manuel Fähndrich:
“<a href="http://drops.dagstuhl.de/opus/volltexte/2015/5238/">Global Sequence Protocol: A Robust
Abstraction for Replicated Shared State</a>,” at <em>29th European Conference on Object-Oriented
Programming</em> (ECOOP), July 2015.
<a href="http://dx.doi.org/10.4230/LIPIcs.ECOOP.2015.568">doi:10.4230/LIPIcs.ECOOP.2015.568</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Soper2015ue">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Soper2015ue-marker">44</a>] Mark Soper:
“<a href="https://medium.com/@marksoper/clearing-up-react-data-management-confusion-with-flux-redux-and-relay-aad504e63cae">Clearing
Up React Data Management Confusion with Flux, Redux, and Relay</a>,” <em>medium.com</em>, December 3, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thereska2016ul">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Thereska2016ul-marker">45</a>] Eno Thereska, Damian Guy, Michael Noll, and Neha Narkhede:
“<a href="http://www.confluent.io/blog/unifying-stream-processing-and-interactive-queries-in-apache-kafka/">Unifying
Stream Processing and Interactive Queries in Apache Kafka</a>,” <em>confluent.io</em>, October 26, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2016vk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McSherry2016vk-marker">46</a>] Frank McSherry:
“<a href="https://github.com/frankmcsherry/blog/blob/master/posts/2016-07-17.md">Dataflow as
Database</a>,” <em>github.com</em>, July 17, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alvaro2015vs">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alvaro2015vs-marker">47</a>] Peter Alvaro:
“<a href="https://www.youtube.com/watch?v=R2Aa4PivG0g">I See What You Mean</a>,” at <em>Strange
Loop</em>, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Marz2012wd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Marz2012wd-marker">48</a>] Nathan Marz:
“<a href="https://blog.twitter.com/2012/trident-a-high-level-abstraction-for-realtime-computation">Trident:
A High-Level Abstraction for Realtime Computation</a>,” <em>blog.twitter.com</em>, August 2, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bice2016vl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bice2016vl-marker">49</a>] Edi Bice:
“<a href="http://www.slideshare.net/edibice/extremely-low-latency-web-scale-fraud-prevention-with-apache-samza-kafka-and-friends">Low
Latency Web Scale Fraud Prevention with Apache Samza, Kafka and Friends</a>,” at <em>Merchant Risk
Council MRC Vegas Conference</em>, March 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Majors2016wo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Majors2016wo-marker">50</a>] Charity Majors:
“<a href="https://charity.wtf/2016/10/02/the-accidental-dba/">The Accidental DBA</a>,” <em>charity.wtf</em>,
October 2, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bernstein2000jk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bernstein2000jk-marker">51</a>] Arthur J. Bernstein, Philip M. Lewis, and Shiyong Lu:
“<a href="http://db.cs.berkeley.edu/cs286/papers/isolation-icde2000.pdf">Semantic Conditions for
Correctness at Different Isolation Levels</a>,” at <em>16th International Conference on Data
Engineering</em> (ICDE), February 2000.
<a href="http://dx.doi.org/10.1109/ICDE.2000.839387">doi:10.1109/ICDE.2000.839387</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jorwekar2007uq_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Jorwekar2007uq_ch12-marker">52</a>] Sudhir Jorwekar, Alan Fekete, Krithi Ramamritham, and
S. Sudarshan: “<a href="http://www.vldb.org/conf/2007/papers/industrial/p1263-jorwekar.pdf">Automating
the Detection of Snapshot Isolation Anomalies</a>,” at <em>33rd International Conference on Very
Large Data Bases</em> (VLDB), September 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="KingsburyJepsen">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#KingsburyJepsen-marker">53</a>] Kyle Kingsbury:
<a href="https://aphyr.com/tags/jepsen">Jepsen blog post series</a>, <em>aphyr.com</em>, 2013–2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Jouravlev2004wh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Jouravlev2004wh-marker">54</a>] Michael Jouravlev:
“<a href="http://www.theserverside.com/news/1365146/Redirect-After-Post">Redirect After Post</a>,”
<em>theserverside.com</em>, August 1, 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saltzer1984do_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Saltzer1984do_ch12-marker">55</a>] Jerome H. Saltzer, David P. Reed, and
David D. Clark: “<a href="http://www.ece.drexel.edu/courses/ECE-C631-501/SalRee1984.pdf">End-to-End
Arguments in System Design</a>,” <em>ACM Transactions on Computer Systems</em>, volume 2, number 4,
pages 277–288, November 1984.
<a href="http://dx.doi.org/10.1145/357401.357402">doi:10.1145/357401.357402</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bailis2014th_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bailis2014th_ch12-marker">56</a>] Peter Bailis, Alan Fekete, Michael J. Franklin, et al.:
“<a href="http://arxiv.org/pdf/1402.2237.pdf">Coordination-Avoiding Database Systems</a>,”
<em>Proceedings of the VLDB Endowment</em>, volume 8, number 3, pages 185–196, November 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Yarmula2016wv">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Yarmula2016wv-marker">57</a>] Alex Yarmula:
“<a href="https://blog.twitter.com/2016/strong-consistency-in-manhattan">Strong Consistency in
Manhattan</a>,” <em>blog.twitter.com</em>, March 17, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terry1995dn_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Terry1995dn_ch12-marker">58</a>] Douglas B Terry, Marvin M Theimer, Karin Petersen, et al.:
“<a href="http://css.csail.mit.edu/6.824/2014/papers/bayou-conflicts.pdf">Managing Update Conflicts
in Bayou, a Weakly Connected Replicated Storage System</a>,” at <em>15th ACM Symposium on Operating
Systems Principles</em> (SOSP), pages 172–182, December 1995.
<a href="http://dx.doi.org/10.1145/224056.224070">doi:10.1145/224056.224070</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray1981wi_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gray1981wi_ch12-marker">59</a>] Jim Gray:
“<a href="http://research.microsoft.com/en-us/um/people/gray/papers/theTransactionConcept.pdf">The
Transaction Concept: Virtues and Limitations</a>,” at <em>7th International Conference on
Very Large Data Bases</em> (VLDB), September 1981.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GarciaMolina1987ca_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#GarciaMolina1987ca_ch12-marker">60</a>] Hector Garcia-Molina and Kenneth Salem:
“<a href="http://www.cs.cornell.edu/andru/cs711/2002fa/reading/sagas.pdf">Sagas</a>,” at
<em>ACM International Conference on Management of Data</em> (SIGMOD), May 1987.
<a href="http://dx.doi.org/10.1145/38713.38742">doi:10.1145/38713.38742</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Helland2007tp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Helland2007tp-marker">61</a>] Pat Helland:
“<a href="http://blogs.msdn.com/b/pathelland/archive/2007/05/15/memories-guesses-and-apologies.aspx">Memories,
Guesses, and Apologies</a>,” <em>blogs.msdn.com</em>, May 15, 2007.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kim2014bn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kim2014bn-marker">62</a>] Yoongu Kim, Ross Daly, Jeremie Kim, et al.:
“<a href="https://users.ece.cmu.edu/~yoonguk/papers/kim-isca14.pdf">Flipping Bits in Memory Without
Accessing Them: An Experimental Study of DRAM Disturbance Errors</a>,” at <em>41st Annual
International Symposium on Computer Architecture</em> (ISCA), June 2014.
<a href="http://dx.doi.org/10.1145/2678373.2665726">doi:10.1145/2678373.2665726</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Seaborn2015ve">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Seaborn2015ve-marker">63</a>] Mark Seaborn and Thomas Dullien:
“<a href="https://googleprojectzero.blogspot.co.uk/2015/03/exploiting-dram-rowhammer-bug-to-gain.html">Exploiting
the DRAM Rowhammer Bug to Gain Kernel Privileges</a>,” <em>googleprojectzero.blogspot.co.uk</em>, March 9,
2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray2005vg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Gray2005vg-marker">64</a>] Jim N. Gray and Catharine van Ingen:
“<a href="https://www.microsoft.com/en-us/research/publication/empirical-measurements-of-disk-failure-rates-and-error-rates/">Empirical
Measurements of Disk Failure Rates and Error Rates</a>,” Microsoft Research, MSR-TR-2005-166,
December 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="MySQL73170">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#MySQL73170-marker">65</a>] Annamalai Gurusami and Daniel Price:
“<a href="http://bugs.mysql.com/bug.php?id=73170">Bug #73170: Duplicates in Unique Secondary Index
Because of Fix of Bug#68021</a>,” <em>bugs.mysql.com</em>, July 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fredericks2015pg_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Fredericks2015pg_ch12-marker">66</a>] Gary Fredericks:
“<a href="https://github.com/gfredericks/pg-serializability-bug">Postgres Serializability Bug</a>,”
<em>github.com</em>, September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chen2016rq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Chen2016rq-marker">67</a>] Xiao Chen:
“<a href="http://blog.cloudera.com/blog/2016/12/hdfs-datanode-scanners-and-disk-checker-explained/">HDFS
DataNode Scanners and Disk Checker Explained</a>,” <em>blog.cloudera.com</em>, December 20,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2012td_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kreps2012td_ch12-marker">68</a>] Jay Kreps:
“<a href="http://blog.empathybox.com/post/19574936361/getting-real-about-distributed-system-reliability">Getting
Real About Distributed System Reliability</a>,” <em>blog.empathybox.com</em>, March 19, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fowler2011wp_ch12">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Fowler2011wp_ch12-marker">69</a>] Martin Fowler:
“<a href="http://martinfowler.com/articles/lmax.html">The LMAX Architecture</a>,”
<em>martinfowler.com</em>, July 12, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Stokes2016ni">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Stokes2016ni-marker">70</a>] Sam Stokes:
“<a href="http://blog.samstokes.co.uk/blog/2016/07/11/move-fast-with-confidence/">Move Fast with
Confidence</a>,” <em>blog.samstokes.co.uk</em>, July 11, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="SawtoothLake">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#SawtoothLake-marker">71</a>] “<a href="http://intelledger.github.io/introduction.html">Sawtooth
Lake Documentation</a>,” Intel Corporation, <em>intelledger.github.io</em>, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Brown2016uo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Brown2016uo-marker">72</a>] Richard Gendal Brown:
“<a href="https://gendal.me/2016/04/05/introducing-r3-corda-a-distributed-ledger-designed-for-financial-services/">Introducing
R3 Corda™: A Distributed Ledger Designed for Financial Services</a>,” <em>gendal.me</em>, April 5, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McConaghy2016vw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#McConaghy2016vw-marker">73</a>] Trent McConaghy, Rodolphe Marques, Andreas Müller, et al.:
“<a href="https://www.bigchaindb.com/whitepaper/bigchaindb-whitepaper.pdf">BigchainDB: A Scalable
Blockchain Database</a>,” <em>bigchaindb.com</em>, June 8, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Merkle1987jk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Merkle1987jk-marker">74</a>] Ralph C. Merkle:
“<a href="https://people.eecs.berkeley.edu/~raluca/cs261-f15/readings/merkle.pdf">A Digital
Signature Based on a Conventional Encryption Function</a>,” at <em>CRYPTO ’87</em>, August 1987.
<a href="http://dx.doi.org/10.1007/3-540-48184-2_32">doi:10.1007/3-540-48184-2_32</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Laurie2014kr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Laurie2014kr-marker">75</a>] Ben Laurie:
“<a href="http://queue.acm.org/detail.cfm?id=2668154">Certificate Transparency</a>,” <em>ACM
Queue</em>, volume 12, number 8, pages 10-19, August 2014.
<a href="http://dx.doi.org/10.1145/2668152.2668154">doi:10.1145/2668152.2668154</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ryan2014iz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ryan2014iz-marker">76</a>] Mark D. Ryan:
“<a href="http://www.internetsociety.org/doc/enhanced-certificate-transparency-and-end-end-encrypted-mail">Enhanced
Certificate Transparency and End-to-End Encrypted Mail</a>,” at <em>Network and Distributed System
Security Symposium</em> (NDSS), February 2014.
<a href="http://dx.doi.org/10.14722/ndss.2014.23379">doi:10.14722/ndss.2014.23379</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ACMEthics">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ACMEthics-marker">77</a>] “<a href="http://www.acm.org/about/se-code">Software Engineering Code of Ethics and Professional
Practice</a>,” Association for Computing Machinery, <em>acm.org</em>, 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chollet2016bo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Chollet2016bo-marker">78</a>] François Chollet:
“<a href="https://twitter.com/fchollet/status/792958695722201088">Software development is starting
to involve important ethical choices</a>,” <em>twitter.com</em>, October 30, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Perisic2016zo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Perisic2016zo-marker">79</a>] Igor Perisic:
“<a href="https://engineering.linkedin.com/blog/2016/11/making-hard-choices--the-quest-for-ethics-in-machine-learning">Making
Hard Choices: The Quest for Ethics in Machine Learning</a>,” <em>engineering.linkedin.com</em>, November
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Naughton2016qk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Naughton2016qk-marker">80</a>] John Naughton:
“<a href="https://www.theguardian.com/commentisfree/2015/dec/06/algorithm-writers-should-have-code-of-conduct">Algorithm
Writers Need a Code of Conduct</a>,” <em>theguardian.com</em>, December 6, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kugler2016hn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Kugler2016hn-marker">81</a>] Logan Kugler:
“<a href="http://cacm.acm.org/magazines/2016/6/202655-what-happens-when-big-data-blunders/fulltext">What
Happens When Big Data Blunders?</a>,” <em>Communications of the ACM</em>, volume 59, number 6, pages
15–16, June 2016. <a href="http://dx.doi.org/10.1145/2911975">doi:10.1145/2911975</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Davidow2014ve">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Davidow2014ve-marker">82</a>] Bill Davidow:
“<a href="http://www.theatlantic.com/technology/archive/2014/02/welcome-to-algorithmic-prison/283985/">Welcome
to Algorithmic Prison</a>,” <em>theatlantic.com</em>, February 20, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Peck2013tr">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Peck2013tr-marker">83</a>] Don Peck:
“<a href="http://www.theatlantic.com/magazine/archive/2013/12/theyre-watching-you-at-work/354681/">They’re
Watching You at Work</a>,” <em>theatlantic.com</em>, December 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alexander2016xa">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Alexander2016xa-marker">84</a>] Leigh Alexander:
“<a href="https://www.theguardian.com/technology/2016/aug/03/algorithm-racist-human-employers-work">Is
an Algorithm Any Less Racist Than a Human?</a>” <em>theguardian.com</em>, August 3, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Emspak2016no">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Emspak2016no-marker">85</a>] Jesse Emspak:
“<a href="https://www.scientificamerican.com/article/how-a-machine-learns-prejudice/">How a Machine
Learns Prejudice</a>,” <em>scientificamerican.com</em>, December 29, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ceglowski2016uw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ceglowski2016uw-marker">86</a>] Maciej Cegłowski:
“<a href="http://idlewords.com/talks/sase_panel.htm">The Moral Economy of Tech</a>,”
<em>idlewords.com</em>, June 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ONeil2016vh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#ONeil2016vh-marker">87</a>] Cathy O’Neil:
<a href="https://weaponsofmathdestructionbook.com/"><em>Weapons of Math Destruction: How Big Data
Increases Inequality and Threatens Democracy</em></a>. Crown Publishing, 2016.
ISBN: 978-0-553-41881-1</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Angwin2016qn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Angwin2016qn-marker">88</a>] Julia Angwin:
“<a href="http://www.nytimes.com/2016/08/01/opinion/make-algorithms-accountable.html">Make
Algorithms Accountable</a>,” <em>nytimes.com</em>, August 1, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Goodman2016vd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Goodman2016vd-marker">89</a>] Bryce Goodman and Seth Flaxman:
“<a href="https://arxiv.org/abs/1606.08813">European Union Regulations on Algorithmic
Decision-Making and a ‘Right to Explanation’</a>,” <em>arXiv:1606.08813</em>, August 31,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="USSenate2013um">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#USSenate2013um-marker">90</a>] “<a href="https://www.commerce.senate.gov/public/index.cfm/reports?ID=57C428EC-8F20-44EE-BFB8-A570E9BE0CCC">A
Review of the Data Broker Industry: Collection, Use, and Sale of Consumer Data for Marketing
Purposes</a>,” Staff Report, <em>United States Senate Committee on Commerce, Science, and
Transportation</em>, <em>commerce.senate.gov</em>, December 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Solon2016vt">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Solon2016vt-marker">91</a>] Olivia Solon:
“<a href="https://www.theguardian.com/technology/2016/nov/10/facebook-fake-news-election-conspiracy-theories">Facebook’s
Failure: Did Fake News and Polarized Politics Get Trump Elected?</a>” <em>theguardian.com</em>, November 10,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Meadows2008wq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Meadows2008wq-marker">92</a>] Donella H. Meadows and Diana Wright:
<em>Thinking in Systems: A Primer</em>. Chelsea Green Publishing, 2008. ISBN: 978-1-603-58055-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bernstein2015bp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Bernstein2015bp-marker">93</a>] Daniel J. Bernstein:
“<a href="https://twitter.com/hashbreaker/status/598076230437568512">Listening to a ‘big data’/‘data
science’ talk</a>,” <em>twitter.com</em>, May 12, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Andreessen2011xa">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Andreessen2011xa-marker">94</a>] Marc Andreessen:
“<a href="http://genius.com/Marc-andreessen-why-software-is-eating-the-world-annotated">Why Software
Is Eating the World</a>,” <em>The Wall Street Journal</em>, 20 August 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Porup2016cx">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Porup2016cx-marker">95</a>] J. M. Porup:
“<a href="http://arstechnica.com/security/2016/01/how-to-search-the-internet-of-things-for-photos-of-sleeping-babies/">‘Internet
of Things’ Security Is Hilariously Broken and Getting Worse</a>,” <em>arstechnica.com</em>, January 23, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schneier2015vf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2015vf-marker">96</a>] Bruce Schneier:
<a href="https://www.schneier.com/books/data_and_goliath/"><em>Data and Goliath: The Hidden Battles
to Collect Your Data and Control Your World</em></a>. W. W. Norton, 2015.
ISBN: 978-0-393-35217-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Grugq2016nw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Grugq2016nw-marker">97</a>] The Grugq:
“<a href="https://grugq.tumblr.com/post/142799983558/nothing-to-hide">Nothing to Hide</a>,”
<em>grugq.tumblr.com</em>, April 15, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Beltramelli2015wk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Beltramelli2015wk-marker">98</a>] Tony Beltramelli:
“<a href="https://arxiv.org/abs/1512.05616">Deep-Spying: Spying Using Smartwatch and Deep
Learning</a>,” Masters Thesis, IT University of Copenhagen, December 2015. Available at
<em>arxiv.org/abs/1512.05616</em></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zuboff2015jd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zuboff2015jd-marker">99</a>] Shoshana Zuboff:
“<a href="http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2594754">Big Other: Surveillance
Capitalism and the Prospects of an Information Civilization</a>,” <em>Journal of Information
Technology</em>, volume 30, number 1, pages 75–89, April 2015.
<a href="http://dx.doi.org/10.1057/jit.2015.5">doi:10.1057/jit.2015.5</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zona2016nj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Zona2016nj-marker">100</a>] Carina C. Zona:
“<a href="https://www.youtube.com/watch?v=YRI40A4tyWU">Consequences of an Insightful Algorithm</a>,”
at <em>GOTO Berlin</em>, November 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schneier2016tf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2016tf-marker">101</a>] Bruce Schneier:
“<a href="https://www.schneier.com/essays/archives/2016/03/data_is_a_toxic_asse.html">Data Is a
Toxic Asset, So Why Not Throw It Out?</a>,” <em>schneier.com</em>, March 1, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dunn2016gy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Dunn2016gy-marker">102</a>] John E. Dunn:
“<a href="http://www.techworld.com/security/uks-most-infamous-data-breaches-2016-3604586/">The UK’s
15 Most Infamous Data Breaches</a>,” <em>techworld.com</em>, November 18, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Scott2016cq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Scott2016cq-marker">103</a>] Cory Scott:
“<a href="https://twitter.com/cory_scott/status/706586399483437056">Data is not toxic - which
implies no benefit - but rather hazardous material, where we must balance need vs. want</a>,”
<em>twitter.com</em>, March 6, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schneier2013qj">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Schneier2013qj-marker">104</a>] Bruce Schneier:
“<a href="https://www.schneier.com/essays/archives/2013/07/mission_creep_when_e.html">Mission Creep:
When Everything Is Terrorism</a>,” <em>schneier.com</em>, July 16, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ulbricht2016gh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ulbricht2016gh-marker">105</a>] Lena Ulbricht and Maximilian von Grafenstein:
“<a href="http://policyreview.info/articles/analysis/big-data-big-power-shifts">Big Data: Big Power
Shifts?</a>,” <em>Internet Policy Review</em>, volume 5, number 1, March 2016.
<a href="http://dx.doi.org/10.14763/2016.1.406">doi:10.14763/2016.1.406</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Goodman2016um">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Goodman2016um-marker">106</a>] Ellen P. Goodman and Julia Powles:
“<a href="https://www.theguardian.com/technology/2016/sep/28/google-facebook-powerful-secretive-empire-transparency">Facebook
and Google: Most Powerful and Secretive Empires We’ve Ever Known</a>,” <em>theguardian.com</em>, September 28,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DataProtectionDirective1995">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#DataProtectionDirective1995-marker">107</a>] <a href="http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:31995L0046">Directive
95/46/EC on the protection of individuals with regard to the processing of personal data and on the
free movement of such data</a>, Official Journal of the European Communities No. L 281/31,
<em>eur-lex.europa.eu</em>, November 1995.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="VanAlsenoy2016wb">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#VanAlsenoy2016wb-marker">108</a>] Brendan Van Alsenoy:
“<a href="https://lirias.kuleuven.be/handle/123456789/545027">Regulating Data Protection: The
Allocation of Responsibility and Risk Among Actors Involved in Personal Data Processing</a>,”
Thesis, KU Leuven Centre for IT and IP Law, August 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rhoen2016ff">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Rhoen2016ff-marker">109</a>] Michiel Rhoen:
“<a href="http://policyreview.info/articles/analysis/beyond-consent-improving-data-protection-through-consumer-protection-law">Beyond
Consent: Improving Data Protection Through Consumer Protection Law</a>,” <em>Internet Policy
Review</em>, volume 5, number 1, March 2016.
<a href="http://dx.doi.org/10.14763/2016.1.404">doi:10.14763/2016.1.404</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Leber2016xy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Leber2016xy-marker">110</a>] Jessica Leber:
“<a href="https://www.fastcoexist.com/3057514/your-data-footprint-is-affecting-your-life-in-ways-you-cant-even-imagine">Your
Data Footprint Is Affecting Your Life in Ways You Can’t Even Imagine</a>,” <em>fastcoexist.com</em>, March 15,
2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ceglowski2015vw">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Ceglowski2015vw-marker">111</a>] Maciej Cegłowski:
“<a href="http://idlewords.com/talks/haunted_by_data.htm">Haunted by Data</a>,” <em>idlewords.com</em>,
October 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thielman2016bt">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Thielman2016bt-marker">112</a>] Sam Thielman:
“<a href="https://www.theguardian.com/us-news/2016/jan/13/us-library-records-purged-data-privacy">You
Are Not What You Read: Librarians Purge User Data to Protect Privacy</a>,” <em>theguardian.com</em>,
January 13, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Friedersdorf2014yc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Friedersdorf2014yc-marker">113</a>] Conor Friedersdorf:
“<a href="http://www.theatlantic.com/politics/archive/2014/05/edward-snowdens-other-motive-for-leaking/370068/">Edward
Snowden’s Other Motive for Leaking</a>,” <em>theatlantic.com</em>, May 13, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Rogaway2015uc">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#Rogaway2015uc-marker">114</a>] Phillip Rogaway:
“<a href="http://web.cs.ucdavis.edu/~rogaway/papers/moral-fn.pdf">The Moral Character of
Cryptographic Work</a>,” Cryptology ePrint 2015/1162, December 2015.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">11. Stream Processing</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/glossary01.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">Glossary</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 69px;">
    <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/preferences/">Settings</a></li>
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    
    
      <img src="https://www.oreilly.com/library/view/oreilly_set_cookie/" alt="" style="display:none;">
    
    
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.9736120972302462"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.5473383292341132" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=ec2f48cd-faeb-71d9-d1ed-2eedabf3e011&amp;pi=1200101525&amp;lg=en-US&amp;sw=1440&amp;sh=900&amp;sc=24&amp;tl=12.%20The%20Future%20of%20Data%20Systems%20-%20Designing%20Data-Intensive%20Applications&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdesigning-data-intensive-applications%2F9781491903063%2Fch12.html&amp;r=&amp;evt=pageLoad&amp;msclkid=N&amp;rn=2569"></div>
    
  

<div class="annotator-notice"></div><div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch12.html#">Reset</a>
</div>
</div></body></html>