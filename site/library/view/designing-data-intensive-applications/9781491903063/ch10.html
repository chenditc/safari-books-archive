<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/designing-data-intensive-applications/9781491903063/ch10.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="3905629"
  data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99"
  data-username="safaribooksonline122"
  data-account-type="Trial"
  
  data-activated-trial-date="12/09/2018"


  data-archive="9781491903063"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch10.html"
  data-epub-title="Designing Data-Intensive Applications" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class="js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/designing-data-intensive-applications/9781491903063/ch10.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="3905629" data-user-uuid="f04af719-1c84-4fc3-9be3-1f1b4622ab99" data-username="safaribooksonline122" data-account-type="Trial" data-activated-trial-date="12/09/2018" data-archive="9781491903063" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch10.html" data-epub-title="Designing Data-Intensive Applications" data-debug="0" data-testing="0" style=""><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781491903063"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.0c29511d2d72.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>10. Batch Processing - Designing Data-Intensive Applications</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/5e586a47a3b7.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.e3b0c44298fc.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td,#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content canvas,#sbo-rt-content details,#sbo-rt-content embed,#sbo-rt-content figure,#sbo-rt-content figcaption,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content output,#sbo-rt-content ruby,#sbo-rt-content section,#sbo-rt-content summary,#sbo-rt-content time,#sbo-rt-content mark,#sbo-rt-content audio,#sbo-rt-content video{margin:0;padding:0;border:0;font-size:100%;font:inherit;vertical-align:baseline}#sbo-rt-content article,#sbo-rt-content aside,#sbo-rt-content details,#sbo-rt-content figcaption,#sbo-rt-content figure,#sbo-rt-content footer,#sbo-rt-content header,#sbo-rt-content hgroup,#sbo-rt-content menu,#sbo-rt-content nav,#sbo-rt-content section{display:block}#sbo-rt-content div{line-height:1}#sbo-rt-content ol,#sbo-rt-content ul{list-style:none}#sbo-rt-content blockquote,#sbo-rt-content q{quotes:none}#sbo-rt-content blockquote:before,#sbo-rt-content blockquote:after,#sbo-rt-content q:before,#sbo-rt-content q:after{content:none}#sbo-rt-content table{border-collapse:collapse;border-spacing:0}@page{margin:5px !important}#sbo-rt-content p{margin:10px 0 0;line-height:125%;text-align:left}#sbo-rt-content p.byline{text-align:left;margin:-33px auto 35px;font-style:italic;font-weight:bold}#sbo-rt-content div.preface p+p.byline{margin:1em 0 0 !important}#sbo-rt-content div.preface p.byline+p.byline{margin:0 !important}#sbo-rt-content div.sect1>p.byline{margin:-.25em 0 1em}#sbo-rt-content div.sect1>p.byline+p.byline{margin-top:-1em}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link,#sbo-rt-content a{text-decoration:none;color:#8e0012}#sbo-rt-content span.lineannotation{font-style:italic;color:#a62a2a;font-family:serif}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#fff}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important;margin-bottom:30px !important}#sbo-rt-content section[data-type="sect1"] h1{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content section[data-type="sect2"] h2{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content section[data-type="sect3"] h3{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content section[data-type="sect4"] h4{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section[data-type="chapter"]>div>h1,#sbo-rt-content section[data-type="preface"]>div>h1,#sbo-rt-content section[data-type="appendix"]>div>h1,#sbo-rt-content section[data-type="glossary"]>div>h1,#sbo-rt-content section[data-type="bibliography"]>div>h1,#sbo-rt-content section[data-type="index"]>div>h1{font-size:2em;line-height:1;margin-bottom:50px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content span.label,#sbo-rt-content span.keep-together{font-size:inherit;font-weight:inherit}#sbo-rt-content div[data-type="part"] h1{font-size:2em;text-align:center;margin-top:0 !important;margin-bottom:50px;padding:50px 0 10px 0;border-bottom:1px solid #000}#sbo-rt-content img.width-ninety{width:90%}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center !important;margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content figure{margin:15px 0 15px 0 !important;page-break-inside:avoid}#sbo-rt-content div.figure h6,#sbo-rt-content figure h6,#sbo-rt-content figure figcaption{font-size:.9rem !important;text-align:center;font-weight:normal !important;font-style:italic;font-family:serif !important;text-transform:none !important;letter-spacing:normal !important;color:#000 !important;padding-top:10px !important;page-break-before:avoid}#sbo-rt-content div.informalfigure{text-align:center !important;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:15px 0 10px 0 !important;border:1px solid #DCDCDC;background-color:#F7F7F7;padding:15px !important;page-break-inside:avoid}#sbo-rt-content aside[data-type="sidebar"]{margin:15px 0 10px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title,#sbo-rt-content aside[data-type="sidebar"] h5{font-weight:bold;font-size:1em;font-family:sans-serif;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar ol,#sbo-rt-content div.sidebar ul,#sbo-rt-content aside[data-type="sidebar"] ol,#sbo-rt-content aside[data-type="sidebar"] ul{margin-left:1.25em !important}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content aside[data-type="sidebar"] figcaption,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif !important;color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div[data-type="tip"],#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div[data-type="note"],#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div[data-type="warning"],#sbo-rt-content div.sidebar div[data-type="caution"],#sbo-rt-content div.sidebar div[data-type="important"]{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content aside[data-type="sidebar"] p.byline{font-size:90%;font-weight:bold;font-style:italic;text-align:center;text-indent:0;margin:5px auto 6px;page-break-after:avoid}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;overflow-wrap:break-word}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div[data-type="example"]{margin:10px 0 15px 0 !important}#sbo-rt-content div[data-type="example"] h1,#sbo-rt-content div[data-type="example"] h2,#sbo-rt-content div[data-type="example"] h3,#sbo-rt-content div[data-type="example"] h4,#sbo-rt-content div[data-type="example"] h5,#sbo-rt-content div[data-type="example"] h6{font-style:italic;font-weight:normal;text-align:left !important;text-transform:none !important;font-family:serif !important;margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],#sbo-rt-content div[data-type="example"] pre[data-type="screen"]{margin:0}#sbo-rt-content section[data-type="titlepage"]>div>h1{font-size:2em;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content section[data-type="titlepage"] h2,#sbo-rt-content section[data-type="titlepage"] p.subtitle,#sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"]{font-size:1.3em;font-weight:normal;text-align:center;margin-top:.5em;color:#555}#sbo-rt-content section[data-type="titlepage"]>div>h2[data-type="author"],#sbo-rt-content section[data-type="titlepage"] p.author{font-size:1.3em;font-family:serif !important;font-weight:bold;margin:50px 0 !important;text-align:center}#sbo-rt-content section[data-type="titlepage"] p.edition{text-align:center;text-transform:uppercase;margin-top:2em}#sbo-rt-content section[data-type="titlepage"]{text-align:center}#sbo-rt-content section[data-type="titlepage"]:after{content:url(css_assets/titlepage_footer_ebook.png);margin:0 auto;max-width:80%}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content ul.stafflist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.stafflist li{list-style-type:none;padding:5px 0}#sbo-rt-content ul.printings li{list-style-type:none}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif !important;font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif !important;margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif !important;font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10px}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif !important}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-style:italic;margin:.75em 0 0 !important}#sbo-rt-content blockquote div.attribution,#sbo-rt-content blockquote p[data-type="attribution"]{margin:5px 0 10px 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p,#sbo-rt-content blockquote p[data-type="attribution"]{font-style:normal;margin-top:5px}#sbo-rt-content blockquote div.attribution p:before,#sbo-rt-content blockquote p[data-type="attribution"]:before{font-style:normal;content:"—";-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div[data-type="footnotes"]{border-top:1px solid black;margin-top:1.5em}#sbo-rt-content sub,#sbo-rt-content sup{font-size:75%;line-height:0;position:relative}#sbo-rt-content sup{top:-.5em}#sbo-rt-content sub{bottom:-.25em}#sbo-rt-content div.refentry p.refname{font-size:1em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin-bottom:5px;overflow:auto;width:100%}#sbo-rt-content div.refentry{width:100%;display:block;margin-top:2em}#sbo-rt-content div.refsynopsisdiv{display:block;clear:both}#sbo-rt-content div.refentry header{page-break-inside:avoid !important;display:block;break-inside:avoid !important;padding-top:0;border-bottom:1px solid #000}#sbo-rt-content div.refsect1 h6{font-size:.9em;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content div.refsect1{margin-top:3em}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dd{margin-left:1.5em !important;margin-bottom:.25em}#sbo-rt-content dd ol,#sbo-rt-content dd ul{padding-left:1em}#sbo-rt-content dd li{margin-top:0;margin-bottom:0}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ul,#sbo-rt-content ul>li,#sbo-rt-content ol ul,#sbo-rt-content ol ul>li,#sbo-rt-content ul ol ul,#sbo-rt-content ul ol ul>li{list-style-type:disc}#sbo-rt-content ul ul,#sbo-rt-content ul ul>li{list-style-type:square}#sbo-rt-content ul ul ul,#sbo-rt-content ul ul ul>li{list-style-type:circle}#sbo-rt-content ol,#sbo-rt-content ol>li,#sbo-rt-content ol ul ol,#sbo-rt-content ol ul ol>li,#sbo-rt-content ul ol,#sbo-rt-content ul ol>li{list-style-type:decimal}#sbo-rt-content ol ol,#sbo-rt-content ol ol>li{list-style-type:lower-alpha}#sbo-rt-content ol ol ol,#sbo-rt-content ol ol ol>li{list-style-type:lower-roman}#sbo-rt-content ol,#sbo-rt-content ul{list-style-position:outside;margin:15px 0 15px 1.25em;padding-left:2.25em}#sbo-rt-content ol li,#sbo-rt-content ul li{margin:.5em 0 .65em;line-height:125%}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist,#sbo-rt-content ul.simplelist{margin:15px 0 15px 20px !important}#sbo-rt-content ul.simplelist li{list-style-type:none;padding:5px 0}#sbo-rt-content table.simplelist td{border:none}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content dl.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content dl.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content dl.calloutlist img,#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"],#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{margin:30px !important;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip ol,#sbo-rt-content div.tip ul,#sbo-rt-content div[data-type="tip"] ol,#sbo-rt-content div[data-type="tip"] ul,#sbo-rt-content div.note ol,#sbo-rt-content div.note ul,#sbo-rt-content div[data-type="note"] ol,#sbo-rt-content div[data-type="note"] ul,#sbo-rt-content div.warning ol,#sbo-rt-content div.warning ul,#sbo-rt-content div[data-type="warning"] ol,#sbo-rt-content div[data-type="warning"] ul,#sbo-rt-content div[data-type="caution"] ol,#sbo-rt-content div[data-type="caution"] ul,#sbo-rt-content div[data-type="important"] ol,#sbo-rt-content div[data-type="important"] ul{margin-left:1.5em !important}#sbo-rt-content div.tip,#sbo-rt-content div[data-type="tip"],#sbo-rt-content div.note,#sbo-rt-content div[data-type="note"]{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div[data-type="warning"],#sbo-rt-content div[data-type="caution"],#sbo-rt-content div[data-type="important"]{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="note"] h1,#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1,#sbo-rt-content div[data-type="important"] h6{font-weight:bold;font-size:110%;font-family:sans-serif !important;text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div[data-type="tip"] figure h6,#sbo-rt-content div[data-type="note"] figure h6,#sbo-rt-content div[data-type="warning"] figure h6,#sbo-rt-content div[data-type="caution"] figure h6,#sbo-rt-content div[data-type="important"] figure h6{font-family:serif !important}#sbo-rt-content div.tip h3,#sbo-rt-content div[data-type="tip"] h6,#sbo-rt-content div.note h3,#sbo-rt-content div[data-type="note"] h6,#sbo-rt-content div[data-type="tip"] h1,#sbo-rt-content div[data-type="note"] h1{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div[data-type="warning"] h6,#sbo-rt-content div[data-type="caution"] h6,#sbo-rt-content div[data-type="important"] h6,#sbo-rt-content div[data-type="warning"] h1,#sbo-rt-content div[data-type="caution"] h1,#sbo-rt-content div[data-type="important"] h1{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,#sbo-rt-content div.safarienabled{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,#sbo-rt-content div.safarienabled h6{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px 0 30px 0 !important;max-width:95%;border:none !important;background:none;display:table !important}#sbo-rt-content div.table,#sbo-rt-content div.informaltable,#sbo-rt-content table{page-break-inside:avoid}#sbo-rt-content tr,#sbo-rt-content tr td{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td,#sbo-rt-content thead th{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif;font-weight:bold}#sbo-rt-content td,#sbo-rt-content th{display:table-cell;padding:.3em;text-align:left;vertical-align:middle;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title,#sbo-rt-content table caption{font-weight:normal;font-style:italic;font-family:serif;font-size:1em;margin:10px 0 10px 0 !important;padding:0;page-break-after:avoid;text-align:left !important}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation,#sbo-rt-content div[data-type="equation"]{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title,#sbo-rt-content div[data-type="equation"] h5{font-style:italic;font-weight:normal;font-family:serif !important;font-size:90%;margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content div[data-type="equation"] math{font-size:calc(.35em + 1vw)}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{text-indent:0}#sbo-rt-content div.index h3{padding:.25em;margin-top:1em !important;background-color:#F0F0F0}#sbo-rt-content div.index li{line-height:130%;list-style-type:none}#sbo-rt-content div.index a.indexterm{color:#8e0012 !important}#sbo-rt-content div.index ul{margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.index ul ul{margin-left:1em !important;margin-top:0 !important}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif;text-align:left}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.map-ebook{page-break-after:always}
    </style><link rel="canonical" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html"><meta name="description" content=" Chapter 10. Batch Processing A system cannot be successful if it is too strongly influenced by a single person. Once the initial design is complete and fairly robust, the real ... "><meta property="og:title" content="10. Batch Processing"><meta itemprop="isPartOf" content="/library/view/designing-data-intensive-applications/9781491903063/"><meta itemprop="name" content="10. Batch Processing"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch10.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781491903063/"><meta property="og:description" itemprop="description" content=" Chapter 10. Batch Processing A system cannot be successful if it is too strongly influenced by a single person. Once the initial design is complete and fairly robust, the real ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449373320"><meta property="og:book:author" itemprop="author" content="Martin Kleppmann"><meta property="og:book:tag" itemprop="about" content="Core Programming"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts subscribe-panel library" data-gr-c-s-loaded="true">

    
  
  
  



    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        





<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/playlists/" class="t-queue-nav l0 nav-icn None"><!--?xml version="1.0" encoding="UTF-8"?--><svg width="21px" height="17px" viewBox="0 0 21 17" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><!-- Generator: Sketch 46.2 (44496) - http://www.bohemiancoding.com/sketch --><title>icon_Playlist_sml</title><desc>Created with Sketch.</desc><defs></defs><g id="Page-1" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="icon_Playlist_sml" fill-rule="nonzero" fill="#000000"><g id="playlist-icon"><g id="Group-6"><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle></g><g id="Group-5" transform="translate(0.000000, 7.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g><g id="Group-5-Copy" transform="translate(0.000000, 14.000000)"><circle id="Oval" cx="1.5" cy="1.5" r="1.5"></circle><rect id="Rectangle-path" x="5" y="0" width="16" height="3" rx="0.5"></rect></g></g></g></g></svg><span>
               Playlists
            </span></a></li><li class="search"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" width="20" height="20" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" width="20" height="20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" width="20" height="20" version="1.1" fill="#4A3C31"><desc>offers icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M35.9 20.6L27 15.5C26.1 15 24.7 15 23.7 15.5L14.9 20.6C13.9 21.1 13.2 22.4 13.2 23.4L13.2 41.4C13.2 42.4 13.9 43.7 14.9 44.2L23.3 49C24.2 49.5 25.6 49.5 26.6 49L35.9 43.6C36.8 43.1 37.6 41.8 37.6 40.8L37.6 23.4C37.6 22.4 36.8 21.1 35.9 20.6L35.9 20.6ZM40 8.2C39.1 7.6 37.6 7.6 36.7 8.2L30.2 11.9C29.3 12.4 29.3 13.2 30.2 13.8L39.1 18.8C40 19.4 40.7 20.6 40.7 21.7L40.7 39C40.7 40.1 41.4 40.5 42.4 40L48.2 36.6C49.1 36.1 49.8 34.9 49.8 33.8L49.8 15.6C49.8 14.6 49.1 13.3 48.2 12.8L40 8.2 40 8.2ZM27 10.1L33.6 6.4C34.5 5.9 34.5 5 33.6 4.5L26.6 0.5C25.6 0 24.2 0 23.3 0.5L16.7 4.2C15.8 4.7 15.8 5.6 16.7 6.1L23.7 10.1C24.7 10.6 26.1 10.6 27 10.1ZM10.1 21.7C10.1 20.6 10.8 19.4 11.7 18.8L20.6 13.8C21.5 13.2 21.5 12.4 20.6 11.9L13.6 7.9C12.7 7.4 11.2 7.4 10.3 7.9L1.6 12.8C0.7 13.3 0 14.6 0 15.6L0 33.8C0 34.9 0.7 36.1 1.6 36.6L8.4 40.5C9.3 41 10.1 40.6 10.1 39.6L10.1 21.7 10.1 21.7Z"></path></g></svg><span>Offers &amp; Deals</span></a><ul class="flyout"><li><a href="https://get.oreilly.com/email-signup.html" target="_blank" class="l2 nav-icn"><span>Newsletters</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/f04af719-1c84-4fc3-9be3-1f1b4622ab99/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" width="20" height="20" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/preferences/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l1 no-icon">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/preferences/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" width="20" height="20" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z"></path></g></svg><span>Settings</span></a><span class="l2 t-nag-notification" id="nav-nag"><strong class="trial-green">10</strong> days left in your trial.
  
  

  
    
      

<a class="" href="https://www.safaribooksonline.com/subscribe/">Subscribe</a>.


    
  

  

</span></li><li><a href="https://www.oreilly.com/online-learning/support/" class="l2">Support</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application" style="height: auto;">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      Designing Data-Intensive Applications
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><div class="js-content-uri" data-content-uri="/api/v1/book/9781491903063/chapter/ch10.html"><div class="js-collections-dropdown collections-dropdown menu-bit-cards"><div data-reactroot="" class="menu-dropdown-wrapper js-menu-dropdown-wrapper align-right"><img class="hidden" src="https://www.safaribooksonline.com/static/images/ajax-transp.gif" alt="loading spinner"><div class="menu-control"><div class="control "><div class="js-playlists-menu"><button class="js-playlist-icon"><svg class="icon-add-to-playlist-sml" viewBox="0 0 16 14" version="1.1" xmlns="http://www.w3.org/2000/svg"><g stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g fill-rule="nonzero" fill="#000000"><g transform="translate(-1.000000, 0.000000)"><rect x="5" y="0" width="12" height="2"></rect><title>Playlists</title><path d="M4.5,14 C6.43299662,14 8,12.4329966 8,10.5 C8,8.56700338 6.43299662,7 4.5,7 C2.56700338,7 1,8.56700338 1,10.5 C1,12.4329966 2.56700338,14 4.5,14 Z M2.5,10 L4,10 L4,8.5 L5,8.5 L5,10 L6.5,10 L6.5,11 L5,11 L5,12.5 L4,12.5 L4,11 L2.5,11 L2.5,10 Z"></path><circle cx="2" cy="5" r="1"></circle><circle cx="1.94117647" cy="1" r="1"></circle><rect x="5" y="4" width="12" height="2"></rect><rect x="9" y="8" width="8" height="2"></rect><rect x="9" y="12" width="8" height="2"></rect></g></g></g></svg><div class="js-playlist-addto-label">Add&nbsp;To</div></button></div></div></div></div></div></div></li><li class="js-font-control-panel font-control-activator"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch10.html&amp;text=Designing%20Data-Intensive%20Applications&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch10.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch10.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%2010.%20Batch%20Processing&amp;body=https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/ch10.html%0D%0Afrom%20Designing%20Data-Intensive%20Applications%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
        
        



 <!--[if lt IE 9]>
  
<![endif]-->



  


        
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/part03.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">III. Derived Data</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Stream Processing</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content" style="transform: none;"><div class="annotator-wrapper"><section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 10. Batch Processing"><div class="chapter" id="ch_batch">
<h1><span class="label">Chapter 10. </span>Batch Processing</h1>

<blockquote data-type="epigraph" epub:type="epigraph">
<p><em>A system cannot be successful if it is too strongly influenced by a single person. Once the initial
design is complete and fairly robust, the real test begins as people with many different viewpoints
undertake their own experiments.</em></p>
<p data-type="attribution">Donald Knuth</p>
</blockquote>

<div class="map-ebook">
 <img id="c276" src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ch10-map.png" width="2100" height="2756" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ch10-map.png">
</div>

<p><a data-type="indexterm" data-primary="batch processing" id="ix_batch"></a>
In the first two parts of this book we talked a lot about <em>requests</em> and <em>queries</em>, and the
corresponding <em>responses</em> or <em>results</em>. This style of data processing is assumed in many modern data
systems: you ask for something, or you send an instruction, and some time later the system
(hopefully) gives you an answer. Databases, caches, search indexes, web servers, and many other
systems work this way.</p>

<p><a data-type="indexterm" data-primary="online systems" data-seealso="services" id="idm140417549501456"></a>
In such <em>online</em> systems, whether it’s a web browser requesting a page or a service calling a
remote API, we generally assume that the request is triggered by a human user, and that the user is
waiting for the response. They shouldn’t have to wait too long, so we pay a lot of
attention to the <em>response time</em> of these systems (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch01.html#sec_introduction_percentiles">“Describing Performance”</a>).</p>

<p>The web, and increasing numbers of HTTP/REST-based APIs, has made the request/response style of
interaction so common that it’s easy to take it for granted. But we should remember that it’s not
the only way of building systems, and that other approaches have their merits too. Let’s distinguish
three different types of systems:</p>
<dl>
<dt>Services (online systems)</dt>
<dd>
<p><a data-type="indexterm" data-primary="services" data-secondary="relation to batch/stream processors" id="idm140417549495952"></a>
<a data-type="indexterm" data-primary="microservices" data-secondary="relation to batch/stream processors" id="idm140417549494832"></a>
<a data-type="indexterm" data-primary="response time" data-secondary="as performance metric for services" id="idm140417549493712"></a>
A service waits for a request or instruction from a client to arrive. When one is received, the
service tries to handle it as quickly as possible and sends a response back. Response time is
usually the primary measure of performance of a service, and availability is often very important
(if the client can’t reach the service, the user will probably get an error message).</p>
</dd>
<dt>Batch processing systems (offline systems)</dt>
<dd>
<p><a data-type="indexterm" data-primary="offline systems" data-seealso="batch processing" id="idm140417549491008"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="measuring performance" id="idm140417549489904"></a>
<a data-type="indexterm" data-primary="throughput" id="idm140417549488800"></a>
A batch processing system takes a large amount of input data, runs a <em>job</em> to process it, and
produces some output data. Jobs often take a while (from a few minutes to several days), so there
normally isn’t a user waiting for the job to finish. Instead, batch jobs are often scheduled to
run periodically (for example, once a day). The primary performance measure of a batch job is
usually <em>throughput</em> (the time it takes to crunch through an input dataset of a certain size). We
discuss batch processing in this chapter.</p>
</dd>
<dt>Stream processing systems (near-real-time systems)</dt>
<dd>
<p><a data-type="indexterm" data-primary="near-real-time (nearline) processing" data-seealso="stream processing" id="idm140417549485408"></a>
<a data-type="indexterm" data-primary="real-time" data-secondary="near-real-time processing" data-seealso="stream processing" id="idm140417549484288"></a>
Stream processing is somewhere between online and offline/batch processing (so it is sometimes
called <em>near-real-time</em> or <em>nearline</em> processing). Like a batch processing system, a stream
processor consumes inputs and produces outputs (rather than responding to requests). However, a
stream job operates on events shortly after they happen, whereas a batch job operates on a fixed
set of input data.  This difference allows stream processing systems to have lower latency than
the equivalent batch systems. As stream processing builds upon batch processing, we discuss it in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>
</dd>
</dl>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" id="idm140417549480272"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="MapReduce" data-see="MapReduce" id="idm140417549479440"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-seealso="MapReduce" id="idm140417549478048"></a>
As we shall see in this chapter, batch processing is an important building block in our quest to
build reliable, scalable, and maintainable applications. For example, MapReduce, a batch processing
algorithm published in 2004
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Dean2004ua_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Dean2004ua_ch10" class="totri-footnote">1</a>], was (perhaps
over-enthusiastically) called “the algorithm that makes Google so massively scalable”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Spolsky2005wm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Spolsky2005wm" class="totri-footnote">2</a>]. It was subsequently
implemented in various open source data systems, including Hadoop, CouchDB, and MongoDB.</p>

<p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to distributed databases" id="idm140417549471504"></a>
MapReduce is a fairly low-level programming model compared to the parallel processing systems that
were developed for data warehouses many years previously
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Babu2013gm_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Babu2013gm_ch10" class="totri-footnote">3</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt2008up-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt2008up" class="totri-footnote">4</a>],
but it was a major step forward in terms of the scale of processing that could be achieved on
commodity hardware. Although the importance of MapReduce is now declining
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Robinson2014vz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Robinson2014vz" class="totri-footnote">5</a>],
it is still worth understanding, because it provides a clear picture of why and how batch processing
is useful.</p>

<p><a data-type="indexterm" data-primary="punch card tabulating machines" id="idm140417549462176"></a>
<a data-type="indexterm" data-primary="Hollerith machines" id="idm140417549461216"></a>
<a data-type="indexterm" data-primary="IBM" data-secondary="electromechanical card-sorting machines" id="idm140417549460384"></a>
<a data-type="indexterm" data-primary="business data processing" id="idm140417549459264"></a>
In fact, batch processing is a very old form of computing. Long before programmable digital
computers were invented, punch card tabulating machines—such as the Hollerith machines used in
the 1890 US Census
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Hollerith-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Hollerith" class="totri-footnote">6</a>]—implemented a
semi-mechanized form of batch processing to compute aggregate statistics from large inputs. And
MapReduce bears an uncanny resemblance to the electromechanical IBM card-sorting machines that were
widely used for business data processing in the 1940s and 1950s
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="IBM1962vz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#IBM1962vz" class="totri-footnote">7</a>]. As usual, history has a tendency of repeating
itself.</p>

<p>In this chapter, we will look at MapReduce and several other batch processing algorithms and
frameworks, and explore how they are used in modern data systems. But first, to get started, we will
look at data processing using standard Unix tools. Even if you are already familiar with them, a
reminder about the Unix philosophy is worthwhile because the ideas and lessons from Unix carry over
to large-scale, heterogeneous distributed data systems.</p>






<section data-type="sect1" data-pdf-bookmark="Batch Processing with Unix Tools"><div class="sect1" id="sec_batch_unix">
<h1>Batch Processing with Unix Tools</h1>

<p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" id="ix_Unixtools"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="using Unix tools (example)" id="ix_batchUnix"></a>
Let’s start with a simple example. Say you have a web server that appends a line to a log file every
time it serves a request. For example, using the nginx default access log format, one line of the
log might look like this:</p>

<pre data-type="programlisting">216.58.210.78 - - [27/Feb/2015:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X
10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115
Safari/537.36"</pre>

<p>(That is actually one line; it’s only broken onto multiple lines here for readability.) There’s a
lot of information in that line. In order to interpret it, you need to look at the definition of
the log format, which is as follows:</p>

<pre data-type="programlisting">$remote_addr - $remote_user [$time_local] "$request"
$status $body_bytes_sent "$http_referer" "$http_user_agent"</pre>

<p>So, this one line of the log indicates that on February 27, 2015, at 17:55:11 UTC, the server
received a request for the file <em>/css/typography.css</em> from the client IP address 216.58.210.78. The
user was not authenticated, so <code>$remote_user</code> is set to a hyphen (<code>-</code>). The response status was 200
(i.e., the request was successful), and the response was 3,377 bytes in size. The web browser was
Chrome 40, and it loaded the file because it was referenced in the page at the URL
<em><a href="http://martin.kleppmann.com/"><em class="hyperlink">http://martin.kleppmann.com/</em></a></em>.</p>








<section data-type="sect2" data-pdf-bookmark="Simple Log Analysis"><div class="sect2" id="sec_batch_log_analysis">
<h2>Simple Log Analysis</h2>

<p>Various tools can take these log files and produce pretty reports about your website traffic, but
for the sake of exercise, let’s build our own, using basic Unix tools. For example, say you want to
find the five most popular pages on your website. You can do this in a Unix shell as
follows:<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417549440016-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549440016" class="totri-footnote">i</a></sup>
<a data-type="indexterm" data-primary="cat (Unix tool)" id="idm140417549438224"></a><a data-type="indexterm" data-primary="awk (Unix tool)" id="idm140417549437440"></a></p>

<pre data-type="programlisting" data-code-language="bash"><code>cat</code><code> </code><code>/var/log/nginx/access.log</code><code> </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-1" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-1"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/1.png" alt="1" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/1.png"></a><code>
  </code><code>awk</code><code> </code><code class="s1">'{print $7}'</code><code> </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-2" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-2"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/2.png" alt="2" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/2.png"></a><code>
  </code><code>sort</code><code>             </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-3" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-3"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/3.png" alt="3" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/3.png"></a><code>
  </code><code>uniq</code><code> </code><code>-c</code><code>          </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-4" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-4"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/4.png" alt="4" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/4.png"></a><code>
  </code><code>sort</code><code> </code><code>-r</code><code> </code><code>-n</code><code>       </code><code class="p">|</code><code> </code><a class="co" id="co_batch_processing_CO1-5" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-5"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/5.png" alt="5" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/5.png"></a><code>
  </code><code>head</code><code> </code><code>-n</code><code> </code><code class="m">5</code><code>          </code><a class="co" id="co_batch_processing_CO1-6" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO1-6"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/6.png" alt="6" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/6.png"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_batch_processing_CO1-1" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-1"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/1.png" alt="1" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/1.png"></a></dt>
<dd><p>Read the log file.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO1-2" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-2"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/2.png" alt="2" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/2.png"></a></dt>
<dd><p>Split each line into fields by whitespace, and output only the seventh such field from each
line, which happens to be the requested URL. In our example line, this request URL is
<em>/css/typography.css</em>.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO1-3" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-3"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/3.png" alt="3" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/3.png"></a></dt>
<dd><p><a data-type="indexterm" data-primary="sort (Unix tool)" id="idm140417549391600"></a> Alphabetically <code>sort</code> the list of requested URLs. If some URL has been
requested <em>n</em> times, then after sorting, the file contains the same URL repeated <em>n</em> times in a row.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO1-4" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-4"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/4.png" alt="4" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/4.png"></a></dt>
<dd><p><a data-type="indexterm" data-primary="uniq (Unix tool)" id="idm140417549358608"></a> The <code>uniq</code> command filters out repeated lines in its input by checking
whether two adjacent lines are the same. The <code>-c</code> option tells it to also output a counter: for
every distinct URL, it reports how many times that URL appeared in the input.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO1-5" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-5"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/5.png" alt="5" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/5.png"></a></dt>
<dd><p>The second <code>sort</code> sorts by the number (<code>-n</code>) at the start of each line, which is the number of
times the URL was requested. It then returns the results in reverse
(<span class="keep-together"><code>-r</code></span>) order, i.e. with the largest number
first.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO1-6" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO1-6"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/6.png" alt="6" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/6.png"></a></dt>
<dd><p><a data-type="indexterm" data-primary="head (Unix tool)" id="idm140417549350384"></a> Finally, <code>head</code> outputs just the first five lines (<code>-n 5</code>) of input,
and discards the rest.</p></dd>
</dl>

<p>The output of that series of commands looks something like this:</p>

<pre data-type="programlisting">4189 /favicon.ico
3631 /2013/05/24/improving-security-of-ssh-private-keys.html
2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
1369 /
 915 /css/typography.css</pre>

<p>Although the preceding command line likely looks a bit obscure if you’re unfamiliar with Unix tools,
it is incredibly powerful. It will process gigabytes of log files in a matter of seconds, and you
can easily modify the analysis to suit your needs. For example, if you want to omit CSS files from
the report, change the <code>awk</code> argument to <code>'$7 !~ /\.css$/ {print $7}'</code>. If you want to count top
client IP addresses instead of top pages, change the <code>awk</code> argument to <code>'{print $1}'</code>. And so on.</p>

<p><a data-type="indexterm" data-primary="grep (Unix tool)" id="idm140417549344784"></a>
<a data-type="indexterm" data-primary="xargs (Unix tool)" id="idm140417549343728"></a>
<a data-type="indexterm" data-primary="sed (Unix tool)" id="idm140417549342896"></a>
We don’t have space in this book to explore Unix tools in detail, but they are very much worth
learning about. Surprisingly many data analyses can be done in a few minutes using some combination
of <code>awk</code>, <code>sed</code>, <code>grep</code>, <code>sort</code>, <code>uniq</code>, and <code>xargs</code>, and they perform surprisingly well
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Drake2014vm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Drake2014vm" class="totri-footnote">8</a>].</p>










<section data-type="sect3" data-pdf-bookmark="Chain of commands versus custom program"><div class="sect3" id="idm140417549337152">
<h3>Chain of commands versus custom program</h3>

<p class="pagebreak-after">Instead of the chain of Unix commands, you could write a simple program to do the same
thing. For example, in Ruby, it might look something like this:</p>

<pre data-type="programlisting" data-code-language="ruby"><code class="n">counts</code><code> </code><code class="o">=</code><code> </code><code class="no">Hash</code><code class="o">.</code><code class="n">new</code><code class="p">(</code><code class="mi">0</code><code class="p">)</code><code> </code><a class="co" id="co_batch_processing_CO2-1" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO2-1"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/1.png" alt="1" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/1.png"></a><code>

</code><code class="no">File</code><code class="o">.</code><code class="n">open</code><code class="p">(</code><code class="s1">'/var/log/nginx/access.log'</code><code class="p">)</code><code> </code><code class="k">do</code><code> </code><code class="o">|</code><code class="n">file</code><code class="o">|</code><code>
  </code><code class="n">file</code><code class="o">.</code><code class="n">each</code><code> </code><code class="k">do</code><code> </code><code class="o">|</code><code class="n">line</code><code class="o">|</code><code>
    </code><code class="n">url</code><code> </code><code class="o">=</code><code> </code><code class="n">line</code><code class="o">.</code><code class="n">split</code><code class="o">[</code><code class="mi">6</code><code class="o">]</code><code> </code><a class="co" id="co_batch_processing_CO2-2" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO2-2"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/2.png" alt="2" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/2.png"></a><code>
    </code><code class="n">counts</code><code class="o">[</code><code class="n">url</code><code class="o">]</code><code> </code><code class="o">+=</code><code> </code><code class="mi">1</code><code> </code><a class="co" id="co_batch_processing_CO2-3" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO2-3"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/3.png" alt="3" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/3.png"></a><code>
  </code><code class="k">end</code><code>
</code><code class="k">end</code><code>

</code><code class="n">top5</code><code> </code><code class="o">=</code><code> </code><code class="n">counts</code><code class="o">.</code><code class="n">map</code><code class="p">{</code><code class="o">|</code><code class="n">url</code><code class="p">,</code><code> </code><code class="n">count</code><code class="o">|</code><code> </code><code class="o">[</code><code class="n">count</code><code class="p">,</code><code> </code><code class="n">url</code><code class="o">]</code><code> </code><code class="p">}</code><code class="o">.</code><code class="n">sort</code><code class="o">.</code><code class="n">reverse</code><code class="o">[</code><code class="mi">0</code><code class="o">.</code><code class="n">.</code><code class="o">.</code><code class="mi">5</code><code class="o">]</code><code> </code><a class="co" id="co_batch_processing_CO2-4" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO2-4"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/4.png" alt="4" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/4.png"></a><code>
</code><code class="n">top5</code><code class="o">.</code><code class="n">each</code><code class="p">{</code><code class="o">|</code><code class="n">count</code><code class="p">,</code><code> </code><code class="n">url</code><code class="o">|</code><code> </code><code class="nb">puts</code><code> </code><code class="s2">"</code><code class="si">#{</code><code class="n">count</code><code class="si">}</code><code class="s2"> </code><code class="si">#{</code><code class="n">url</code><code class="si">}</code><code class="s2">"</code><code> </code><code class="p">}</code><code> </code><a class="co" id="co_batch_processing_CO2-5" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#callout_batch_processing_CO2-5"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/5.png" alt="5" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/5.png"></a></pre>
<dl class="calloutlist">
<dt><a class="co" id="callout_batch_processing_CO2-1" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO2-1"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/1.png" alt="1" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/1.png"></a></dt>
<dd><p><code>counts</code> is a hash table that keeps a counter for the number of times we’ve seen each URL. A
counter is zero by default.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO2-2" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO2-2"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/2.png" alt="2" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/2.png"></a></dt>
<dd><p>From each line of the log, we take the URL to be the seventh whitespace-separated field (the
array index here is 6 because Ruby’s arrays are zero-indexed).</p></dd>
<dt><a class="co" id="callout_batch_processing_CO2-3" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO2-3"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/3.png" alt="3" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/3.png"></a></dt>
<dd><p>Increment the counter for the URL in the current line of the log.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO2-4" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO2-4"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/4.png" alt="4" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/4.png"></a></dt>
<dd><p>Sort the hash table contents by counter value (descending), and take the top five entries.</p></dd>
<dt><a class="co" id="callout_batch_processing_CO2-5" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#co_batch_processing_CO2-5"><img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/5.png" alt="5" width="12" height="12" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/5.png"></a></dt>
<dd><p>Print out those top five entries.</p></dd>
</dl>

<p>This program is not as concise as the chain of Unix pipes, but it’s fairly readable, and which of
the two you prefer is partly a matter of taste. However, besides the superficial syntactic
differences between the two, there is a big difference in the execution flow, which becomes apparent
if you run this analysis on a large file.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Sorting versus in-memory aggregation"><div class="sect3" id="idm140417549169264">
<h3>Sorting versus in-memory aggregation</h3>

<p>The Ruby script keeps an in-memory hash table of URLs, where each URL is mapped to the number of
times it has been seen. The Unix pipeline example does not have such a hash table, but instead
relies on sorting a list of URLs in which multiple occurrences of the same URL are simply repeated.</p>

<p><a data-type="indexterm" data-primary="working set" id="idm140417549167328"></a>
Which approach is better? It depends how many different URLs you have. For most small to mid-sized
websites, you can probably fit all distinct URLs, and a counter for each URL, in (say) 1&nbsp;GB of
memory. In this example, the <em>working set</em> of the job (the amount of memory to which the job needs
random access) depends only on the number of distinct URLs: if there are a million log entries for a
single URL, the space required in the hash table is still just one URL plus the size of the counter.
If this working set is small enough, an in-memory hash table works fine—even on a laptop.</p>

<p>On the other hand, if the job’s working set is larger than the available memory, the sorting approach has the
advantage that it can make efficient use of disks. It’s the same principle as we discussed in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>: chunks of data can be sorted in memory and written out to disk as segment
files, and then multiple sorted segments can be merged into a larger sorted file. Mergesort has
sequential access patterns that perform well on disks. (Remember that optimizing for sequential I/O
was a recurring theme in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#ch_storage">Chapter&nbsp;3</a>. The same pattern reappears here.)</p>

<p><a data-type="indexterm" data-primary="GNU Coreutils (Linux)" id="idm140417549162944"></a>
<a data-type="indexterm" data-primary="sort (Unix tool)" id="idm140417549161984"></a>
The <code>sort</code> utility in GNU Coreutils (Linux) automatically handles larger-than-memory datasets by
spilling to disk, and automatically parallelizes sorting across multiple CPU cores
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="GNUCoreutils-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#GNUCoreutils" class="totri-footnote">9</a>].
This means that the simple chain of Unix commands we saw earlier easily scales to large datasets, without
running out of memory. The bottleneck is likely to be the rate at which the input file can be read
from disk.
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" data-startref="ix_Unixtools" id="idm140417549158944"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="using Unix tools (example)" data-startref="ix_batchUnix" id="idm140417549157728"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="The Unix Philosophy"><div class="sect2" id="sec_batch_unix_philosophy">
<h2>The Unix Philosophy</h2>

<p><a data-type="indexterm" data-primary="Unix philosophy" id="ix_Unixtoolsphil"></a>
<a data-type="indexterm" data-primary="pipelined execution" data-secondary="in Unix" id="idm140417549154224"></a>
It’s no coincidence that we were able to analyze a log file quite easily, using a chain of commands
like in the previous example: this was in fact one of the key design ideas of Unix, and it remains
astonishingly relevant today. Let’s look at it in some more depth so that we can borrow some ideas from
Unix [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2015tz_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kleppmann2015tz_ch10">10</a>].</p>

<p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="pipes" id="idm140417549150560"></a>
Doug McIlroy, the inventor of Unix pipes, first described them like this in 1964
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="RichieMcIlroy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#RichieMcIlroy">11</a>]: “We should have
some ways of connecting programs like [a] garden hose—screw in another segment when it becomes
necessary to massage data in another way. This is the way of I/O also.”
The plumbing analogy stuck, and the idea of connecting programs with pipes became part of what is
now known as the <em>Unix philosophy</em>—a set of design principles that became popular among the
developers and users of Unix. The philosophy was described in 1978 as follows
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McIlroy1978te-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#McIlroy1978te">12</a>, <a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Raymond2003wn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Raymond2003wn">13</a>]:</p>
<blockquote><ol>
<li>
<p>Make each program do one thing well. To do a new job, build afresh rather than complicate old
programs by adding new “features”.</p>
</li>
<li>
<p>Expect the output of every program to become the input to another, as yet unknown, program. Don’t
clutter output with extraneous information. Avoid stringently columnar or binary input formats.
Don’t insist on interactive input.</p>
</li>
<li>
<p>Design and build software, even operating systems, to be tried early, ideally within weeks. Don’t
hesitate to throw away the clumsy parts and rebuild them.</p>
</li>
<li>
<p>Use tools in preference to unskilled help to lighten a programming task, even if you have to
detour to build the tools and expect to throw some of them out after you’ve finished using them.</p>
</li>

</ol></blockquote>

<p><a data-type="indexterm" data-primary="Agile" data-secondary="Unix philosophy" id="idm140417549138224"></a>
<a data-type="indexterm" data-primary="DevOps" id="idm140417549137216"></a>
This approach—automation, rapid prototyping, incremental iteration, being friendly to
experimentation, and breaking down large projects into manageable chunks—sounds remarkably like
the Agile and DevOps movements of today. Surprisingly little has changed in four decades.</p>

<p><a data-type="indexterm" data-primary="sort (Unix tool)" id="idm140417549135840"></a>
The <code>sort</code> tool is a great example of a program that does one thing well. It is arguably a better
sorting implementation than most programming languages have in their standard libraries (which do not
spill to disk and do not use multiple threads, even when that would be beneficial). And yet, <code>sort</code>
is barely useful in isolation. It only becomes powerful in combination with the other Unix tools,
such as <code>uniq</code>.</p>

<p><a data-type="indexterm" data-primary="bash shell (Unix)" id="idm140417549133008"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="composability and uniform interfaces" id="idm140417549132176"></a>
A Unix shell like <code>bash</code> lets us easily <em>compose</em> these small programs into surprisingly powerful
data processing jobs. Even though many of these programs are written by different groups of people,
they can be joined together in flexible ways. What does Unix do to enable this composability?</p>










<section data-type="sect3" data-pdf-bookmark="A uniform interface"><div class="sect3" id="idm140417549129808">
<h3>A uniform interface</h3>

<p>If you expect the output of one program to become the input to another program, that means those
programs must use the same data format—in other words, a compatible interface. If you want to be
able to connect <em>any</em> program’s output to <em>any</em> program’s input, that means that <em>all</em> programs must
use the same input/output interface.</p>

<p><a data-type="indexterm" data-primary="file descriptors (Unix)" id="idm140417549126448"></a>
<a data-type="indexterm" data-primary="stdin, stdout" id="idm140417549125392"></a>
<a data-type="indexterm" data-primary="uniform interfaces" id="idm140417549124560"></a>
In Unix, that interface is a file (or, more precisely, a file descriptor). A file is just an ordered
sequence of bytes. Because that is such a simple interface, many different things can be represented
using the same interface: an actual file on the filesystem, a communication channel to another
process (Unix socket, <code>stdin</code>, <code>stdout</code>), a device driver (say <code>/dev/audio</code> or <code>/dev/lp0</code>), a socket
representing a TCP connection, and so on. It’s easy to take this for granted, but it’s actually
quite remarkable that these very different things can share a uniform interface, so they can easily
be plugged together.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417549121440-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549121440">ii</a></sup></p>

<p><a data-type="indexterm" data-primary="ASCII text" id="idm140417549119536"></a>
By convention, many (but not all) Unix programs treat this sequence of bytes as ASCII text. Our log
analysis example used this fact: <code>awk</code>, <code>sort</code>, <code>uniq</code>, and <code>head</code> all treat their input file
as a list of records separated by the <code>\n</code> (newline, ASCII <code>0x0A</code>) character. The choice of <code>\n</code> is
arbitrary—arguably, the ASCII record separator <code>0x1E</code> would have been a better choice, since it’s
intended for this purpose [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Duncan2009ts-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Duncan2009ts">14</a>]—but in any case, the fact that
all these programs have standardized on using the same record separator allows them to interoperate.</p>

<p><a data-type="indexterm" data-primary="CSV (comma-separated values)" id="idm140417549112384"></a>
<a data-type="indexterm" data-primary="xargs (Unix tool)" id="idm140417549111536"></a>
The parsing of each record (i.e., a line of input) is more vague. Unix tools commonly split a line
into fields by whitespace or tab characters, but CSV (comma-separated), pipe-separated, and other
encodings are also used. Even a fairly simple tool like <code>xargs</code> has half a dozen command-line
options for specifying how its input should be parsed.</p>

<p>The uniform interface of ASCII text mostly works, but it’s not exactly beautiful: our log analysis
example used <code>{print $7}</code> to extract the URL, which is not very readable. In an ideal world this
could have perhaps been <code>{print $request_url}</code> or something of that sort. We will return to this
idea later.</p>

<p>Although it’s not perfect, even decades later, the uniform interface of Unix is still something
remarkable. Not many pieces of software interoperate and compose as well as Unix tools do: you can’t
easily pipe the contents of your email account and your online shopping history through a custom
analysis tool into a spreadsheet and post the results to a social network or a wiki. Today it’s an
exception, not the norm, to have programs that work together as smoothly as Unix tools do.</p>

<p>Even databases with the <em>same data model</em> often don’t make it easy to get data out of one and into
the other. This lack of integration leads to Balkanization of data.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Separation of logic and wiring"><div class="sect3" id="sec_batch_logic_wiring">
<h3>Separation of logic and wiring</h3>

<p><a data-type="indexterm" data-primary="stdin, stdout" id="idm140417549104736"></a>
Another characteristic feature of Unix tools is their use of standard input (<code>stdin</code>) and standard
output (<code>stdout</code>). If you run a program and don’t specify anything else, <code>stdin</code> comes from the
keyboard and <code>stdout</code> goes to the screen. However, you can also take input from a file and/or
redirect output to a file. Pipes let you attach the <code>stdout</code> of one process to the <code>stdin</code> of
another process (with a small in-memory buffer, and without writing the entire intermediate data
stream to disk).</p>

<p><a data-type="indexterm" data-primary="Unix philosophy" data-secondary="loose coupling" id="idm140417549100672"></a>
<a data-type="indexterm" data-primary="loose coupling" id="idm140417549099200"></a>
<a data-type="indexterm" data-primary="late binding" id="idm140417549098368"></a>
<a data-type="indexterm" data-primary="inversion of control" id="idm140417549097536"></a>
A program can still read and write files directly if it needs to, but the Unix approach works best
if a program doesn’t worry about particular file paths and simply uses <code>stdin</code> and <code>stdout</code>. This
allows a shell user to wire up the input and output in whatever way they want; the program doesn’t
know or care where the input is coming from and where the output is going to. (One could say this is
a form of <em>loose coupling</em>, <em>late binding</em> [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="KayOxymoron-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#KayOxymoron">15</a>], or <em>inversion of control</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Fowler2005tp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Fowler2005tp">16</a>].) Separating the input/output wiring from the
program logic makes it easier to compose small tools into bigger systems.</p>

<p>You can even write your own programs and combine them with the tools provided by the operating
system. Your program just needs to read input from <code>stdin</code> and write output to <code>stdout</code>, and it can
participate in data processing pipelines. In the log analysis example, you could write a tool that
translates user-agent strings into more sensible browser identifiers, or a tool that translates IP
addresses into country codes, and simply plug it into the pipeline. The <code>sort</code> program doesn’t care
whether it’s communicating with another part of the operating system or with a program written by
you.</p>

<p>However, there are limits to what you can do with <code>stdin</code> and <code>stdout</code>. Programs that need multiple
inputs or outputs are possible but tricky. You can’t pipe a program’s output into a network
connection [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DJBTwoFD-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DJBTwoFD">17</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Pike1999ui-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Pike1999ui">18</a>].<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417549081328-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549081328">iii</a></sup>
<a data-type="indexterm" data-primary="netcat (Unix tool)" id="idm140417549076192"></a>
<a data-type="indexterm" data-primary="curl (Unix tool)" id="idm140417549075360"></a>
If a program directly opens files for reading and writing, or starts another program as a
subprocess, or opens a network connection, then that I/O is wired up by the program itself. It can
still be configurable (through command-line options, for example), but the flexibility of wiring up
inputs and outputs in a shell is reduced.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Transparency and experimentation"><div class="sect3" id="idm140417549074176">
<h3>Transparency and experimentation</h3>

<p>Part of what makes Unix tools so successful is that they make it quite easy to see what is going on:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="immutability" data-secondary="inputs to Unix commands" id="idm140417549071632"></a>
The input files to Unix commands are normally treated as immutable. This means you can run the
commands as often as you want, trying various command-line options, without damaging the input
files.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="less (Unix tool)" id="idm140417549069616"></a>
You can end the pipeline at any point, pipe the output into <code>less</code>, and look at it to see if it
has the expected form. This ability to inspect is great for debugging.</p>
</li>
<li>
<p>You can write the output of one pipeline stage to a file and use that file as input to the next
stage. This allows you to restart the later stage without rerunning the entire pipeline.</p>
</li>
</ul>

<p>Thus, even though Unix tools are quite blunt, simple tools compared to a query optimizer of a
relational database, they remain amazingly useful, especially for experimentation.</p>

<p>However, the biggest limitation of Unix tools is that they run only on a single machine—and
that’s where tools like Hadoop come in.
<a data-type="indexterm" data-primary="Unix philosophy" data-startref="ix_Unixtoolsphil" id="idm140417549065600"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="MapReduce and Distributed Filesystems"><div class="sect1" id="sec_batch_mapreduce">
<h1>MapReduce and Distributed Filesystems</h1>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="MapReduce and distributed filesystems" data-seealso="MapReduce" id="ix_batchMRdist"></a>
MapReduce is a bit like Unix tools, but distributed across potentially thousands of machines. Like
Unix tools, it is a fairly blunt, brute-force, but surprisingly effective tool. A single MapReduce
job is comparable to a single Unix process: it takes one or more inputs and produces one or more
outputs.</p>

<p>As with most Unix tools, running a MapReduce job normally does not modify the input and does not
have any side effects other than producing the output. The output files are written once, in a
sequential fashion (not modifying any existing part of a file once it has been written).</p>

<p><a data-type="indexterm" data-primary="distributed filesystems" id="ix_distfs"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="HDFS distributed filesystem" data-see="HDFS" id="idm140417549058704"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-seealso="distributed filesystems" id="ix_hdfs"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="GFS (distributed file system)" id="idm140417549055952"></a>
While Unix tools use <code>stdin</code> and <code>stdout</code> as input and output, MapReduce jobs read and write files
on a distributed filesystem. In Hadoop’s implementation of MapReduce, that filesystem is called HDFS
(Hadoop Distributed File System), an open source reimplementation of the Google File System (GFS)
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ghemawat2003dy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ghemawat2003dy">19</a>].</p>

<p><a data-type="indexterm" data-primary="GlusterFS (distributed filesystem)" id="idm140417549050720"></a>
<a data-type="indexterm" data-primary="Quantcast File System (distributed filesystem)" id="idm140417549049728"></a>
<a data-type="indexterm" data-primary="Amazon Web Services (AWS)" data-secondary="S3 (object storage)" id="idm140417549048768"></a>
<a data-type="indexterm" data-primary="Microsoft" data-secondary="Azure Storage" id="idm140417549047648"></a>
<a data-type="indexterm" data-primary="OpenStack" data-secondary="Swift (object storage)" id="idm140417549046544"></a>
Various other distributed filesystems besides HDFS exist, such as GlusterFS and the Quantcast File System (QFS)
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ovsiannikov2013da-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ovsiannikov2013da">20</a>].
Object storage services such as Amazon S3, Azure Blob Storage, and OpenStack Swift
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="OpenStackSwift-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#OpenStackSwift">21</a>]
are similar in many ways.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417549040384-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549040384">iv</a></sup>
In this chapter we will mostly use HDFS as a running example, but the principles apply to any
distributed filesystem.</p>

<p><a data-type="indexterm" data-primary="shared-nothing architecture" data-secondary="distributed filesystems" data-seealso="distributed filesystems" id="idm140417549037904"></a>
<a data-type="indexterm" data-primary="shared-disk architecture" id="idm140417549036336"></a>
<a data-type="indexterm" data-primary="Network Attached Storage (NAS)" id="idm140417549035536"></a>
<a data-type="indexterm" data-primary="Storage Area Network (SAN)" id="idm140417549034688"></a>
<a data-type="indexterm" data-primary="Fibre Channel (networks)" id="idm140417549033840"></a>
HDFS is based on the <em>shared-nothing</em> principle (see the introduction to <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/part02.html#part_distributed_data">Part&nbsp;II</a>), in
contrast to the shared-disk approach of <em>Network Attached Storage</em> (NAS) and <em>Storage Area Network</em>
(SAN) architectures. Shared-disk storage is implemented by a centralized storage appliance, often
using custom hardware and special network infrastructure such as Fibre Channel. On the other hand,
the shared-nothing approach requires no special hardware, only computers connected by a conventional
datacenter network.</p>

<p><a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="NameNode" id="idm140417549030368"></a>
HDFS consists of a daemon process running on each machine, exposing a network service that allows
other nodes to access files stored on that machine (assuming that every general-purpose machine in a
datacenter has some disks attached to it). A central server called the <em>NameNode</em> keeps track of which
file blocks are stored on which machine. Thus, HDFS conceptually creates one big filesystem that can
use the space on the disks of all machines running the daemon.</p>

<p><a data-type="indexterm" data-primary="replication" data-secondary="in distributed filesystems" id="idm140417549027952"></a>
<a data-type="indexterm" data-primary="replication" data-secondary="using erasure coding" id="idm140417549026656"></a>
<a data-type="indexterm" data-primary="erasure coding (error correction)" id="idm140417549025552"></a>
<a data-type="indexterm" data-primary="error-correcting codes" id="idm140417549024704"></a>
<a data-type="indexterm" data-primary="Reed–Solomon codes (error correction)" id="idm140417549023872"></a>
<a data-type="indexterm" data-primary="RAID (Redundant Array of Independent Disks)" id="idm140417549023024"></a>
In order to tolerate machine and disk failures, file blocks are replicated on multiple machines.
Replication may mean simply several copies of the same data on multiple machines, as in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch05.html#ch_replication">Chapter&nbsp;5</a>, or an <em>erasure coding</em> scheme such as Reed–Solomon codes, which allows lost data
to be recovered with lower storage overhead than full replication
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ovsiannikov2013da">20</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zhang2015vi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Zhang2015vi">22</a>].
The techniques are similar to RAID, which provides redundancy across several disks attached to the
same machine; the difference is that in a distributed filesystem, file access and replication are
done over a conventional datacenter network without special hardware.</p>

<p>HDFS has scaled well: at the time of writing, the biggest HDFS deployments run on tens of thousands
of machines, with combined storage capacity of hundreds of petabytes
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cnudde2016tm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cnudde2016tm">23</a>]. Such large scale has become viable because the
cost of data storage and access on HDFS, using commodity hardware and open source software, is much
lower than that of the equivalent capacity on a dedicated storage appliance
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Baldeschwieler2012ue-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Baldeschwieler2012ue">24</a>].
<a data-type="indexterm" data-primary="distributed filesystems" data-startref="ix_distfs" id="idm140417549012016"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-startref="ix_hdfs" id="idm140417549010880"></a></p>








<section data-type="sect2" data-pdf-bookmark="MapReduce Job Execution"><div class="sect2" id="idm140417549009760">
<h2>MapReduce Job Execution</h2>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" id="ix_madred"></a>
MapReduce is a programming framework with which you can write code to process large datasets in a
distributed filesystem like HDFS. The easiest way of understanding it is by referring back to the
web server log analysis example in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_log_analysis">“Simple Log Analysis”</a>. The pattern of data processing in
MapReduce is very similar to this example:</p>
<ol>
<li>
<p><a data-type="indexterm" data-primary="records" id="idm140417549005248"></a> Read a set of input files, and break it up into <em>records</em>. In the web server log
example, each record is one line in the log (that is, <code>\n</code> is the record separator).</p>
</li>
<li>
<p>Call the mapper function to extract a key and value from each input record. In the preceding
example, the mapper function is <code>awk '{print $7}'</code>: it extracts the URL (<code>$7</code>) as the key, and
leaves the value empty.</p>
</li>
<li>
<p>Sort all of the key-value pairs by key. In the log example, this is done by the first <code>sort</code>
command.</p>
</li>
<li>
<p>Call the reducer function to iterate over the sorted key-value pairs. If there are multiple
occurrences of the same key, the sorting has made them adjacent in the list, so it is easy to
combine those values without having to keep a lot of state in memory. In the preceding example,
the reducer is implemented by the command <code>uniq -c</code>, which counts the number of adjacent records
with the same key.</p>
</li>

</ol>

<p>Those four steps can be performed by one MapReduce job. Steps 2 (map) and 4 (reduce) are where you
write your custom data processing code. Step 1 (breaking files into records) is handled by the input
format parser. Step 3, the <code>sort</code> step, is implicit in MapReduce—you don’t have to write it, because the
output from the mapper is always sorted before it is given to the reducer.</p>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="mapper and reducer functions" id="idm140417548996544"></a>
To create a MapReduce job, you need to implement two callback functions, the mapper and reducer, which
behave as follows (see also <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_mapreduce">“MapReduce Querying”</a>):</p>
<dl>
<dt>Mapper</dt>
<dd>
<p>The mapper is called once for every input record, and its job is to extract the key and
value from the input record. For each input, it may generate any number of key-value pairs
(including none). It does not keep any state from one input record to the next, so each record is
handled independently.</p>
</dd>
<dt>Reducer</dt>
<dd>
<p>The MapReduce framework takes the key-value pairs produced by the mappers, collects all the values
belonging to the same key, and calls the reducer with an iterator over that collection of
values. The reducer can produce output records (such as the number of occurrences of the same
URL).</p>
</dd>
</dl>

<p>In the web server log example, we had a second <code>sort</code> command in step 5, which ranked URLs by number
of requests. In MapReduce, if you need a second sorting stage, you can implement it by writing a
second MapReduce job and using the output of the first job as input to the second job. Viewed like
this, the role of the mapper is to prepare the data by putting it into a form that is suitable for
sorting, and the role of the reducer is to process the data that has been sorted.
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-startref="ix_madred" id="idm140417548989360"></a></p>










<section data-type="sect3" data-pdf-bookmark="Distributed execution of MapReduce"><div class="sect3" id="sec_batch_mapreduce_dist">
<h3>Distributed execution of MapReduce</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" id="ix_mapredhadoop"></a>
The main difference from pipelines of Unix commands is that MapReduce can parallelize a computation
across many machines, without you having to write code to explicitly handle the parallelism. The
mapper and reducer only operate on one record at a time; they don’t need to know where their input
is coming from or their output is going to, so the framework can handle the complexities of moving
data between machines.</p>

<p><a data-type="indexterm" data-primary="CouchDB (database)" data-secondary="MapReduce support" id="idm140417548984320"></a>
<a data-type="indexterm" data-primary="MongoDB (database)" data-secondary="MapReduce support" id="idm140417548983216"></a>
It is possible to use standard Unix tools as mappers and reducers in a distributed computation
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gregg2013wz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gregg2013wz">25</a>], but more commonly they are implemented as
functions in a conventional programming language. In Hadoop MapReduce, the mapper and reducer are
each a Java class that implements a particular interface. In MongoDB and CouchDB, mappers and
reducers are JavaScript functions (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_mapreduce">“MapReduce Querying”</a>).</p>

<p><a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_mapreduce">Figure&nbsp;10-1</a> shows the dataflow in a Hadoop MapReduce job. Its parallelization is based
on partitioning (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#ch_partitioning">Chapter&nbsp;6</a>): the input to a job is typically a directory in HDFS, and
each file or file block within the input directory is considered to be a separate partition that can
be processed by a separate map task (marked by <em>m&nbsp;1</em>, <em>m&nbsp;2</em>, and <em>m&nbsp;3</em> in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_mapreduce">Figure&nbsp;10-1</a>).</p>

<p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm140417548973952"></a>
<a data-type="indexterm" data-primary="putting computation near data" id="idm140417548972816"></a>
Each input file is typically hundreds of megabytes in size. The MapReduce scheduler (not shown in
the diagram) tries to run each mapper on one of the machines that stores a replica of the input
file, provided that machine has enough spare RAM and CPU resources to run the map task
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="White2015vl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2015vl">26</a>].
This principle is known as <em>putting the computation near the data</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gray2003vx-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gray2003vx">27</a>]: it saves copying the input file over the
network, reducing network load and increasing locality.</p>

<figure><div id="fig_batch_mapreduce" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1001.png" alt="ddia 1001" width="2880" height="1920" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1001.png">
<h6><span class="label">Figure 10-1. </span>A MapReduce job with three mappers and three reducers.</h6>
</div></figure>

<p>In most cases, the application code that should run in the map task is not yet present on the
machine that is assigned the task of running it, so the MapReduce framework first copies the code
(e.g., JAR files in the case of a Java program) to the appropriate machines. It then starts the map
task and begins reading the input file, passing one record at a time to the mapper callback. The
output of the mapper consists of key-value pairs.</p>

<p>The reduce side of the computation is also partitioned. While the number of map tasks is determined
by the number of input file blocks, the number of reduce tasks is configured by the job author (it
can be different from the number of map tasks). To ensure that all key-value pairs with the same key
end up at the same reducer, the framework uses a hash of the key to determine which reduce task
should receive a particular key-value pair (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_hash">“Partitioning by Hash of Key”</a>).</p>

<p>The key-value pairs must be sorted, but the dataset is likely too large to be sorted with a
conventional sorting algorithm on a single machine. Instead, the sorting is performed in stages.
First, each map task partitions its output by reducer, based on the hash of the key. Each of
these partitions is written to a sorted file on the mapper’s local disk, using a technique similar
to what we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>.</p>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" data-tertiary="the shuffle" id="idm140417548961424"></a>
Whenever a mapper finishes reading its input file and writing its sorted output files, the MapReduce
scheduler notifies the reducers that they can start fetching the output files from that mapper. The
reducers connect to each of the mappers and download the files of sorted key-value pairs for their
partition. The process of partitioning by reducer, sorting, and copying data partitions from mappers
to reducers is known as the <em>shuffle</em> [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2015vl">26</a>] (a
confusing term—unlike shuffling a deck of cards, there is no randomness in MapReduce).</p>

<p><a data-type="indexterm" data-primary="algorithms" data-secondary="mergesort" id="idm140417548957904"></a><a data-type="indexterm" data-primary="merging sorted files" id="idm140417548956928"></a>
The reduce task takes the files from the mappers and merges them together, preserving the sort
order. Thus, if different mappers produced records with the same key, they will be adjacent in the
merged reducer input.</p>

<p>The reducer is called with a key and an iterator that incrementally scans over all records with the
same key (which may in some cases not all fit in memory). The reducer can use arbitrary logic to
process these records, and can generate any number of output records. These output records are
written to a file on the distributed filesystem (usually, one copy on the local disk of the machine
running the reducer, with replicas on other machines).</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="MapReduce workflows"><div class="sect3" id="sec_batch_workflows">
<h3>MapReduce workflows</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="workflows" id="idm140417548953456"></a>
<a data-type="indexterm" data-primary="workflows (MapReduce)" id="idm140417548952336"></a>
The range of problems you can solve with a single MapReduce job is limited. Referring back to the
log analysis example, a single MapReduce job could determine the number of page views per URL, but
not the most popular URLs, since that requires a second round of sorting.</p>

<p><a data-type="indexterm" data-primary="distributed filesystems" data-secondary="use by MapReduce" id="idm140417548950960"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="use by MapReduce" id="idm140417548949856"></a>
Thus, it is very common for MapReduce jobs to be chained together into <em>workflows</em>, such that the
output of one job becomes the input to the next job. The Hadoop MapReduce framework does not have
any particular support for workflows, so this chaining is done implicitly by directory name: the
first job must be configured to write its output to a designated directory in HDFS, and the second
job must be configured to read that same directory name as its input. From the MapReduce framework’s
point of view, they are two independent jobs.</p>

<p>Chained MapReduce jobs are therefore less like pipelines of Unix commands (which pass the output of
one process as input to another process directly, using only a small in-memory buffer) and more
like a sequence of commands where each command’s output is written to a temporary file, and the next
command reads from the temporary file. This design has advantages and disadvantages, which we will
discuss in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_materialize">“Materialization of Intermediate State”</a>.</p>

<p><a data-type="indexterm" data-primary="Oozie (workflow scheduler)" id="idm140417548945904"></a><a data-type="indexterm" data-primary="Apache Oozie" data-see="Oozie" id="idm140417548945008"></a>
<a data-type="indexterm" data-primary="LinkedIn" data-secondary="Azkaban (workflow scheduler)" id="idm140417548943936"></a>
<a data-type="indexterm" data-primary="Luigi (workflow scheduler)" id="idm140417548942816"></a>
<a data-type="indexterm" data-primary="Airflow (workflow scheduler)" id="idm140417548941968"></a>
<a data-type="indexterm" data-primary="Pinball (workflow scheduler)" id="idm140417548941120"></a>
A batch job’s output is only considered valid when the job has completed successfully (MapReduce
discards the partial output of a failed job). Therefore, one job in a workflow can only start when
the prior jobs—that is, the jobs that produce its input directories—have completed
successfully. To handle these dependencies between job executions, various workflow schedulers for
Hadoop have been developed, including Oozie, Azkaban, Luigi, Airflow, and Pinball
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Trencseni2016vn-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Trencseni2016vn">28</a>].</p>

<p><a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch workflows" id="idm140417548937472"></a>
These schedulers also have management features that are useful when maintaining a large collection
of batch jobs. Workflows consisting of 50 to 100 MapReduce jobs are common when building
recommendation systems
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sumbaly2013eh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2013eh">29</a>],
and in a large organization, many different teams may be running different jobs that read each
other’s output. Tool support is important for managing such complex dataflows.</p>

<p><a data-type="indexterm" data-primary="Application Programming Interfaces (APIs)" data-secondary="for batch processing" id="idm140417548932768"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" id="idm140417548931296"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="higher-level tools" id="idm140417548930176"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="higher-level tools" id="idm140417548929056"></a>
<a data-type="indexterm" data-primary="Cascading (batch processing)" data-secondary="workflows" id="idm140417548927936"></a>
<a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="workflows" id="idm140417548926816"></a>
<a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="workflows" id="idm140417548925696"></a>
<a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="workflows" id="idm140417548924592"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="FlumeJava (dataflow library)" id="idm140417548923488"></a>
<a data-type="indexterm" data-primary="FlumeJava (dataflow library)" id="idm140417548922368"></a>
Various higher-level tools for Hadoop, such as Pig
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gates2009vg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gates2009vg">30</a>], Hive
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Thusoo2010hp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Thusoo2010hp">31</a>],
Cascading
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="CascadingDocs-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#CascadingDocs">32</a>], Crunch
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ApacheCrunch-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ApacheCrunch">33</a>], and FlumeJava
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Chambers2010dp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Chambers2010dp">34</a>],
also set up workflows of multiple MapReduce stages that are automatically wired together
appropriately.
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="implementation in Hadoop" data-startref="ix_mapredhadoop" id="idm140417548908848"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Reduce-Side Joins and Grouping"><div class="sect2" id="sec_batch_reduce_joins">
<h2>Reduce-Side Joins and Grouping</h2>

<p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="join algorithms" data-seealso="MapReduce" id="ix_hadoopjoins"></a>
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" id="ix_joinsMRreduce"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" id="ix_madredredjoins"></a>
We discussed joins in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#ch_datamodels">Chapter&nbsp;2</a> in the context of data models and query languages, but we
have not delved into how joins are actually implemented. It is time that we pick up that thread
again.</p>

<p><a data-type="indexterm" data-primary="foreign keys" id="idm140417548900496"></a>
<a data-type="indexterm" data-primary="document data model" data-secondary="document references" id="idm140417548899440"></a>
<a data-type="indexterm" data-primary="edges (in graphs)" id="idm140417548898336"></a>
<a data-type="indexterm" data-primary="normalization (data representation)" data-secondary="executing joins" id="idm140417548897504"></a>
In many datasets it is common for one record to have an association with another record: a <em>foreign
key</em> in a relational model, a <em>document reference</em> in a document model, or an <em>edge</em> in a graph
model. A join is necessary whenever you have some code that needs to access records on both sides of
that association (both the record that holds the reference and the record being referenced). As
discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#ch_datamodels">Chapter&nbsp;2</a>, denormalization can reduce the need for joins but generally not
remove it entirely.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417548893984-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417548893984" class="totri-footnote">v</a></sup><a data-type="indexterm" data-primary="equi-joins" id="idm140417548891760"></a></p>

<p><a data-type="indexterm" data-primary="joins" data-secondary="by index lookup" id="idm140417548890960"></a>
In a database, if you execute a query that involves only a small number of records, the database
will typically use an <em>index</em> to quickly locate the records of interest (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#ch_storage">Chapter&nbsp;3</a>). If
the query involves joins, it may require multiple index lookups. However, MapReduce has no concept
of indexes—at least not in the usual sense.</p>

<p><a data-type="indexterm" data-primary="full table scans" id="idm140417548888016"></a>
When a MapReduce job is given a set of files as input, it reads the entire content of all of those
files; a database would call this operation a <em>full table scan</em>. If you only want to read a small
number of records, a full table scan is outrageously expensive compared to an index lookup.
However, in analytic queries (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>) it is common to want to calculate
aggregates over a large number of records. In this case, scanning the entire input might be quite a
reasonable thing to do, especially if you can parallelize the processing across multiple machines.</p>

<p>When we talk about joins in the context of batch processing, we mean resolving all occurrences of
some association within a dataset. For example, we assume that a job is processing the data for all
users simultaneously, not merely looking up the data for one particular user (which would be done
far more efficiently with an index).</p>










<section data-type="sect3" data-pdf-bookmark="Example: analysis of user activity events"><div class="sect3" id="sec_batch_join_example">
<h3>Example: analysis of user activity events</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="analysis of user activity events (example)" id="idm140417548882832"></a>
<a data-type="indexterm" data-primary="analytics" data-secondary="using MapReduce, analysis of user activity events (example)" id="idm140417548881168"></a>
<a data-type="indexterm" data-primary="clickstream data, analysis of" id="idm140417548880016"></a>
A typical example of a join in a batch job is illustrated in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_example">Figure&nbsp;10-2</a>. On the left
is a log of events describing the things that logged-in users did on a website (known as <em>activity
events</em> or <em>clickstream data</em>), and on the right is a database of users. You can think of this
example as being part of a star schema (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_analytics_schemas">“Stars and Snowflakes: Schemas for Analytics”</a>): the log of events is
the fact table, and the user database is one of the dimensions.</p>

<figure><div id="fig_batch_join_example" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1002.png" alt="ddia 1002" width="2880" height="1238" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1002.png">
<h6><span class="label">Figure 10-2. </span>A join between a log of user activity events and a database of user profiles.</h6>
</div></figure>

<p>An analytics task may need to correlate user activity with user profile information: for example, if
the profile contains the user’s age or date of birth, the system could determine which pages are
most popular with which age groups. However, the activity events contain only the user ID, not the
full user profile information. Embedding that profile information in every single activity event
would most likely be too wasteful. Therefore, the activity events need to be joined with the user
profile database.</p>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="accessing external services within job" id="idm140417548873264"></a>
The simplest implementation of this join would go over the activity events one by one and query the
user database (on a remote server) for every user ID it encounters. This is possible, but it would
most likely suffer from very poor performance: the processing throughput would be limited by the
round-trip time to the database server, the effectiveness of a local cache would depend very much on
the distribution of data, and running a large number of queries in parallel could easily overwhelm
the database [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2014wm_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kreps2014wm_ch10">35</a>].</p>

<p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm140417548868752"></a>
In order to achieve good throughput in a batch process, the computation must be (as much as
possible) local to one machine. Making random-access requests over the network for every record you
want to process is too slow. Moreover, querying a remote database would mean that the batch job
becomes nondeterministic, because the data in the remote database might change.</p>

<p><a data-type="indexterm" data-primary="backups" data-secondary="use for ETL processes" id="idm140417548867120"></a>
<a data-type="indexterm" data-primary="ETL (extract-transform-load)" id="idm140417548866016"></a>
Thus, a better approach would be to take a copy of the user database (for example, extracted from a
database backup using an ETL process—see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_dwh">“Data Warehousing”</a>) and to put it in the same distributed
filesystem as the log of user activity events. You would then have the user database in one set of
files in HDFS and the user activity records in another set of files, and could use MapReduce to
bring together all of the relevant records in the same place and process them efficiently.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Sort-merge joins"><div class="sect3" id="idm140417548863744">
<h3>Sort-merge joins</h3>

<p><a data-type="indexterm" data-primary="algorithms" data-secondary="mergesort" id="idm140417548862432"></a><a data-type="indexterm" data-primary="merging sorted files" id="idm140417548861232"></a>
<a data-type="indexterm" data-primary="sort-merge joins (MapReduce)" id="idm140417548860432"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="sort-merge joins" id="idm140417548859632"></a>
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-tertiary="sort-merge joins" id="idm140417548858240"></a>
Recall that the purpose of the mapper is to extract a key and value from each input record. In the
case of <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_example">Figure&nbsp;10-2</a>, this key would be the user ID: one set of mappers would go over
the activity events (extracting the user ID as the key and the activity event as the value), while
another set of mappers would go over the user database (extracting the user ID as the key and the
user’s date of birth as the value). This process is illustrated in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_reduce">Figure&nbsp;10-3</a>.</p>

<figure><div id="fig_batch_join_reduce" class="figure">
<img src="https://www.safaribooksonline.com/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1003.png" alt="ddia 1003" width="2880" height="1308" data-mfp-src="/library/view/designing-data-intensive-applications/9781491903063/assets/ddia_1003.png">
<h6><span class="label">Figure 10-3. </span>A reduce-side sort-merge join on user ID. If the input datasets are partitioned into multiple files, each could be processed with multiple mappers in parallel.</h6>
</div></figure>

<p><a data-type="indexterm" data-primary="secondary sorts" id="idm140417548852464"></a>
When the MapReduce framework partitions the mapper output by key and then sorts the key-value
pairs, the effect is that all the activity events and the user record with the same user ID become
adjacent to each other in the reducer input. The MapReduce job can even arrange the records to be
sorted such that the reducer always sees the record from the user database first, followed by the
activity events in timestamp order—this technique is known as a <em>secondary sort</em>
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2015vl">26</a>].</p>

<p>The reducer can then perform the actual join logic easily: the reducer function is called once for
every user ID, and thanks to the secondary sort, the first value is expected to be the date-of-birth
record from the user database. The reducer stores the date of birth in a local variable and then
iterates over the activity events with the same user ID, outputting pairs of <em>viewed-url</em> and
<em>viewer-age-in-years</em>. Subsequent MapReduce jobs could then calculate the distribution of viewer ages
for each URL, and cluster by age group.</p>

<p>Since the reducer processes all of the records for a particular user ID in one go, it only needs to
keep one user record in memory at any one time, and it never needs to make any requests over the
network. This algorithm is known as a <em>sort-merge join</em>, since mapper output is sorted by key, and
the reducers then merge together the sorted lists of records from both sides of the join.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Bringing related data together in the same place"><div class="sect3" id="idm140417548846736">
<h3>Bringing related data together in the same place</h3>

<p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm140417548845568"></a>
In a sort-merge join, the mappers and the sorting process make sure that all the necessary data to
perform the join operation for a particular user ID is brought together in the same place: a single
call to the reducer. Having lined up all the required data in advance, the reducer can be a fairly
simple, single-threaded piece of code that can churn through records with high throughput and low
memory overhead.</p>

<p>One way of looking at this architecture is that mappers “send messages” to the reducers. When a
mapper emits a key-value pair, the key acts like the destination address to which the value should be
delivered. Even though the key is just an arbitrary string (not an actual network address like an
IP address and port number), it behaves like an address: all key-value pairs with the same key will be
delivered to the same destination (a call to the reducer).</p>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm140417548842912"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm140417548841808"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm140417548840736"></a>
Using the MapReduce programming model has separated the physical network communication aspects of
the computation (getting the data to the right machine) from the application logic (processing the
data once you have it). This separation contrasts with the typical use of databases, where a request to
fetch data from a database often occurs somewhere deep inside a piece of application code
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kleppmann2012ts-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kleppmann2012ts">36</a>].
Since MapReduce handles all network communication, it also shields the application code from having
to worry about partial failures, such as the crash of another node: MapReduce transparently retries
failed tasks without affecting the application logic.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="GROUP BY"><div class="sect3" id="sec_batch_grouping">
<h3>GROUP BY</h3>

<p><a data-type="indexterm" data-primary="aggregation" data-secondary="in batch processes" id="idm140417548834816"></a>
<a data-type="indexterm" data-primary="grouping records in MapReduce" id="idm140417548833712"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="grouping records by same key" id="idm140417548832816"></a>
<a data-type="indexterm" data-primary="GROUP BY clause (SQL)" id="idm140417548831408"></a>
Besides joins, another common use of the “bringing related data to the same place” pattern is
grouping records by some key (as in the <code>GROUP BY</code> clause in SQL). All records with the same key
form a group, and the next step is often to perform some kind of aggregation within each group—for
example:</p>

<ul>
<li>
<p>Counting the number of records in each group (like in our example of counting page views,
which you would express as a <code>COUNT(*)</code> aggregation in SQL)</p>
</li>
<li>
<p>Adding up the values in one particular field (<code>SUM(fieldname)</code>) in SQL</p>
</li>
<li>
<p>Picking the top <em>k</em> records according to some ranking function</p>
</li>
</ul>

<p>The simplest way of implementing such a grouping operation with MapReduce is to set up the mappers
so that the key-value pairs they produce use the desired grouping key. The partitioning and sorting
process then brings together all the records with the same key in the same reducer. Thus, grouping
and joining look quite similar when implemented on top of MapReduce.</p>

<p><a data-type="indexterm" data-primary="sessionization" id="idm140417548824128"></a>
Another common use for grouping is collating all the activity events for a particular user session,
in order to find out the sequence of actions that the user took—a process called <em>sessionization</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Grover2015tl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>].
For example, such analysis could be used to work out whether users who were shown a new version of
your website are more likely to make a purchase than those who were shown the old version (A/B
testing), or to calculate whether some marketing activity is worthwhile.</p>

<p>If you have multiple web servers handling user requests, the activity events for a particular user
are most likely scattered across various different servers’ log files. You can implement
sessionization by using a session cookie, user ID, or similar identifier as the grouping key and
bringing all the activity events for a particular user together in one place, while distributing
different users’ events across different partitions.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Handling skew"><div class="sect3" id="sec_batch_skew">
<h3>Handling skew</h3>

<p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-tertiary="handling skew" id="idm140417548817888"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-tertiary="handling skew" id="idm140417548816272"></a>
<a data-type="indexterm" data-primary="skew" data-secondary="unbalanced workload" data-tertiary="in batch processing" id="idm140417548814880"></a>
<a data-type="indexterm" data-primary="grouping records in MapReduce" data-secondary="handling skew" id="idm140417548813504"></a>
<a data-type="indexterm" data-primary="hot spots" data-secondary="in batch processing" id="idm140417548812384"></a>
The pattern of “bringing all records with the same key to the same place” breaks down if there is a
very large amount of data related to a single key. For example, in a social network, most users
might be connected to a few hundred people, but a small number of celebrities may have many
millions of followers. Such disproportionately active database records are known as <em>linchpin objects</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ajoux2015wh_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ajoux2015wh_ch10">38</a>] or <em>hot keys</em>.</p>

<p>Collecting all activity related to a celebrity (e.g., replies to something they posted) in a single
reducer can lead to significant <em>skew</em> (also known as <em>hot spots</em>)—that is, one
reducer that must process significantly more records than the others (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_skew">“Skewed Workloads and Relieving Hot Spots”</a>). Since a MapReduce job is only complete when all of its mappers and
reducers have completed, any subsequent jobs must wait for the slowest reducer to complete before
they can start.</p>

<p><a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="skewed joins" id="idm140417548805088"></a>
If a join input has hot keys, there are a few algorithms you can use to compensate. For example, the
<em>skewed join</em> method in Pig first runs a sampling job to determine which keys are hot
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Manjunath2009bh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Manjunath2009bh">39</a>]. When performing the actual join, the mappers send any records relating to a hot key
to one of several reducers, chosen at random (in contrast to conventional MapReduce, which chooses a
reducer deterministically based on a hash of the key). For the other input to the join, records
relating to the hot key need to be replicated to <em>all</em> reducers handling that key
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt1992ws-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt1992ws">40</a>].</p>

<p><a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="sharded joins" id="idm140417548797856"></a>
This technique spreads the work of handling the hot key over several reducers, which allows it to be
parallelized better, at the cost of having to replicate the other join input to multiple reducers.
The <em>sharded join</em> method in Crunch is similar, but requires the hot keys to be specified explicitly
rather than using a sampling job. This technique is also very similar to one we discussed in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_skew">“Skewed Workloads and Relieving Hot Spots”</a>, using randomization to alleviate hot spots in a partitioned database.</p>

<p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="skewed joins" id="idm140417548794768"></a>
Hive’s skewed join optimization takes an alternative approach. It requires hot keys to be specified
explicitly in the table metadata, and it stores records related to those keys in separate files from
the rest. When performing a join on that table, it uses a map-side join (see the next section) for
the hot keys.</p>

<p>When grouping records by a hot key and aggregating them, you can perform the grouping in two
stages. The first MapReduce stage sends records to a random reducer, so that each reducer performs
the grouping on a subset of records for the hot key and outputs a more compact aggregated value
per key. The second MapReduce job then combines the values from all of the first-stage reducers into
a single value per key.
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce reduce-side joins" data-startref="ix_joinsMRreduce" id="idm140417548792512"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="reduce-side processing" data-startref="ix_madredredjoins" id="idm140417548791120"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Map-Side Joins"><div class="sect2" id="sec_batch_map_joins">
<h2>Map-Side Joins</h2>

<p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" id="ix_joinsMRmap"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" id="ix_mapredmapjoin"></a>
The join algorithms described in the last section perform the actual join logic in the reducers, and
are hence known as <em>reduce-side joins</em>. The mappers take the role of preparing the input data:
extracting the key and value from each input record, assigning the key-value pairs to a reducer
partition, and sorting by key.</p>

<p>The reduce-side approach has the advantage that you do not need to make any assumptions about the
input data: whatever its properties and structure, the mappers can prepare the data to be ready for
joining. However, the downside is that all that sorting, copying to reducers, and merging of reducer
inputs can be quite expensive. Depending on the available memory buffers, data may be written to
disk several times as it passes through the stages of MapReduce
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>].</p>

<p>On the other hand, if you <em>can</em> make certain assumptions about your input data, it is possible to
make joins faster by using a so-called <em>map-side join</em>. This approach uses a cut-down MapReduce job
in which there are no reducers and no sorting. Instead, each mapper simply reads one input file
block from the distributed filesystem and writes one output file to the filesystem—that is all.</p>










<section data-type="sect3" data-pdf-bookmark="Broadcast hash joins"><div class="sect3" id="idm140417548781312">
<h3>Broadcast hash joins</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="broadcast hash joins" id="idm140417548779936"></a>
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="broadcast hash joins" id="idm140417548778544"></a>
The simplest way of performing a map-side join applies in the case where a large dataset is joined
with a small dataset. In particular, the small dataset needs to be small enough that it can be
loaded entirely into memory in each of the mappers.</p>

<p><a data-type="indexterm" data-primary="hash indexes" data-secondary="broadcast hash joins" id="idm140417548776640"></a>
For example, imagine in the case of <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_example">Figure&nbsp;10-2</a> that the user database is small
enough to fit in memory. In this case, when a mapper starts up, it can first read the user
database from the distributed filesystem into an in-memory hash table. Once this is done, the mapper
can scan over the user activity events and simply look up the user ID for each event in the hash
table.<sup xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook"><a data-type="noteref" id="idm140417548774384-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417548774384">vi</a></sup></p>

<p>There can still be several map tasks: one for each file block of the large input to the join (in
the example of <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_example">Figure&nbsp;10-2</a>, the activity events are the large input). Each of these
mappers loads the small input entirely into memory.</p>

<p><a data-type="indexterm" data-primary="broadcast hash joins" id="idm140417548771536"></a>
<a data-type="indexterm" data-primary="Cascading (batch processing)" data-secondary="hash joins" id="idm140417548770480"></a>
<a data-type="indexterm" data-primary="Crunch (batch processing)" data-secondary="hash joins" id="idm140417548769360"></a>
<a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="map-side joins" id="idm140417548768320"></a>
<a data-type="indexterm" data-primary="Pig (dataflow language)" data-secondary="replicated joins" id="idm140417548767344"></a>
<a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="hash joins" id="idm140417548766304"></a>
This simple but effective algorithm is called a <em>broadcast hash join</em>: the word <em>broadcast</em> reflects
the fact that each mapper for a partition of the large input reads the entirety of the small input
(so the small input is effectively “broadcast” to all partitions of the large input), and the word
<em>hash</em> reflects its use of a hash table. This join method is supported by Pig (under the name
“replicated join”), Hive (“MapJoin”), Cascading, and Crunch. It is also used in data warehouse query
engines such as Impala
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kornacker2015uv_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kornacker2015uv_ch10">41</a>].</p>

<p>Instead of loading the small join input into an in-memory hash table, an alternative is to store the
small join input in a read-only index on the local disk
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Monsch2015vb-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Monsch2015vb">42</a>].
The frequently used parts of this index will remain in the operating system’s page cache, so this
approach can provide random-access lookups almost as fast as an in-memory hash table, but without
actually requiring the dataset to fit in memory.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Partitioned hash joins"><div class="sect3" id="idm140417548757696">
<h3>Partitioned hash joins</h3>

<p><a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="partitioned hash joins" id="idm140417548756384"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="partitioned hash joins" id="idm140417548754992"></a>
<a data-type="indexterm" data-primary="hash indexes" data-secondary="partitioned hash joins" id="idm140417548753600"></a>
If the inputs to the map-side join are partitioned in the same way, then the hash join approach can
be applied to each partition independently. In the case of <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#fig_batch_join_example">Figure&nbsp;10-2</a>, you might
arrange for the activity events and the user database to each be partitioned based on the last
decimal digit of the user ID (so there are 10 partitions on either side). For example, mapper 3
first loads all users with an ID ending in 3 into a hash table, and then scans over all the activity
events for each user whose ID ends in 3.</p>

<p>If the partitioning is done correctly, you can be sure that all the records you might want to join
are located in the same numbered partition, and so it is sufficient for each mapper to only read one
partition from each of the input datasets. This has the advantage that each mapper can load a
smaller amount of data into its hash table.</p>

<p>This approach only works if both of the join’s inputs have the same number of partitions, with
records assigned to partitions based on the same key and the same hash function. If the inputs are
generated by prior MapReduce jobs that already perform this grouping, then this can be a reasonable
assumption to make.</p>

<p>Partitioned hash joins are known as <em>bucketed map joins</em> in Hive
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Map-side merge joins"><div class="sect3" id="idm140417548747840">
<h3>Map-side merge joins</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-tertiary="merge joins" id="idm140417548746640"></a>
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-tertiary="merge joins" id="idm140417548745248"></a>
<a data-type="indexterm" data-primary="merge joins, MapReduce map-side" id="idm140417548743856"></a>
Another variant of a map-side join applies if the input datasets are not only partitioned in the
same way, but also <em>sorted</em> based on the same key. In this case, it does not matter whether the
inputs are small enough to fit in memory, because a mapper can perform the same merging operation
that would normally be done by a reducer: reading both input files incrementally, in order of
ascending key, and matching records with the same key.</p>

<p>If a map-side merge join is possible, it probably means that prior MapReduce jobs brought the input
datasets into this partitioned and sorted form in the first place. In principle, this join could
have been performed in the reduce stage of the prior job. However, it may still be appropriate to
perform the merge join in a separate map-only job, for example if the partitioned and sorted
datasets are also needed for other purposes besides this particular join.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="MapReduce workflows with map-side joins"><div class="sect3" id="idm140417548741008">
<h3>MapReduce workflows with map-side joins</h3>

<p><a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="with map-side joins" id="idm140417548739840"></a>
When the output of a MapReduce join is consumed by downstream jobs, the choice of map-side or
reduce-side join affects the structure of the output. The output of a reduce-side join is
partitioned and sorted by the join key, whereas the output of a map-side join is partitioned and
sorted in the same way as the large input (since one map task is started for each file block of the
join’s large input, regardless of whether a partitioned or broadcast join is used).</p>

<p>As discussed, map-side joins also make more assumptions about the size, sorting, and partitioning of
their input datasets. Knowing about the physical layout of datasets in the distributed
filesystem becomes important when optimizing join strategies: it is not sufficient to just know the
encoding format and the name of the directory in which the data is stored; you must also know the number of
partitions and the keys by which the data is partitioned and sorted.</p>

<p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="HCatalog and metastore" id="idm140417548737104"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="metadata about datasets" id="idm140417548735776"></a>
In the Hadoop ecosystem, this kind of metadata about the partitioning of datasets is often
maintained in HCatalog and the Hive metastore [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>].
<a data-type="indexterm" data-primary="joins" data-secondary="MapReduce map-side joins" data-startref="ix_joinsMRmap" id="idm140417548733696"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="map-side processing" data-startref="ix_mapredmapjoin" id="idm140417548732304"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="join algorithms" data-startref="ix_hadoopjoins" id="idm140417548730912"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="The Output of Batch Workflows"><div class="sect2" id="sec_batch_output">
<h2>The Output of Batch Workflows</h2>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" id="ix_batchoutputs"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" id="ix_madredout"></a>
<a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" id="ix_workout"></a>
We have talked a lot about the various algorithms for implementing workflows of MapReduce jobs, but
we neglected an important question: what is the result of all of that processing, once it is done?
Why are we running all these jobs in the first place?</p>

<p><a data-type="indexterm" data-primary="analytics" data-secondary="relation to batch processing" id="idm140417548723264"></a>
<a data-type="indexterm" data-primary="transactions" data-secondary="OLTP versus analytics queries" id="idm140417548722144"></a>
<a data-type="indexterm" data-primary="OLTP (online transaction processing)" data-secondary="analytics queries versus" id="idm140417548721024"></a>
In the case of database queries, we distinguished transaction processing (OLTP) purposes from
analytic purposes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_analytics">“Transaction Processing or Analytics?”</a>). We saw that OLTP queries generally look up a
small number of records by key, using indexes, in order to present them to a user (for example, on a
web page). On the other hand, analytic queries often scan over a large number of records, performing
groupings and aggregations, and the output often has the form of a report: a graph showing the
change in a metric over time, or the top 10 items according to some ranking, or a breakdown of some
quantity into subcategories. The consumer of such a report is often an analyst or a manager who
needs to make business decisions.</p>

<p>Where does batch processing fit in? It is not transaction processing, nor is it analytics. It is
closer to analytics, in that a batch process typically scans over large portions of an input
dataset. However, a workflow of MapReduce jobs is not the same as a SQL query used for analytic
purposes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>). The output of a batch process is often not a report, but some
<span class="keep-together">other kind</span> of structure.</p>










<section data-type="sect3" data-pdf-bookmark="Building search indexes"><div class="sect3" id="idm140417548715968">
<h3>Building search indexes</h3>

<p><a data-type="indexterm" data-primary="searches" data-secondary="building search indexes in batch processes" id="idm140417548714560"></a>
<a data-type="indexterm" data-primary="indexes" data-secondary="building in batch processes" id="idm140417548713024"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-tertiary="building search indexes" id="idm140417548711904"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-tertiary="search indexes" id="idm140417548710496"></a>
<a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-tertiary="search indexes" id="idm140417548709120"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-tertiary="building search indexes" id="idm140417548707744"></a>
<a data-type="indexterm" data-primary="Lucene (storage engine)" data-secondary="building indexes in batch processes" id="idm140417548706352"></a>
<a data-type="indexterm" data-primary="Solr (search server)" data-secondary="building indexes in batch processes" id="idm140417548705232"></a>
<a data-type="indexterm" data-primary="full-text search" data-secondary="building search indexes" id="idm140417548704112"></a>
Google’s original use of MapReduce was to build indexes for its search engine, which was
implemented as a workflow of 5 to 10 MapReduce jobs
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Dean2004ua_ch10" class="totri-footnote">1</a>].
Although Google later moved away from using MapReduce for this purpose
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Peng2010ub-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Peng2010ub">43</a>],
it helps to understand MapReduce if you look at it through the lens of building a search index.
(Even today, Hadoop MapReduce remains a good way of building indexes for Lucene/Solr
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ClouderaSearch-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ClouderaSearch">44</a>].)</p>

<p>We saw briefly in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_full_text">“Full-text search and fuzzy indexes”</a> how a full-text search index such as Lucene works: it is
a file (the term dictionary) in which you can efficiently look up a particular keyword and find the
list of all the document IDs containing that keyword (the postings list). This is a very simplified
view of a search index—in reality it requires various additional data, in order to rank search
results by relevance, correct misspellings, resolve synonyms, and so on—but the principle holds.</p>

<p><a data-type="indexterm" data-primary="document-partitioned indexes" id="idm140417548696064"></a>
If you need to perform a full-text search over a fixed set of documents, then a batch process is a
very effective way of building the indexes: the mappers partition the set of documents as needed,
each reducer builds the index for its partition, and the index files are written to the distributed
filesystem. Building such document-partitioned indexes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch06.html#sec_partitioning_secondary_indexes">“Partitioning and Secondary Indexes”</a>)
parallelizes very well. Since querying a search index by keyword is a read-only operation, these
index files are immutable once they have been created.</p>

<p>If the indexed set of documents changes, one option is to periodically rerun the entire indexing
workflow for the entire set of documents, and replace the previous index files wholesale with the
new index files when it is done. This approach can be computationally expensive if only a small number of
documents have changed, but it has the advantage that the indexing process is very easy to reason
about: documents in, indexes out.</p>

<p>Alternatively, it is possible to build indexes incrementally. As discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#ch_storage">Chapter&nbsp;3</a>, if you
want to add, remove, or update documents in an index, Lucene writes out new segment
files and asynchronously merges and compacts segment files in the background. We will see more
on such incremental processing in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a>.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Key-value stores as batch process output"><div class="sect3" id="idm140417548690608">
<h3>Key-value stores as batch process output</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-tertiary="key-value stores" id="idm140417548689264"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-tertiary="key-value stores" id="idm140417548687728"></a>
<a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-tertiary="key-value stores" id="idm140417548686352"></a>
<a data-type="indexterm" data-primary="key-value stores" data-secondary="as batch process output" id="idm140417548684976"></a>
<a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch process outputs" id="idm140417548683872"></a>
Search indexes are just one example of the possible outputs of a batch processing workflow. Another
common use for batch processing is to build machine learning systems such as classifiers (e.g., spam
filters, anomaly detection, image recognition) and recommendation systems (e.g., people you may know,
products you may be interested in, or related searches
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2013eh">29</a>]).</p>

<p><a data-type="indexterm" data-primary="databases" data-secondary="output from batch workflows" id="idm140417548681456"></a>
The output of those batch jobs is often some kind of database: for example, a database that can be
queried by user ID to obtain suggested friends for that user, or a database that can be queried by
product ID to get a list of related products
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Wu2014tm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Wu2014tm">45</a>].</p>

<p>These databases need to be queried from the web application that handles user requests, which is
usually separate from the Hadoop infrastructure. So how does the output from the batch process get
back into a database where the web application can query it?</p>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="accessing external services within job" id="idm140417548676640"></a>
The most obvious choice might be to use the client library for your favorite database directly
within a mapper or reducer, and to write from the batch job directly to the database server, one
record at a time. This will work (assuming your firewall rules allow direct access from your Hadoop
environment to your production databases), but it is a bad idea for several reasons:</p>

<ul>
<li>
<p>As discussed previously in the context of joins, making a network request for every single record
is orders of magnitude slower than the normal throughput of a batch task. Even if the client
library supports batching, performance is likely to be poor.</p>
</li>
<li>
<p>MapReduce jobs often run many tasks in parallel. If all the mappers or reducers concurrently write
to the same output database, with a rate expected of a batch process, that database can easily be
overwhelmed, and its performance for queries is likely to suffer. This can in turn cause operational
problems in other parts of the system [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kreps2014wm_ch10">35</a>].</p>
</li>
<li>
<p>Normally, MapReduce provides a clean all-or-nothing guarantee for job output: if a job succeeds,
the result is the output of running every task exactly once, even if some tasks failed and had to
be retried along the way; if the entire job fails, no output is produced. However, writing to an
external system from inside a job produces externally visible side effects that cannot be hidden
in this way. Thus, you have to worry about the results from partially completed jobs being visible
to other systems, and the complexities of Hadoop task attempts and speculative execution.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="Voldemort (database)" data-secondary="building read-only stores in batch processes" id="idm140417548669760"></a>
<a data-type="indexterm" data-primary="Terrapin (database)" id="idm140417548668336"></a>
<a data-type="indexterm" data-primary="ElephantDB (database)" id="idm140417548667504"></a>
<a data-type="indexterm" data-primary="HBase (database)" data-secondary="bulk loading" id="idm140417548666672"></a>
A much better solution is to build a brand-new database <em>inside</em> the batch job and write it as
files to the job’s output directory in the distributed filesystem, just like the search indexes in
the last section. Those data files are then immutable once written, and can be loaded in bulk into
servers that handle read-only queries. Various key-value stores support building database files in
MapReduce jobs, including Voldemort
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sumbaly2012wi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2012wi">46</a>],
Terrapin [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Sharma2015tp-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sharma2015tp">47</a>],
ElephantDB [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="ElephantDB-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ElephantDB">48</a>],
and HBase bulk loading [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cryans2013wo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cryans2013wo">49</a>].</p>

<p>Building these database files is a good use of MapReduce: using a mapper to extract a key and then
sorting by that key is already a lot of the work required to build an index. Since most of these
key-value stores are read-only (the files can only be written once by a batch job and are then
immutable), the data structures are quite simple. For example, they do not require a WAL (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_btree_wal">“Making B-trees reliable”</a>).</p>

<p>When loading data into Voldemort, the server continues serving requests to the old data files while
the new data files are copied from the distributed filesystem to the server’s local disk. Once the
copying is complete, the server atomically switches over to querying the new files. If anything goes
wrong in this process, it can easily switch back to the old files again, since they are still there
and immutable [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2012wi">46</a>].
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="output of batch workflows" data-startref="ix_madredout" id="idm140417548652624"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="MapReduce and distributed filesystems" data-startref="ix_batchMRdist" id="idm140417548651216"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="outputs" data-startref="ix_batchoutputs" id="idm140417548649824"></a></p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Philosophy of batch process outputs"><div class="sect3" id="sec_batch_philosophy">
<h3>Philosophy of batch process outputs</h3>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to Unix" id="ix_batchphil"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to Unix" id="ix_hadoopunixphil"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to Hadoop" id="ix_unixphilhadoop"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to Unix" id="ix_mapredunixphil"></a>
The Unix philosophy that we discussed earlier in this chapter (<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_unix_philosophy">“The Unix Philosophy”</a>)
encourages experimentation by being very explicit about dataflow: a program reads its input and
writes its output. In the process, the input is left unchanged, any previous output is completely
replaced with the new output, and there are no other side effects. This means that you can rerun a
command as often as you like, tweaking or debugging it, without messing up the state of your system.</p>

<p>The handling of output from MapReduce jobs follows the same philosophy. By treating inputs as
immutable and avoiding side effects (such as writing to external databases), batch jobs not only
achieve good performance but also become much easier to maintain:</p>

<ul>
<li>
<p><a data-type="indexterm" data-primary="corruption of data" data-secondary="recovering from" id="idm140417548638224"></a>
If you introduce a bug into the code and the output is wrong or corrupted, you can simply roll
back to a previous version of the code and rerun the job, and the output will be correct again. Or,
even simpler, you can keep the old output in a different directory and simply switch back to it.
Databases with read-write transactions do not have this property: if you deploy buggy code that
writes bad data to the database, then rolling back the code will do nothing to fix the data in the
database.
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="human fault tolerance" id="idm140417548636608"></a>
<a data-type="indexterm" data-primary="human errors" id="idm140417548635536"></a>
(The idea of being able to recover from buggy code has been called <em>human fault tolerance</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Marz2011vq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Marz2011vq">50</a>].)</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="Agile" data-secondary="minimizing irreversibility" id="idm140417548631072"></a>
As a consequence of this ease of rolling back, feature development can proceed more quickly than
in an environment where mistakes could mean irreversible damage. This principle of <em>minimizing
irreversibility</em> is beneficial for Agile software development
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Bartlett2015wv_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Bartlett2015wv_ch10">51</a>].</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm140417548626240"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm140417548625248"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm140417548624080"></a>
If a map or reduce task fails, the MapReduce framework automatically re-schedules it and runs it
again on the same input. If the failure is due to a bug in the code, it will keep crashing and
eventually cause the job to fail after a few attempts; but if the failure is due to a transient
issue, the fault is tolerated. This automatic retry is only safe because inputs are immutable and
outputs from failed tasks are discarded by the MapReduce framework.</p>
</li>
<li>
<p>The same set of files can be used as input for various different jobs, including monitoring jobs
that calculate metrics and evaluate whether a job’s output has the expected characteristics (for
example, by comparing it to the output from the previous run and measuring discrepancies).</p>
</li>
<li>
<p>Like Unix tools, MapReduce jobs separate logic from wiring (configuring the input and output
directories), which provides a separation of concerns and enables potential reuse of code: one
team can focus on implementing a job that does one thing well, while other teams can decide where
and when to run that job.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="Avro (data format)" data-secondary="use in Hadoop" id="idm140417548619808"></a>
<a data-type="indexterm" data-primary="Avro (data format)" data-secondary="object container files" id="idm140417548618336"></a>
<a data-type="indexterm" data-primary="Parquet (data format)" data-secondary="use in Hadoop" id="idm140417548617232"></a>
In these areas, the design principles that worked well for Unix also seem to be working well for
Hadoop—but Unix and Hadoop also differ in some ways. For example, because most Unix tools assume
untyped text files, they have to do a lot of input parsing (our log analysis example at the
beginning of the chapter used <code>{print $7}</code> to extract the URL).
<a data-type="indexterm" data-primary="column-oriented storage" data-secondary="Parquet" id="idm140417548615408"></a> On Hadoop, some of those low-value syntactic conversions
are eliminated by using more structured file formats: Avro (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_avro">“Avro”</a>) and Parquet
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_column">“Column-Oriented Storage”</a>) are often used, as they provide efficient schema-based encoding and
allow evolution of their schemas over time (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#ch_encoding">Chapter&nbsp;4</a>).
<a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to Unix" data-startref="ix_batchphil" id="idm140417548611568"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to Unix" data-startref="ix_hadoopunixphil" id="idm140417548610192"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="comparison to Hadoop" data-startref="ix_unixphilhadoop" id="idm140417548608848"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to Unix" data-startref="ix_mapredunixphil" id="idm140417548607472"></a>
<a data-type="indexterm" data-primary="workflows (MapReduce)" data-secondary="outputs" data-startref="ix_workout" id="idm140417548606080"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Comparing Hadoop to Distributed Databases"><div class="sect2" id="sec_batch_mr_vs_db">
<h2>Comparing Hadoop to Distributed Databases</h2>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to MPP databases" id="ix_batchmpp"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to MPP databases" id="ix_medredcomp"></a>
<a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" id="ix_distsyscompmr"></a>
<a data-type="indexterm" data-primary="MPP" data-see="massively parallel processing" id="idm140417548599072"></a>
<a data-type="indexterm" data-primary="parallel databases" data-see="massively parallel processing" id="idm140417548597952"></a>
As we have seen, Hadoop is somewhat like a distributed version of Unix, where HDFS is the
filesystem and MapReduce is a quirky implementation of a Unix process (which happens to always run
the <code>sort</code> utility between the map phase and the reduce phase). We saw how you can implement various
join and grouping operations on top of these primitives.</p>

<p><a data-type="indexterm" data-primary="joins" data-secondary="parallel execution of" id="idm140417548595952"></a>
When the MapReduce paper [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Dean2004ua_ch10" class="totri-footnote">1</a>] was
published, it was—in some sense—not at all new. All of the processing and parallel join
algorithms that we discussed in the last few sections had already been implemented in so-called
<em>massively parallel processing</em> (MPP) databases more than a decade previously
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Babu2013gm_ch10" class="totri-footnote">3</a>, <a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt1992ws">40</a>].
For example, the Gamma database machine, Teradata, and Tandem NonStop SQL were pioneers in this area
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="DeWitt1992fn_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt1992fn_ch10">52</a>].</p>

<p><a data-type="indexterm" data-primary="analytics" data-secondary="parallel query execution in MPP databases" id="idm140417548588544"></a>
The biggest difference is that MPP databases focus on parallel execution of analytic SQL queries on
a cluster of machines, while the combination of MapReduce and a distributed filesystem
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ghemawat2003dy">19</a>] provides something much more like a
general-purpose operating system that can run arbitrary programs.</p>










<section data-type="sect3" data-pdf-bookmark="Diversity of storage"><div class="sect3" id="idm140417548586096">
<h3>Diversity of storage</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="diversity of storage" id="idm140417548584752"></a>
<a data-type="indexterm" data-primary="storage" data-secondary="diversity of, in MapReduce" id="idm140417548583344"></a>
Databases require you to structure data according to a particular model (e.g., relational or
documents), whereas files in a distributed filesystem are just byte sequences, which can be written
using any data model and encoding. They might be collections of database records, but they can
equally well be text, images, videos, sensor readings, sparse matrices, feature vectors, genome
sequences, or any other kind of data.</p>

<p><a data-type="indexterm" data-primary="distributed filesystems" data-secondary="indiscriminately dumping data into" id="idm140417548581536"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="indiscriminately dumping data into" id="idm140417548580416"></a>
To put it bluntly, Hadoop opened up the possibility of indiscriminately dumping data into HDFS, and
only later figuring out how to process it further
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kreps2014qz-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kreps2014qz">53</a>]. By
contrast, MPP databases typically require careful up-front modeling of the data and query patterns
before importing the data into the database’s proprietary storage format.</p>

<p>From a purist’s point of view, it may seem that this careful modeling and import is desirable, because
it means users of the database have better-quality data to work with. However, in practice, it
appears that simply making data available quickly—even if it is in a quirky, difficult-to-use,
raw format—is often more valuable than trying to decide on the ideal data model up front
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Cohen2009fv-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cohen2009fv">54</a>].</p>

<p><a data-type="indexterm" data-primary="data warehousing" data-secondary="comparison to data lakes" id="idm140417548572672"></a>
<a data-type="indexterm" data-primary="data lakes" id="idm140417548571504"></a>
The idea is similar to a data warehouse (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_dwh">“Data Warehousing”</a>): simply bringing data from various
parts of a large organization together in one place is valuable, because it enables joins across
datasets that were previously disparate. The careful schema design required by an MPP database slows
down that centralized data collection; collecting data in its raw form, and worrying about schema
design later, allows the data collection to be speeded up (a concept sometimes known as a “data
lake” or “enterprise data hub” [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Terrizzano2015tk-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Terrizzano2015tk">55</a>]).</p>

<p><a data-type="indexterm" data-primary="schema-on-read" data-secondary="in distributed filesystems" id="idm140417548566736"></a>
Indiscriminate data dumping shifts the burden of interpreting the data: instead of forcing the
producer of a dataset to bring it into a standardized format, the interpretation of the data becomes
the consumer’s problem (the schema-on-read approach [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Roberts2015tl-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Roberts2015tl">56</a>];
see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_schema_flexibility">“Schema flexibility in the document model”</a>).
This can be an advantage if the producer and consumers are different teams with different
priorities. There may not even be one ideal data model, but rather different views onto the data
that are suitable for different purposes. Simply dumping data in its raw form allows for several
such transformations. This approach has been dubbed the <em>sushi principle</em>: “raw data is better”
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Johnson2015ua-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Johnson2015ua">57</a>].</p>

<p><a data-type="indexterm" data-primary="data warehousing" data-secondary="ETL (extract-transform-load)" id="idm140417548558976"></a>
<a data-type="indexterm" data-primary="ETL (extract-transform-load)" data-secondary="use of Hadoop for" id="idm140417548557808"></a>
Thus, Hadoop has often been used for implementing ETL processes (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_dwh">“Data Warehousing”</a>): data from
transaction processing systems is dumped into the distributed filesystem in some raw form, and then
MapReduce jobs are written to clean up that data, transform it into a relational form, and import it
into an MPP data warehouse for analytic purposes. Data modeling still happens, but it is in a
separate step, decoupled from the data collection. This decoupling is possible because a distributed
filesystem supports data encoded in any format.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Diversity of processing models"><div class="sect3" id="idm140417548555200">
<h3>Diversity of processing models</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="diversity of processing models" id="idm140417548553920"></a>
<a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="advantages and limitations of" id="idm140417548552448"></a>
<a data-type="indexterm" data-primary="Tableau (data visualization software)" id="idm140417548551312"></a>
MPP databases are monolithic, tightly integrated pieces of software that take care of storage layout
on disk, query planning, scheduling, and execution. Since these components can all be tuned and
optimized for the specific needs of the database, the system as a whole can achieve very good
performance on the types of queries for which it is designed. Moreover, the SQL query language
allows expressive queries and elegant semantics without the need to write code, making it accessible
to graphical tools used by business analysts (such as Tableau).</p>

<p>On the other hand, not all kinds of processing can be sensibly expressed as SQL queries. For
example, if you are building machine learning and recommendation systems, or full-text search
indexes with relevance ranking models, or performing image analysis, you most likely need a more
general model of data processing. These kinds of processing are often very specific to a particular
application (e.g., feature engineering for machine learning, natural language models for machine
translation, risk estimation functions for fraud prediction), so they inevitably require writing
code, not just queries.</p>

<p><a data-type="indexterm" data-primary="SQL (Structured Query Language)" data-secondary="query execution on Hadoop" id="idm140417548548608"></a>
MapReduce gave engineers the ability to easily run their own code over large datasets. If you have
HDFS and MapReduce, you <em>can</em> build a SQL query execution engine on top of it, and indeed this is
what the Hive project did [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Thusoo2010hp">31</a>]. However, you
can also write many other forms of batch processes that do not lend themselves to being expressed as
a SQL query.</p>

<p><a data-type="indexterm" data-primary="YARN (job scheduler)" id="idm140417548545600"></a>
Subsequently, people found that MapReduce was too limiting and performed too badly for some types of
processing, so various other processing models were developed on top of Hadoop (we will see some of
them in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_beyond_mr">“Beyond MapReduce”</a>). Having two processing models, SQL and MapReduce, was not enough:
even <span class="keep-together">more different</span> models were needed! And due to the
openness of the Hadoop platform, it was feasible to implement a whole range of approaches, which
would not have been possible within the confines of a monolithic MPP database
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Vavilapalli2013eu-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Vavilapalli2013eu">58</a>].</p>

<p><a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="diverse processing models in ecosystem" id="idm140417548539632"></a>
Crucially, those various processing models can all be run on a single shared-use cluster of
machines, all accessing the same files on the distributed filesystem. In the Hadoop approach, there
is no need to import the data into several different specialized systems for different kinds of
processing: the system is flexible enough to support a diverse set of workloads within the same
cluster. Not having to move data around makes it a lot easier to derive value from the data, and a
lot easier to experiment with new processing models.</p>

<p><a data-type="indexterm" data-primary="HBase (database)" data-secondary="use of HDFS" id="idm140417548537632"></a>
<a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="use of HDFS" id="idm140417548536304"></a>
<a data-type="indexterm" data-primary="distributed filesystems" data-secondary="decoupling from query engines" id="idm140417548535200"></a>
<a data-type="indexterm" data-primary="HDFS (Hadoop Distributed File System)" data-secondary="decoupling from query engines" id="idm140417548534128"></a>
The Hadoop ecosystem includes both random-access OLTP databases such as HBase (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_lsm_trees">“SSTables and LSM-Trees”</a>) and MPP-style analytic databases such as Impala
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kornacker2015uv_ch10">41</a>]. Neither HBase nor Impala uses
MapReduce, but both use HDFS for storage. They are very different approaches to accessing and
processing data, but they can nevertheless coexist and be integrated in the same system.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Designing for frequent faults"><div class="sect3" id="idm140417548531168">
<h3>Designing for frequent faults</h3>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="comparison to distributed databases" data-tertiary="designing for frequent faults" id="idm140417548529696"></a>
<a data-type="indexterm" data-primary="faults" data-secondary="in batch processing versus distributed databases" id="idm140417548528320"></a>
When comparing MapReduce to MPP databases, two more differences in design approach stand out: the
handling of faults and the use of memory and disk. Batch processes are less sensitive to faults
than online systems, because they do not immediately affect users if they fail and they can always
be run again.</p>

<p>If a node crashes while a query is executing, most MPP databases abort the entire query, and
either let the user resubmit the query or automatically run it again
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Babu2013gm_ch10" class="totri-footnote">3</a>]. As queries normally run for a few
seconds or a few minutes at most, this way of handling errors is acceptable, since the cost of
retrying is not too great.  MPP databases also prefer to keep as much data as possible in memory
(e.g., using hash joins) to avoid the cost of reading from disk.</p>

<p>On the other hand, MapReduce can tolerate the failure of a map or reduce task without it affecting the
job as a whole by retrying work at the granularity of an individual task. It is also very eager to
write data to disk, partly for fault tolerance, and partly on the assumption that the dataset will
be too big to fit in memory anyway.</p>

<p>The MapReduce approach is more appropriate for larger jobs: jobs that process so much data and run
for such a long time that they are likely to experience at least one task failure along the way. In
that case, rerunning the entire job due to a single task failure would be wasteful. Even if
recovery at the granularity of an individual task introduces overheads that make fault-free
processing slower, it can still be a reasonable trade-off if the rate of task failures is high
enough.</p>

<p>But how realistic are these assumptions? In most clusters, machine failures do occur, but they are
not very frequent—probably rare enough that most jobs will not experience a machine failure. Is
it really worth incurring significant overheads for the sake of fault tolerance?</p>

<p><a data-type="indexterm" data-primary="preemption" data-secondary="of datacenter resources" id="idm140417548522464"></a>
To understand the reasons for MapReduce’s sparing use of memory and task-level recovery, it is
helpful to look at the environment for which MapReduce was originally designed. Google has mixed-use
datacenters, in which online production services and offline batch jobs run on the same machines.
Every task has a resource allocation (CPU cores, RAM, disk space, etc.) that is enforced using
containers. Every task also has a priority, and if a higher-priority task needs more resources,
lower-priority tasks on the same machine can be terminated (preempted) in order to free up
resources. Priority also determines pricing of the computing resources: teams must pay for the
resources they use, and higher-priority processes cost more
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Verma2015gi-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Verma2015gi">59</a>].</p>

<p>This architecture allows non-production (low-priority) computing resources to be overcommitted,
because the system knows that it can reclaim the resources if necessary. Overcommitting resources in
turn allows better utilization of machines and greater efficiency compared to systems that segregate
production and non-production tasks. However, as MapReduce jobs run at low priority, they run the
risk of being preempted at any time because a higher-priority process requires their resources.
Batch jobs effectively “pick up the scraps under the table,” using any computing resources that
remain after the high-priority processes have taken what they need.</p>

<p><a data-type="indexterm" data-primary="Google" data-secondary="MapReduce (batch processing)" data-tertiary="task preemption" id="idm140417548516288"></a>
At Google, a MapReduce task that runs for an hour has an approximately 5% risk of being terminated
to make space for a higher-priority process. This rate is more than an order of magnitude higher
than the rate of failures due to hardware issues, machine reboot, or other reasons
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Verma2015gi">59</a>]. At this rate of preemptions, if a job has
100 tasks that each run for 10 minutes, there is a risk greater than 50% that at least one task will be
terminated before it is finished.</p>

<p>And this is why MapReduce is designed to tolerate frequent unexpected task termination: it’s not
because the hardware is particularly unreliable, it’s because the freedom to arbitrarily terminate
processes enables better resource utilization in a computing cluster.</p>

<p><a data-type="indexterm" data-primary="YARN (job scheduler)" data-secondary="preemption of jobs" id="idm140417548512592"></a>
<a data-type="indexterm" data-primary="Mesos (cluster manager)" id="idm140417548511264"></a>
<a data-type="indexterm" data-primary="Kubernetes (cluster manager)" id="idm140417548510432"></a>
Among open source cluster schedulers, preemption is less widely used. YARN’s CapacityScheduler
supports preemption for balancing the resource allocation of different queues
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Vavilapalli2013eu">58</a>],
but general priority preemption is not supported in YARN, Mesos, or Kubernetes at the time of writing
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Schwarzkopf2016un-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Schwarzkopf2016un">60</a>].
In an environment where tasks are not so often terminated, the design decisions of MapReduce make
less sense. In the next section, we will look at some alternatives to MapReduce that make different
design decisions.
<a data-type="indexterm" data-primary="batch processing" data-secondary="comparison to MPP databases" data-startref="ix_batchmpp" id="idm140417548506160"></a>
<a data-type="indexterm" data-primary="Hadoop (data infrastructure)" data-secondary="comparison to MPP databases" data-startref="ix_medredcomp" id="idm140417548504768"></a>
<a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" data-startref="ix_distsyscompmr" id="idm140417548503360"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Beyond MapReduce"><div class="sect1" id="sec_batch_beyond_mr">
<h1>Beyond MapReduce</h1>

<p>Although MapReduce became very popular and received a lot of hype in the late 2000s, it is just one
among many possible programming models for distributed systems. Depending on the volume of data, the
structure of the data, and the type of processing being done with it, other tools may be more
appropriate for expressing a computation.</p>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="disadvantages and limitations of" id="idm140417548499984"></a>
We nevertheless spent a lot of time in this chapter discussing MapReduce because it is a useful
learning tool, as it is a fairly clear and simple abstraction on top of a distributed filesystem.
That is, <em>simple</em> in the sense of being able to understand what it is doing, not in the sense of
being easy to use. Quite the opposite: implementing a complex processing job using the raw MapReduce
APIs is actually quite hard and laborious—for instance, you would need to implement any join
algorithms from scratch [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>].</p>

<p><a data-type="indexterm" data-primary="Pig (dataflow language)" id="idm140417548496976"></a>
<a data-type="indexterm" data-primary="Hive (query engine)" id="idm140417548496064"></a>
<a data-type="indexterm" data-primary="Cascading (batch processing)" id="idm140417548495232"></a>
<a data-type="indexterm" data-primary="Crunch (batch processing)" id="idm140417548494432"></a>
In response to the difficulty of using MapReduce directly, various higher-level programming models
(Pig, Hive, Cascading, Crunch) were created as abstractions on top of MapReduce. If you understand
how MapReduce works, they are fairly easy to learn, and their higher-level constructs make many
common batch processing tasks significantly easier to implement.</p>

<p>However, there are also problems with the MapReduce execution model itself, which are not fixed by
adding another level of abstraction and which manifest themselves as poor performance for some
kinds of processing. On the one hand, MapReduce is very robust: you can use it to process almost
arbitrarily large quantities of data on an unreliable multi-tenant system with frequent task
terminations, and it will still get the job done (albeit slowly). On the other hand, other tools are
sometimes orders of magnitude faster for some kinds of processing.</p>

<p>In the rest of this chapter, we will look at some of those alternatives for batch processing. In
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html#ch_stream">Chapter&nbsp;11</a> we will move to stream processing, which can be regarded as another way of speeding up
batch processing.</p>








<section data-type="sect2" data-pdf-bookmark="Materialization of Intermediate State"><div class="sect2" id="sec_batch_materialize">
<h2>Materialization of Intermediate State</h2>

<p><a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="materialization of intermediate state" id="ix_batchbeymater"></a>
As discussed previously, every MapReduce job is independent from every other job. The main contact
points of a job with the rest of the world are its input and output directories on the distributed
filesystem. If you want the output of one job to become the input to a second job, you need to
configure the second job’s input directory to be the same as the first job’s output directory, and
an external workflow scheduler must start the second job only once the first job has completed.</p>

<p><a data-type="indexterm" data-primary="loose coupling" id="idm140417548486688"></a>
This setup is reasonable if the output from the first job is a dataset that you want to publish
widely within your organization. In that case, you need to be able to refer to it by name and reuse
it as input to several different jobs (including jobs developed by other teams). Publishing data to
a well-known location in the distributed filesystem allows loose coupling so that jobs don’t need
to know who is producing their input or consuming their output (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_logic_wiring">“Separation of logic and wiring”</a>).</p>

<p><a data-type="indexterm" data-primary="intermediate state, materialization of" id="ix_intermed"></a>
<a data-type="indexterm" data-primary="materialization" data-secondary="intermediate state (batch processing)" id="ix_materialize"></a>
<a data-type="indexterm" data-primary="recommendation engines" data-secondary="batch workflows" id="idm140417548481776"></a>
However, in many cases, you know that the output of one job is only ever used as input to one other
job, which is maintained by the same team. In this case, the files on the distributed filesystem are
simply <em>intermediate state</em>: a means of passing data from one job to the next. In the complex
workflows used to build recommendation systems consisting of 50 or 100 MapReduce jobs
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2013eh">29</a>], there is a lot of such intermediate
state.</p>

<p>The process of writing out this intermediate state to files is called <em>materialization</em>. (We came
across the term previously in the context of materialized views, in
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_materialized_views">“Aggregation: Data Cubes and Materialized Views”</a>. It means to eagerly compute the result of some operation and
write it out, rather than computing it on demand when requested.)</p>

<p>By contrast, the log analysis example at the beginning of the chapter used Unix pipes to connect the
output of one command with the input of another. Pipes do not fully materialize the intermediate
state, but instead <em>stream</em> the output to the input incrementally, using only a small in-memory
buffer.</p>

<p>MapReduce’s approach of fully materializing intermediate state has downsides compared to Unix pipes:</p>

<ul>
<li>
<p>A MapReduce job can only start when all tasks in the preceding jobs (that generate its inputs)
have completed, whereas processes connected by a Unix pipe are started at the same time, with
output being consumed as soon as it is produced. Skew or varying load on different machines means
that a job often has a few straggler tasks that take much longer to complete than the others.
Having to wait until all of the preceding job’s tasks have completed slows down the execution of
the workflow as a whole.</p>
</li>
<li>
<p>Mappers are often redundant: they just read back the same file that was just written by a reducer,
and prepare it for the next stage of partitioning and sorting. In many cases, the mapper code
could be part of the previous reducer: if the reducer output was partitioned and sorted in the
same way as mapper output, then reducers could be chained together directly, without interleaving
with mapper stages.</p>
</li>
<li>
<p>Storing intermediate state in a distributed filesystem means those files are replicated across
several nodes, which is often overkill for such temporary data.</p>
</li>
</ul>










<section data-type="sect3" data-pdf-bookmark="Dataflow engines"><div class="sect3" id="sec_batch_dataflow">
<h3>Dataflow engines</h3>

<p><a data-type="indexterm" data-primary="Spark (processing framework)" id="ix_sparkintro"></a><a data-type="indexterm" data-primary="Apache Spark" data-see="Spark" id="idm140417548467840"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" id="ix_flinkintro"></a><a data-type="indexterm" data-primary="Apache Flink" data-see="Flink" id="idm140417548465776"></a>
<a data-type="indexterm" data-primary="Tez (dataflow engine)" id="ix_tezintro"></a><a data-type="indexterm" data-primary="Apache Tez" data-see="Tez" id="idm140417548463728"></a>
<a data-type="indexterm" data-primary="dataflow engines" id="ix_dflowengines"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="dataflow engines" id="ix_batchdataflow"></a>
In order to fix these problems with MapReduce, several new execution engines for distributed batch
computations were developed, the most well known of which are Spark
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Zaharia2012ve-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Zaharia2012ve">61</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Karau2015wf-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Karau2015wf">62</a>],
Tez [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saha2014vd-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Saha2014vd">63</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Saha2015dh-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Saha2015dh">64</a>],
and Flink [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Tzoumas2015ws-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Tzoumas2015ws">65</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Alexandrov2014jb-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Alexandrov2014jb">66</a>].
There are various differences in the way they are designed, but they have one thing in common: they
handle an entire workflow as one job, rather than breaking it up into independent subjobs.</p>

<p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm140417548445552"></a>
Since they explicitly model the flow of data through several processing stages, these systems are
known as <em>dataflow engines</em>. Like MapReduce, they work by repeatedly calling a user-defined function
to process one record at a time on a single thread. They parallelize work by partitioning inputs,
and they copy the output of one function over the network to become the input to another function.</p>

<p><a data-type="indexterm" data-primary="operators" id="idm140417548443408"></a>
Unlike in MapReduce, these functions need not take the strict roles of alternating map and reduce,
but instead can be assembled in more flexible ways. We call these functions <em>operators</em>, and the
dataflow engine provides several different options for connecting one operator’s output to another’s
input:</p>

<ul>
<li>
<p>One option is to repartition and sort records by key, like in the shuffle stage of MapReduce
(see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mapreduce_dist">“Distributed execution of MapReduce”</a>). This feature enables sort-merge joins and grouping in the same
way as in MapReduce.</p>
</li>
<li>
<p>Another possibility is to take several inputs and to partition them in the same way, but skip
the sorting. This saves effort on partitioned hash joins, where the partitioning of records is
important but the order is irrelevant because building the hash table randomizes the order
anyway.</p>
</li>
<li>
<p>For broadcast hash joins, the same output from one operator can be sent to all partitions of
the join operator.</p>
</li>
</ul>

<p><a data-type="indexterm" data-primary="Dryad (dataflow engine)" id="idm140417548436976"></a>
<a data-type="indexterm" data-primary="Nephele (dataflow engine)" id="idm140417548436144"></a>
This style of processing engine is based on research systems like Dryad
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Isard2007fe-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Isard2007fe">67</a>]
and Nephele [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Warneke2009en-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Warneke2009en">68</a>], and it
offers several advantages compared to the MapReduce model:</p>

<ul>
<li>
<p>Expensive work such as sorting need only be performed in places where it is actually required,
rather than always happening by default between every map and reduce stage.</p>
</li>
<li>
<p>There are no unnecessary map tasks, since the work done by a mapper can often be
incorporated into the preceding reduce operator (because a mapper does not change the partitioning
of a dataset).</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="locality (data access)" data-secondary="in batch processing" id="idm140417548426064"></a>
Because all joins and data dependencies in a workflow are explicitly declared, the scheduler has
an overview of what data is required where, so it can make locality optimizations. For example, it
can try to place the task that consumes some data on the same machine as the task that produces
it, so that the data can be exchanged through a shared memory buffer rather than having to copy
it over the network.</p>
</li>
<li>
<p>It is usually sufficient for intermediate state between operators to be kept in memory or written
to local disk, which requires less I/O than writing it to HDFS (where it must be replicated to
several machines and written to disk on each replica). MapReduce already uses this optimization
for mapper output, but dataflow engines generalize the idea to all intermediate state.</p>
</li>
<li>
<p>Operators can start executing as soon as their input is ready; there is no need to wait for the
entire preceding stage to finish before the next one starts.</p>
</li>
<li>
<p><a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="process reuse in batch processors" id="idm140417548421536"></a>
Existing Java Virtual Machine (JVM) processes can be reused to run new operators, reducing startup
overheads compared to MapReduce (which launches a new JVM for each task).</p>
</li>
</ul>

<p>You can use dataflow engines to implement the same computations as MapReduce workflows, and they
usually execute significantly faster due to the optimizations described here. Since operators are a
generalization of map and reduce, the same processing code can run on either execution engine:
workflows implemented in Pig, Hive, or Cascading can be switched from MapReduce to Tez or Spark with
a simple configuration change, without modifying code
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Saha2015dh">64</a>].</p>

<p>Tez is a fairly thin library that relies on the YARN shuffle service for the actual copying of data
between nodes [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Vavilapalli2013eu">58</a>], whereas Spark and
Flink are big frameworks that include their own network communication layer, scheduler, and
user-facing APIs. We will discuss those high-level APIs shortly.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Fault tolerance"><div class="sect3" id="sec_batch_materialize_ft">
<h3>Fault tolerance</h3>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="fault tolerance" id="idm140417548415312"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="fault tolerance" id="idm140417548414208"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm140417548413088"></a>
An advantage of fully materializing intermediate state to a distributed filesystem is that it is
durable, which makes fault tolerance fairly easy in MapReduce: if a task fails, it can just be
restarted on another machine and read the same input again from the filesystem.</p>

<p><a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="fault tolerance" id="idm140417548411408"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="fault tolerance" id="idm140417548410336"></a>
<a data-type="indexterm" data-primary="Tez (dataflow engine)" data-secondary="fault tolerance" id="idm140417548409216"></a>
Spark, Flink, and Tez avoid writing intermediate state to HDFS, so they take a different approach to
tolerating faults: if a machine fails and the intermediate state on that machine is lost, it is
recomputed from other data that is still available (a prior intermediary stage if possible, or
otherwise the original input data, which is normally on HDFS).</p>

<p><a data-type="indexterm" data-primary="checkpointing" data-secondary="in batch processors" id="idm140417548407488"></a>
To enable this recomputation, the framework must keep track of how a given piece of data was
computed—which input partitions it used, and which operators were applied to it. Spark uses the
resilient distributed dataset (RDD) abstraction for tracking the ancestry of data
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Zaharia2012ve">61</a>], while Flink checkpoints operator state,
allowing it to resume running an operator that ran into a fault during its execution
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Alexandrov2014jb">66</a>].</p>

<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="and fault tolerance" id="idm140417548404064"></a>
When recomputing data, it is important to know whether the computation is <em>deterministic</em>: that is,
given the same input data, do the operators always produce the same output? This question matters if some of
the lost data has already been sent to downstream operators. If the operator is restarted and the
recomputed data is not the same as the original lost data, it becomes very hard for downstream
operators to resolve the contradictions between the old and new data. The solution in the case of
nondeterministic operators is normally to kill the downstream operators as well, and run them again
on the new data.</p>

<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="accidental nondeterminism" id="idm140417548401728"></a>
<a data-type="indexterm" data-primary="nondeterministic operations" data-secondary="accidental nondeterminism" id="idm140417548400592"></a>
In order to avoid such cascading faults, it is better to make operators deterministic. Note however
that it is easy for nondeterministic behavior to accidentally creep in: for example, many
programming languages do not guarantee any particular order when iterating over elements of a hash
table, many probabilistic and statistical <span class="keep-together">algorithms</span>
explicitly rely on using random numbers, and any use of the system clock or external data sources is
nondeterministic. Such causes of nondeterminism need to be removed in order to reliably recover from
faults, for example by generating <span class="keep-together">pseudorandom</span> numbers
using a fixed seed.</p>

<p>Recovering from faults by recomputing data is not always the right answer: if the intermediate data
is much smaller than the source data, or if the computation is very CPU-intensive, it is probably
cheaper to materialize the intermediate data to files than to recompute it.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Discussion of materialization"><div class="sect3" id="idm140417548396672">
<h3>Discussion of materialization</h3>

<p><a data-type="indexterm" data-primary="pipelined execution" id="idm140417548395504"></a>
<a data-type="indexterm" data-primary="Unix philosophy" data-secondary="command-line batch processing" data-tertiary="Unix pipes versus dataflow engines" id="idm140417548394672"></a>
Returning to the Unix analogy, we saw that MapReduce is like writing the output of each command to a
temporary file, whereas dataflow engines look much more like Unix pipes. Flink especially is built
around the idea of pipelined execution: that is, incrementally passing the output of an operator to
other operators, and not waiting for the input to be complete before starting to process it.</p>

<p>A sorting operation inevitably needs to consume its entire input before it can produce any output,
because it’s possible that the very last input record is the one with the lowest key and thus needs
to be the very first output record. Any operator that requires sorting will thus need to accumulate
state, at least temporarily. But many other parts of a workflow can be executed in a pipelined
manner.</p>

<p>When the job completes, its output needs to go somewhere durable so that users can find it and use
it—most likely, it is written to the distributed filesystem again. Thus, when using a dataflow
engine, materialized datasets on HDFS are still usually the inputs and the final outputs of a job.
Like with MapReduce, the inputs are immutable and the output is completely replaced. The improvement
over MapReduce is that you save yourself writing all the intermediate state to the filesystem as
well.
<a data-type="indexterm" data-primary="intermediate state, materialization of" data-startref="ix_intermed" id="idm140417548390848"></a>
<a data-type="indexterm" data-primary="materialization" data-secondary="intermediate state (batch processing)" data-startref="ix_materialize" id="idm140417548389776"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="materialization of intermediate state" data-startref="ix_batchbeymater" id="idm140417548388384"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-startref="ix_sparkintro" id="idm140417548386976"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-startref="ix_flinkintro" id="idm140417548385856"></a>
<a data-type="indexterm" data-primary="Tez (dataflow engine)" data-startref="ix_tezintro" id="idm140417548384736"></a>
<a data-type="indexterm" data-primary="dataflow engines" data-startref="ix_dflowengines" id="idm140417548383632"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="dataflow engines" data-startref="ix_batchdataflow" id="idm140417548382528"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="Graphs and Iterative Processing"><div class="sect2" id="sec_batch_graph">
<h2>Graphs and Iterative Processing</h2>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="graphs and iterative processing" id="ix_batchbeygraph"></a>
<a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" id="ix_graph"></a>
In <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_graph">“Graph-Like Data Models”</a> we discussed using graphs for modeling data, and using graph query
languages to traverse the edges and vertices in a graph. The discussion in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#ch_datamodels">Chapter&nbsp;2</a> was
focused around OLTP-style use: quickly executing queries to find a small number of vertices matching
certain criteria.</p>

<p><a data-type="indexterm" data-primary="machine learning" data-secondary="iterative processing" id="idm140417548374640"></a>
<a data-type="indexterm" data-primary="recommendation engines" data-secondary="iterative processing" id="idm140417548373536"></a>
<a data-type="indexterm" data-primary="ranking algorithms" id="idm140417548372432"></a>
<a data-type="indexterm" data-primary="PageRank (algorithm)" id="idm140417548371600"></a>
It is also interesting to look at graphs in a batch processing context, where the goal is to perform
some kind of offline processing or analysis on an entire graph. This need often arises in machine
learning applications such as recommendation engines, or in ranking systems. For example, one of the
most famous graph analysis algorithms is PageRank
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Page1999wg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Page1999wg">69</a>],
which tries to estimate the popularity of a web page based on what other web pages link to it. It is
used as part of the formula that determines the order in which web search engines present their
results.</p>
<div data-type="note" epub:type="note"><h6>Note</h6>
<p><a data-type="indexterm" data-primary="operators" data-secondary="flow of data between" id="idm140417548366672"></a>
<a data-type="indexterm" data-primary="directed acyclic graphs (DAGs)" id="idm140417548365344"></a>
<a data-type="indexterm" data-primary="dataflow engines" data-secondary="directed acyclic graphs (DAG)" id="idm140417548364544"></a>
Dataflow engines like Spark, Flink, and Tez (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_materialize">“Materialization of Intermediate State”</a>) typically arrange the
operators in a job as a directed acyclic graph (DAG). This is not the same as graph processing: in
dataflow engines, the <em>flow of data from one operator to another</em> is structured as a graph, while
the data itself typically consists of relational-style tuples. In graph processing, the <em>data
itself</em> has the form of a graph. Another unfortunate naming confusion!</p>
</div>

<p><a data-type="indexterm" data-primary="transitive closure (graph algorithm)" id="idm140417548361136"></a>
Many graph algorithms are expressed by traversing one edge at a time, joining one vertex with an
adjacent vertex in order to propagate some information, and repeating until some condition is
met—for example, until there are no more edges to follow, or until some metric converges. We saw an
example in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#fig_datalog_naive">Figure&nbsp;2-6</a>, which made a list of all the locations in North America contained
in a database by repeatedly following edges indicating which location is within which other location
(this kind of algorithm is called a <em>transitive closure</em>).</p>

<p><a data-type="indexterm" data-primary="iterative processing" id="ix_iterate"></a>
It is possible to store a graph in a distributed filesystem (in files containing lists of vertices
and edges), but this idea of “repeating until done” cannot be expressed in plain MapReduce, since it
only performs a single pass over the data. This kind of algorithm is thus often implemented in an
<em>iterative</em> style:</p>
<ol>
<li>
<p>An external scheduler runs a batch process to calculate one step of the algorithm.</p>
</li>
<li>
<p>When the batch process completes, the scheduler checks whether it has finished (based on the
completion condition—e.g., there are no more edges to follow, or the change compared to the last
iteration is below some threshold).</p>
</li>
<li>
<p>If it has not yet finished, the scheduler goes back to step 1 and runs another round of the batch
process.</p>
</li>

</ol>

<p>This approach works, but implementing it with MapReduce is often very inefficient, because MapReduce
does not account for the iterative nature of the algorithm: it will always read the entire input
dataset and produce a completely new output dataset, even if only a small part of the graph has
changed compared to the last iteration.</p>










<section data-type="sect3" data-pdf-bookmark="The Pregel processing model"><div class="sect3" id="idm140417548352048">
<h3>The Pregel processing model</h3>

<p><a data-type="indexterm" data-primary="Application Programming Interfaces (APIs)" data-secondary="for graph processing" id="idm140417548350704"></a>
<a data-type="indexterm" data-primary="Giraph (graph processing)" id="idm140417548349168"></a><a data-type="indexterm" data-primary="Apache Giraph" data-see="Giraph" id="idm140417548348400"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="GraphX API (graph processing)" id="idm140417548347328"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="Gelly API (graph processing)" id="idm140417548346192"></a>
<a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-tertiary="Pregel processing model" id="idm140417548345056"></a>
<a data-type="indexterm" data-primary="Pregel processing model" id="idm140417548343680"></a>
<a data-type="indexterm" data-primary="bulk synchronous parallel (BSP) model" id="idm140417548342848"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="Pregel (graph processing)" id="idm140417548342000"></a>
As an optimization for batch processing graphs, the <em>bulk synchronous parallel</em> (BSP) model of
computation [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Valiant1990ce-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Valiant1990ce">70</a>]
has become popular. Among others, it is implemented by Apache Giraph
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl">37</a>], Spark’s GraphX API, and Flink’s Gelly API
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Ewen2012cm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ewen2012cm">71</a>].
It is also known as the <em>Pregel</em> model, as Google’s Pregel paper popularized this approach for
processing graphs [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Malewicz2010jq-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Malewicz2010jq">72</a>].</p>

<p>Recall that in MapReduce, mappers conceptually “send a message” to a particular call of the reducer
because the framework collects together all the mapper outputs with the same key. A similar idea is
behind Pregel: one vertex can “send a message” to another vertex, and typically those messages are
sent along the edges in a graph.</p>

<p>In each iteration, a function is called for each vertex, passing it all the messages that were sent
to it—much like a call to the reducer. The difference from MapReduce is that in the Pregel
model, a vertex remembers its state in memory from one iteration to the next, so the function only
needs to process new incoming messages. If no messages are being sent in some part of the graph, no
work needs to be done.</p>

<p><a data-type="indexterm" data-primary="actor model" data-secondary="comparison to Pregel model" id="idm140417548328800"></a>
It’s a bit similar to the actor model (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch04.html#sec_encoding_actors">“Distributed actor frameworks”</a>), if you think of each vertex as
an actor, except that vertex state and messages between vertices are fault-tolerant and durable, and
communication proceeds in fixed rounds: at every iteration, the framework delivers all messages sent
in the previous iteration. Actors normally have no such timing guarantee.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Fault tolerance"><div class="sect3" id="idm140417548326400">
<h3>Fault tolerance</h3>

<p><a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-tertiary="fault tolerance" id="idm140417548325264"></a>
<a data-type="indexterm" data-primary="fault tolerance" data-secondary="in batch processing" id="idm140417548323888"></a>
The fact that vertices can only communicate by message passing (not by querying each other directly)
helps improve the performance of Pregel jobs, since messages can be batched and there is less
waiting for communication. The only waiting is between iterations: since the Pregel model guarantees
that all messages sent in one iteration are delivered in the next iteration, the prior iteration
must completely finish, and all of its messages must be copied over the network, before the next one
can start.</p>

<p>Even though the underlying network may drop, duplicate, or arbitrarily delay messages (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch08.html#sec_distributed_networks">“Unreliable Networks”</a>), Pregel implementations guarantee that messages are processed exactly
once at their destination vertex in the following iteration. Like MapReduce, the framework
transparently recovers from faults in order to simplify the programming model for algorithms on top
of Pregel.</p>

<p><a data-type="indexterm" data-primary="deterministic operations" data-secondary="and fault tolerance" id="idm140417548320368"></a>
<a data-type="indexterm" data-primary="checkpointing" data-secondary="in batch processors" id="idm140417548319248"></a>
This fault tolerance is achieved by periodically checkpointing the state of all vertices at the end
of an iteration—i.e., writing their full state to durable storage. If a node fails and its in-memory
state is lost, the simplest solution is to roll back the entire graph computation to the last
checkpoint and restart the computation. If the algorithm is deterministic and messages are
logged, it is also possible to selectively recover only the partition that was lost (like we
previously discussed for dataflow engines) [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Malewicz2010jq">72</a>].</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Parallel execution"><div class="sect3" id="idm140417548316544">
<h3>Parallel execution</h3>

<p><a data-type="indexterm" data-primary="parallel execution" data-secondary="of graph analysis algorithms" id="idm140417548315168"></a>
A vertex does not need to know on which physical machine it is executing; when it sends messages to
other vertices, it simply sends them to a vertex ID. It is up to the framework to partition the
graph—i.e., to decide which vertex runs on which machine, and how to route messages over the
network so that they end up in the right place.</p>

<p>Because the programming model deals with just one vertex at a time (sometimes called “thinking like a
vertex”), the framework may partition the graph in arbitrary ways. Ideally it would be partitioned
such that vertices are colocated on the same machine if they need to communicate a lot. However,
finding such an optimized partitioning is hard—in practice, the graph is often simply partitioned
by an arbitrarily assigned vertex ID, making no attempt to group related vertices together.</p>

<p>As a result, graph algorithms often have a lot of cross-machine communication overhead, and the
intermediate state (messages sent between nodes) is often bigger than the original graph. The
overhead of sending messages over the network can significantly slow down distributed graph
algorithms.</p>

<p><a data-type="indexterm" data-primary="single-threaded execution" data-secondary="in batch processing" id="idm140417548311792"></a>
<a data-type="indexterm" data-primary="GraphChi (graph processing)" id="idm140417548310496"></a>
For this reason, if your graph can fit in memory on a single computer, it’s quite likely that a
single-machine (maybe even single-threaded) algorithm will outperform a distributed batch process
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="McSherry2015vx_ch10-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#McSherry2015vx_ch10">73</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Gog2015et-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gog2015et">74</a>].
Even if the graph is bigger than memory, it can fit on the disks of a single computer,
single-machine processing using a framework such as GraphChi is a viable option
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Kyrola2012uo-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kyrola2012uo">75</a>].
If the graph is too big to fit on a single machine, a distributed approach such as Pregel is
unavoidable; efficiently parallelizing graph algorithms is an area of ongoing research
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Lenharth2016je-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Lenharth2016je">76</a>].
<a data-type="indexterm" data-primary="iterative processing" data-startref="ix_iterate" id="idm140417548298656"></a>
<a data-type="indexterm" data-primary="batch processing" data-secondary="graphs and iterative processing" data-startref="ix_batchbeygraph" id="idm140417548297552"></a>
<a data-type="indexterm" data-primary="graphs" data-secondary="processing and analysis" data-startref="ix_graph" id="idm140417548296160"></a></p>
</div></section>



</div></section>













<section data-type="sect2" data-pdf-bookmark="High-Level APIs and Languages"><div class="sect2" id="sec_batch_highlevel">
<h2>High-Level APIs and Languages</h2>

<p><a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" id="ix_batchbeyAPI"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="higher-level tools" id="idm140417548292160"></a>
Over the years since MapReduce first became popular, the execution engines for distributed batch
processing have matured. By now, the infrastructure has become robust enough to store and process
many petabytes of data on clusters of over 10,000 machines. As the problem of physically operating
batch processes at such scale has been considered more or less solved, attention has turned to other
areas: improving the programming model, improving the efficiency of processing, and broadening the
set of problems that these technologies can solve.</p>

<p><a data-type="indexterm" data-primary="Hive (query engine)" id="idm140417548290224"></a>
<a data-type="indexterm" data-primary="Pig (dataflow language)" id="idm140417548289168"></a>
<a data-type="indexterm" data-primary="Cascading (batch processing)" id="idm140417548288336"></a>
<a data-type="indexterm" data-primary="Crunch (batch processing)" id="idm140417548287536"></a>
<a data-type="indexterm" data-primary="Tez (dataflow engine)" data-secondary="support by higher-level tools" id="idm140417548286688"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="dataflow APIs" id="idm140417548285568"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="dataflow APIs" id="idm140417548284448"></a>
<a data-type="indexterm" data-primary="Google" data-secondary="FlumeJava (dataflow library)" id="idm140417548283328"></a>
<a data-type="indexterm" data-primary="FlumeJava (dataflow library)" id="idm140417548282208"></a>
As discussed previously, higher-level languages and APIs such as Hive, Pig, Cascading, and Crunch
became popular because programming MapReduce jobs by hand is quite laborious. As Tez emerged, these
high-level languages had the additional benefit of being able to move to the new dataflow execution
engine without the need to rewrite job code. Spark and Flink also include their own high-level
dataflow APIs, often taking inspiration from FlumeJava
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Chambers2010dp">34</a>].</p>

<p>These dataflow APIs generally use relational-style building blocks to express a computation: joining
datasets on the value of some field; grouping tuples by key; filtering by some condition; and
aggregating tuples by counting, summing, or other functions. Internally, these operations are
implemented using the various join and grouping algorithms that we discussed earlier in this
chapter.</p>

<p>Besides the obvious advantage of requiring less code, these high-level interfaces also allow
interactive use, in which you write analysis code incrementally in a shell and run it frequently to
observe what it is doing. This style of development is very helpful when exploring a dataset and
experimenting with approaches for processing it. It is also reminiscent of the Unix philosophy,
which we discussed in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_unix_philosophy">“The Unix Philosophy”</a>.</p>

<p>Moreover, these high-level interfaces not only make the humans using the system more productive, but
they also improve the job execution efficiency at a machine level.</p>










<section data-type="sect3" data-pdf-bookmark="The move toward declarative query languages"><div class="sect3" id="idm140417548277040">
<h3>The move toward declarative query languages</h3>

<p><a data-type="indexterm" data-primary="Hive (query engine)" data-secondary="query optimizer" id="idm140417548275632"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="query optimizer" id="idm140417548274304"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="query optimizer" id="idm140417548273232"></a>
<a data-type="indexterm" data-primary="joins" data-secondary="expressing as relational operators" id="idm140417548272112"></a>
<a data-type="indexterm" data-primary="query optimizers" id="idm140417548270992"></a>
An advantage of specifying joins as relational operators, compared to spelling out the code that
performs the join, is that the framework can analyze the properties of the join inputs and
automatically decide which of the aforementioned join algorithms would be most suitable for the task
at hand. Hive, Spark, and Flink have cost-based query optimizers that can do this, and even change
the order of joins so that the amount of intermediate state is minimized
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Alexandrov2014jb">66</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Huske2015vm-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Huske2015vm">77</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Mokhtar2015vg-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Mokhtar2015vg">78</a>,
<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Armbrust2015dy-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Armbrust2015dy">79</a>].</p>

<p><a data-type="indexterm" data-primary="dataflow engines" data-secondary="support for declarative queries" id="idm140417548261264"></a>
<a data-type="indexterm" data-primary="declarative languages" data-secondary="for batch processing" id="idm140417548260144"></a>
The choice of join algorithm can make a big difference to the performance of a batch job, and it is
nice not to have to understand and remember all the various join algorithms we discussed in this
chapter. This is possible if joins are specified in a <em>declarative</em> way: the application simply
states which joins are required, and the query optimizer decides how they can best be executed.
We previously came across this idea in <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch02.html#sec_datamodels_query">“Query Languages for Data”</a>.</p>

<p>However, in other ways, MapReduce and its dataflow successors are very different from the fully
declarative query model of SQL. MapReduce was built around the idea of function callbacks: for each
record or group of records, a user-defined function (the mapper or reducer) is called, and that
function is free to call arbitrary code in order to decide what to output. This approach has the
advantage that you can draw upon a large ecosystem of existing libraries to do things like parsing,
natural language analysis, image analysis, and running numerical or statistical algorithms.</p>

<p><a data-type="indexterm" data-primary="massively parallel processing (MPP)" data-secondary="comparison to Hadoop" id="idm140417548256080"></a>
<a data-type="indexterm" data-primary="package managers" id="idm140417548254784"></a>
<a data-type="indexterm" data-primary="Maven (Java build tool)" id="idm140417548253952"></a>
<a data-type="indexterm" data-primary="npm (package manager)" id="idm140417548253120"></a>
<a data-type="indexterm" data-primary="Rubygems (package manager)" id="idm140417548252288"></a>
The freedom to easily run arbitrary code is what has long distinguished batch processing systems of
MapReduce heritage from MPP databases (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#sec_batch_mr_vs_db">“Comparing Hadoop to Distributed Databases”</a>); although databases have
facilities for writing user-defined functions, they are often cumbersome to use and not well
integrated with the package managers and dependency management systems that are widely used in most
programming languages (such as Maven for Java, npm for JavaScript, and Rubygems for Ruby).</p>

<p><a data-type="indexterm" data-primary="CPUs" data-secondary="caching and pipelining" id="idm140417548250032"></a>
<a data-type="indexterm" data-primary="caches" data-secondary="in CPUs" id="idm140417548248624"></a>
<a data-type="indexterm" data-primary="vectorized processing" id="idm140417548247520"></a>
<a data-type="indexterm" data-primary="column-oriented storage" data-secondary="in batch processors" id="idm140417548246688"></a>
<a data-type="indexterm" data-primary="column-oriented storage" data-secondary="vectorized processing" id="idm140417548245584"></a>
<a data-type="indexterm" data-primary="Java Virtual Machine (JVM)" data-secondary="bytecode generation" id="idm140417548244480"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="bytecode generation" id="idm140417548243360"></a>
<a data-type="indexterm" data-primary="Impala (query engine)" data-secondary="native code generation" id="idm140417548242240"></a>
However, dataflow engines have found that there are also advantages to incorporating more
declarative features in areas besides joins. For example, if a callback function contains only a
simple filtering condition, or it just selects some fields from a record, then there is significant
CPU overhead in calling the function on every record. If such simple filtering and mapping
operations are expressed in a declarative way, the query optimizer can take advantage of
column-oriented storage layouts (see <a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_column">“Column-Oriented Storage”</a>) and read only the required columns from
disk. Hive, Spark DataFrames, and Impala also use vectorized execution (see
<a data-type="xref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch03.html#sec_storage_vectorized">“Memory bandwidth and vectorized processing”</a>): iterating over data in a tight inner loop that is friendly to CPU
caches, and avoiding function calls. Spark generates JVM bytecode
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Armbrust2015dy">79</a>] and Impala uses LLVM to generate native
code for these inner loops [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kornacker2015uv_ch10">41</a>].</p>

<p>By incorporating declarative aspects in their high-level APIs, and having query optimizers that can
take advantage of them during execution, batch processing frameworks begin to look more like MPP
databases (and can achieve comparable performance). At the same time, by having the extensibility of
being able to run arbitrary code and read data in arbitrary formats, they retain their flexibility
advantage.</p>
</div></section>













<section data-type="sect3" data-pdf-bookmark="Specialization for different domains"><div class="sect3" id="idm140417548236368">
<h3>Specialization for different domains</h3>

<p>While the extensibility of being able to run arbitrary code is useful, there are also many common
cases where standard processing patterns keep reoccurring, and so it is worth having reusable
implementations of the common building blocks. Traditionally, MPP databases have served the needs of
business intelligence analysts and business reporting, but that is just one among many domains in
which batch processing is used.</p>

<p><a data-type="indexterm" data-primary="statistical and numerical algorithms" id="idm140417548234176"></a>
<a data-type="indexterm" data-primary="machine learning" data-secondary="statistical and numerical algorithms" id="idm140417548233008"></a>
<a data-type="indexterm" data-primary="recommendation engines" data-secondary="statistical and numerical algorithms" id="idm140417548231920"></a>
<a data-type="indexterm" data-primary="Mahout (machine learning toolkit)" id="idm140417548230800"></a><a data-type="indexterm" data-primary="Apache Mahout" data-see="Mahout" id="idm140417548230080"></a>
<a data-type="indexterm" data-primary="MADlib (machine learning toolkit)" id="idm140417548229008"></a><a data-type="indexterm" data-primary="Apache MADlib" data-see="MADlib" id="idm140417548228288"></a>
<a data-type="indexterm" data-primary="HAWQ (database)" id="idm140417548227312"></a><a data-type="indexterm" data-primary="Apache HAWQ" data-see="HAWQ" id="idm140417548226704"></a>
<a data-type="indexterm" data-primary="MapReduce (batch processing)" data-secondary="machine learning" id="idm140417548225696"></a>
<a data-type="indexterm" data-primary="Spark (processing framework)" data-secondary="machine learning" id="idm140417548224448"></a>
<a data-type="indexterm" data-primary="Flink (processing framework)" data-secondary="machine learning" id="idm140417548223328"></a>
Another domain of increasing importance is statistical and numerical algorithms, which are needed
for machine learning applications such as classification and recommendation systems. Reusable
implementations are emerging: for example, Mahout implements various algorithms for machine learning
on top of MapReduce, Spark, and Flink, while MADlib implements similar functionality inside a
relational MPP database (Apache HAWQ) [<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cohen2009fv">54</a>].</p>

<p><a data-type="indexterm" data-primary="spatial algorithms" id="idm140417548220960"></a>
<a data-type="indexterm" data-primary="k-nearest neighbors" id="idm140417548219904"></a>
<a data-type="indexterm" data-primary="genome analysis" id="idm140417548219072"></a>
<a data-type="indexterm" data-primary="searches" data-secondary="k-nearest neighbors" id="idm140417548218240"></a>
<a data-type="indexterm" data-primary="similarity search" data-secondary="k-nearest neighbors" id="idm140417548217136"></a>
Also useful are spatial algorithms such as <em>k-nearest neighbors</em>
[<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="Blazevski2016ve-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Blazevski2016ve">80</a>], which searches for items
that are close to a given item in some multi-dimensional space—a kind of similarity search.
Approximate search is also important for genome analysis algorithms, which need to find strings that
are similar but not identical [<a xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="noteref" id="White2016ua-marker" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2016ua">81</a>].</p>

<p>Batch processing engines are being used for distributed execution of algorithms from an increasingly
wide range of domains. As batch processing systems gain built-in functionality and high-level
declarative operators, and as MPP databases become more programmable and flexible, the two are
beginning to look more alike: in the end, they are all just systems for storing and processing data.
<a data-type="indexterm" data-primary="batch processing" data-secondary="high-level APIs and languages" data-startref="ix_batchbeyAPI" id="idm140417548210064"></a></p>
</div></section>



</div></section>





</div></section>













<section data-type="sect1" data-pdf-bookmark="Summary"><div class="sect1" id="idm140417548501968">
<h1>Summary</h1>

<p>In this chapter we explored the topic of batch processing. We started by looking at Unix tools such
as <code>awk</code>, <code>grep</code>, and <code>sort</code>, and we saw how the design philosophy of those tools is carried forward
into MapReduce and more recent dataflow engines. Some of those design principles are that inputs are
immutable, outputs are intended to become the input to another (as yet unknown) program, and complex
problems are solved by composing small tools that “do one thing well.”</p>

<p>In the Unix world, the uniform interface that allows one program to be composed with another is
files and pipes; in MapReduce, that interface is a distributed filesystem. We saw that dataflow
engines add their own pipe-like data transport mechanisms to avoid materializing intermediate state
to the distributed filesystem, but the initial input and final output of a job is still usually
HDFS.</p>

<p><a data-type="indexterm" data-primary="partitioning" data-secondary="in batch processing" id="idm140417548204288"></a>
The two main problems that distributed batch processing frameworks need to solve are:</p>
<dl>
<dt>Partitioning</dt>
<dd>
<p>In MapReduce, mappers are partitioned according to input file blocks. The output of mappers is
repartitioned, sorted, and merged into a configurable number of reducer partitions. The purpose of
this process is to bring all the related data—e.g., all the records with the same key—together in
the same place.</p>

<p><a data-type="indexterm" data-primary="dataflow engines" data-secondary="partitioning, approach to" id="idm140417548200912"></a>
Post-MapReduce dataflow engines try to avoid sorting unless it is required, but they otherwise take
a broadly similar approach to partitioning.</p>
</dd>
<dt>Fault tolerance</dt>
<dd>
<p>MapReduce frequently writes to disk, which makes it easy to recover from an individual
failed task without restarting the entire job but slows down execution in the failure-free
case. Dataflow engines perform less materialization of intermediate state and keep more in memory,
which means that they need to recompute more data if a node fails. Deterministic operators reduce
the amount of data that needs to be recomputed.</p>
</dd>
</dl>

<p>We discussed several join algorithms for MapReduce, most of which are also internally used in MPP
databases and dataflow engines. They also provide a good illustration of how partitioned algorithms
work:</p>
<dl>
<dt>Sort-merge joins</dt>
<dd>
<p>Each of the inputs being joined goes through a mapper that extracts the join key. By partitioning,
sorting, and merging, all the records with the same key end up going to the same call of the
reducer. This function can then output the joined records.</p>
</dd>
<dt>Broadcast hash joins</dt>
<dd>
<p>One of the two join inputs is small, so it is not partitioned and it can be entirely loaded into a
hash table. Thus, you can start a mapper for each partition of the large join input, load the hash
table for the small input into each mapper, and then scan over the large input one record at a
time, querying the hash table for each record.</p>
</dd>
<dt>Partitioned hash joins</dt>
<dd>
<p>If the two join inputs are partitioned in the same way (using the same key, same hash function,
and same number of partitions), then the hash table approach can be used independently for each
partition.</p>
</dd>
</dl>

<p>Distributed batch processing engines have a deliberately restricted programming model: callback
functions (such as mappers and reducers) are assumed to be stateless and to have no externally
visible side effects besides their designated output. This restriction allows the framework to hide
some of the hard distributed systems problems behind its abstraction: in the face of crashes and
network issues, tasks can be retried safely, and the output from any failed tasks is discarded. If
several tasks for a partition succeed, only one of them actually makes its output visible.</p>

<p>Thanks to the framework, your code in a batch processing job does not need to worry about
implementing fault-tolerance mechanisms: the framework can guarantee that the final output of a job
is the same as if no faults had occurred, even though in reality various tasks perhaps had to be
retried. These reliable semantics are much stronger than what you usually have in online services
that handle user requests and that write to databases as a side effect of processing a request.</p>

<p><a data-type="indexterm" data-primary="bounded datasets" id="idm140417548189536"></a>
The distinguishing feature of a batch processing job is that it reads some input data and produces
some output data, without modifying the input—in other words, the output is derived from the
input. Crucially, the input data is <em>bounded</em>: it has a known, fixed size (for example, it
consists of a set of log files at some point in time, or a snapshot of a database’s contents).
Because it is bounded, a job knows when it has finished reading the entire input, and so a job
eventually completes when it is done.</p>

<p>In the next chapter, we will turn to stream processing, in which the input is <em>unbounded</em>—that
is, you still have a job, but its inputs are never-ending streams of data. In this case, a job is
never complete, because at any time there may still be more work coming in. We shall see that stream
and batch processing are similar in some respects, but the assumption of unbounded streams also changes
a lot about how we build systems.
<a data-type="indexterm" data-primary="batch processing" data-startref="ix_batch" id="idm140417548186592"></a></p>
</div></section>







<div data-type="footnotes"><h5>Footnotes</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417549440016"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549440016-marker" class="totri-footnote">i</a></sup> Some people love to point out that
<code>cat</code> is unnecessary here, as the input file could be given directly as an argument to
<code>awk</code>. However, the linear pipeline is more apparent when written like this.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417549121440"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549121440-marker">ii</a></sup> Another example of a
uniform interface is URLs and HTTP, the foundations of the web. A URL identifies a particular thing
(resource) on a website, and you can link to any URL from any other website. A user with a web
browser can thus seamlessly jump between websites by following links, even though the servers may be
operated by entirely unrelated organizations. This principle seems obvious today, but it was a key insight
in making the web the success that it is today. Prior systems were not so uniform: for example,
in the era of bulletin board systems (BBSs), each system had its own phone number and baud rate
configuration. A reference from one BBS to another would have to be in the form of a phone number
and modem settings; the user would have to hang up, dial the other BBS, and then manually find the
information they were looking for. It wasn’t possible to link directly to some piece of content
inside another BBS.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417549081328"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549081328-marker">iii</a></sup> Except by
using a separate tool, such as <code>netcat</code> or <code>curl</code>. Unix started out trying to
represent everything as files, but the BSD sockets API deviated from that convention
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DJBTwoFD">17</a>]. The research operating systems <em>Plan
9</em> and <em>Inferno</em> are more consistent in their use of files: they represent a TCP
connection as a file in <code>/net/tcp</code>
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Pike1999ui">18</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417549040384"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417549040384-marker">iv</a></sup> One difference is that
with HDFS, computing tasks can be scheduled to run on the machine that stores a copy of a particular
file, whereas object stores usually keep storage and computation separate. Reading from a local disk
has a performance advantage if network bandwidth is a bottleneck. Note however that if erasure coding is
used, the locality advantage is lost, because the data from several machines must be combined in
order to reconstitute the original file
[<a data-type="noteref" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ovsiannikov2013da">20</a>].</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417548893984"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417548893984-marker" class="totri-footnote">v</a></sup> The joins we talk about
in this book are generally <em>equi-joins</em>, the most common type of join, in which a record is
associated with other records that have <em>an identical value</em> in a particular field (such as
an ID). Some databases support more general types of joins, for example using a less-than operator
instead of an equality operator, but we do not have space to cover them here.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="idm140417548774384"><sup><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#idm140417548774384-marker">vi</a></sup> This example assumes that there is
exactly one entry for each key in the hash table, which is probably true with a user database (a
user ID uniquely identifies a user). In general, the hash table may need to contain several entries
with the same key, and the join operator will output all matches for a key.</p></div><div data-type="footnotes"><h5>References</h5><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Dean2004ua_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Dean2004ua_ch10-marker" class="totri-footnote">1</a>] Jeffrey Dean and Sanjay Ghemawat:
“<a href="http://research.google.com/archive/mapreduce.html">MapReduce: Simplified Data
Processing on Large Clusters</a>,” at <em>6th USENIX Symposium on Operating System Design
and Implementation</em> (OSDI), December 2004.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Spolsky2005wm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Spolsky2005wm-marker" class="totri-footnote">2</a>] Joel Spolsky:
“<a href="http://www.joelonsoftware.com/articles/ThePerilsofJavaSchools.html">The Perils of
JavaSchools</a>,” <em>joelonsoftware.com</em>, December 25, 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Babu2013gm_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Babu2013gm_ch10-marker" class="totri-footnote">3</a>] Shivnath Babu and Herodotos Herodotou:
“<a href="http://research.microsoft.com/pubs/206464/db-mr-survey-final.pdf">Massively Parallel
Databases and MapReduce Systems</a>,” <em>Foundations and Trends in Databases</em>,
volume 5, number 1, pages 1–104, November 2013.
<a href="http://dx.doi.org/10.1561/1900000036">doi:10.1561/1900000036</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt2008up">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt2008up-marker" class="totri-footnote">4</a>] David J. DeWitt and Michael Stonebraker:
“<a href="https://homes.cs.washington.edu/~billhowe/mapreduce_a_major_step_backwards.html">MapReduce: A
Major Step Backwards</a>,” originally published at <em>databasecolumn.vertica.com</em>, January 17, 2008.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Robinson2014vz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Robinson2014vz-marker" class="totri-footnote">5</a>] Henry Robinson:
“<a href="http://the-paper-trail.org/blog/the-elephant-was-a-trojan-horse-on-the-death-of-map-reduce-at-google/">The
Elephant Was a Trojan Horse: On the Death of Map-Reduce at Google</a>,”
<em>the-paper-trail.org</em>, June 25, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Hollerith">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Hollerith-marker" class="totri-footnote">6</a>] “<a href="https://www.census.gov/history/www/innovations/technology/the_hollerith_tabulator.html">The
Hollerith Machine</a>,” United States Census Bureau, <em>census.gov</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="IBM1962vz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#IBM1962vz-marker" class="totri-footnote">7</a>] “<a href="http://www.textfiles.com/bitsavers/pdf/ibm/punchedCard/Sorter/A24-1034-1_82-83-84_sorters.pdf">IBM
82, 83, and 84 Sorters Reference Manual</a>,” Edition A24-1034-1, International Business
Machines Corporation, July 1962.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Drake2014vm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Drake2014vm-marker" class="totri-footnote">8</a>] Adam Drake:
“<a href="http://aadrake.com/command-line-tools-can-be-235x-faster-than-your-hadoop-cluster.html">Command-Line
Tools Can Be 235x Faster than Your Hadoop Cluster</a>,” <em>aadrake.com</em>, January 25, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="GNUCoreutils">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#GNUCoreutils-marker" class="totri-footnote">9</a>] “<a href="http://www.gnu.org/software/coreutils/manual/html_node/index.html">GNU
Coreutils 8.23 Documentation</a>,” Free Software Foundation, Inc., 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2015tz_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kleppmann2015tz_ch10-marker">10</a>] Martin Kleppmann:
“<a href="http://martin.kleppmann.com/2015/08/05/kafka-samza-unix-philosophy-distributed-data.html">Kafka,
Samza, and the Unix Philosophy of Distributed Data</a>,” <em>martin.kleppmann.com</em>, August 5, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="RichieMcIlroy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#RichieMcIlroy-marker">11</a>] Doug McIlroy:
<a href="http://cm.bell-labs.com/cm/cs/who/dmr/mdmpipe.pdf">Internal Bell Labs memo</a>,
October 1964. Cited in: Dennis M. Richie:
“<a href="https://www.bell-labs.com/usr/dmr/www/mdmpipe.html">Advice from Doug McIlroy</a>,”
<em>cm.bell-labs.com</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McIlroy1978te">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#McIlroy1978te-marker">12</a>] M. D. McIlroy, E. N. Pinson, and B. A. Tague:
“<a href="https://archive.org/details/bstj57-6-1899">UNIX Time-Sharing System: Foreword</a>,”
<em>The Bell System Technical Journal</em>, volume 57, number 6, pages 1899–1904,
July 1978.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Raymond2003wn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Raymond2003wn-marker">13</a>] Eric S. Raymond:
<a href="http://www.catb.org/~esr/writings/taoup/html/"><em>The Art of UNIX Programming</em></a>.
Addison-Wesley, 2003. ISBN: 978-0-13-142901-7</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Duncan2009ts">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Duncan2009ts-marker">14</a>] Ronald Duncan:
“<a href="https://ronaldduncan.wordpress.com/2009/10/31/text-file-formats-ascii-delimited-text-not-csv-or-tab-delimited-text/">Text
File Formats – ASCII Delimited Text – Not CSV or TAB Delimited Text</a>,”
<em>ronaldduncan.wordpress.com</em>, October 31, 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="KayOxymoron">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#KayOxymoron-marker">15</a>] Alan Kay:
“<a href="http://tinlizzie.org/~takashi/IsSoftwareEngineeringAnOxymoron.pdf">Is ‘Software
Engineering’ an Oxymoron?</a>,” <em>tinlizzie.org</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Fowler2005tp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Fowler2005tp-marker">16</a>] Martin Fowler:
“<a href="http://martinfowler.com/bliki/InversionOfControl.html">InversionOfControl</a>,”
<em>martinfowler.com</em>, June 26, 2005.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DJBTwoFD">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DJBTwoFD-marker">17</a>] Daniel J. Bernstein:
“<a href="http://cr.yp.to/tcpip/twofd.html">Two File Descriptors for Sockets</a>,” <em>cr.yp.to</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Pike1999ui">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Pike1999ui-marker">18</a>] Rob Pike and Dennis M. Ritchie:
“<a href="http://doc.cat-v.org/inferno/4th_edition/styx">The Styx Architecture for Distributed
Systems</a>,” <em>Bell Labs Technical Journal</em>, volume 4, number 2, pages
146–152, April 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ghemawat2003dy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ghemawat2003dy-marker">19</a>] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak
Leung: “<a href="http://research.google.com/archive/gfs-sosp2003.pdf">The Google File System</a>,”
at <em>19th ACM Symposium on Operating Systems Principles</em> (SOSP), October 2003.
<a href="http://dx.doi.org/10.1145/945445.945450">doi:10.1145/945445.945450</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ovsiannikov2013da">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ovsiannikov2013da-marker">20</a>] Michael Ovsiannikov, Silvius Rus, Damian Reeves, et al.:
“<a href="http://db.disi.unitn.eu/pages/VLDBProgram/pdf/industry/p808-ovsiannikov.pdf">The Quantcast File
System</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 6, number 11, pages 1092–1101, August 2013.
<a href="http://dx.doi.org/10.14778/2536222.2536234">doi:10.14778/2536222.2536234</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="OpenStackSwift">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#OpenStackSwift-marker">21</a>] “<a href="http://docs.openstack.org/developer/swift/">OpenStack
Swift 2.6.1 Developer Documentation</a>,” OpenStack Foundation, <em>docs.openstack.org</em>, March 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zhang2015vi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Zhang2015vi-marker">22</a>] Zhe Zhang, Andrew Wang, Kai Zheng, et al.:
“<a href="http://blog.cloudera.com/blog/2015/09/introduction-to-hdfs-erasure-coding-in-apache-hadoop/">Introduction
to HDFS Erasure Coding in Apache Hadoop</a>,” <em>blog.cloudera.com</em>, September 23, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cnudde2016tm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cnudde2016tm-marker">23</a>] Peter Cnudde:
“<a href="http://yahoohadoop.tumblr.com/post/138739227316/hadoop-turns-10">Hadoop Turns 10</a>,”
<em>yahoohadoop.tumblr.com</em>, February 5, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Baldeschwieler2012ue">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Baldeschwieler2012ue-marker">24</a>] Eric Baldeschwieler:
“<a href="http://hortonworks.com/blog/thinking-about-the-hdfs-vs-other-storage-technologies/">Thinking
About the HDFS vs. Other Storage Technologies</a>,” <em>hortonworks.com</em>, July 25, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gregg2013wz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gregg2013wz-marker">25</a>] Brendan Gregg:
“<a href="http://dtrace.org/blogs/brendan/2013/06/25/manta-unix-meets-map-reduce/">Manta: Unix Meets
Map Reduce</a>,” <em>dtrace.org</em>, June 25, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="White2015vl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2015vl-marker">26</a>] Tom White: <em>Hadoop: The Definitive Guide</em>,
4th edition. O’Reilly Media, 2015. ISBN: 978-1-491-90163-2</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gray2003vx">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gray2003vx-marker">27</a>] Jim N. Gray:
“<a href="http://arxiv.org/pdf/cs/0403019.pdf">Distributed Computing Economics</a>,” Microsoft
Research Tech Report MSR-TR-2003-24, March 2003.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Trencseni2016vn">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Trencseni2016vn-marker">28</a>] Márton Trencséni:
“<a href="http://bytepawn.com/luigi-airflow-pinball.html">Luigi vs Airflow vs Pinball</a>,”
<em>bytepawn.com</em>, February 6, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sumbaly2013eh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2013eh-marker">29</a>] Roshan Sumbaly, Jay Kreps, and Sam Shah:
“<a href="http://www.slideshare.net/s_shah/the-big-data-ecosystem-at-linkedin-23512853">The ‘Big
Data’ Ecosystem at LinkedIn</a>,” at <em>ACM International Conference on Management of Data</em>
(SIGMOD), July 2013.
<a href="http://dx.doi.org/10.1145/2463676.2463707">doi:10.1145/2463676.2463707</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gates2009vg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gates2009vg-marker">30</a>] Alan F. Gates, Olga Natkovich, Shubham Chopra, et al.:
“<a href="http://www.vldb.org/pvldb/2/vldb09-1074.pdf">Building a High-Level Dataflow System on Top
of Map-Reduce: The Pig Experience</a>,” at <em>35th International Conference on Very Large Data
Bases</em> (VLDB), August 2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Thusoo2010hp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Thusoo2010hp-marker">31</a>] Ashish Thusoo, Joydeep Sen Sarma, Namit Jain, et al.:
“<a href="http://i.stanford.edu/~ragho/hive-icde2010.pdf">Hive – A Petabyte Scale Data Warehouse
Using Hadoop</a>,” at <em>26th IEEE International Conference on Data Engineering</em> (ICDE), March 2010.
<a href="http://dx.doi.org/10.1109/ICDE.2010.5447738">doi:10.1109/ICDE.2010.5447738</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="CascadingDocs">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#CascadingDocs-marker">32</a>] “<a href="http://docs.cascading.org/cascading/3.0/userguide/">Cascading
3.0 User Guide</a>,” Concurrent, Inc., <em>docs.cascading.org</em>, January 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ApacheCrunch">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ApacheCrunch-marker">33</a>] “<a href="https://crunch.apache.org/user-guide.html">Apache
Crunch User Guide</a>,” Apache Software Foundation, <em>crunch.apache.org</em>.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Chambers2010dp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Chambers2010dp-marker">34</a>] Craig Chambers, Ashish Raniwala, Frances
Perry, et al.: “<a href="https://research.google.com/pubs/archive/35650.pdf">FlumeJava: Easy,
Efficient Data-Parallel Pipelines</a>,” at <em>31st ACM SIGPLAN Conference on Programming Language
Design and Implementation</em> (PLDI), June 2010.
<a href="http://dx.doi.org/10.1145/1806596.1806638">doi:10.1145/1806596.1806638</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2014wm_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kreps2014wm_ch10-marker">35</a>] Jay Kreps:
“<a href="https://www.oreilly.com/ideas/why-local-state-is-a-fundamental-primitive-in-stream-processing">Why
Local State is a Fundamental Primitive in Stream Processing</a>,” <em>oreilly.com</em>, July 31, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kleppmann2012ts">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kleppmann2012ts-marker">36</a>] Martin Kleppmann:
“<a href="http://martin.kleppmann.com/2012/10/01/rethinking-caching-in-web-apps.html">Rethinking
Caching in Web Apps</a>,” <em>martin.kleppmann.com</em>, October 1, 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Grover2015tl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Grover2015tl-marker">37</a>] Mark Grover, Ted Malaska, Jonathan
Seidman, and Gwen Shapira: <em><a href="http://shop.oreilly.com/product/0636920033196.do">Hadoop
Application Architectures</a></em>. O’Reilly Media, 2015. ISBN: 978-1-491-90004-8</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ajoux2015wh_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ajoux2015wh_ch10-marker">38</a>] Philippe Ajoux, Nathan Bronson,
Sanjeev Kumar, et al.:
“<a href="https://www.usenix.org/system/files/conference/hotos15/hotos15-paper-ajoux.pdf">Challenges
to Adopting Stronger Consistency at Scale</a>,” at <em>15th USENIX Workshop on Hot Topics in
Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Manjunath2009bh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Manjunath2009bh-marker">39</a>] Sriranjan Manjunath:
“<a href="https://wiki.apache.org/pig/PigSkewedJoinSpec">Skewed Join</a>,” <em>wiki.apache.org</em>,
2009.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt1992ws">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt1992ws-marker">40</a>] David J. DeWitt, Jeffrey F. Naughton, Donovan A.
Schneider, and S. Seshadri: “<a href="http://www.vldb.org/conf/1992/P027.PDF">Practical Skew Handling
in Parallel Joins</a>,” at <em>18th International Conference on Very Large Data Bases</em> (VLDB), August 1992.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kornacker2015uv_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kornacker2015uv_ch10-marker">41</a>] Marcel Kornacker, Alexander Behm, Victor
Bittorf, et al.: “<a href="http://pandis.net/resources/cidr15impala.pdf">Impala: A Modern,
Open-Source SQL Engine for Hadoop</a>,” at <em>7th Biennial Conference on Innovative Data Systems
Research</em> (CIDR), January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Monsch2015vb">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Monsch2015vb-marker">42</a>] Matthieu Monsch:
“<a href="https://engineering.linkedin.com/blog/2015/10/open-sourcing-paldb--a-lightweight-companion-for-storing-side-da">Open-Sourcing
PalDB, a Lightweight Companion for Storing Side Data</a>,” <em>engineering.linkedin.com</em>, October 26, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Peng2010ub">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Peng2010ub-marker">43</a>] Daniel Peng and Frank Dabek:
“<a href="https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf">Large-Scale
Incremental Processing Using Distributed Transactions and Notifications</a>,” at <em>9th USENIX
conference on Operating Systems Design and Implementation</em> (OSDI), October 2010.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ClouderaSearch">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ClouderaSearch-marker">44</a>] “<a href="http://www.cloudera.com/documentation/cdh/5-1-x/Search/Cloudera-Search-User-Guide/Cloudera-Search-User-Guide.html">“Cloudera
Search User Guide,”</a> Cloudera, Inc., September 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Wu2014tm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Wu2014tm-marker">45</a>] Lili Wu, Sam Shah, Sean Choi, et al.:
“<a href="http://ls13-www.cs.uni-dortmund.de/homepage/rsweb2014/papers/rsweb2014_submission_3.pdf">The
Browsemaps: Collaborative Filtering at LinkedIn</a>,” at <em>6th Workshop on Recommender Systems and
the Social Web</em> (RSWeb), October 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sumbaly2012wi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sumbaly2012wi-marker">46</a>] Roshan Sumbaly, Jay Kreps, Lei Gao, et al.:
“<a href="http://static.usenix.org/events/fast12/tech/full_papers/Sumbaly.pdf">Serving Large-Scale
Batch Computed Data with Project Voldemort</a>,” at <em>10th USENIX Conference on File and Storage
Technologies</em> (FAST), February 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Sharma2015tp">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Sharma2015tp-marker">47</a>] Varun Sharma:
“<a href="https://engineering.pinterest.com/blog/open-sourcing-terrapin-serving-system-batch-generated-data-0">Open-Sourcing
Terrapin: A Serving System for Batch Generated Data</a>,” <em>engineering.pinterest.com</em>, September 14, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="ElephantDB">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#ElephantDB-marker">48</a>] Nathan Marz:
“<a href="http://www.slideshare.net/nathanmarz/elephantdb">ElephantDB</a>,” <em>slideshare.net</em>, May 30, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cryans2013wo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cryans2013wo-marker">49</a>] Jean-Daniel (JD) Cryans:
“<a href="http://blog.cloudera.com/blog/2013/09/how-to-use-hbase-bulk-loading-and-why/">How-to: Use
HBase Bulk Loading, and Why</a>,” <em>blog.cloudera.com</em>, September 27, 2013.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Marz2011vq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Marz2011vq-marker">50</a>] Nathan Marz:
“<a href="http://nathanmarz.com/blog/how-to-beat-the-cap-theorem.html">How to Beat the CAP
Theorem</a>,” <em>nathanmarz.com</em>, October 13, 2011.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Bartlett2015wv_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Bartlett2015wv_ch10-marker">51</a>] Molly Bartlett Dishman and Martin Fowler:
“<a href="http://conferences.oreilly.com/software-architecture/sa2015/public/schedule/detail/40388">Agile
Architecture</a>,” at <em>O’Reilly Software Architecture Conference</em>, March 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="DeWitt1992fn_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#DeWitt1992fn_ch10-marker">52</a>] David J. DeWitt and Jim N. Gray:
“<a href="http://www.cs.cmu.edu/~pavlo/courses/fall2013/static/papers/dewittgray92.pdf">Parallel
Database Systems: The Future of High Performance Database Systems</a>,”
<em>Communications of the ACM</em>, volume 35, number 6, pages 85–98, June 1992.
<a href="http://dx.doi.org/10.1145/129888.129894">doi:10.1145/129888.129894</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kreps2014qz">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kreps2014qz-marker">53</a>] Jay Kreps:
“<a href="https://twitter.com/jaykreps/status/528235702480142336">But the multi-tenancy thing is
actually really really hard</a>,” tweetstorm, <em>twitter.com</em>, October 31, 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Cohen2009fv">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Cohen2009fv-marker">54</a>] Jeffrey Cohen, Brian Dolan, Mark Dunlap, et al.: “<a href="http://www.vldb.org/pvldb/2/vldb09-219.pdf">MAD Skills: New
Analysis Practices for Big Data</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 2, number
2, pages 1481–1492, August 2009.
<a href="http://dx.doi.org/10.14778/1687553.1687576">doi:10.14778/1687553.1687576</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Terrizzano2015tk">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Terrizzano2015tk-marker">55</a>] Ignacio
Terrizzano, Peter Schwarz, Mary Roth, and John E. Colino:
“<a href="http://cidrdb.org/cidr2015/Papers/CIDR15_Paper2.pdf">Data Wrangling: The Challenging
Journey from the Wild to the Lake</a>,” at <em>7th Biennial Conference on Innovative Data Systems
Research</em> (CIDR), January 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Roberts2015tl">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Roberts2015tl-marker">56</a>] Paige Roberts:
“<a href="http://adaptivesystemsinc.com/blog/to-schema-on-read-or-to-schema-on-write-that-is-the-hadoop-data-lake-question/">To
Schema on Read or to Schema on Write, That Is the Hadoop Data Lake Question</a>,” <em>adaptivesystemsinc.com</em>, July 2, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Johnson2015ua">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Johnson2015ua-marker">57</a>] Bobby Johnson and Joseph Adler:
“<a href="https://vimeo.com/123985284">The Sushi Principle: Raw Data Is Better</a>,” at
<em>Strata+Hadoop World</em>, February 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Vavilapalli2013eu">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Vavilapalli2013eu-marker">58</a>] Vinod Kumar Vavilapalli, Arun C. Murthy, Chris Douglas, et al.:
“<a href="http://www.socc2013.org/home/program/a5-vavilapalli.pdf">Apache Hadoop YARN: Yet Another
Resource Negotiator</a>,” at <em>4th ACM Symposium on Cloud Computing</em> (SoCC), October 2013.
<a href="http://dx.doi.org/10.1145/2523616.2523633">doi:10.1145/2523616.2523633</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Verma2015gi">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Verma2015gi-marker">59</a>] Abhishek Verma, Luis Pedrosa, Madhukar Korupolu, et al.:
“<a href="http://research.google.com/pubs/pub43438.html">Large-Scale Cluster Management at Google
with Borg</a>,” at <em>10th European Conference on Computer Systems</em> (EuroSys), April 2015.
<a href="http://dx.doi.org/10.1145/2741948.2741964">doi:10.1145/2741948.2741964</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Schwarzkopf2016un">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Schwarzkopf2016un-marker">60</a>] Malte Schwarzkopf:
“<a href="http://www.firmament.io/blog/scheduler-architectures.html">The Evolution of Cluster
Scheduler Architectures</a>,” <em>firmament.io</em>, March 9, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Zaharia2012ve">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Zaharia2012ve-marker">61</a>] Matei Zaharia, Mosharaf Chowdhury, Tathagata Das, et al.:
“<a href="https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf">Resilient
Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing</a>,” at <em>9th
USENIX Symposium on Networked Systems Design and Implementation</em> (NSDI), April 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Karau2015wf">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Karau2015wf-marker">62</a>] Holden Karau, Andy Konwinski, Patrick Wendell, and Matei
Zaharia: <em>Learning Spark</em>. O’Reilly Media, 2015. ISBN: 978-1-449-35904-1</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saha2014vd">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Saha2014vd-marker">63</a>] Bikas Saha and Hitesh Shah:
“<a href="http://www.slideshare.net/Hadoop_Summit/w-1205phall1saha">Apache Tez: Accelerating Hadoop
Query Processing</a>,” at <em>Hadoop Summit</em>, June 2014.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Saha2015dh">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Saha2015dh-marker">64</a>] Bikas Saha, Hitesh Shah, Siddharth Seth, et al.:
“<a href="http://home.cse.ust.hk/~weiwa/teaching/Fall15-COMP6611B/reading_list/Tez.pdf">Apache Tez:
A Unifying Framework for Modeling and Building Data Processing Applications</a>,” at <em>ACM
International Conference on Management of Data</em> (SIGMOD), June 2015.
<a href="http://dx.doi.org/10.1145/2723372.2742790">doi:10.1145/2723372.2742790</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Tzoumas2015ws">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Tzoumas2015ws-marker">65</a>] Kostas Tzoumas:
“<a href="http://www.slideshare.net/KostasTzoumas/apache-flink-api-runtime-and-project-roadmap">Apache
Flink: API, Runtime, and Project Roadmap</a>,” <em>slideshare.net</em>, January 14, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Alexandrov2014jb">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Alexandrov2014jb-marker">66</a>] Alexander Alexandrov, Rico Bergmann, Stephan Ewen, et al.:
“<a href="https://ssc.io/pdf/2014-VLDBJ_Stratosphere_Overview.pdf">The Stratosphere Platform for Big
Data Analytics</a>,” <em>The VLDB Journal</em>, volume 23, number 6, pages 939–964, May 2014.
<a href="http://dx.doi.org/10.1007/s00778-014-0357-y">doi:10.1007/s00778-014-0357-y</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Isard2007fe">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Isard2007fe-marker">67</a>] Michael Isard, Mihai Budiu, Yuan Yu, et al.:
“<a href="http://research.microsoft.com/en-us/projects/dryad/eurosys07.pdf">Dryad: Distributed
Data-Parallel Programs from Sequential Building Blocks</a>,” at <em>European Conference on Computer
Systems</em> (EuroSys), March 2007.
<a href="http://dx.doi.org/10.1145/1272996.1273005">doi:10.1145/1272996.1273005</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Warneke2009en">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Warneke2009en-marker">68</a>] Daniel Warneke and Odej Kao:
“<a href="https://stratosphere2.dima.tu-berlin.de/assets/papers/Nephele_09.pdf">Nephele: Efficient
Parallel Data Processing in the Cloud</a>,” at <em>2nd Workshop on Many-Task Computing on Grids and
Supercomputers</em> (MTAGS), November 2009.
<a href="http://dx.doi.org/10.1145/1646468.1646476">doi:10.1145/1646468.1646476</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Page1999wg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Page1999wg-marker">69</a>] Lawrence Page, Sergey Brin, Rajeev
Motwani, and Terry Winograd: “<a href="http://ilpubs.stanford.edu:8090/422/">The <span class="keep-together">PageRank</span> Citation
Ranking: Bringing Order to the Web</a>,” Stanford InfoLab Technical Report 422, 1999.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Valiant1990ce">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Valiant1990ce-marker">70</a>] Leslie G. Valiant:
“<a href="http://dl.acm.org/citation.cfm?id=79181">A Bridging Model for Parallel Computation</a>,”
<em>Communications of the ACM</em>, volume 33, number 8, pages 103–111, August 1990.
<a href="http://dx.doi.org/10.1145/79173.79181">doi:10.1145/79173.79181</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Ewen2012cm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Ewen2012cm-marker">71</a>] Stephan Ewen, Kostas Tzoumas, Moritz Kaufmann, and Volker Markl:
“<a href="http://vldb.org/pvldb/vol5/p1268_stephanewen_vldb2012.pdf">Spinning Fast Iterative Data
Flows</a>,” <em>Proceedings of the VLDB Endowment</em>, volume 5, number 11, pages 1268-1279, July 2012.
<a href="http://dx.doi.org/10.14778/2350229.2350245">doi:10.14778/2350229.2350245</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Malewicz2010jq">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Malewicz2010jq-marker">72</a>] Grzegorz Malewicz, Matthew H.
Austern, Aart J. C. Bik, et al.: “<a href="https://kowshik.github.io/JPregel/pregel_paper.pdf">Pregel:
A System for Large-Scale Graph Processing</a>,” at <em>ACM International Conference on Management of
Data</em> (SIGMOD), June 2010.
<a href="http://dx.doi.org/10.1145/1807167.1807184">doi:10.1145/1807167.1807184</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="McSherry2015vx_ch10">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#McSherry2015vx_ch10-marker">73</a>] Frank McSherry, Michael Isard, and Derek G. Murray:
“<a href="http://www.frankmcsherry.org/assets/COST.pdf">Scalability! But at What COST?</a>,” at
<em>15th USENIX Workshop on Hot Topics in Operating Systems</em> (HotOS), May 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Gog2015et">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Gog2015et-marker">74</a>] Ionel Gog, Malte Schwarzkopf, Natacha Crooks, et al.:
“<a href="http://www.cl.cam.ac.uk/research/srg/netos/camsas/pubs/eurosys15-musketeer.pdf">Musketeer:
All for One, One for All in Data Processing Systems</a>,” at <em>10th European Conference on
Computer Systems</em> (EuroSys), April 2015.
<a href="http://dx.doi.org/10.1145/2741948.2741968">doi:10.1145/2741948.2741968</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Kyrola2012uo">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Kyrola2012uo-marker">75</a>] Aapo Kyrola, Guy Blelloch, and Carlos Guestrin:
“<a href="https://www.usenix.org/system/files/conference/osdi12/osdi12-final-126.pdf">GraphChi:
Large-Scale Graph Computation on Just a PC</a>,” at <em>10th USENIX Symposium on Operating Systems
Design and Implementation</em> (OSDI), October 2012.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Lenharth2016je">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Lenharth2016je-marker">76</a>] Andrew Lenharth, Donald Nguyen, and Keshav Pingali:
“<a href="http://cacm.acm.org/magazines/2016/5/201591-parallel-graph-analytics/fulltext">Parallel
Graph Analytics</a>,” <em>Communications of the ACM</em>, volume 59, number 5, pages 78–87, May
2016. <a href="http://dx.doi.org/10.1145/2901919">doi:10.1145/2901919</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Huske2015vm">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Huske2015vm-marker">77</a>] Fabian Hüske:
“<a href="http://flink.apache.org/news/2015/03/13/peeking-into-Apache-Flinks-Engine-Room.html">Peeking
into Apache Flink’s Engine Room</a>,” <em>flink.apache.org</em>, March 13, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Mokhtar2015vg">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Mokhtar2015vg-marker">78</a>] Mostafa Mokhtar:
“<a href="http://hortonworks.com/blog/hive-0-14-cost-based-optimizer-cbo-technical-overview/">Hive
0.14 Cost Based Optimizer (CBO) Technical Overview</a>,” <em>hortonworks.com</em>, March 2, 2015.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Armbrust2015dy">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Armbrust2015dy-marker">79</a>] Michael Armbrust, Reynold S Xin, Cheng Lian, et al.:
“<a href="http://people.csail.mit.edu/matei/papers/2015/sigmod_spark_sql.pdf">Spark SQL: Relational
Data Processing in Spark</a>,” at <em>ACM International Conference on Management of Data</em> (SIGMOD), June 2015.
<a href="http://dx.doi.org/10.1145/2723372.2742797">doi:10.1145/2723372.2742797</a></p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="Blazevski2016ve">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#Blazevski2016ve-marker">80</a>] Daniel Blazevski:
“<a href="http://insightdataengineering.com/blog/flink-knn/">Planting Quadtrees for Apache
Flink</a>,” <em>insightdataengineering.com</em>, March 25, 2016.</p><p xmlns:htmlbook="https://github.com/oreillymedia/HTMLBook" data-type="footnote" id="White2016ua">[<a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#White2016ua-marker">81</a>] Tom White:
“<a href="http://blog.cloudera.com/blog/2016/04/genome-analysis-toolkit-now-using-apache-spark-for-data-processing/">Genome
Analysis Toolkit: Now Using Apache Spark for Data Processing</a>,” <em>blog.cloudera.com</em>, April 6, 2016.</p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#">Add Highlight</a></li>
		<li class="add-note"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#">
			
				Add Note
			
		</a></li>
		
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/part03.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">III. Derived Data</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch11.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">11. Stream Processing</div>
        </a>
    
  
  </div>


        
    </section>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  




    
    
      <div id="js-subscribe-nag" class="subscribe-nag clearfix trial-panel t-subscribe-nag collapsed slideUp">
        
        
          
          
            <p class="usage-data">Find answers on the fly, or master something new. Subscribe today. <a href="https://www.safaribooksonline.com/subscribe/" class="ga-active-trial-subscribe-nag">See pricing options.</a></p>
          

          
        
        

      </div>

    
    



        
      </div>
      




  <footer class="pagefoot t-pagefoot" style="padding-bottom: 69px;">
    <a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li>
      <a class="t-queue-footer" href="https://www.safaribooksonline.com/playlists/">Playlists</a>
      </li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/preferences/">Settings</a></li>
      <li class="full-support"><a href="https://www.oreilly.com/online-learning/support/">Support</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2018 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    
    
      <img src="https://www.oreilly.com/library/view/oreilly_set_cookie/" alt="" style="display:none;">
    
    
    
  

<div class="annotator-notice"></div><div class="font-flyout" style="top: 151.007px; left: 1356px;"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="/site/library/view/designing-data-intensive-applications/9781491903063/ch10.html#">Reset</a>
</div>
</div>
<div style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.12616520025029265"><img style="width:0px; height:0px; display:none; visibility:hidden;" id="batBeacon0.6876318827313341" width="0" height="0" alt="" src="https://bat.bing.com/action/0?ti=5794699&amp;Ver=2&amp;mid=0e95e049-36bb-695e-d450-7cd9d3ff9c5e&amp;pi=1200101525&amp;lg=en-US&amp;sw=1440&amp;sh=900&amp;sc=24&amp;tl=10.%20Batch%20Processing%20-%20Designing%20Data-Intensive%20Applications&amp;p=https%3A%2F%2Fwww.safaribooksonline.com%2Flibrary%2Fview%2Fdesigning-data-intensive-applications%2F9781491903063%2Fch10.html&amp;r=&amp;lt=22082&amp;evt=pageLoad&amp;msclkid=N&amp;rn=230752"></div></body></html>