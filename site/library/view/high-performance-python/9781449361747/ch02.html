<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage"" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/high-performance-python/9781449361747/ch02.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="859452"
  data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36"
  data-username="dchen267"
  data-account-type="B2B"
  
  data-activated-trial-date="04/25/2016"


  data-archive="9781449361747"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch02.html"
  data-epub-title="High Performance Python" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" "="" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/high-performance-python/9781449361747/ch02.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="859452" data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36" data-username="dchen267" data-account-type="B2B" data-activated-trial-date="04/25/2016" data-archive="9781449361747" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch02.html" data-epub-title="High Performance Python" data-debug="0" data-testing="0"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781449361747"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.8cc2fd27400e.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>2. Profiling to find bottlenecks - High Performance Python</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/e4b0fef39b55.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.min.fd58f69f4908.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content font,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}@page{margin:5px !important}#sbo-rt-content p{margin:8px 0 0;line-height:125%;text-align:left}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link{text-decoration:none;color:#8e0012}#sbo-rt-content sup{font-size:x-small;vertical-align:super}#sbo-rt-content sub{font-size:smaller;vertical-align:sub}#sbo-rt-content span.lineannotation{font-style:italic;color:#A62A2A;font-family:serif,"DejaVuSerif"}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#FFF}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content h1,#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important}#sbo-rt-content h2{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content h3{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content h4{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content h5{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section.chapter div.titlepage,#sbo-rt-content section.appendix div.titlepage,#sbo-rt-content section.preface div.titlepage{page-break-inside:avoid;page-break-after:avoid}#sbo-rt-content section.chapter>div.titlepage,#sbo-rt-content section.preface>div.titlepage,#sbo-rt-content section.appendix>div.titlepage{margin-bottom:50px}#sbo-rt-content section.chapter>div.titlepage h2.title,#sbo-rt-content section.preface>div.titlepage h2.title,#sbo-rt-content section.appendix>div.titlepage h2.title{font-size:2em;line-height:1;margin-bottom:15px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content div.toc-title{margin-bottom:30px !important}#sbo-rt-content div.part h1{font-size:2em;text-align:center;margin-top:0 !important;padding:50px 0 20px 0;border-bottom:1px solid #000}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;padding:15px 5px 15px 5px !important;margin:30px 0 30px 0 !important;max-height:100%;page-break-inside:avoid}#sbo-rt-content div.figure-title,#sbo-rt-content div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.informalfigure{text-align:center;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:30px 0 20px 0 !important;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;background-color:#F7F7F7;font-size:90%;padding:15px !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title{font-weight:bold;font-size:1em;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar ol{margin-left:15px}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div.caution,#sbo-rt-content div.sidebar div.important{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content div.sidebar div.figure{border:none}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div.example{margin:10px 0 15px 0 !important}#sbo-rt-content div.example-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div.example-contents pre.programlisting,#sbo-rt-content div.example-contents pre.screen{margin:0}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.book div.titlepage h1.title{font-size:3em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content div.book div.titlepage h2.subtitle{text-align:center;color:#000;margin:0 !important;font-style:italic;font-family:serif;font-size:1.5em}#sbo-rt-content div.book div.titlepage div.author h3{font-size:2em;font-family:sans-serif,"DejaVuSans";font-weight:bold;color:#8e0012;margin:50px 0 !important;text-align:center}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif,"DejaVuSerif";font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif,"DejaVuSerif";margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10pt}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-family:serif,"DejaVuSerif";font-style:italic}#sbo-rt-content blockquote div.attribution{margin:5px 0 0 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p{font-style:normal}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div.footnote{font-size:90%}#sbo-rt-content div.refnamediv h2,#sbo-rt-content div.refnamediv h3,#sbo-rt-content div.refsynopsisdiv h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refentry div.refsect1 h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refsect2 h3{font-size:1em;color:#000;margin-top:10px !important;margin-bottom:0 !important}#sbo-rt-content div.refnamediv p{margin-left:15px !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dt span.term{font-weight:bold;font-style:italic}#sbo-rt-content dt span.term code.literal{font-style:normal;font-weight:normal}#sbo-rt-content dd{margin-left:1.5em !important}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ol{list-style-type:decimal;margin-top:8px !important;margin-bottom:8px !important;margin-left:20px !important;padding-left:25px !important}#sbo-rt-content ol ol{list-style-type:lower-alpha}#sbo-rt-content ol ol ol{list-style-type:lower-roman}#sbo-rt-content ul{list-style-type:square;margin-top:8px !important;margin-bottom:8px !important;margin-left:5px !important;padding-left:20px !important}#sbo-rt-content ul ul{list-style-type:none;padding-left:0 !important;margin-left:0 !important}#sbo-rt-content ol li,#sbo-rt-content ul li,#sbo-rt-content dd{margin-bottom:1em}#sbo-rt-content ul ul li p:before{content:"— "}#sbo-rt-content ul ul ul li p:before{content:""}#sbo-rt-content ul ul ul{list-style-type:square;margin-left:20px !important;padding-left:30px !important}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist{margin-left:20px !important;margin-bottom:10px}#sbo-rt-content table.simplelist td{border:none;font-size:90%}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content div.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content div.calloutlist img{padding:0}#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.calloutlist>dl>dd>div.orderedlist{margin-top:25pt}#sbo-rt-content div.calloutlist>dl>dd>div.orderedlist>ol.orderedlist>li{margin-bottom:25pt}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div.note,#sbo-rt-content div.warning,#sbo-rt-content div.caution,#sbo-rt-content div.important{margin:30px !important;-webkit-border-radius:5px;border-radius:5px;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip,#sbo-rt-content div.note,#sbo-rt-content div.important{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div.caution{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div.note h3,#sbo-rt-content div.warning h3,#sbo-rt-content div.caution h3,#sbo-rt-content div.important h3{font:bold 90%;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div.tip h3,#sbo-rt-content div.note h3,#sbo-rt-content div.important h3{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div.caution h3{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;-webkit-border-radius:0;border-radius:0;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px auto 30px auto !important;max-width:95%;border-collapse:collapse;border-spacing:0}#sbo-rt-content div.table,#sbo-rt-content div.informaltable{page-break-inside:avoid}#sbo-rt-content tr{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif,"DejaVuSans";color:#000;font-weight:bold}#sbo-rt-content td{padding:.3em;text-align:left;vertical-align:baseline;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title{font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";margin:20px 0 0 0 !important;text-align:center;padding:0}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{font-weight:bold}#sbo-rt-content div.index dt{line-height:140%}#sbo-rt-content div.index a.indexterm{color:#8e0012}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif,"DejaVuSerif";text-align:left}
    </style><link rel="canonical" href="/Library/view/high-performance-python/9781449361747/ch02.html"><meta name="description" content="Chapter&nbsp;2.&nbsp;Profiling to find bottlenecks Questions you’ll be able to answer after this chapter How can we identify speed and RAM bottlnecks in our code? How do we ... "><meta property="og:title" content="2. Profiling to find bottlenecks"><meta itemprop="isPartOf" content="/library/view/high-performance-python/9781449361747/"><meta itemprop="name" content="2. Profiling to find bottlenecks"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch02.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781449361747/"><meta property="og:description" itemprop="description" content="Chapter&nbsp;2.&nbsp;Profiling to find bottlenecks Questions you’ll be able to answer after this chapter How can we identify speed and RAM bottlnecks in our code? How do we ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449361594"><meta property="og:book:author" itemprop="author" content="Ian Ozsvald"><meta property="og:book:author" itemprop="author" content="Micha Gorelick"><meta property="og:book:tag" itemprop="about" content="Python"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><!--[if lt IE 9]><script src="/static/js/src/respond.min.cf5c9b7980e5.js"></script><![endif]--><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts library" data-gr-c-s-loaded="true">

    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        




<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#container" class="skip">Skip to content</a><header class="topbar t-topbar"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="20" height="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z" fill="currentColor"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/s/" class="t-queue-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z" fill="currentColor"></path></g></svg><span>Queue</span></a></li><li class="search"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z" fill="currentColor"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z" fill="currentColor"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z" fill="currentColor"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z" fill="currentColor"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>offers icon</desc><path d="M10.8 43.7L0 39 0 10.2 13.6 4.6 23.3 8.7 11.5 13.5C11 13.6 10.8 13.9 10.8 14.3L10.8 43.7 10.8 43.7Z"></path><polygon points="12.3 44.4 25 50 38 44.3 38 14.7 25.2 9.4 12.3 14.7 12.3 44.4"></polygon><path d="M36.6 4.7L50 10.2 50 39 39.5 43.6 39.5 14.3C39.5 13.8 39.2 13.6 38.8 13.5L27 8.7 36.6 4.7 36.6 4.7Z"></path><polygon points="34.8 4 25 0 15.4 3.9 25.2 7.9 34.8 4"></polygon></svg><span>Offers</span></a><ul class="flyout"><li><a href="https://www.safaribooksonline.com/oreilly-conferences/" class="l2 nav-icn"><span>Conferences</span></a></li><li><a href="https://www.safaribooksonline.com/oreilly-newsletters/" class="l2 nav-icn"><span>Newsletter</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/003o000000t5q9fAAA/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z" fill="currentColor"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l1 no-icon">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l2">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      High Performance Python
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781449361747/chapter/ch02.html" data-for-analytics="9781449361747:ch02.html"><span>Add to Queue</span></button></li><li class="js-font-control-panel font-control-activator"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch02.html&amp;text=High%20Performance%20Python&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch02.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch02.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%202.%20Profiling%20to%20find%20bottlenecks&amp;body=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch02.html%0D%0Afrom%20High%20Performance%20Python%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
      
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/Library/view/high-performance-python/9781449361747/ch01.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">1. Understanding Performant Python</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/Library/view/high-performance-python/9781449361747/ch03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">3. Lists and Tuples</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section class="chapter" epub:type="chapter" id="chapter-profiling"><div class="titlepage"><div><div><h2 class="title">Chapter&nbsp;2.&nbsp;Profiling to find bottlenecks</h2></div></div></div><div class="sidebar"><div class="sidebar-title">Questions you’ll be able to answer after this chapter</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
How can we identify speed and RAM bottlnecks in our code?
</li><li class="listitem">
How do we profile CPU and memory usage?
</li><li class="listitem">
What depth of profiling should we use?
</li><li class="listitem">
How can we profile a long-running application?
</li><li class="listitem">
What’s happening under the hood with CPython?
</li><li class="listitem">
How do we keep our code correct whilst tuning performance?
</li></ul></div></div><p>Profiling lets us find bottlenecks so we can do the least amount of work to get the biggest practical performance gain. Often we’d like huge gains in speed and resource usage with little work, practically you’ll aim for your code to run “fast enough” and “lean enough” to fit your needs. Profiling will let you make the most pragmatic decisions for the least overall effort.</p><p>Any measurable resource can be profiled (not just the CPU!) - in this chapter we look at both CPU time and memory usage. You could apply similar techniques to measure network bandwidth and disk I/O too.</p><p>If a program is running too slowly or using too much RAM then you’ll want to fix whichever parts of your code are responsible. You could of course skip profiling and fix what you believe might be the problem - <span class="emphasis"><em>be wary</em></span> as you’ll often end up fixing the wrong thing. Rather than using your intuition it is far more sensible to first profile having defined a hypothesis, before making changes to the structure of your code.</p><p>A good habit for a programmer is laziness. By profiling first you can quickly identify the bottlenecks that need to be solved and then you can solve just enough of these to achieve the performance you need. If you avoid profiling and jump to optimization then it is quite likely that you’ll do more work in the long-run. Always be driven by the results of profiling.</p><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_profiling_efficiently">Profiling efficiently</h2></div></div></div><p>The first aim of profiling is to test a representative system to identify what’s slow (or using too much RAM, disk I/O or network I/O). Profiling typically adds an overhead (10* to 100* slowdowns can be typical) and you still want your code to be used as similarly to a real world situation as possible. Extract a test case and isolate the piece of the system that you need to test, preferably it’ll have been written to be in its own set of modules already.</p><p>The basic techniques that are introduced first in this chapter include the <code class="literal">%timeit</code> magic in IPython, <code class="literal">time.time()</code> and a timing decorator. You can use these techniques to understand the behavior of statements and functions.</p><p>We will cover <code class="literal">cProfile</code> (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-cprofile">Using the cProfile module</a>) to show you how to use this built-in tool to understand which functions in your code take the longest to run. This will give you a high level view of the problem so you can direct your attention to the critical functions.</p><p>Next we’ll look at <code class="literal">line_profiler</code> (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-line-profiler">line_profiler for line-by-line measurements</a>) which will profile your chosen functions on a line-by-line basis. The result will include a count of the number of times each line is called and the percentage of time spent on each line. This is exactly the information you need to understand what’s running slowly and why.</p><p>Armed with the results of <code class="literal">line_profiler</code> you’ll have the information you need to move on to using a compiler (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch07.html">Chapter&nbsp;7</a>).</p><p><code class="literal">perf stat</code> is later introduced in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch06.html#matrix_perf_python_memory">Example&nbsp;6-8</a>, there you’ll learn how to understand the number of instructions that are ultimately executed on a CPU and how efficiently the CPU’s caches are utilized. This allows for advanced-level tuning of matrix operations. You should read this when you’re done with this chapter.</p><p>After <code class="literal">line_profiler</code> we show you <code class="literal">heapy</code> (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-heapy">Inspecting objects on the heap with heapy</a>) which can track all of the objects inside Python’s memory - this is great for hunting strange memory-leaks. If you’re working with long-running systems then <code class="literal">dowser</code> (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dowser">Dowser for live graphing of instantiated variables</a>) will interest you - you can introspect live objects in a long-running process via a web-browser interface.</p><p>To understand why your RAM usage is high we’ll show you <code class="literal">memory_profiler</code> (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#memory_profiler">memory_profiler for diagnosing memory usage</a>). It is particularly useful to chart RAM usage over time on a labelled chart to explain to colleagues why certain functions use more RAM than expected.</p><div class="note"><h3 class="title">Note</h3><p>Whatever approach you take to profile your code, you must remember to have adequate unit test coverage in your code. Unit tests help you to avoid silly mistakes and help to keep your results reproducible. Avoid them at your peril.</p><p><span class="strong"><strong>Always</strong></span> profile your code before compiling or re-writing your algorithm, you need evidence to determine the most efficient ways to make your code run faster.</p></div><p>Finally we’ll give you an introduction to the Python bytecode inside CPython (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dis">The dis module to examine CPython bytecode</a>) so you can understand what’s happening “under the hood”. In particular having an understanding of how Python’s stack based virtual machine operates will help you to understand why certain coding styles run more slowly than others.</p><p>Before the end we review how to integrate unit tests whilst profiling (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-unit-testing">Unit testing during optimization to maintain correctness</a>) to preserve the correctness of your code whilst you make it run more efficiently.</p><p>We’ll finish with a discussion of profiling strategies (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-strategies">Strategies to profile your code successfully</a>) so you can reliably profile your code and gather the correct data to test your hypotheses. Here you’ll learn about how dynamic CPU frequency scaling and features like TurboBoost can skew your profiling results and how they can be disabled.</p><p>To walk through all of these steps we need an easy-to-analyze function. The next section introduces the Julia Set - it is a CPU-bound function that’s a little hungry for RAM, it also exhibits non-linear behavior (so we can’t easily predict the outcomes) which means we need to profile it at run-time rather than analyze it off-line.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_introducing_the_julia_set">Introducing the Julia Set</h2></div></div></div><p>The Julia Set <sup>[<a id="id314061" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#ftn.id314061" class="footnote totri-footnote">4</a>]</sup> is an interesting CPU-bound problem for us to begin with, it is a fractal sequence that generates a complex output image. It is named after Gaston Julia.</p><p>The code that follows is a little longer than a version you might write yourself, it has a CPU-bound component and a very explicit set of inputs. This configuration allows us to profile both the CPU usage and the RAM usage so we understand which parts of our code are consuming two of our scarce computing resources. This implementation is <span class="emphasis"><em>deliberately</em></span> sub-optimal so we can identify memory-consuming operations and slow statements. Later in this chapter we fix a slow logic statement and a memory-consuming statement and then in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch07.html">Chapter&nbsp;7</a> we significantly speed-up the overall execution time of this function.</p><p>We will analyse a block of code that produces both a false greyscale plot (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-example">Figure&nbsp;2-1</a>) and a pure greyscale variant of the Julia Set (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-example-greyscale">Figure&nbsp;2-3</a>), at the complex point <code class="literal">c=-0.62772-0.42193j</code>. A Julia Set is produced by calculating each pixel in isolation, this is an “embarrassingly parallel problem” as no data is shared between points.</p><p>If we chose a different <code class="literal">c</code> then we’d get a different image. The location we have chosen has regions that are quick to calculate and others that are slow to calculate, this is useful for our analysis.</p><p>The problem is interesting because each pixel is calculated by applying a loop that could be applied an indeterminate number of times. On each iteration we test to see if this coordinate’s value escapes towards infinity or if it seems to be held by an attractor. Coordinates that cause few iterations are colored darkly, a high number of iterations are colored white. White regions are more complex to calculate and so take longer to generate.</p><div class="figure"><a id="FIG-julia-example"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_example_image.png" alt="Julia set at -0.62772-0.42193i" width="1000" height="1000" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_example_image.png"></div></div><div class="figure-title">Figure&nbsp;2-1.&nbsp;Julia Set plot with a false greyscale to highlight detail.</div></div><p>We define a set of <code class="literal">z</code> coordinates that we’ll test. The function that we calculate squares the complex number <code class="literal">z</code> and adds <code class="literal">c</code>:</p><p><span class="inlinemediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/inleq_0201.png" alt="" width="129" height="29" data-mfp-src="/library/view/high-performance-python/9781449361747/inleq_0201.png"></span></p><p>We iterate on this function whilst testing to see if the escape condition holds using <code class="literal">abs</code>. If the escape function is <code class="literal">False</code> then we break out of the loop and record the number of iterations we performed at this coordinate. If the escape function is never <code class="literal">False</code> then we stop after <code class="literal">maxiter</code> iterations. We will later turn this <code class="literal">z</code>’s result into a coloured pixel representing this complex location.</p><p>In pseudocode it might look like:</p><pre class="programlisting"><code class="k">for</code> <code class="n">z</code> <code class="ow">in</code> <code class="n">coordinates</code><code class="p">:</code>
    <code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">maxiter</code><code class="p">):</code>  <code class="c"># limited iterations per point</code>
        <code class="k">if</code> <code class="nb">abs</code><code class="p">(</code><code class="n">z</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mf">2.0</code><code class="p">:</code>  <code class="c"># has the escape condition been broken?</code>
            <code class="n">z</code> <code class="o">=</code> <code class="n">z</code><code class="o">*</code><code class="n">z</code> <code class="o">+</code> <code class="n">c</code>
        <code class="k">else</code><code class="p">:</code>
            <code class="k">break</code>
    <code class="c"># store the iteration count for each z and draw later</code></pre><p>To explain this function, let’s try two coordinates.</p><p>Let us use the co-ordinate that we draw in the top-left corner of the plot at <code class="literal">-1.8-1.8j</code>. We must test <code class="literal">abs(z) &lt; 2</code> before we can try the update rule:</p><pre class="programlisting"><code class="n">z</code> <code class="o">=</code> <code class="o">-</code><code class="mf">1.8</code><code class="o">-</code><code class="mf">1.8j</code>
<code class="k">print</code> <code class="nb">abs</code><code class="p">(</code><code class="n">z</code><code class="p">)</code></pre><pre class="screen">2.54558441227</pre><p>We can see that for the top-left co-ordinate the <code class="literal">abs(z)</code> test will be <code class="literal">False</code> on the 0th iteration, so we do not perform the update rule. The <code class="literal">output</code> value for this co-ordinate is <code class="literal">0</code>.</p><p>Now let’s jump to the center of the plot at <code class="literal">z=0+0j</code> and try a few iterations:</p><pre class="programlisting"><code class="n">c</code> <code class="o">=</code> <code class="o">-</code><code class="mf">0.62772</code><code class="o">-</code><code class="mf">0.42193j</code>
<code class="n">z</code> <code class="o">=</code> <code class="mi">0</code><code class="o">+</code><code class="mi">0j</code>
<code class="k">for</code> <code class="n">n</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">9</code><code class="p">):</code>
    <code class="n">z</code> <code class="o">=</code> <code class="n">z</code><code class="o">*</code><code class="n">z</code> <code class="o">+</code> <code class="n">c</code>
    <code class="k">print</code> <code class="s">"{}: z={:33}, abs(z)={:0.2f}, c={}"</code><code class="o">.</code><code class="n">format</code><code class="p">(</code><code class="n">n</code><code class="p">,</code> <code class="n">z</code><code class="p">,</code> <code class="nb">abs</code><code class="p">(</code><code class="n">z</code><code class="p">),</code> <code class="n">c</code><code class="p">)</code></pre><pre class="screen">0: z=              (-0.62772-0.42193j), abs(z)=0.76, c=(-0.62772-0.42193j)
1: z=    (-0.4117125265+0.1077777992j), abs(z)=0.43, c=(-0.62772-0.42193j)
2: z=(-0.469828849523-0.510676940018j), abs(z)=0.69, c=(-0.62772-0.42193j)
3: z=(-0.667771789222+0.057931518414j), abs(z)=0.67, c=(-0.62772-0.42193j)
4: z=(-0.185156898345-0.499300067407j), abs(z)=0.53, c=(-0.62772-0.42193j)
5: z=(-0.842737480308-0.237032296351j), abs(z)=0.88, c=(-0.62772-0.42193j)
6: z=(0.026302151203-0.0224179996428j), abs(z)=0.03, c=(-0.62772-0.42193j)
7: z= (-0.62753076355-0.423109283233j), abs(z)=0.76, c=(-0.62772-0.42193j)
8: z=(-0.412946606356+0.109098183144j), abs(z)=0.43, c=(-0.62772-0.42193j)</pre><p>We can see that each update to <code class="literal">z</code> for these first iterations leaves it with a value where <code class="literal">abs(z) &lt; 2</code> is <code class="literal">True</code>. For this co-ordinate we can iterate 300 times and still the test will be True. We cannot tell how many iterations we must perform before the condition becomes <code class="literal">False</code> (and possibly this is an infinite sequence) so the maximum iteration (<code class="literal">maxiter</code>) break clause will stop us iterating potentially forever.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-non-convergence">Figure&nbsp;2-2</a> we see the first 50 iterations of the above sequence. For <code class="literal">0+0j</code> (solid line with circle markers) the sequence appears to repeat every 8th iteration but each sequence of 7 calculations has a minor deviation to the previous sequence - we can’t tell if this point will iterate forever within the boundary condition or for a long time or maybe for just a few more iterations. The dashed <code class="literal">cutoff</code> line shows the boundary at <code class="literal">+2</code>.</p><div class="figure"><a id="FIG-julia-non-convergence"></a><div class="figure-contents"><div class="mediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_nonconvergence.png" alt="julia non convergence" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_nonconvergence.png"></div></div><div class="figure-title">Figure&nbsp;2-2.&nbsp;Two co-ordinate examples evolving for the Julia set.</div></div><p>For <code class="literal">-0.82+0j</code> (the dashed line with diamond markers) we can see that after the 9th update the absolute result has exceeded the <code class="literal">+2</code> cutoff and we’d stop updating this value.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_calculating_the_full_julia_set">Calculating the full Julia Set</h2></div></div></div><p>Below we break down the code that generates the Julia Set, we’ll analyse it in various ways throughout this chapter. As shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-intro1">Example&nbsp;2-1</a> at the start of our module we import the <code class="literal">time</code> module for our first profiling approach and define some coordinate constants.</p><div class="example"><a id="profiling-juliaset-intro1"></a><div class="example-title">Example&nbsp;2-1.&nbsp;Defining global constants for the co-ordinate space</div><div class="example-contents"><pre class="screen">"""Julia set generator without optional PIL-based image drawing"""
import time

# area of complex space to investigate
x1, x2, y1, y2 = -1.8, 1.8, -1.8, 1.8
c_real, c_imag = -0.62772, -.42193</pre></div></div><p>To generate the plot we create two lists of input data - the first is <code class="literal">zs</code> (complex z co-ordinates), the second is <code class="literal">cs</code> (a complex initial condition). Neither list varies and we could optimise <code class="literal">cs</code> to a single <code class="literal">c</code> value as a constant. The rationale for building two input lists is that we have some reasonable-looking data to profile when we profile RAM usage later in this chapter.</p><p>To build the <code class="literal">zs</code> and <code class="literal">cs</code> lists we need to know the coordinates for each <code class="literal">z</code>, in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-intro2">Example&nbsp;2-2</a> we build this up using <code class="literal">xcoord</code> and <code class="literal">ycoord</code> and a specified <code class="literal">x_step</code> and <code class="literal">y_step</code>. The somewhat verbose nature of this setup is useful when porting the code to other tools (e.g. to <code class="literal">numpy</code>) and to other Python environments as it helps to have everything <span class="emphasis"><em>very</em></span> clearly defined for debugging.</p><div class="example"><a id="profiling-juliaset-intro2"></a><div class="example-title">Example&nbsp;2-2.&nbsp;Code to establish the co-ordinate lists as inputs to our calculation function</div><div class="example-contents"><pre class="screen">def calc_pure_python(desired_width, max_iterations):
    """Create a list of complex co-ordinates (zs) and complex
    parameters (cs), build Julia set and display"""
    x_step = (float(x2 - x1) / float(desired_width))
    y_step = (float(y1 - y2) / float(desired_width))
    x = []
    y = []
    ycoord = y2
    while ycoord &gt; y1:
        y.append(ycoord)
        ycoord += y_step
    xcoord = x1
    while xcoord &lt; x2:
        x.append(xcoord)
        xcoord += x_step
    # build a list of co-ordinates and the initial condition for each cell.
    # Note that our initial condition is a constant and could easily be removed,
    # we use it to simulate a real-world scenario with several inputs to our function
    zs = []
    cs = []
    for ycoord in y:
        for xcoord in x:
            zs.append(complex(xcoord, ycoord))
            cs.append(complex(c_real, c_imag))

    print "Length of x:", len(x)
    print "Total elements:", len(zs)
    start_time = time.time()
    output = calculate_z_serial_purepython(max_iterations, zs, cs)
    end_time = time.time()
    secs = end_time - start_time
    print calculate_z_serial_purepython.func_name + " took", secs, "seconds"

    # this sum is expected for 1000^2 grid with 300 iterations
    # it catches minor errors we might introduce when we're
    # working on a fixed set of inputs
    assert sum(output) == 33219980</pre></div></div><p>Having built the <code class="literal">zs</code> and <code class="literal">cs</code> lists we output some information about the size of the lists and calculate the <code class="literal">output</code> list  via <code class="literal">calculate_z_serial_purepython</code>. Finally we <code class="literal">sum</code> the contents of <code class="literal">output</code> and <code class="literal">assert</code> that it matches the expected output value. Ian uses it here to confirm that no errors creep into the book.</p><p>As the code is deterministic we can verify that the function works as we expect by summing all the calculated values. This is useful as a sanity check because when we make changes to numerical code it is <span class="emphasis"><em>very</em></span> sensible to check that we haven’t broken the algorithm. Ideally we would use unit tests and we’d test more than one configuration of the problem.</p><p>Next in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-intro3">Example&nbsp;2-3</a> we define the <code class="literal">calculate_z_serial_purepython</code> function, it expands on the algorithm we discussed above. Notably we also define an <code class="literal">output</code> list at the start which has the same length as the input <code class="literal">zs</code> and <code class="literal">cs</code> lists. You may also wonder why we’re using <code class="literal">range</code> rather than <code class="literal">xrange</code> - this is so in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#memory_profiler">memory_profiler for diagnosing memory usage</a> we can show how wasteful <code class="literal">range</code> can be!</p><div class="example"><a id="profiling-juliaset-intro3"></a><div class="example-title">Example&nbsp;2-3.&nbsp;Our CPU-bound calculation function</div><div class="example-contents"><pre class="screen">def calculate_z_serial_purepython(maxiter, zs, cs):
    """Calculate output list using Julia update rule"""
    output = [0] * len(zs)
    for i in range(len(zs)):
        n = 0
        z = zs[i]
        c = cs[i]
        while abs(z) &lt; 2 and n &lt; maxiter:
            z = z * z + c
            n += 1
        output[i] = n
    return output</pre></div></div><p>Now we call the calculation routine in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-intro4">Example&nbsp;2-4</a>, by wrapping it in a <code class="literal">__main__</code> check we can safely import the module without starting the calculations for some of the profiling methods. Note that here we’re not showing the method used to plot the output.</p><div class="example"><a id="profiling-juliaset-intro4"></a><div class="example-title">Example&nbsp;2-4.&nbsp;<span class="emphasis"><em>main</em></span> for our code</div><div class="example-contents"><pre class="screen">if __name__ == "__main__":
    # Calculate the Julia set using a pure Python solution with
    # reasonable defaults for a laptop
    calc_pure_python(desired_width=1000, max_iterations=300)</pre></div></div><p>Once we run the code we see some output about the complexity of the problem:</p><pre class="programlisting"><code class="c"># running the above produces:</code>
<code class="n">Length</code> <code class="n">of</code> <code class="n">x</code><code class="p">:</code> <code class="mi">1000</code>
<code class="n">Total</code> <code class="n">elements</code><code class="p">:</code> <code class="mi">1000000</code>
<code class="n">calculate_z_serial_purepython</code> <code class="n">took</code> <code class="mf">12.3479790688</code> <code class="n">seconds</code></pre><p>In the false-color plot (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-example">Figure&nbsp;2-1</a>) the high-contrast color changes give us an idea of where the cost function is slow-changing or fast-changing. Here in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-example-greyscale">Figure&nbsp;2-3</a> we have a linear color map, black is quick to calculate and white is expensive to calculate.</p><div class="figure"><a id="FIG-julia-example-greyscale"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_example_greyscale.png" alt="Julia set at -0.62772-0.42193i" width="1000" height="1000" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_example_greyscale.png"></div></div><div class="figure-title">Figure&nbsp;2-3.&nbsp;Julia plot example using a pure greyscale.</div></div><p>By showing two representations of the same data we can see that lots of detail is lost in the linear mapping. Sometimes it can be useful to have various representations in mind when investigating the cost of a function.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_simple_approaches_to_timing_print_and_a_decorator">Simple approaches to timing - print and a decorator</h2></div></div></div><p>At the end of the code example above we see the output that is generated by several <code class="literal">print</code> statements. On Ian’s laptop this code takes approximately 12 seconds to run using CPython 2.7. It is useful to note that there is always some variation in execution time. You must observe the normal variation when you’re timing your code or you might incorrectly attribute an improvement in your code simply to a random variation in execution time.</p><p>Your computer will be running other tasks such as accessing the network, disk or RAM and these factors can cause variations in the execution time of your program.</p><p>Ian’s laptop is a Dell E6420 with an Intel Core I7-2720QM CPU (2.20GHz, 6MB cache, Quad Core) and 8GB of RAM running Ubuntu 13.10.</p><p>In <code class="literal">calc_pure_python</code> we can see several <code class="literal">print</code> statements. This is the simplest way to measure the execution time of a piece of code <span class="emphasis"><em>inside</em></span> a function. It is a very basic approach, despite being quick and dirty it can be very useful when you’re first looking at a piece of code.</p><p>Using <code class="literal">print</code> statements is commonplace when debugging and profiling code. It quickly becomes unmanageable but is useful for short investigations. Try to tidy them up when you’re done with them or they will clutter your <code class="literal">stdout</code>.</p><p>A slightly cleaner approach is to use a <code class="literal">decorator</code> - here we add 1 line of code above the function that we care about. Our <code class="literal">decorator</code> can be very simple and just replicate the effect of the <code class="literal">print</code> statements. Later we can make it more advanced.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-timefn">Example&nbsp;2-5</a> we define a new function <code class="literal">timefn</code> which takes a function as an argument, the inner function <code class="literal">measure_time</code> takes <code class="literal">*args</code> (a variable number of positional arguments) and <code class="literal">**kwargs</code> (a variable number of key/value arguments) and passes them through to <code class="literal">fn</code> for execution. Around the execution of <code class="literal">fn</code> we capture <code class="literal">time.time()</code> and then <code class="literal">print</code> the result along with <code class="literal">fn.func_name</code>. The overhead of using this decorator is small but if you’re calling <code class="literal">fn</code> millions of times the overhead might become noticeable.</p><p>We use <code class="literal">@wraps(fn)</code> to expose the function name and docstring to the caller of the decorated function (otherwise we would see the function name and docstring for the decorator, not the function it decorates).</p><div class="example"><a id="profiling-juliaset-timefn"></a><div class="example-title">Example&nbsp;2-5.&nbsp;Defining a decorator to automate timing measurements</div><div class="example-contents"><pre class="screen">from functools import wraps

def timefn(fn):
    @wraps(fn)
    def measure_time(*args, **kwargs):
        t1 = time.time()
        result = fn(*args, **kwargs)
        t2 = time.time()
        print ("@timefn:" + fn.func_name + " took " + str(t2 - t1) + " seconds")
        return result
    return measure_time


@timefn
def calculate_z_serial_purepython(maxiter, zs, cs):
    ...</pre></div></div><p>When we run the above version (we keep the <code class="literal">print</code> statements from before) we can see that the execution time in the decorated version is very slightly quicker than the call from <code class="literal">calc_pure_python</code>, this is due to the overhead of calling a function (the difference is very tiny).</p><pre class="programlisting"><code class="n">Length</code> <code class="n">of</code> <code class="n">x</code><code class="p">:</code> <code class="mi">1000</code>
<code class="n">Total</code> <code class="n">elements</code><code class="p">:</code> <code class="mi">1000000</code>
<code class="nd">@timefn</code><code class="p">:</code><code class="n">calculate_z_serial_purepython</code> <code class="n">took</code> <code class="mf">12.2218790054</code> <code class="n">seconds</code>
<code class="n">calculate_z_serial_purepython</code> <code class="n">took</code> <code class="mf">12.2219250043</code> <code class="n">seconds</code></pre><div class="note"><h3 class="title">Note</h3><p>The addition of profiling information will inevitably slow your code down, some profiling options are very informative and induce a heavy speed penalty. The trade between profiling detail and speed will be something you have to consider.</p></div><p>We can use the <code class="literal">timeit</code> module as another way to get a coarse measurement of the execution speed of our function. More typically you would use this when timing different types of simple expressions as you experiment with ways to solve a problem. Here we can use it to time the execution time for our CPU-bound function.</p><div class="warning" epub:type="warning"><h3 class="title">Warning</h3><p>Note that the <code class="literal">timeit</code> module temporarily disables the garbage collector, this might impact the speed you’ll see with real-world operations if the garbage collector would normally be invoked by your operation. See the Python documentation for help on this <sup>[<a id="id322178" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#ftn.id322178" class="footnote totri-footnote">5</a>]</sup>.</p></div><p>From the command line you can run <code class="literal">timeit</code> using:</p><p><code class="literal">$ python -m timeit -n 5 -r 5 -s "import julia1"
   "julia1.calc_pure_python(
   desired_width=1000,
   max_iterations=300)"</code></p><p>and note that you have to import the module as a setup step using <code class="literal">-s</code> as <code class="literal">calc_pure_python</code> is inside that module. <code class="literal">timeit</code> has some sensible defaults for short sections of code, for longer-running functions it can be sensible to specify the number of loops which are averaged (<code class="literal">-n 5</code>) and a number of repetitions of the loops (<code class="literal">-r 5</code>). The best result of all the repetitions is given as the answer, each result is an average of the loops in that test.</p><p>By default if I run <code class="literal">timeit</code> without specifying <code class="literal">-n</code> and <code class="literal">-r</code> then it runs 10 loops with 5 repetitions and this takes 6 minutes, so overriding the defaults can make sense if you want to get your results a little faster.</p><p>We’re only interested in the best case results as the average and worst case are probably a result of interference by other processes. Choosing the best of five repetitions of five averaged results should give us a fairly stable result. Try running the benchmark several times to check if you get varying results - you may need more repetitions to settle on a stable fastest-result time. There is no “correct” configuration - if you see a wide variation in your timing results then do more repetitions until your final result is stable.</p><pre class="programlisting"><code class="mi">5</code> <code class="n">loops</code><code class="p">,</code> <code class="n">best</code> <code class="n">of</code> <code class="mi">5</code><code class="p">:</code> <code class="mf">13.1</code> <code class="n">sec</code> <code class="n">per</code> <code class="n">loop</code></pre><p>The overall cost of calling <code class="literal">calc_pure_python</code> is 13.1 seconds (as the best case) whilst single calls to <code class="literal">calculate_z_serial_purepython</code> take 12.2 seconds as measured by the <code class="literal">@timefn</code> decorator. The difference is mainly the time taken to create the lists of <code class="literal">zs</code> and <code class="literal">cs</code>.</p><p>Inside <code class="literal">IPython</code> we can use the magic <code class="literal">%timeit</code> in the same way. If you are developing your code interactively in <code class="literal">IPython</code> and the functions are in the local namespace (probably because you’re using <code class="literal">%run</code>) then you can use:</p><pre class="programlisting"><code class="o">%</code><code class="n">timeit</code> <code class="n">calc_pure_python</code><code class="p">(</code><code class="n">desired_width</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">max_iterations</code><code class="o">=</code><code class="mi">300</code><code class="p">)</code></pre><p>It is worth considering the variation in load that you get on a normal computer. Many background tasks are running (e.g. Dropbox, backups) that could impact the CPU and disk resources at random. Scripts in web pages can also cause unpredictable resource usage. <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-set-system-monitor-trimmed">Figure&nbsp;2-4</a> shows the single CPU being used at 100% for some of the timing steps above, the other cores on this machine are each lightly working on other tasks.</p><div class="figure"><a id="FIG-julia-set-system-monitor-trimmed"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 405; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_set_system_monitor_trimmed.png" alt="System Monitor (Ubuntu) showing background CPU usage during timings" width="883" height="254" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_set_system_monitor_trimmed.png"></div></div><div class="figure-title">Figure&nbsp;2-4.&nbsp;System Monitor on Ubuntu showing variation in background CPU usage whilst we time our function.</div></div><p>Occasionally the System Monitor shows spikes of activity on this machine, it is sensible to watch your System Monitor to check that nothing else is interfering with your critical resources (CPU, Disk, Network).</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_simple_timing_using_the_unix_time_command">Simple timing using the Unix time command</h2></div></div></div><p>We can step outside of Python for a moment to use a standard system utility on Unix-like systems. The following will record various views on the execution time of your program and it won’t care about the internal structure of your code. Note that we specifically use <code class="literal">/usr/bin/time</code> rather than <code class="literal">time</code> so we get the system’s <code class="literal">time</code> and not the simpler (and less useful) version built into our shell.</p><p>If you try <code class="literal">time --verbose</code> and you get an error, you’re probably looking at the shell’s built-in <code class="literal">time</code> command and not the system command.</p><pre class="programlisting"><code class="err">$</code> <code class="o">/</code><code class="n">usr</code><code class="o">/</code><code class="nb">bin</code><code class="o">/</code><code class="n">time</code> <code class="o">-</code><code class="n">p</code> <code class="n">python</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code>
<code class="n">Length</code> <code class="n">of</code> <code class="n">x</code><code class="p">:</code> <code class="mi">1000</code>
<code class="n">Total</code> <code class="n">elements</code><code class="p">:</code> <code class="mi">1000000</code>
<code class="n">calculate_z_serial_purepython</code> <code class="n">took</code> <code class="mf">12.7298331261</code> <code class="n">seconds</code>
<code class="n">real</code> <code class="mf">13.46</code>
<code class="n">user</code> <code class="mf">13.40</code>
<code class="n">sys</code> <code class="mf">0.04</code></pre><p>Using the <code class="literal">-p</code> portability flag we get three results:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
<code class="literal">real</code> records the wall-clock or elapsed time
</li><li class="listitem">
<code class="literal">user</code> records the amount of time the CPU spent on your task outside of kernel functions
</li><li class="listitem">
<code class="literal">sys</code> records the time spent in kernel-level functions
</li></ul></div><p>By adding <code class="literal">user</code> and <code class="literal">sys</code> you get a sense of how much time was spent in the CPU. The difference between this and <code class="literal">real</code> might tell you about the amount of time waiting for I/O, it might also suggest that your system is busy running other tasks that are distorting your measurements.</p><p><code class="literal">time</code> is useful because it isn’t specific to Python. It includes the time taken starting the <code class="literal">python</code> executable which might be significant if you start lots of fresh processes (rather than having a long-running single process). If you often have short-running scripts where the start-up time is a significant part of the overall runtime then <code class="literal">time</code> can be a more useful measure.</p><p>We can add the <code class="literal">--verbose</code> flag to get even more output:</p><pre class="programlisting"><code class="err">$</code> <code class="o">/</code><code class="n">usr</code><code class="o">/</code><code class="nb">bin</code><code class="o">/</code><code class="n">time</code> <code class="o">--</code><code class="n">verbose</code> <code class="n">python</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code>
<code class="n">Length</code> <code class="n">of</code> <code class="n">x</code><code class="p">:</code> <code class="mi">1000</code>
<code class="n">Total</code> <code class="n">elements</code><code class="p">:</code> <code class="mi">1000000</code>
<code class="n">calculate_z_serial_purepython</code> <code class="n">took</code> <code class="mf">12.3145110607</code> <code class="n">seconds</code>
        <code class="n">Command</code> <code class="n">being</code> <code class="n">timed</code><code class="p">:</code> <code class="s">"python julia1_nopil.py"</code>
        <code class="n">User</code> <code class="n">time</code> <code class="p">(</code><code class="n">seconds</code><code class="p">):</code> <code class="mf">13.46</code>
        <code class="n">System</code> <code class="n">time</code> <code class="p">(</code><code class="n">seconds</code><code class="p">):</code> <code class="mf">0.05</code>
        <code class="n">Percent</code> <code class="n">of</code> <code class="n">CPU</code> <code class="n">this</code> <code class="n">job</code> <code class="n">got</code><code class="p">:</code> <code class="mi">99</code><code class="o">%</code>
        <code class="n">Elapsed</code> <code class="p">(</code><code class="n">wall</code> <code class="n">clock</code><code class="p">)</code> <code class="n">time</code> <code class="p">(</code><code class="n">h</code><code class="p">:</code><code class="n">mm</code><code class="p">:</code><code class="n">ss</code> <code class="ow">or</code> <code class="n">m</code><code class="p">:</code><code class="n">ss</code><code class="p">):</code> <code class="mi">0</code><code class="p">:</code><code class="mf">13.53</code>
        <code class="n">Average</code> <code class="n">shared</code> <code class="n">text</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">0</code>
        <code class="n">Average</code> <code class="n">unshared</code> <code class="n">data</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">0</code>
        <code class="n">Average</code> <code class="n">stack</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">0</code>
        <code class="n">Average</code> <code class="n">total</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">0</code>
        <code class="n">Maximum</code> <code class="n">resident</code> <code class="nb">set</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">131952</code>
        <code class="n">Average</code> <code class="n">resident</code> <code class="nb">set</code> <code class="n">size</code> <code class="p">(</code><code class="n">kbytes</code><code class="p">):</code> <code class="mi">0</code>
        <code class="n">Major</code> <code class="p">(</code><code class="n">requiring</code> <code class="n">I</code><code class="o">/</code><code class="n">O</code><code class="p">)</code> <code class="n">page</code> <code class="n">faults</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">Minor</code> <code class="p">(</code><code class="n">reclaiming</code> <code class="n">a</code> <code class="n">frame</code><code class="p">)</code> <code class="n">page</code> <code class="n">faults</code><code class="p">:</code> <code class="mi">58974</code>
        <code class="n">Voluntary</code> <code class="n">context</code> <code class="n">switches</code><code class="p">:</code> <code class="mi">3</code>
        <code class="n">Involuntary</code> <code class="n">context</code> <code class="n">switches</code><code class="p">:</code> <code class="mi">26</code>
        <code class="n">Swaps</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">File</code> <code class="n">system</code> <code class="n">inputs</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">File</code> <code class="n">system</code> <code class="n">outputs</code><code class="p">:</code> <code class="mi">1968</code>
        <code class="n">Socket</code> <code class="n">messages</code> <code class="n">sent</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">Socket</code> <code class="n">messages</code> <code class="n">received</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">Signals</code> <code class="n">delivered</code><code class="p">:</code> <code class="mi">0</code>
        <code class="n">Page</code> <code class="n">size</code> <code class="p">(</code><code class="nb">bytes</code><code class="p">):</code> <code class="mi">4096</code>
        <code class="n">Exit</code> <code class="n">status</code><code class="p">:</code> <code class="mi">0</code></pre><p>Probably the most useful indicator above is <code class="literal">Major (requiring I/O) page faults</code> as this indicates whether the operating system is having to load pages of data from the disk because it no longer resides in RAM. This will cause a speed penalty.</p><p>In our example the code and data requirement is small so no page faults occur. If you have a memory-bound process, or several programs that use variable and large amounts of RAM, you might find that this gives you a clue as to which program is slowed down by disk accesses at the operating system level because parts of it have been swapped out of RAM to disk.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-cprofile">Using the cProfile module</h2></div></div></div><p><code class="literal">cProfile</code> is a built-in profiling tool in the standard library. It hooks into the virtual machine in CPython to measure the time taken running every function that it sees. This introduces a greater overhead and you get more information as a result. Sometimes the additional information can lead to surprising insights into your code.</p><p><code class="literal">cProfile</code> is one of three profilers in the standard library (the others are <code class="literal">hotshot</code> and <code class="literal">profile</code>). <code class="literal">hotshot</code> is experimental code, <code class="literal">profile</code> is the original pure-Python profiler. <code class="literal">cProfile</code> has the same interface as <code class="literal">profile</code>, is supported and is the default profiling tool. If you’re curious about the history of these libraries see Armin Rigo’s request to include <code class="literal">cProfile</code> in the standard library from 2005 <sup>[<a id="id312621" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#ftn.id312621" class="footnote totri-footnote">6</a>]</sup>.</p><p>A good practice when profiling is to generate a <span class="emphasis"><em>hypothesis</em></span> about the speed of parts of your code before you profile it. Ian likes to print out the code snippet in question and to annotate it. Forming a hypothesis ahead of time means you can measure how wrong you are (you will get it wrong!) and improve your intuition about certain coding styles.</p><div class="warning" epub:type="warning"><h3 class="title">Warning</h3><p>You should never avoid profiling in favor of a gut instinct (we warn you - you <span class="emphasis"><em>will</em></span> get it wrong!). It is definitely worth forming a hypothesis ahead of profiling to help you learn to spot possible slow choices in your code and always back up your choices with evidence.</p></div><p>Always be driven by results that you have measured and always start with some quick-and-dirty profiling to make sure you’re addressing the right area. There’s nothing more humbling than cleverly optimizing a section of code only to realise (hours or days later) that you’d missed the slowest part of the process and haven’t really addressed the underlying problem at all.</p><p>Our hypothesis: We know that <code class="literal">calculate_z_serial_purepython</code> is likely to be the slowest part of the code. In that function we do a lot of dereferencing and make many calls to basic arithmetic operators and the <code class="literal">abs</code> function. These will probably show up as consumers of CPU resources.</p><p>Below we’ll use the <code class="literal">cProfile</code> module to run a variant of the code. The output is spartan but helps us figure out where to analyse further.</p><p>The <code class="literal">-s cumulative</code> flag tells <code class="literal">cProfile</code> to sort by cumulative time spent inside each function - this gives us a view into the slowest parts of a section of code. The <code class="literal">cProfile</code> output is written to screen directly after our usual <code class="literal">print</code> results.</p><pre class="programlisting"><code class="err">$</code> <code class="n">python</code> <code class="o">-</code><code class="n">m</code> <code class="n">cProfile</code> <code class="o">-</code><code class="n">s</code> <code class="n">cumulative</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code>
<code class="o">...</code>
         <code class="mi">36221992</code> <code class="n">function</code> <code class="n">calls</code> <code class="ow">in</code> <code class="mf">19.664</code> <code class="n">seconds</code>

   <code class="n">Ordered</code> <code class="n">by</code><code class="p">:</code> <code class="n">cumulative</code> <code class="n">time</code>

   <code class="n">ncalls</code>  <code class="n">tottime</code>  <code class="n">percall</code>  <code class="n">cumtime</code>  <code class="n">percall</code> <code class="n">filename</code><code class="p">:</code><code class="n">lineno</code><code class="p">(</code><code class="n">function</code><code class="p">)</code>
        <code class="mi">1</code>    <code class="mf">0.034</code>    <code class="mf">0.034</code>   <code class="mf">19.664</code>   <code class="mf">19.664</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">1</code><code class="p">(</code><code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code><code class="p">)</code>
        <code class="mi">1</code>    <code class="mf">0.843</code>    <code class="mf">0.843</code>   <code class="mf">19.630</code>   <code class="mf">19.630</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
        <code class="mi">1</code>   <code class="mf">14.121</code>   <code class="mf">14.121</code>   <code class="mf">18.627</code>   <code class="mf">18.627</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code>
                                              <code class="p">(</code><code class="n">calculate_z_serial_purepython</code><code class="p">)</code>
 <code class="mi">34219980</code>    <code class="mf">4.487</code>    <code class="mf">0.000</code>    <code class="mf">4.487</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="nb">abs</code><code class="p">}</code>
  <code class="mi">2002000</code>    <code class="mf">0.150</code>    <code class="mf">0.000</code>    <code class="mf">0.150</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">method</code> <code class="s">'append'</code> <code class="n">of</code> <code class="s">'list'</code> <code class="n">objects</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code> <code class="p">{</code><code class="nb">range</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code> <code class="p">{</code><code class="nb">sum</code><code class="p">}</code>
        <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">}</code>
        <code class="mi">4</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="nb">len</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">method</code> <code class="s">'disable'</code> <code class="n">of</code>
                                               <code class="s">'_lsprof.Profiler'</code> <code class="n">objects</code><code class="p">}</code></pre><p>Sorting by cumulative time gives us an idea about where the majority of execution time is spent. The above result shows us that 36221992 function calls occurred in just over 19 seconds (this time includes the overhead of using <code class="literal">cProfile</code>). Previously our code took around 13 seconds to execute - we’ve just added a 5 second penalty by measuring how long each function takes to execute.</p><p>We can see that the entry point to the code <code class="literal">julia1_cprofile.py</code> on line 1 takes a total of 19 seconds, this is just the <code class="literal">__main__</code> call to <code class="literal">calc_pure_python</code>. <code class="literal">ncalls</code> is 1, this line is only executed once.</p><p>Inside <code class="literal">calc_pure_python</code> the call to <code class="literal">calculate_z_serial_purepython</code> consumes 18.6 seconds, both functions are only called once. We can derive that approximately 1 second is spent on lines of code inside <code class="literal">calc_pure_python</code> separate to calling the CPU intensive <code class="literal">calculate_z_serial_purepython</code> function. We can’t derive <span class="emphasis"><em>which</em></span> lines however take the time inside the function using <code class="literal">cProfile</code>.</p><p>Inside <code class="literal">calculate_z_serial_purepython</code> the time spent on lines of code (without calling other functions) is 14.1 seconds. This function makes 34,219,980 calls to <code class="literal">abs</code> which take a total of 4.4 seconds along with some other calls which do not cost much time.</p><p>What about the <code class="literal">{abs}</code> call? This line is measuring the individual calls to the <code class="literal">abs</code> function inside <code class="literal">calculate_z_serial_purepython</code>. Whilst the per-call cost is negligible (it is recorded as 0.000 seconds) the total time for 34,219,980 calls is 4.4 seconds. We couldn’t predict in advance exactly how many calls would be made to <code class="literal">abs</code> as the Julia function has non-predictable dynamics (that’s why it is so interesting to look at).</p><p>At best we could have said that it will be called a minimum of 1,000,000 times as we’re calculating <code class="literal">1000*1000</code> pixels. At most it would be 300,000,000 as we calculate 1,000,000 pixels with a maximum of 300 iterations. 34 million calls is roughly 10% of the worst case.</p><p>If we looked at the original greyscale image and in our mind’s eye we squashed the white parts together and into a corner, we can estimate that the expensive white region accounts for roughly 10% of the rest of the image.</p><p>The next line in the profiled output <code class="literal">{method 'append' of 'list' objects}</code> details the creation of 2,002,000 list items.</p><div class="tip"><h3 class="title">Tip</h3><p>Why 2,002,000 items? Before you read below think about how many list items are being constructed.</p></div><p>This creation of the 2,002,000 items is occurring in <code class="literal">calc_pure_python</code> during the setup phase.</p><p>The <code class="literal">zs</code> and <code class="literal">cs</code> will be <code class="literal">1000*1000</code> items each and these are built from a list of 1,000 <code class="literal">x</code> and 1,000 <code class="literal">y</code> co-ordinates, in total this is 2,002,000 calls to append.</p><p>It is important to note that this <code class="literal">cProfile</code> output is not ordered by parent functions, it is summarising the expense of all functions in the executed block of code. Figuring out what is happening on a line-by-line basis is very hard with <code class="literal">cProfile</code> as we only get profile information for the function calls themselves, not each line within the function.</p><p>Inside <code class="literal">calculate_z_serial_purepython</code> we can now account for <code class="literal">{abs}</code> and <code class="literal">{range}</code> and in total these two functions cost approximately 4.5 seconds. We know that <code class="literal">calculate_z_serial_purepython</code> costs 18.6 seconds.</p><p>The final line of the profiling output refers to <code class="literal">lsprof</code> - this is the original name of the tool that evolved into <code class="literal">cProfile</code> and can be ignored.</p><p>To get some more control over the results of <code class="literal">cProfile</code> we can write a statistics file and then analyse it in Python:</p><p><code class="literal">$ python -m cProfile -o profile.stats julia1.py</code></p><p>We can load this into Python with the following, it’ll give us the same cumulative time report as before:</p><pre class="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">1</code><code class="p">]:</code> <code class="kn">import</code> <code class="nn">pstats</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">2</code><code class="p">]:</code> <code class="n">p</code> <code class="o">=</code> <code class="n">pstats</code><code class="o">.</code><code class="n">Stats</code><code class="p">(</code><code class="s">"profile.stats"</code><code class="p">)</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="n">p</code><code class="o">.</code><code class="n">sort_stats</code><code class="p">(</code><code class="s">"cumulative"</code><code class="p">)</code>
<code class="n">Out</code><code class="p">[</code><code class="mi">3</code><code class="p">]:</code> <code class="o">&lt;</code><code class="n">pstats</code><code class="o">.</code><code class="n">Stats</code> <code class="n">instance</code> <code class="n">at</code> <code class="mh">0x177dcf8</code><code class="o">&gt;</code>
<code class="n">In</code> <code class="p">[</code><code class="mi">4</code><code class="p">]:</code> <code class="n">p</code><code class="o">.</code><code class="n">print_stats</code><code class="p">()</code>
<code class="n">Tue</code> <code class="n">Jan</code>  <code class="mi">7</code> <code class="mi">21</code><code class="p">:</code><code class="mo">00</code><code class="p">:</code><code class="mi">56</code> <code class="mi">2014</code>    <code class="n">profile</code><code class="o">.</code><code class="n">stats</code>

         <code class="mi">36221992</code> <code class="n">function</code> <code class="n">calls</code> <code class="ow">in</code> <code class="mf">19.983</code> <code class="n">seconds</code>

   <code class="n">Ordered</code> <code class="n">by</code><code class="p">:</code> <code class="n">cumulative</code> <code class="n">time</code>

   <code class="n">ncalls</code>  <code class="n">tottime</code>  <code class="n">percall</code>  <code class="n">cumtime</code>  <code class="n">percall</code> <code class="n">filename</code><code class="p">:</code><code class="n">lineno</code><code class="p">(</code><code class="n">function</code><code class="p">)</code>
        <code class="mi">1</code>    <code class="mf">0.033</code>    <code class="mf">0.033</code>   <code class="mf">19.983</code>   <code class="mf">19.983</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">1</code><code class="p">(</code><code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code><code class="p">)</code>
        <code class="mi">1</code>    <code class="mf">0.846</code>    <code class="mf">0.846</code>   <code class="mf">19.950</code>   <code class="mf">19.950</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
        <code class="mi">1</code>   <code class="mf">13.585</code>   <code class="mf">13.585</code>   <code class="mf">18.944</code>   <code class="mf">18.944</code> <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_z_serial_purepython</code><code class="p">)</code>
 <code class="mi">34219980</code>    <code class="mf">5.340</code>    <code class="mf">0.000</code>    <code class="mf">5.340</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="nb">abs</code><code class="p">}</code>
  <code class="mi">2002000</code>    <code class="mf">0.150</code>    <code class="mf">0.000</code>    <code class="mf">0.150</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">method</code> <code class="s">'append'</code> <code class="n">of</code> <code class="s">'list'</code> <code class="n">objects</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code> <code class="p">{</code><code class="nb">range</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code> <code class="p">{</code><code class="nb">sum</code><code class="p">}</code>
        <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">}</code>
        <code class="mi">4</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="nb">len</code><code class="p">}</code>
        <code class="mi">1</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code> <code class="p">{</code><code class="n">method</code> <code class="s">'disable'</code> <code class="n">of</code>
                                               <code class="s">'_lsprof.Profiler'</code> <code class="n">objects</code><code class="p">}</code></pre><p>To trace which functions we’re profiling we can print the caller information. In the following two listings we can see that <code class="literal">calculate_z_serial_purepython</code> is the most expensive function and it is called from one place. If it was called from many places, these listings might help us narrow down the location of the most expensive parents:</p><pre class="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">5</code><code class="p">]:</code> <code class="n">p</code><code class="o">.</code><code class="n">print_callers</code><code class="p">()</code>
   <code class="n">Ordered</code> <code class="n">by</code><code class="p">:</code> <code class="n">cumulative</code> <code class="n">time</code>

<code class="n">Function</code>                                <code class="n">was</code> <code class="n">called</code> <code class="n">by</code><code class="o">...</code>
                                                 <code class="n">ncalls</code>  <code class="n">tottime</code>  <code class="n">cumtime</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">1</code><code class="p">(</code><code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code><code class="p">)</code>                   <code class="o">&lt;-</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>          <code class="o">&lt;-</code>       <code class="mi">1</code>    <code class="mf">0.846</code>   <code class="mf">19.950</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">1</code><code class="p">(</code><code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code><code class="p">)</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_z_serial_purepython</code><code class="p">)</code>  <code class="o">&lt;-</code>       <code class="mi">1</code>   <code class="mf">13.585</code>   <code class="mf">18.944</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="p">{</code><code class="nb">abs</code><code class="p">}</code>                                         <code class="o">&lt;-</code> <code class="mi">34219980</code>    <code class="mf">5.340</code>    <code class="mf">5.340</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_</code><code class="o">...</code><code class="n">_purepython</code><code class="p">)</code>
<code class="p">{</code><code class="n">method</code> <code class="s">'append'</code> <code class="n">of</code> <code class="s">'list'</code> <code class="n">objects</code><code class="p">}</code>           <code class="o">&lt;-</code> <code class="mi">2002000</code>    <code class="mf">0.150</code>    <code class="mf">0.150</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="p">{</code><code class="nb">range</code><code class="p">}</code>                                       <code class="o">&lt;-</code>       <code class="mi">1</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_</code><code class="o">...</code><code class="n">_purepython</code><code class="p">)</code>
<code class="p">{</code><code class="nb">sum</code><code class="p">}</code>                                         <code class="o">&lt;-</code>       <code class="mi">1</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="p">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">}</code>                                   <code class="o">&lt;-</code>       <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="p">{</code><code class="nb">len</code><code class="p">}</code>                                         <code class="o">&lt;-</code>       <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_</code><code class="o">...</code><code class="n">_purepython</code><code class="p">)</code>
                                                       <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                  <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="p">{</code><code class="n">method</code> <code class="s">'disable'</code> <code class="n">of</code> <code class="s">'_lsprof.Profiler'</code> <code class="n">objects</code><code class="p">}</code>  <code class="o">&lt;-</code></pre><p>We can flip this around the other way to show which functions call other functions:</p><pre class="programlisting"><code class="n">In</code> <code class="p">[</code><code class="mi">6</code><code class="p">]:</code> <code class="n">p</code><code class="o">.</code><code class="n">print_callees</code><code class="p">()</code>
   <code class="n">Ordered</code> <code class="n">by</code><code class="p">:</code> <code class="n">cumulative</code> <code class="n">time</code>

<code class="n">Function</code>                                          <code class="n">called</code><code class="o">...</code>
                                                      <code class="n">ncalls</code>  <code class="n">tottime</code>  <code class="n">cumtime</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">1</code><code class="p">(</code><code class="o">&lt;</code><code class="n">module</code><code class="o">&gt;</code><code class="p">)</code>                       <code class="o">-&gt;</code>       <code class="mi">1</code>    <code class="mf">0.846</code>   <code class="mf">19.950</code>
                                                      <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">23</code><code class="p">(</code><code class="n">calc_pure_python</code><code class="p">)</code>              <code class="o">-&gt;</code>       <code class="mi">1</code>   <code class="mf">13.585</code>   <code class="mf">18.944</code>
                                                      <code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_</code><code class="o">...</code><code class="n">_purepython</code><code class="p">)</code>
                                                           <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                      <code class="p">{</code><code class="nb">len</code><code class="p">}</code>
                                                     <code class="mi">2002000</code>    <code class="mf">0.150</code>    <code class="mf">0.150</code>
                                                      <code class="p">{</code><code class="n">method</code> <code class="s">'append'</code> <code class="n">of</code> <code class="s">'list'</code> <code class="n">objects</code><code class="p">}</code>
                                                           <code class="mi">1</code>    <code class="mf">0.010</code>    <code class="mf">0.010</code>
                                                      <code class="p">{</code><code class="nb">sum</code><code class="p">}</code>
                                                           <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                      <code class="p">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">}</code>
<code class="n">julia1_nopil</code><code class="o">.</code><code class="n">py</code><code class="p">:</code><code class="mi">9</code><code class="p">(</code><code class="n">calculate_z_serial_purepython</code><code class="p">)</code>  <code class="o">-&gt;</code> <code class="mi">34219980</code>    <code class="mf">5.340</code>    <code class="mf">5.340</code>
                                                      <code class="p">{</code><code class="nb">abs</code><code class="p">}</code>
                                                           <code class="mi">2</code>    <code class="mf">0.000</code>    <code class="mf">0.000</code>
                                                      <code class="p">{</code><code class="nb">len</code><code class="p">}</code>
                                                           <code class="mi">1</code>    <code class="mf">0.019</code>    <code class="mf">0.019</code>
                                                      <code class="p">{</code><code class="nb">range</code><code class="p">}</code>
<code class="p">{</code><code class="nb">abs</code><code class="p">}</code>                                             <code class="o">-&gt;</code>
<code class="p">{</code><code class="n">method</code> <code class="s">'append'</code> <code class="n">of</code> <code class="s">'list'</code> <code class="n">objects</code><code class="p">}</code>               <code class="o">-&gt;</code>
<code class="p">{</code><code class="nb">range</code><code class="p">}</code>                                           <code class="o">-&gt;</code>
<code class="p">{</code><code class="nb">sum</code><code class="p">}</code>                                             <code class="o">-&gt;</code>
<code class="p">{</code><code class="n">time</code><code class="o">.</code><code class="n">time</code><code class="p">}</code>                                       <code class="o">-&gt;</code>
<code class="p">{</code><code class="nb">len</code><code class="p">}</code>                                             <code class="o">-&gt;</code>
<code class="p">{</code><code class="n">method</code> <code class="s">'disable'</code> <code class="n">of</code> <code class="s">'_lsprof.Profiler'</code> <code class="n">objects</code><code class="p">}</code>  <code class="o">-&gt;</code></pre><p><code class="literal">cProfile</code> is rather verbose and you need a side screen to see it without lots of word-wrap. Since it is built-in it is a convenient tool for quickly identifying bottlenecks. Tools like <code class="literal">line_profiler</code>, <code class="literal">heapy</code> and <code class="literal">memory_profiler</code> which we discuss later in this chapter will then help you to drill-down to the specific lines that you should pay attention to.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_runsnakerun_to_visualise_cprofile_output">runsnakerun to visualise cProfile output</h2></div></div></div><p><code class="literal">runsnake</code> is a visualisation tool for the profile statistics created by <code class="literal">cProfile</code> - you can quickly get a sense of which functions are most expensive just by looking at the diagram that’s generated.</p><p>Use <code class="literal">runsnake</code> to get a high level understanding of a <code class="literal">cProfile</code> statistics file especially when you’re investigating a new and large code-base, it’ll give you a feel for the areas that you should focus on. It might also reveal areas that you hadn’t realized would be expensive so it might highlight some quick wins for you to focus on.</p><p>You can also use it when discussing the slow areas of code in a team as it is easy to discuss the results.</p><p>Installation: <code class="literal">pip install runsnake</code></p><p>Note that it requires <code class="literal">wxPython</code> and this can be a pain to install into a <code class="literal">virtualenv</code>. Ian has resorted to installing this globally on more than one occasion just to analyse a profile file, rather than to try to get it running in a <code class="literal">virtualenv</code>.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-set-runsnakerun">Figure&nbsp;2-5</a> we have the visual plot of the previous <code class="literal">cProfile</code> data. The visual inspection should make it easier to quickly understand that <code class="literal">calculate_z_serial_purepython</code> takes the majority of the time and that only a part of the execution time is due to calling other functions (the only one that is significant is <code class="literal">abs</code>). Visually you can see that there’s little point investing time in the setup routine as the vast majority of the execution time is in the calculation routine.</p><div class="figure"><a id="FIG-julia-set-runsnakerun"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 459; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_set_runsnakerun.png" alt="RunSnakeRun" width="1000" height="681" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_set_runsnakerun.png"></div></div><div class="figure-title">Figure&nbsp;2-5.&nbsp;RunSnakeRun visualising a cProfile profile file.</div></div><p>With <code class="literal">runsnake</code> you can click into functions and drill into complex nested calls. When discussing the reasons for slow execution in a piece of code in a team this tool is invaluable.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-line-profiler">line_profiler for line-by-line measurements</h2></div></div></div><p>In Ian’s opinion Robert Kern’s line_profiler is the strongest tool for identifying the cause of CPU-bound problems in Python code. It works by profiling individual functions on a line-by-line basis so you should start with <code class="literal">cProfile</code> and use the high-level view to guide which functions to profile with <code class="literal">line_profiler</code>.</p><p>It is worthwhile printing and annotating versions of the output from this tool as you modify your code so you have a record of changes (successful or not) that you can quickly refer to. Don’t rely on your memory when you’re working on line-by-line changes.</p><p>Installation: <code class="literal">pip install line_profiler</code></p><p>A decorator (<code class="literal">@profile</code>) is used to mark the chosen function. The <code class="literal">kernprof.py</code> script is used to execute your code, the CPU time and other statistics for each line of the chosen function are recorded.</p><div class="note"><h3 class="title">Note</h3><p>The requirement to modify the source code is a minor annoyance as the addition of a decorator will break your unit tests unless you make a dummy decorator - see <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#no_op_profile_decorator">No-op @profile decorator</a> later in this chapter.</p></div><p>The arguments are <code class="literal">-l</code> for line-by-line (rather than function level) profiling and <code class="literal">-v</code> for a verbose output. Without <code class="literal">-v</code> you receive an <code class="literal">.lprof</code> output which you can later analyse with the <code class="literal">line_profiler</code> module. Below in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-kernprof-run1">Example&nbsp;2-6</a> we’ll do a full run on our CPU-bound function.</p><div class="example"><a id="profiling-kernprof-run1"></a><div class="example-title">Example&nbsp;2-6.&nbsp;Running kernprof with line-by-line output on a decorated function to record the CPU cost of each line’s execution</div><div class="example-contents"><pre class="screen">$ kernprof.py -l -v julia1_lineprofiler.py
...
Wrote profile results to julia1_lineprofiler.py.lprof
Timer unit: 1e-06 s

File: julia1_lineprofiler.py
Function: calculate_z_serial_purepython at line 9
Total time: 100.81 s

Line #      Hits   Per Hit   % Time  Line Contents
==================================================
     9                               @profile
    10                               def calculate_z_serial_purepython(maxiter, zs, cs):
    11                                   """Calculate output list using
                                         Julia update rule"""
    12         1    6870.0      0.0      output = [0] * len(zs)
    13   1000001       0.8      0.8      for i in range(len(zs)):
    14   1000000       0.8      0.8          n = 0
    15   1000000       0.8      0.8          z = zs[i]
    16   1000000       0.8      0.8          c = cs[i]
    17  34219980       1.1     36.2          while abs(z) &lt; 2 and n &lt; maxiter:
    18  33219980       1.0     32.6              z = z * z + c
    19  33219980       0.8     27.2              n += 1
    20   1000000       0.9      0.9          output[i] = n
    21         1       4.0      0.0      return output</pre></div></div><p>Introducing <code class="literal">kernprof.py</code> adds a substational time cost to the run time. In the above example <code class="literal">calculate_z_serial_purepython</code> takes 100 seconds, this is up from 13 seconds using simple <code class="literal">print</code> statements and 19 seconds using <code class="literal">cProfile</code>. The gain is that we get a line-by-line breakdown of where the time is spent inside the function.</p><p>The <code class="literal">% Time</code> column is the most helpful - we can see that 36% of the time is spent on the <code class="literal">while</code> testing. We don’t know whether the first statement (<code class="literal">abs(z) &lt; 2</code>) is more expensive than the second (<code class="literal">n &lt; maxiter</code>). Inside the loop we see that the update to <code class="literal">z</code> is also fairly expensive. Even <code class="literal">n += 1</code> is expensive! Python’s dynamic lookup machinery is at work for every loop even though we’re using the same types for each variable in each loop - this is where compiling and type specialization (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch07.html">Chapter&nbsp;7</a>) give us a massive win. The creation of the <code class="literal">output</code> list and the updates on line 20 are relatively cheap compared to the cost of the <code class="literal">while</code> loop.</p><p>The obvious way to further analyse the <code class="literal">while</code> statement is to break it up. Whilst there has been some discussion in the Python community around the idea of re-writing the <code class="literal">pyc</code> files with more detailed information for multi-part single line statements, we are unaware of any production tools that offer a more fine-grained analysis than <code class="literal">line_profiler</code>.</p><p>Below in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-kernprof-run2">Example&nbsp;2-7</a> we break the <code class="literal">while</code> logic into several statements. This additional complexity will increase the run time of the function as we have more lines of code to execute and it <span class="emphasis"><em>might</em></span> help us to understand the costs incurred in this part of the code.</p><div class="tip"><h3 class="title">Tip</h3><p><span class="emphasis"><em>Before you look at the code</em></span> - do you think we’ll learn about the costs of the fundamental operations this way? Might other factors complicate the analysis?</p></div><div class="example"><a id="profiling-kernprof-run2"></a><div class="example-title">Example&nbsp;2-7.&nbsp;Breaking the compound while statement into individual statements to record the cost of each part of the original parts</div><div class="example-contents"><pre class="screen">$ kernprof.py -l -v julia1_lineprofiler2.py
...
Wrote profile results to julia1_lineprofiler2.py.lprof
Timer unit: 1e-06 s

File: julia1_lineprofiler2.py
Function: calculate_z_serial_purepython at line 9
Total time: 184.739 s

Line #      Hits    Per Hit   % Time  Line Contents
===================================================
     9                                @profile
    10                                def calculate_z_serial_purepython(maxiter, zs, cs):
    11                                    """Calculate output list using
                                          Julia update rule"""
    12         1     6831.0      0.0      output = [0] * len(zs)
    13   1000001        0.8      0.4      for i in range(len(zs)):
    14   1000000        0.8      0.4          n = 0
    15   1000000        0.9      0.5          z = zs[i]
    16   1000000        0.8      0.4          c = cs[i]
    17  34219980        0.8     14.9          while True:
    18  34219980        1.0     19.0              not_yet_escaped = abs(z) &lt; 2
    19  34219980        0.8     15.5              iterations_left = n &lt; maxiter
    20  34219980        0.8     15.1              if not_yet_escaped
                                                      and iterations_left:
    21  33219980        1.0     17.5                  z = z * z + c
    22  33219980        0.9     15.3                  n += 1
    23                                            else:
    24   1000000        0.8      0.4                  break
    25   1000000        0.9      0.5          output[i] = n
    26         1        5.0      0.0      return output</pre></div></div><p>This version takes 184 seconds to execute whilst the previous version took 100 seconds. Other factors <span class="emphasis"><em>did</em></span> complicate the analysis. In this case having extra statements that have to be executed 34,219,980 times each slows down the code. If we hadn’t used <code class="literal">kernprof.py</code> to investigate the line-by-line effect of this change we might have drawn other conclusions about the reason for the slowdown as we’d have lacked the necessary evidence.</p><p>At this point it makes sense to step back to the earlier <code class="literal">timeit</code> technique to test the cost of individual expressions.</p><pre class="programlisting"><code class="o">&gt;&gt;&gt;</code> <code class="n">z</code> <code class="o">=</code> <code class="mi">0</code><code class="o">+</code><code class="mi">0j</code>  <code class="c"># a point in the middle of our image</code>
<code class="o">&gt;&gt;&gt;</code> <code class="o">%</code><code class="n">timeit</code> <code class="nb">abs</code><code class="p">(</code><code class="n">z</code><code class="p">)</code> <code class="o">&lt;</code> <code class="mi">2</code>  <code class="c"># tested inside IPython</code></pre><pre class="screen">10000000 loops, best of 3: 119 ns per loop</pre><pre class="programlisting"><code class="o">&gt;&gt;&gt;</code> <code class="n">n</code> <code class="o">=</code> <code class="mi">1</code>
<code class="o">&gt;&gt;&gt;</code> <code class="n">maxiter</code> <code class="o">=</code> <code class="mi">300</code>
<code class="o">&gt;&gt;&gt;%</code><code class="n">timeit</code> <code class="n">n</code> <code class="o">&lt;</code> <code class="n">maxiter</code></pre><pre class="screen">10000000 loops, best of 3: 77 ns per loop</pre><p>From this simple analysis it looks as though the logic test on <code class="literal">n</code> is almost two times faster than the call to <code class="literal">abs</code>. Since the order of evaluation for Python statements is both left-to-right and opportunistic it makes sense to put the cheapest test on the left side of the equation. On every 1 in 301 tests for each co-ordinate the <code class="literal">n &lt; maxiter</code> test will be <code class="literal">False</code> so Python wouldn’t need to evaluate the other side of the <code class="literal">and</code> operator.</p><p>We never know whether <code class="literal">abs(z) &lt; 2</code> will be false until we evaluate it and our earlier observations for this region of the complex plane suggest it is <code class="literal">True</code> around 10% of the time for all 300 iterations. If we wanted to have a strong understanding of the time complexity of this part of the code it would make sense to continue the numerical analysis. In this situation however we want an easy check to see if we can get a quick win.</p><p>We can form a new hypothesis stating “by swapping the order of the operators in the <code class="literal">while</code> statement we will achieve a reliable speed-up”. We <span class="emphasis"><em>can</em></span> test this hypothesis using <code class="literal">kernprof.py</code> but the additional overheads of profiling this way might add too much noise. Instead we can use an earlier version of the code, running a test comparing these two versions with <code class="literal">while abs(z) &lt; 2 and n &lt; maxiter:</code> against <code class="literal">while n &lt; maxiter and abs(z) &lt; 2:</code>.</p><p>The result was a fairly stable improvement of approximately 0.4 seconds. This result is obviously minor and is very problem specific, using a more suitable approach to solve this problem (e.g. swapping to use Cython or PyPy - see <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch07.html">Chapter&nbsp;7</a>) would yield greater gains.</p><p>We can be confident in our result because:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
we stated a hypothesis that was easy to test
</li><li class="listitem">
we changed our code so that only the hypothesis would be tested (never test two things at once!)
</li><li class="listitem">
we gathered enough evidence to support our conclusion
</li></ul></div><p>For completeness we can run a final <code class="literal">kernprof.py</code> on the two main functions including our optimization to confirm that we have a complete picture of the overall complexity of our code:</p><div class="example"><a id="profiling-kernprof-run3"></a><div class="example-title">Example&nbsp;2-8.&nbsp;Swapping the order of the compound while statement to make the test fractionally faster</div><div class="example-contents"><pre class="screen">$ kernprof.py -l -v julia1_lineprofiler3.py
...
Wrote profile results to julia1_lineprofiler3.py.lprof
Timer unit: 1e-06 s

File: julia1_lineprofiler3.py
Function: calculate_z_serial_purepython at line 9
Total time: 99.7097 s

Line #      Hits   Per Hit   % Time  Line Contents
==================================================
     9                               @profile
    10                               def calculate_z_serial_purepython(maxiter,
                                                                       zs, cs):
    11                                   """Calculate output list using
                                         Julia update rule"""
    12         1    6831.0      0.0      output = [0] * len(zs)
    13   1000001       0.8      0.8      for i in range(len(zs)):
    14   1000000       0.8      0.8          n = 0
    15   1000000       0.9      0.9          z = zs[i]
    16   1000000       0.8      0.8          c = cs[i]
    17  34219980       1.0     35.9          while n &lt; maxiter and abs(z) &lt; 2:
    18  33219980       1.0     32.0              z = z * z + c
    19  33219980       0.8     27.9              n += 1
    20   1000000       0.9      0.9          output[i] = n
    21         1       5.0      0.0      return output</pre></div></div><p>Having swapped the two components of the <code class="literal">while</code> test on line 17 in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-kernprof-run3">Example&nbsp;2-8</a> we see a modest improvement from 36.1% of the execution time to 35.9% (this result was stable over repeated runs).</p><div class="example"><a id="profiling-kernprof-run4"></a><div class="example-title">Example&nbsp;2-9.&nbsp;Testing the line-by-line costs of the setup routine</div><div class="example-contents"><pre class="screen">File: julia1_lineprofiler3.py
Function: calc_pure_python at line 24
Total time: 195.218 s

Line #      Hits   Per Hit   % Time  Line Contents
==================================================
    24                               @profile
    25                               def calc_pure_python(draw_output,
                                                          desired_width, max_iterations):
...
    44         1       1.0      0.0      zs = []
    45         1       1.0      0.0      cs = []
    46      1001       1.1      0.0      for ycoord in y:
    47   1001000       1.1      0.5          for xcoord in x:
    48   1000000       1.5      0.8              zs.append(
                                                        complex(xcoord, ycoord))
    49   1000000       1.6      0.8              cs.append(
                                                        complex(c_real, c_imag))
    50
    51         1      51.0      0.0      print "Length of x:", len(x)
    52         1      11.0      0.0      print "Total elements:", len(zs)
    53         1       6.0      0.0      start_time = time.time()
    54         1  191031307.0     97.9   output =
                                           calculate_z_serial_purepython
                                           (max_iterations, zs, cs)
    55         1       4.0      0.0      end_time = time.time()
    56         1       2.0      0.0      secs = end_time - start_time
    57         1      58.0      0.0      print calculate_z_serial_purepython
                                           .func_name + " took", secs, "seconds"
    58
                                         # this sum is expected for 1000^2 grid with 300 iterations
    59         1    9799.0      0.0      assert sum(output) == 33219980</pre></div></div><p>As expected in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-kernprof-run4">Example&nbsp;2-9</a> <code class="literal">calculate_z_serial_purepython</code> takes most (97%) of the time of its parent function. The <code class="literal">list</code> creation steps are minor in comparison.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="memory_profiler">memory_profiler for diagnosing memory usage</h2></div></div></div><p>Just as Robert Kern’s <code class="literal">line_profiler</code> package measures CPU usage, the <code class="literal">memory_profiler</code> module by Fabian Pedregosa and Philippe Gervais measures memory usage on a line-by-line basis. By understanding the memory usage characteristics of your code you could ask yourself two questions:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
Could we use <span class="emphasis"><em>less</em></span> RAM by re-writing this function to work more efficiently?
</li><li class="listitem">
Could we use <span class="emphasis"><em>more</em></span> RAM and save CPU cycles by caching?
</li></ul></div><p><code class="literal">memory_profiler</code> operates in a very similar way to <code class="literal">line_profiler</code> but runs far more slowly. If you install the <code class="literal">psutil</code> package (optional but recommended) then <code class="literal">memory_profiler</code> will run faster. Memory profiling may easily make your code run 10 to 100 times slower. In practice you will probably use <code class="literal">memory_profiler</code> occasionally and <code class="literal">line_profiler</code> (for CPU profiling) more frequently.</p><p>Installation: <code class="literal">pip install memory_profiler</code> (and optionally <code class="literal">pip install psutil</code>)</p><p>As mentioned above the implementation of <code class="literal">memory_profiler</code> is not as performant as the implementation of <code class="literal">line_profiler</code>, it may make sense to run your tests on a smaller problem which completes in a useful amount of time. Overnight runs might be sensible for validation but you need quick and reasonable iterations to diagnose problems and hypothesise solutions.</p><div class="note"><h3 class="title">Note</h3><p>The requirement to modify the source code is a minor annoyance as the addition of a decorator will break your unit tests unless you make a dummy decorator - see <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#no_op_profile_decorator">No-op @profile decorator</a> later in this chapter.</p></div><p>The code in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-memoryprofiler1">Example&nbsp;2-10</a> uses the full 1000x1000 grid, the statistics took about 1.5 hours to collect on Ian’s laptop. When dealing with memory allocation you must be aware that the situation is not as clear-cut as it is with CPU usage.</p><p>Generally it is more efficient to over-allocate memory to a process into a local pool which can be used at leisure as memory allocation operations are relatively expensive. Garbage collection is not instant and so objects may be unavailable but still in the garbage collection pool for some time.</p><p>The outcome of this is that it is hard to really understand what is happening with memory usage and release inside a Python program as a line of code may not allocate a deterministic amount of memory <span class="emphasis"><em>as observed from outside the process</em></span>. Observing the gross trend over a set of lines is likely to lead to better insight than observing the behaviour of just one line.</p><p>Inside <code class="literal">calculate_z_serial_purepython</code> on line 12 below we see that the allocation of 1,000,000 items causes approximately 7MB of RAM to be added to this process. This does not mean that the <code class="literal">output</code> list is definitely 7MB in size, just that the process grew by approximately 7MB during the internal allocation of the list.  On line 13 we see that the process grew by approximately a further 32MB inside the loop, this can be attributed to the call to <code class="literal">range</code>. RAM tracking is further discussed in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch11.html#lessram-memory-profiler-1e8-same-items">Example&nbsp;11-1</a>, the difference between 7MB and 32MB is due to the contents of the two lists.</p><p>In the parent process on line 46 we see that the allocation of the <code class="literal">zs</code> and <code class="literal">cs</code> lists consumes approximately 79MB. Again it is worth noting that this is not necessarily the true size of the arrays, just the size that the process grew by once these lists had been created.</p><div class="example"><a id="profiling-memoryprofiler1"></a><div class="example-title">Example&nbsp;2-10.&nbsp;memory_profiler’s result on both of our main functions showing an unexpected memory use in <code class="literal">calculate_z_serial_purepython</code></div><div class="example-contents"><pre class="screen">$ python -m memory_profiler julia1_memoryprofiler.py
...
Line #    Mem usage    Increment   Line Contents
================================================
     9   89.934 MiB    0.000 MiB   @profile
    10                             def calculate_z_serial_purepython(maxiter, zs, cs):
    11                                 """Calculate output list using...
    12   97.566 MiB    7.633 MiB       output = [0] * len(zs)
    13  130.215 MiB   32.648 MiB       for i in range(len(zs)):
    14  130.215 MiB    0.000 MiB           n = 0
    15  130.215 MiB    0.000 MiB           z = zs[i]
    16  130.215 MiB    0.000 MiB           c = cs[i]
    17  130.215 MiB    0.000 MiB           while n &lt; maxiter and abs(z) &lt; 2:
    18  130.215 MiB    0.000 MiB               z = z * z + c
    19  130.215 MiB    0.000 MiB               n += 1
    20  130.215 MiB    0.000 MiB           output[i] = n
    21  122.582 MiB   -7.633 MiB       return output

Line #    Mem usage    Increment   Line Contents
================================================
    24   10.574 MiB -112.008 MiB   @profile
    25                             def calc_pure_python(draw_output,
                                                        desired_width,
                                                        max_iterations):
    26                                 """Create a list of complex ...
    27   10.574 MiB    0.000 MiB       x_step = (float(x2 - x1) / ...
    28   10.574 MiB    0.000 MiB       y_step = (float(y1 - y2) / ...
    29   10.574 MiB    0.000 MiB       x = []
    30   10.574 MiB    0.000 MiB       y = []
    31   10.574 MiB    0.000 MiB       ycoord = y2
    32   10.574 MiB    0.000 MiB       while ycoord &gt; y1:
    33   10.574 MiB    0.000 MiB           y.append(ycoord)
    34   10.574 MiB    0.000 MiB           ycoord += y_step
    35   10.574 MiB    0.000 MiB       xcoord = x1
    36   10.582 MiB    0.008 MiB       while xcoord &lt; x2:
    37   10.582 MiB    0.000 MiB           x.append(xcoord)
    38   10.582 MiB    0.000 MiB           xcoord += x_step
    ...
    44   10.582 MiB    0.000 MiB       zs = []
    45   10.582 MiB    0.000 MiB       cs = []
    46   89.926 MiB   79.344 MiB       for ycoord in y:
    47   89.926 MiB    0.000 MiB           for xcoord in x:
    48   89.926 MiB    0.000 MiB               zs.append(complex(xcoord, ycoord))
    49   89.926 MiB    0.000 MiB               cs.append(complex(c_real, c_imag))
    50
    51   89.934 MiB    0.008 MiB       print "Length of x:", len(x)
    52   89.934 MiB    0.000 MiB       print "Total elements:", len(zs)
    53   89.934 MiB    0.000 MiB       start_time = time.time()
    54                                 output = calculate_z_serial...
    55  122.582 MiB   32.648 MiB       end_time = time.time()
    ...</pre></div></div><p>Another way to visualise the change in memory use is to sample over time and plot the result. <code class="literal">memory_profiler</code> has a utility called <code class="literal">mprof</code>, it is used once to sample the memory usage and a second time to visualise the samples. It samples by time and not by line so it barely impacts the run-time of the code.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-memoryprofiler-mprof">Figure&nbsp;2-6</a> is created using <code class="literal">mprof run julia1_memoryprofiler.py</code>, this writes a statistics file which  is then visualised using <code class="literal">mprof plot</code>.</p><p>Our two functions are bracketed, this shows where in time they are entered and we can see the growth in RAM as they run. Inside <code class="literal">calculate_z_serial_purepython</code> we can see the steady increase in RAM usage throughout the execution of the function, this is caused by all the small objects (<code class="literal">int</code> and <code class="literal">float</code> types) that are created.</p><div class="figure"><a id="FIG-julia-memoryprofiler-mprof"></a><div class="figure-contents"><div class="mediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler_mprof.png" alt="memory_profiler report using mprof" width="1000" height="532" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler_mprof.png"></div></div><div class="figure-title">Figure&nbsp;2-6.&nbsp;memory_profiler report using mprof.</div></div><p>In addition to observing the behaviour at the function level we can add labels using a context manager. The following snippet in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-memoryprofiler2">Example&nbsp;2-11</a> is used to generate the graph in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-memoryprofiler-mprof-labels">Figure&nbsp;2-7</a>. The <code class="literal">time.sleep(1)</code> is an artificial addition to make the graph easier to understand. We can see the <code class="literal">create_output_list</code> label, it occurs momentarily after <code class="literal">calculate_z_serial_purepython</code> and the process has been allocated more RAM. We then pause for a second to make the graph clearer.</p><p>After the label <code class="literal">create_range_of_zs</code> we see a large and quick increase in RAM usage, in the modified code below you can see this label when we create the <code class="literal">iterations</code> list. Rather than using <code class="literal">xrange</code> instead we’ve used <code class="literal">range</code> - the diagram should make it clear that a large list of 1,000,000 elements is instantiated just for the purposes of generating an index and that this is an inefficient approach that will not scale to larger list sizes (we will run out of RAM!). The allocation of the memory used to hold this list will itself take a small amount of time which contributes nothing useful to this function.</p><div class="note"><h3 class="title">Note</h3><p>In Python 3 the behavior of <code class="literal">range</code> changes - it works like <code class="literal">xrange</code> from Python 2. <code class="literal">xrange</code> is deprecated in Python 3 and the <code class="literal">2to3</code> conversion tool takes care of this change automatically.</p></div><div class="example"><a id="profiling-memoryprofiler2"></a><div class="example-title">Example&nbsp;2-11.&nbsp;Using a context manager to add labels to the mprof graph</div><div class="example-contents"><pre class="screen">@profile
def calculate_z_serial_purepython(maxiter, zs, cs):
    """Calculate output list using Julia update rule"""
    with profile.timestamp("create_output_list"):
        output = [0] * len(zs)
    time.sleep(1)
    with profile.timestamp("create_range_of_zs"):
        iterations = range(len(zs))
        with profile.timestamp("calculate_output"):
            for i in iterations:
                n = 0
                z = zs[i]
                c = cs[i]
                while n &lt; maxiter and abs(z) &lt; 2:
                    z = z * z + c
                    n += 1
                output[i] = n
    return output</pre></div></div><p>In the <code class="literal">calculate_output</code> block which runs for most of the graph we see a very slow, linear increase in RAM usage, this will be from all of the temporary numbers used in the inner loops. Using the labels really helps us to understand at a fine-grained level where memory is being consumed.</p><div class="figure"><a id="FIG-julia-memoryprofiler-mprof-labels"></a><div class="figure-contents"><div class="mediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler2_mprof.png" alt="memory_profiler report using mprof with labels" width="1000" height="532" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler2_mprof.png"></div></div><div class="figure-title">Figure&nbsp;2-7.&nbsp;memory_profiler report using mprof with labels.</div></div><p>Finally we could change the <code class="literal">range</code> call into an <code class="literal">xrange</code> and in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-memoryprofiler2-xrange-mprof-labels">Figure&nbsp;2-8</a> we see the corresponding decrease in RAM usage in our inner loop.</p><div class="figure"><a id="FIG-julia-memoryprofiler2-xrange-mprof-labels"></a><div class="figure-contents"><div class="mediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler2_xrange_mprof.png" alt="memory_profiler with xrange using mprof with labels" width="1000" height="532" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_memoryprofiler2_xrange_mprof.png"></div></div><div class="figure-title">Figure&nbsp;2-8.&nbsp;memory_profiler report using mprof with labels.</div></div><p>If we want to measure the RAM used by several statements we can use the IPython magic <code class="literal">%memit</code>, it works just like <code class="literal">%timeit</code>. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch11.html">Chapter&nbsp;11</a> we look at using <code class="literal">%memit</code> to measure the memory cost of lists and discuss various ways of using RAM more efficiently.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-heapy">Inspecting objects on the heap with heapy</h2></div></div></div><p>The Guppy project has a heap inspection tool, it lets us look at the number and size of each object on Python’s heap. Looking inside the interpreter and understanding what’s held in memory is extremely useful in rare but difficult debugging sessions where you really need to know how many objects are in use and whether they get garbage collected at appropriate times. If you have a difficult memory leak (probably because references to your objects remain hidden in a complex system) then this is the tool that’ll get you to the root of the problem.</p><p>If you’re reviewing your code to see if it is generating as many objects as you predict then this tool is very useful - the results might surprise you and that could lead to new optimization opportunities.</p><p>Installation: <code class="literal">pip install guppy</code></p><p>The listing in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-heapy1">Example&nbsp;2-12</a> is a slightly modified version of the Julia code, the heap object <code class="literal">hpy</code> is included in <code class="literal">calc_pure_python</code> and we print out the state of the heap at three places.</p><div class="example"><a id="profiling-heapy1"></a><div class="example-title">Example&nbsp;2-12.&nbsp;Using heapy to see how object counts evolve during the run of our code</div><div class="example-contents"><pre class="screen">def calc_pure_python(draw_output, desired_width, max_iterations):
    ...
    while xcoord &lt; x2:
        x.append(xcoord)
        xcoord += x_step

    from guppy import hpy; hp = hpy()
    print "heapy after creating y and x lists of floats"
    h = hp.heap()
    print h
    print

    zs = []
    cs = []
    for ycoord in y:
        for xcoord in x:
            zs.append(complex(xcoord, ycoord))
            cs.append(complex(c_real, c_imag))

    print "heapy after creating zs and cs using complex numbers"
    h = hp.heap()
    print h
    print

    print "Length of x:", len(x)
    print "Total elements:", len(zs)
    start_time = time.time()
    output = calculate_z_serial_purepython(max_iterations, zs, cs)
    end_time = time.time()
    secs = end_time - start_time
    print calculate_z_serial_purepython.func_name + " took", secs, "seconds"

    print
    print "heapy after calling calculate_z_serial_purepython"
    h = hp.heap()
    print h
    print</pre></div></div><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-heapy2">Example&nbsp;2-13</a> shows that after creating the <code class="literal">zs</code> and <code class="literal">cs</code> lists the memory use is more interesting as it has grown by approximately 80MB due to 2,000,000 <code class="literal">complex</code> objects consuming 64,000,000 bytes. These complex numbers represent the majority of the memory usage at this stage. If you wanted to optimize the memory usage in this program then this result would be revealing - we can see both how many objects are being stored and their overall size.</p><div class="example"><a id="profiling-heapy2"></a><div class="example-title">Example&nbsp;2-13.&nbsp;Looking at the output of heapy to see how our object counts increase at each major stage of our code’s execution</div><div class="example-contents"><pre class="screen">guppy $ python julia1_guppy.py
heapy after creating y and x lists of floats
Partition of a set of 27293 objects. Total size = 3416032 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0  10960  40  1050376  31   1050376  31 str
     1   5768  21   465016  14   1515392  44 tuple
     2    199   1   210856   6   1726248  51 dict of type
     3     72   0   206784   6   1933032  57 dict of module
     4   1592   6   203776   6   2136808  63 types.CodeType
     5    313   1   201304   6   2338112  68 dict (no owner)
     6   1557   6   186840   5   2524952  74 function
     7    199   1   177008   5   2701960  79 type
     8    124   0   135328   4   2837288  83 dict of class
     9   1045   4    83600   2   2920888  86 __builtin__.wrapper_descriptor
&lt;91 more rows. Type e.g. '_.more' to view.&gt;

heapy after creating zs and cs using complex numbers
Partition of a set of 2027301 objects. Total size = 83671256 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0 2000000  99 64000000  76  64000000  76 complex
     1    185   0 16295368  19  80295368  96 list
     2  10962   1  1050504   1  81345872  97 str
     3   5767   0   464952   1  81810824  98 tuple
     4    199   0   210856   0  82021680  98 dict of type
     5     72   0   206784   0  82228464  98 dict of module
     6   1592   0   203776   0  82432240  99 types.CodeType
     7    319   0   202984   0  82635224  99 dict (no owner)
     8   1556   0   186720   0  82821944  99 function
     9    199   0   177008   0  82998952  99 type
&lt;92 more rows. Type e.g. '_.more' to view.&gt;

Length of x: 1000
Total elements: 1000000
calculate_z_serial_purepython took 13.2436609268 seconds

heapy after calling calculate_z_serial_purepython
Partition of a set of 2127696 objects. Total size = 94207376 bytes.
 Index  Count   %     Size   % Cumulative  % Kind (class / dict of class)
     0 2000000  94 64000000  68  64000000  68 complex
     1    186   0 24421904  26  88421904  94 list
     2 100965   5  2423160   3  90845064  96 int
     3  10962   1  1050504   1  91895568  98 str
     4   5767   0   464952   0  92360520  98 tuple
     5    199   0   210856   0  92571376  98 dict of type
     6     72   0   206784   0  92778160  98 dict of module
     7   1592   0   203776   0  92981936  99 types.CodeType
     8    319   0   202984   0  93184920  99 dict (no owner)
     9   1556   0   186720   0  93371640  99 function
&lt;92 more rows. Type e.g. '_.more' to view.&gt;</pre></div></div><p>In the third section after calculating the Julia result we have used 94MB. In addition to the complex numbers we now have a large collection of integers in addition to more items stored in lists.</p><p><code class="literal">hpy.setrelheap()</code> could be used to create a check-point of the memory configuration so subsequent calls to <code class="literal">hpy.heap()</code> will generate a delta from this check-point. This way you can avoid seeing the internals of Python and prior memory setup before the point of the program that you’re analysing.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-dowser">Dowser for live graphing of instantiated variables</h2></div></div></div><p>Rober Brewer’s <code class="literal">dowser</code> hooks into the namespace of the running code and provides a real-time view of instantiated variables via a CherryPy interface in a web-browser. Each object that is being tracked has an associated sparkline graphic so you can watch to see if the quantities of certain objects are growing. This is useful for debugging long-running processes.</p><p>If you have a long-running process and you expect different memory behavior to occur depending on the actions you take in the application (e.g. with a web server you might upload data or cause complex queries to run) you can confirm this interactively, there’s an example in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-dowser-sparklines">Figure&nbsp;2-9</a>.</p><div class="figure"><a id="FIG-julia-dowser-sparklines"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 135; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_dowser_sparklines.png" alt="sparklines in dowser" width="265" height="254" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_dowser_sparklines.png"></div></div><div class="figure-title">Figure&nbsp;2-9.&nbsp;Several sparklines shown through CherryPy with <code class="literal">dowser</code>.</div></div><p>To use it we add a convenience function, shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dowser1">Example&nbsp;2-14</a>, to the Julia code which can start the CherryPy server.</p><div class="example"><a id="profiling-dowser1"></a><div class="example-title">Example&nbsp;2-14.&nbsp;Helper function to start <code class="literal">dowser</code> in your application</div><div class="example-contents"><pre class="screen">def launch_memory_usage_server(port=8080):
    import cherrypy
    import dowser

    cherrypy.tree.mount(dowser.Root())
    cherrypy.config.update({
        'environment': 'embedded',
        'server.socket_port': port
    })

    cherrypy.engine.start()</pre></div></div><p>Before we begin our intensive calculations we launch the CherryPy server, shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dowser2">Example&nbsp;2-15</a>. Once we have completed our calculations we can keep the console open using the <code class="literal">sleep</code>, this leaves the CherryPy process running so we can continue to introspect the state of namespace.</p><div class="example"><a id="profiling-dowser2"></a><div class="example-title">Example&nbsp;2-15.&nbsp;Launching dowser at an appropriate point in our application which will launch a webserver</div><div class="example-contents"><pre class="screen"> ...
        for xcoord in x:
            zs.append(complex(xcoord, ycoord))
            cs.append(complex(c_real, c_imag))

    launch_memory_usage_server()

...
    output = calculate_z_serial_purepython(max_iterations, zs, cs)
...
    print "now waiting..."
    while True:
        time.sleep(1)</pre></div></div><p>By following the <code class="literal">TRACE</code> links in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#FIG-julia-dowser-1000000-lists">Figure&nbsp;2-10</a> we can view the contents of each <code class="literal">list</code> object. We could further drill down into each <code class="literal">list</code>, this is like using an interactive debugger in an IDE but you can do this on a deployed server <span class="emphasis"><em>without</em></span> an IDE.</p><div class="figure"><a id="FIG-julia-dowser-1000000-lists"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 405; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/julia_dowser_1000000_lists.png" alt="1000000 items in a list" width="641" height="203" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/julia_dowser_1000000_lists.png"></div></div><div class="figure-title">Figure&nbsp;2-10.&nbsp;1,000,000 items in a list with <code class="literal">dowser</code>.</div></div><div class="note"><h3 class="title">Note</h3><p>We prefer to extract blocks of code that can be profiled in controlled conditions. Sometimes this isn’t practical and sometimes you just need simpler diagnostics - watching a live trace of a running process can be a perfect half-way house to gather the necessary evidence without doing lots of engineering.</p></div><p>So far we’ve reviewed various ways to measure the cost of Python code (both for CPU and RAM usage). We haven’t yet looked at the underlying bytecode used by the virtual machine, understanding what’s happening “under the hood” helps to build a mental model of what’s happening in slow functions and it’ll help when you come to compile your code. Let’s introduce some bytecode.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-dis">The dis module to examine CPython bytecode</h2></div></div></div><p>The <code class="literal">dis</code> module lets us inspect the underlying bytecode that we run inside the stack based CPython virtual machine. Having an understanding of what’s happening in the virtual machine that runs your higher level Python code will help you to understand why some styles of coding are faster than others. It will also help when you come to use a tool like Cython which steps outside of Python and generates C code.</p><p>The <code class="literal">dis</code> module is built-in. You can pass it code or a module and it will print out a disassembly. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dis1">Example&nbsp;2-16</a> we disassemble the outer loop of our CPU-bound function.</p><div class="tip"><h3 class="title">Tip</h3><p>You should try to disassemble one of your own functions and attempt to <span class="emphasis"><em>exactly</em></span> follow how the disassembled code matches to the disassembled output. Can you match the following <code class="literal">dis</code> output to the original function?</p></div><div class="example"><a id="profiling-dis1"></a><div class="example-title">Example&nbsp;2-16.&nbsp;Using the built-in <code class="literal">dis</code> to understand the underlying stack-based virtual machine that runs our Python code</div><div class="example-contents"><pre class="screen">In [1]: import dis
In [2]: import julia1_nopil

In [3]: dis.dis(julia1_nopil.calculate_z_serial_purepython)
 11           0 LOAD_CONST               1 (0)
              3 BUILD_LIST               1
              6 LOAD_GLOBAL              0 (len)
              9 LOAD_FAST                1 (zs)
             12 CALL_FUNCTION            1
             15 BINARY_MULTIPLY
             16 STORE_FAST               3 (output)

 12          19 SETUP_LOOP             123 (to 145)
             22 LOAD_GLOBAL              1 (range)
             25 LOAD_GLOBAL              0 (len)
             28 LOAD_FAST                1 (zs)
             31 CALL_FUNCTION            1
             34 CALL_FUNCTION            1
             37 GET_ITER
        &gt;&gt;   38 FOR_ITER               103 (to 144)
             41 STORE_FAST               4 (i)

 13          44 LOAD_CONST               1 (0)
             47 STORE_FAST               5 (n)

# ...
# We'll snip the rest of the inner loop for brevity!
# ...

 19     &gt;&gt;  131 LOAD_FAST                5 (n)
            134 LOAD_FAST                3 (output)
            137 LOAD_FAST                4 (i)
            140 STORE_SUBSCR
            141 JUMP_ABSOLUTE           38
        &gt;&gt;  144 POP_BLOCK

 20     &gt;&gt;  145 LOAD_FAST                3 (output)
            148 RETURN_VALUE</pre></div></div><p>The output of the above code is fairly straightforward, if terse. The first column contains line numbers that relate to our original file. The second column contains several <code class="literal">&gt;&gt;</code> symbols, these are the destinations for jump points elsewhere in the code. The third column is the operation address along with the operation name. The fourth column contains the parameters for the operation. The fifth column contains annotations to help line up the bytecodes with the original Python parameters.</p><p>Refer back to <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-juliaset-intro3">Example&nbsp;2-3</a> to match the bytecode to the corresponding Python code. The bytecode starts by putting the constant value 0 onto the stack and then it builds a single element list. Next it searches the namespaces to find the <code class="literal">len</code> function, puts it on the stack, searches the namespaces again to find <code class="literal">zs</code> and then puts that onto the stack. On line 12 it calls the <code class="literal">len</code> function from the stack which consumes the <code class="literal">zs</code> reference in the stack, then it applies a binary multiply to the last two arguments (the length of <code class="literal">zs</code> and the single element list) and stores the result in <code class="literal">output</code>. That’s the first line of our Python function now dealt with. Follow the next block of bytecode to understand the behavior of the second line of Python code (the outer <code class="literal">for</code> loop).</p><div class="tip"><h3 class="title">Tip</h3><p>The jump points <code class="literal">&gt;&gt;</code> match to instructions like <code class="literal">JUMP_ABSOLUTE</code> and <code class="literal">POP_JUMP_IF_FALSE</code>. Go through your own disassembled function and match the jump points to the jump instructions.</p></div><p>Having introduced bytecode we can now ask - what’s the bytecode and time cost of writing an function out explicitly versus using built-ins to perform the same task?</p><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_different_approaches_different_complexity">Different approaches, different complexity</h3></div></div></div><div class="blockquote"><blockquote class="blockquote"><p>…
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you’re Dutch.
…</p><div class="attribution"><p>—<span class="attribution">
Tim Peters and The Zen of Python
</span></p></div></blockquote></div><p>There will be various ways to express your idea using Python. Generally there should be one sensible way to express your idea but if you came from an older version of Python or another programming language then you may have other methods in mind. Some of these ways of expressing an idea may be slower than others.</p><p>You probably care more about readability than speed for most of your code so your team can code quickly, without being puzzled by performant but opaque code. Some times you will want performance, without sacrificing readability. Some speed testing might be what you need.</p><p>Take a look at the following two code snippets in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dis2">Example&nbsp;2-17</a>, they both do the same job but the first will generate a lot of additional Python bytecode which will cause more overhead.</p><div class="example"><a id="profiling-dis2"></a><div class="example-title">Example&nbsp;2-17.&nbsp;A naive and a more efficient way to solve the same summation problem</div><div class="example-contents"><pre class="screen">def fn_expressive(upper = 1000000):
    total = 0
    for n in xrange(upper):
        total += n
    return total

def fn_terse(upper = 1000000):
    return sum(xrange(upper))

print "Functions return the same result:", fn_expressive() == fn_terse()</pre><pre class="screen">Functions return the same result:
True</pre></div></div><p>Both function calculate the sum of a range of integers. A simple rule of thumb (but one which you <span class="emphasis"><em>must</em></span> backup using profiling!) is that more lines of bytecode will execute more slowly than fewer equivalent lines of bytecode. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dis3">Example&nbsp;2-18</a> we can use IPython’s <code class="literal">%timeit</code> magic function to measure the best execution time from a set of runs.</p><div class="example"><a id="profiling-dis3"></a><div class="example-title">Example&nbsp;2-18.&nbsp;Using <code class="literal">%timeit</code> to test our hypothesis that using built-in functions should be faster than writing our own functions</div><div class="example-contents"><pre class="screen">%timeit fn_expressive()
%timeit fn_terse()

10 loops, best of 3: 42 ms per loop
100 loops, best of 3: 12.3 ms per loop</pre></div></div><p>If we use the <code class="literal">dis</code> module to investigate the code for each function in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-dis4">Example&nbsp;2-19</a> we can see that the Virtual Machine has 17 lines to execute with the more expressive function and only 6 to execute with the very readable but more terse second function:</p><div class="example"><a id="profiling-dis4"></a><div class="example-title">Example&nbsp;2-19.&nbsp;Using <code class="literal">dis</code> to view the number of bytecode instructions involved in our two functions</div><div class="example-contents"><pre class="screen">import dis
print fn_expressive.func_name
dis.dis(fn_expressive)

fn_expressive
  2           0 LOAD_CONST               1 (0)
              3 STORE_FAST               1 (total)

  3           6 SETUP_LOOP              30 (to 39)
              9 LOAD_GLOBAL              0 (xrange)
             12 LOAD_FAST                0 (upper)
             15 CALL_FUNCTION            1
             18 GET_ITER
        &gt;&gt;   19 FOR_ITER                16 (to 38)
             22 STORE_FAST               2 (n)

  4          25 LOAD_FAST                1 (total)
             28 LOAD_FAST                2 (n)
             31 INPLACE_ADD
             32 STORE_FAST               1 (total)
             35 JUMP_ABSOLUTE           19
        &gt;&gt;   38 POP_BLOCK

  5     &gt;&gt;   39 LOAD_FAST                1 (total)
             42 RETURN_VALUE

print fn_terse.func_name
dis.dis(fn_terse)

fn_terse
  8           0 LOAD_GLOBAL              0 (sum)
              3 LOAD_GLOBAL              1 (xrange)
              6 LOAD_FAST                0 (upper)
              9 CALL_FUNCTION            1
             12 CALL_FUNCTION            1
             15 RETURN_VALUE</pre></div></div><p>The difference between the two code blocks is striking. Inside <code class="literal">fn_expressive()</code> we maintain two local variables and iterate over a list using a <code class="literal">for</code> statement. The <code class="literal">for</code> loop will be checking to see if a <code class="literal">StopIteration</code> exception has been raised on each loop. Each iteration applies the <code class="literal">total.__add__</code> function which will check the type of the second variable (<code class="literal">n</code>) on each iteration. These checks all add a little expense.</p><p>Inside <code class="literal">fn_terse()</code> we call out to an optimized C list comprehension function which knows how to generate the final result without creating intermediate Python objects. This is much faster, although each iteration must still check for the types of the object that are being added together (in a later chapters we look at ways of fixing the type so we don’t need to check it on each iteration).</p><p>As noted above you <span class="emphasis"><em>must</em></span> profile your code, if you just rely on this heuristic then you will inevitably write slower code at some point. It is definitely worth learning if Python has a shorter and still readable way to solve your problem, it is more likely to be easily read by another programmer and it will <span class="emphasis"><em>probably</em></span> run faster.</p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-unit-testing">Unit testing during optimization to maintain correctness</h2></div></div></div><p>If you aren’t already unit-testing your code then you are probably hurting your longer-term productivity. Ian (blushing) is embarrassed to note that once he spent a day optimising his code, having disabled unit-tests because they were inconvenient, only to discover that his significant speed-up result was due to breaking a part of the algorithm he was improving. You do not need to make this mistake even once.</p><p>In addition to unit-testing you should also strongly consider <code class="literal">coverage.py</code>. It checks to see which lines of code are exercised by your tests and identifies the sections that have no coverage. This quickly lets you figure out whether you’re testing the code that you’re about to optimize, such that any mistakes that might creep in during the optimisation process are quickly caught.</p><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="no_op_profile_decorator">No-op @profile decorator</h3></div></div></div><p>Your unit-tests will fail with a <code class="literal">NameError</code> exception if your code uses <code class="literal">@profile</code> from the <code class="literal">line_profiler</code> or <code class="literal">memory_profiler</code>. The reason is that the unit-test framework will not be injecting the <code class="literal">profile</code> decorator into the local namespace. The no-op decorator below solves this problem, it is easiest to add it to the block of code that you’re testing and to remove it when done.</p><p>With the no-op decorator you can run your tests without modifying the code that you’re testing. This means you can run your tests after every profile-led optimization you make so you’ll never be caught out by a bad optimization step.</p><p>Let’s say we have the following trivial <code class="literal">ex.py</code> module in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-noop1">Example&nbsp;2-20</a>, it has a test (for <code class="literal">nosetests</code>) and a function that we’ve been profiling with either the <code class="literal">line_profiler</code> or <code class="literal">memory_profiler</code>:</p><div class="example"><a id="profiling-noop1"></a><div class="example-title">Example&nbsp;2-20.&nbsp;Simple function and test case where we wish to use <code class="literal">@profile</code></div><div class="example-contents"><pre class="screen"># ex.py
import unittest

@profile
def some_fn(nbr):
    return nbr * 2


class TestCase(unittest.TestCase):
    def test(self):
        result = some_fn(2)
        self.assertEquals(result, 4)</pre></div></div><p>If we run <code class="literal">nosetests</code> on our code we’ll have a <code class="literal">NameError</code>:</p><pre class="programlisting"><code class="err">$</code> <code class="n">nosetests</code> <code class="n">ex</code><code class="o">.</code><code class="n">py</code>
<code class="n">E</code>
<code class="o">======================================================================</code>
<code class="n">ERROR</code><code class="p">:</code> <code class="n">Failure</code><code class="p">:</code> <code class="ne">NameError</code> <code class="p">(</code><code class="n">name</code> <code class="s">'profile'</code> <code class="ow">is</code> <code class="ow">not</code> <code class="n">defined</code><code class="p">)</code>
<code class="o">...</code>
<code class="ne">NameError</code><code class="p">:</code> <code class="n">name</code> <code class="s">'profile'</code> <code class="ow">is</code> <code class="ow">not</code> <code class="n">defined</code>

<code class="n">Ran</code> <code class="mi">1</code> <code class="n">test</code> <code class="ow">in</code> <code class="mf">0.001</code><code class="n">s</code>

<code class="n">FAILED</code> <code class="p">(</code><code class="n">errors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre><p>The solution is to add a no-op decorator at the start of <code class="literal">ex.py</code> (you can remove it once you’re done with profiling). If the <code class="literal">profile</code> decorator is not found in one of the namespaces (because <code class="literal">line_profiler</code> or <code class="literal">memory_profiler</code> are not being used) then the no-op version we’ve written is added. If <code class="literal">line_profiler</code> or <code class="literal">memory_profiler</code> have injected the new function into the namespace then our no-op version is ignored.</p><p>For <code class="literal">line_profiler</code> we can add the code in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-noop2">Example&nbsp;2-21</a>.</p><div class="example"><a id="profiling-noop2"></a><div class="example-title">Example&nbsp;2-21.&nbsp;<code class="literal">line_profiler</code> fix to add a no-op <code class="literal">@profile</code> decorator to the namespace whilst unit-testing</div><div class="example-contents"><pre class="screen"># for line_profiler
if '__builtin__' not in dir() or not hasattr(__builtin__, 'profile'):
    def profile(func):
        def inner(*args, **kwargs):
            return func(*args, **kwargs)
        return inner</pre></div></div><p>The <code class="literal">__builtin__</code> test is for <code class="literal">nosetests</code>, the <code class="literal">hasattr</code> test is for identifying when the <code class="literal">profile</code> decorator has been injected into the namespace.</p><pre class="programlisting"><code class="err">$</code> <code class="n">kernprof</code><code class="o">.</code><code class="n">py</code> <code class="o">-</code><code class="n">v</code> <code class="o">-</code><code class="n">l</code> <code class="n">ex</code><code class="o">.</code><code class="n">py</code>
<code class="n">Line</code> <code class="c">#      Hits         Time  Per %%HTMLit   % Time  Line Contents</code>
<code class="o">==============================================================</code>
    <code class="mi">11</code>                                           <code class="nd">@profile</code>
    <code class="mi">12</code>                                           <code class="k">def</code> <code class="nf">some_fn</code><code class="p">(</code><code class="n">nbr</code><code class="p">):</code>
    <code class="mi">13</code>         <code class="mi">1</code>            <code class="mi">3</code>      <code class="mf">3.0</code>    <code class="mf">100.0</code>      <code class="k">return</code> <code class="n">nbr</code> <code class="o">*</code> <code class="mi">2</code>


<code class="err">$</code> <code class="n">nosetests</code> <code class="n">ex</code><code class="o">.</code><code class="n">py</code>
<code class="o">.</code>
<code class="n">Ran</code> <code class="mi">1</code> <code class="n">test</code> <code class="ow">in</code> <code class="mf">0.000</code><code class="n">s</code></pre><p>For <code class="literal">memory_profiler</code> we use the code in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#profiling-noop3">Example&nbsp;2-22</a>.</p><div class="example"><a id="profiling-noop3"></a><div class="example-title">Example&nbsp;2-22.&nbsp;<code class="literal">memory_profiler</code> fix to add a no-op <code class="literal">@profile</code> decorator to the namespace whilst unit-testing</div><div class="example-contents"><pre class="screen"># for memory_profiler
if 'profile' not in dir():
    def profile(func):
        def inner(*args, **kwargs):
            return func(*args, **kwargs)
        return inner</pre></div></div><p>We’d expect it to generate output like this:</p><pre class="programlisting"> <code class="err">$</code> <code class="n">python</code> <code class="o">-</code><code class="n">m</code> <code class="n">memory_profiler</code> <code class="n">ex</code><code class="o">.</code><code class="n">py</code>
<code class="o">...</code>
<code class="n">Line</code> <code class="c">#    Mem usage    Increment   Line Contents</code>
<code class="o">================================================</code>
    <code class="mi">11</code>   <code class="mf">10.809</code> <code class="n">MiB</code>    <code class="mf">0.000</code> <code class="n">MiB</code>   <code class="nd">@profile</code>
    <code class="mi">12</code>                             <code class="k">def</code> <code class="nf">some_fn</code><code class="p">(</code><code class="n">nbr</code><code class="p">):</code>
    <code class="mi">13</code>   <code class="mf">10.809</code> <code class="n">MiB</code>    <code class="mf">0.000</code> <code class="n">MiB</code>       <code class="k">return</code> <code class="n">nbr</code> <code class="o">*</code> <code class="mi">2</code>


<code class="err">$</code> <code class="n">nosetests</code> <code class="n">ex</code><code class="o">.</code><code class="n">py</code>
<code class="o">.</code>
<code class="n">Ran</code> <code class="mi">1</code> <code class="n">test</code> <code class="ow">in</code> <code class="mf">0.000</code><code class="n">s</code></pre><p>You can save yourself a few minutes by avoiding the use of these decorators but once you’ve lost hours making a false optimization which breaks your code, you’ll want to integrate this into your workflow.</p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="profiling-strategies">Strategies to profile your code successfully</h2></div></div></div><p>Profiling requires some time and concentration. You will stand a better chance of understanding your code if you separate the section you want to test from the main body of your code. By separating it you can unit-test the code to preserve correctness and you can pass in realistic fabricated data to exercise the inefficiencies you want to address.</p><p>Do remember to disable any BIOS-based accelerators, they will only confuse your results. On Ian’s laptop the Intel TurboBoost feature can temporarily accelerate a CPU above its normal maximum speed if it is cool enough. This means that a cool CPU may run the same block of code faster than a hot CPU. Your Operating System may also control the clock speed - a laptop on battery power is likely to more aggressively control CPU speed than a laptop on mains power. To create a more stable benchmarking configuration we:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
disable TurboBoost in the BIOS
</li><li class="listitem">
disable the Operating System’s ability to override the BIOS’ SpeedStep in the BIOS
</li><li class="listitem">
only used mains power (never battery power)
</li><li class="listitem">
disabled background tools like backups and Dropbox whilst running experiments
</li><li class="listitem">
run the experiments many times to obtain a stable measurement
</li><li class="listitem">
possibly drop to run-level 1 (Unix) so that no other tasks are running
</li><li class="listitem">
reboot and rerun the experiments to double-confirm the results
</li></ul></div><p>Try to hypothesise the expected behaviour of your code and then validate (or disprove!) the hypothesis with the result of a profiling step. Your choices will not change (you can only use the profiled results to drive your decisions) but your intuitive understanding of the code will improve and this will pay off in future projects as you are more likely to make performant decisions. Of course you will verify these performant decisions by profiling as you go.</p><p>Do not skimp on the preparation. If you try to performance test code deep inside a larger project without separating it from the larger project then you are likely to witness side effects which will side-track your efforts. It is likely to be harder to unit-test a larger project when you’re making fine-grained changes and this may further hamper your efforts. Side effects could include other threads and processes impacting CPU, memory, network and disk activity which will skew your results.</p><p>For web servers investigate <code class="literal">dowser</code> and <code class="literal">dozer</code>, you can use these to visualise in real-time the behavior of objects in the namespace. Definitely consider separating the code you want to test out of the main web application if possible, it will make profiling significantly easier.</p><p>Make sure your unit-tests exercise all the code paths in the code that you’re analysing. Anything you don’t test but is used in your benchmarking may cause subtle errors that will slow down your progress. Use <code class="literal">coverage.py</code> to confirm that your tests are covering all the code paths.</p><p>Unit-testing a complicated section of code that generates a large numerical output may be difficult. Do not be afraid to output a text file of results to run through <code class="literal">diff</code> or to use a <code class="literal">pickled</code> object. For numeric optimization problems Ian likes to create long text files of floating point numbers and to use <code class="literal">diff</code> - minor rounding errors show up immediately and can be rare in the output.</p><p>If your code might be subject to numerical rounding issues due to subtle changes then you are better off with a large output that can be used for a before-and-after comparison. One cause of rounding errors is the difference in floating point precision between CPU registers and main memory. Running your code through a different code path can cause subtle rounding errors which might later confound you - it is better to be aware of this as soon as they occur.</p><p>Obviously it makes sense to use a source code control tool whilst you a profiling and optimizing. Branching is cheap and it will preserve your sanity.</p><p>Having looked at profiling techniques you should have all the tools you need to identify bottlenecks around CPU and RAM usage in your code. Next we’ll look at how Python implements the most common containers so you can make sensible decisions about representing larger collections of data.</p></div><div class="footnotes" epub:type="footnotes"><br><hr style="width: 100; align: left;"><div class="footnote" id="ftn.id314061"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#id314061" class="simpara totri-footnote">4</a>] </sup><a class="ulink" href="http://en.wikipedia.org/wiki/Julia_set" target="_top">http://en.wikipedia.org/wiki/Julia_set</a></p></div><div class="footnote" id="ftn.id322178"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#id322178" class="simpara totri-footnote">5</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/timeit.html" target="_top">http://docs.python.org/2/library/timeit.html</a></p></div><div class="footnote" id="ftn.id312621"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#id312621" class="simpara totri-footnote">6</a>] </sup><a class="ulink" href="https://mail.python.org/pipermail/python-dev/2005-November/058212.html" target="_top">https://mail.python.org/pipermail/python-dev/2005-November/058212.html</a></p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#">
			
				Add Note
			
		</a></li>
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/Library/view/high-performance-python/9781449361747/ch01.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">1. Understanding Performant Python</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/Library/view/high-performance-python/9781449361747/ch03.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">3. Lists and Tuples</div>
        </a>
    
  
  </div>


      
    </section>
    <div class="reading-controls-bottom">
      <ul class="interface-controls js-bitlist">
        <li class="queue-control">
            <button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781449361747/chapter/ch02.html" data-for-analytics="9781449361747:ch02.html">
      <span>Add to Queue</span>
  </button>
        </li>
      </ul>
    </div>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  





    
    



        
      </div>
      



  <footer class="pagefoot t-pagefoot">
    <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li><a class="t-queue-footer" href="https://www.safaribooksonline.com/s/">Queue</a></li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li><a href="https://www.safaribooksonline.com/blog/">Blog</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://community.safaribooksonline.com/">Feedback</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2016 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    <div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch02.html#">Reset</a>
</div>
</div>
    
  

<div class="annotator-notice"></div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>