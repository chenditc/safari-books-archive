<!--[if IE]><![endif]--><!DOCTYPE html><!--[if IE 8]><html class="no-js ie8 oldie" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#"

    
        itemscope itemtype="http://schema.org/Book http://schema.org/ItemPage"" data-login-url="/accounts/login/"
data-offline-url="/"
data-url="/library/view/high-performance-python/9781449361747/ch09.html"
data-csrf-cookie="csrfsafari"
data-highlight-privacy=""


  data-user-id="859452"
  data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36"
  data-username="dchen267"
  data-account-type="B2B"
  
  data-activated-trial-date="04/25/2016"


  data-archive="9781449361747"
  data-publishers="O&#39;Reilly Media, Inc."



  data-htmlfile-name="ch09.html"
  data-epub-title="High Performance Python" data-debug=0 data-testing=0><![endif]--><!--[if gt IE 8]><!--><html class=" js flexbox flexboxlegacy no-touch websqldatabase indexeddb history csscolumns csstransforms localstorage sessionstorage applicationcache svg inlinesvg zoom gr__safaribooksonline_com" lang="en" prefix="og: http://ogp.me/ns/# og:book: http://ogp.me/ns/book# og:video: http://ogp.me/ns/video#" itemscope="" itemtype="http://schema.org/Book http://schema.org/ItemPage" "="" data-login-url="/accounts/login/" data-offline-url="/" data-url="/library/view/high-performance-python/9781449361747/ch09.html" data-csrf-cookie="csrfsafari" data-highlight-privacy="" data-user-id="859452" data-user-uuid="4d373bec-fada-4717-9823-769db3ed3a36" data-username="dchen267" data-account-type="B2B" data-activated-trial-date="04/25/2016" data-archive="9781449361747" data-publishers="O'Reilly Media, Inc." data-htmlfile-name="ch09.html" data-epub-title="High Performance Python" data-debug="0" data-testing="0"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="author" content="Safari Books Online"><meta name="format-detection" content="telephone=no"><meta http-equiv="cleartype" content="on"><meta name="HandheldFriendly" content="True"><meta name="MobileOptimized" content="320"><meta name="apple-itunes-app" content="app-id=881697395, app-argument=safaridetail://9781449361747"><meta name="viewport" content="width=device-width, minimum-scale=1.0, initial-scale=1.0, maximum-scale=1.0"><meta property="twitter:account_id" content="4503599627559754"><link rel="apple-touch-icon" href="https://www.safaribooksonline.com/static/images/apple-touch-icon.8cc2fd27400e.png"><link rel="shortcut icon" href="https://www.safaribooksonline.com/favicon.ico" type="image/x-icon"><link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:200,300,400,600,700,900,200italic,300italic,400italic,600italic,700italic,900italic" rel="stylesheet" type="text/css"><title>9. The multiprocessing module - High Performance Python</title><link rel="stylesheet" href="https://www.safaribooksonline.com/static/CACHE/css/e4b0fef39b55.css" type="text/css"><link rel="stylesheet" type="text/css" href="https://www.safaribooksonline.com/static/css/annotator.min.fd58f69f4908.css"><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css"><style type="text/css" title="ibis-book">
    @charset "utf-8";#sbo-rt-content html,#sbo-rt-content div,#sbo-rt-content div,#sbo-rt-content span,#sbo-rt-content applet,#sbo-rt-content object,#sbo-rt-content iframe,#sbo-rt-content h1,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5,#sbo-rt-content h6,#sbo-rt-content p,#sbo-rt-content blockquote,#sbo-rt-content pre,#sbo-rt-content a,#sbo-rt-content abbr,#sbo-rt-content acronym,#sbo-rt-content address,#sbo-rt-content big,#sbo-rt-content cite,#sbo-rt-content code,#sbo-rt-content del,#sbo-rt-content dfn,#sbo-rt-content em,#sbo-rt-content font,#sbo-rt-content img,#sbo-rt-content ins,#sbo-rt-content kbd,#sbo-rt-content q,#sbo-rt-content s,#sbo-rt-content samp,#sbo-rt-content small,#sbo-rt-content strike,#sbo-rt-content strong,#sbo-rt-content sub,#sbo-rt-content sup,#sbo-rt-content tt,#sbo-rt-content var,#sbo-rt-content b,#sbo-rt-content u,#sbo-rt-content i,#sbo-rt-content center,#sbo-rt-content dl,#sbo-rt-content dt,#sbo-rt-content dd,#sbo-rt-content ol,#sbo-rt-content ul,#sbo-rt-content li,#sbo-rt-content fieldset,#sbo-rt-content form,#sbo-rt-content label,#sbo-rt-content legend,#sbo-rt-content table,#sbo-rt-content caption,#sbo-rt-content tdiv,#sbo-rt-content tfoot,#sbo-rt-content thead,#sbo-rt-content tr,#sbo-rt-content th,#sbo-rt-content td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}@page{margin:5px !important}#sbo-rt-content p{margin:8px 0 0;line-height:125%;text-align:left}#sbo-rt-content em{font-style:italic;font-family:inherit}#sbo-rt-content em.replaceable{font-style:italic}#sbo-rt-content em strong,#sbo-rt-content strong em{font-weight:bold;font-style:italic;font-family:inherit}#sbo-rt-content strong.userinput{font-weight:bold;font-style:normal}#sbo-rt-content span.bolditalic{font-weight:bold;font-style:italic}#sbo-rt-content strong,#sbo-rt-content span.bold{font-weight:bold}#sbo-rt-content a.ulink,#sbo-rt-content a.xref,#sbo-rt-content a.email,#sbo-rt-content a.link{text-decoration:none;color:#8e0012}#sbo-rt-content sup{font-size:x-small;vertical-align:super}#sbo-rt-content sub{font-size:smaller;vertical-align:sub}#sbo-rt-content span.lineannotation{font-style:italic;color:#A62A2A;font-family:serif,"DejaVuSerif"}#sbo-rt-content span.underline{text-decoration:underline}#sbo-rt-content span.strikethrough{text-decoration:line-through}#sbo-rt-content span.smallcaps{font-variant:small-caps}#sbo-rt-content span.cursor{background:#000;color:#FFF}#sbo-rt-content span.smaller{font-size:75%}#sbo-rt-content .boxedtext,#sbo-rt-content .keycap{border-style:solid;border-width:1px;border-color:#000;padding:1px}#sbo-rt-content span.gray50{color:#7F7F7F;}#sbo-rt-content .gray-background,#sbo-rt-content .reverse-video{background:#2E2E2E;color:#FFF}#sbo-rt-content .light-gray-background{background:#A0A0A0}#sbo-rt-content .preserve-whitespace{white-space:pre-wrap}#sbo-rt-content h1,#sbo-rt-content div.toc-title,#sbo-rt-content h2,#sbo-rt-content h3,#sbo-rt-content h4,#sbo-rt-content h5{-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none;font-weight:bold;text-align:left;page-break-after:avoid !important;font-family:sans-serif,"DejaVuSans"}#sbo-rt-content h1,#sbo-rt-content div.toc-title{font-size:1.5em;margin-top:20px !important}#sbo-rt-content h2{font-size:1.3em;color:#8e0012;margin:40px 0 8px 0}#sbo-rt-content h3{font-size:1.1em;margin:30px 0 8px 0 !important}#sbo-rt-content h4{font-size:1em;color:#555;margin:20px 0 8px 0 !important}#sbo-rt-content h5{font-size:1em;font-weight:normal;font-style:italic;margin:15px 0 6px 0 !important}#sbo-rt-content section.chapter div.titlepage,#sbo-rt-content section.appendix div.titlepage,#sbo-rt-content section.preface div.titlepage{page-break-inside:avoid;page-break-after:avoid}#sbo-rt-content section.chapter>div.titlepage,#sbo-rt-content section.preface>div.titlepage,#sbo-rt-content section.appendix>div.titlepage{margin-bottom:50px}#sbo-rt-content section.chapter>div.titlepage h2.title,#sbo-rt-content section.preface>div.titlepage h2.title,#sbo-rt-content section.appendix>div.titlepage h2.title{font-size:2em;line-height:1;margin-bottom:15px;color:#000;padding-bottom:10px;border-bottom:1px solid #000}#sbo-rt-content div.toc-title{margin-bottom:30px !important}#sbo-rt-content div.part h1{font-size:2em;text-align:center;margin-top:0 !important;padding:50px 0 20px 0;border-bottom:1px solid #000}#sbo-rt-content img{max-width:95%;margin:0 auto;padding:0}#sbo-rt-content div.figure{background-color:transparent;text-align:center;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;padding:15px 5px 15px 5px !important;margin:30px 0 30px 0 !important;max-height:100%;page-break-inside:avoid}#sbo-rt-content div.figure-title,#sbo-rt-content div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.informalfigure{text-align:center;padding:5px 0 !important}#sbo-rt-content div.sidebar{margin:30px 0 20px 0 !important;-webkit-border-radius:5px;border-radius:5px;border:1px solid #DCDCDC;background-color:#F7F7F7;font-size:90%;padding:15px !important;page-break-inside:avoid}#sbo-rt-content div.sidebar-title{font-weight:bold;font-size:1em;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px 0 !important;page-break-inside:avoid}#sbo-rt-content div.sidebar div.figure p.title,#sbo-rt-content div.sidebar div.informalfigure div.caption{font-size:90%;text-align:center;font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";color:#000;padding:5px !important;page-break-before:avoid;page-break-after:avoid}#sbo-rt-content div.sidebar ol{margin-left:15px}#sbo-rt-content div.sidebar div.tip,#sbo-rt-content div.sidebar div.note,#sbo-rt-content div.sidebar div.warning,#sbo-rt-content div.sidebar div.caution,#sbo-rt-content div.sidebar div.important{margin:20px auto 20px auto !important;font-size:90%;width:85%}#sbo-rt-content div.sidebar div.figure{border:none}#sbo-rt-content pre{white-space:pre-wrap;font-family:"Ubuntu Mono",monospace;margin:25px 0 25px 20px;font-size:85%;display:block;-webkit-hyphens:none;hyphens:none;adobe-hyphenate:none}#sbo-rt-content div.note pre.programlisting,#sbo-rt-content div.tip pre.programlisting,#sbo-rt-content div.warning pre.programlisting,#sbo-rt-content div.caution pre.programlisting,#sbo-rt-content div.important pre.programlisting{margin-bottom:0}#sbo-rt-content code{font-family:"Ubuntu Mono",monospace}#sbo-rt-content code strong em,#sbo-rt-content code em strong,#sbo-rt-content pre em strong,#sbo-rt-content pre strong em,#sbo-rt-content strong code em code,#sbo-rt-content em code strong code,#sbo-rt-content span.bolditalic code{font-weight:bold;font-style:italic;font-family:"Ubuntu Mono BoldItal",monospace}#sbo-rt-content code em,#sbo-rt-content em code,#sbo-rt-content pre em,#sbo-rt-content em.replaceable{font-family:"Ubuntu Mono Ital",monospace;font-style:italic}#sbo-rt-content code strong,#sbo-rt-content strong code,#sbo-rt-content pre strong,#sbo-rt-content strong.userinput{font-family:"Ubuntu Mono Bold",monospace;font-weight:bold}#sbo-rt-content div.example{margin:10px 0 15px 0 !important}#sbo-rt-content div.example-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:10px 0 5px 0 !important;border-bottom:1px solid #000}#sbo-rt-content li pre.example{padding:10px 0 !important}#sbo-rt-content div.example-contents pre.programlisting,#sbo-rt-content div.example-contents pre.screen{margin:0}#sbo-rt-content span.gray{color:#4C4C4C}#sbo-rt-content div.book div.titlepage h1.title{font-size:3em;font-family:sans-serif,"DejaVuSans";font-weight:bold;margin:50px 0 10px 0 !important;line-height:1;text-align:center}#sbo-rt-content div.book div.titlepage h2.subtitle{text-align:center;color:#000;margin:0 !important;font-style:italic;font-family:serif;font-size:1.5em}#sbo-rt-content div.book div.titlepage div.author h3{font-size:2em;font-family:sans-serif,"DejaVuSans";font-weight:bold;color:#8e0012;margin:50px 0 !important;text-align:center}#sbo-rt-content div.book div.titlepage div.publishername{margin-top:60%;margin-bottom:20px;text-align:center;font-size:1.25em}#sbo-rt-content div.book div.titlepage div.locations p{margin:0;text-align:center}#sbo-rt-content div.book div.titlepage div.locations p.cities{font-size:80%;text-align:center;margin-top:5px}#sbo-rt-content section.preface[title="Dedication"]>div.titlepage h2.title{text-align:center;text-transform:uppercase;font-size:1.5em;margin-top:50px;margin-bottom:50px}#sbo-rt-content section.preface[title="Dedication"] p{font-style:italic;text-align:center}#sbo-rt-content div.colophon h1.title{font-size:1.3em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon h2.subtitle{margin:0 !important;color:#000;font-family:serif,"DejaVuSerif";font-size:1em;font-weight:normal}#sbo-rt-content div.colophon div.author h3.author{font-size:1.1em;font-family:serif,"DejaVuSerif";margin:10px 0 0 !important;font-weight:normal}#sbo-rt-content div.colophon div.editor h4,#sbo-rt-content div.colophon div.editor h3.editor{color:#000;font-size:.8em;margin:15px 0 0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.editor h3.editor{font-size:.8em;margin:0 !important;font-family:serif,"DejaVuSerif";font-weight:normal}#sbo-rt-content div.colophon div.publisher{margin-top:10px}#sbo-rt-content div.colophon div.publisher p,#sbo-rt-content div.colophon div.publisher span.publishername{margin:0;font-size:.8em}#sbo-rt-content div.legalnotice p,#sbo-rt-content div.timestamp p{font-size:.8em}#sbo-rt-content div.timestamp p{margin-top:10pt}#sbo-rt-content div.colophon[title="About the Author"] h1.title,#sbo-rt-content div.colophon[title="Colophon"] h1.title{font-size:1.5em;margin:0 !important;font-family:sans-serif,"DejaVuSans";font-weight:bold}#sbo-rt-content section.chapter div.titlepage div.author{margin:10px 0 10px 0}#sbo-rt-content section.chapter div.titlepage div.author div.affiliation{font-style:italic}#sbo-rt-content div.attribution{margin:5px 0 0 50px !important}#sbo-rt-content h3.author span.orgname{display:none}#sbo-rt-content div.epigraph{margin:10px 0 10px 20px !important;page-break-inside:avoid;font-size:90%}#sbo-rt-content div.epigraph p{font-style:italic}#sbo-rt-content blockquote,#sbo-rt-content div.blockquote{margin:10px !important;page-break-inside:avoid;font-size:95%}#sbo-rt-content blockquote p,#sbo-rt-content div.blockquote p{font-family:serif,"DejaVuSerif";font-style:italic}#sbo-rt-content blockquote div.attribution{margin:5px 0 0 30px !important;text-align:right;width:80%}#sbo-rt-content blockquote div.attribution p{font-style:normal}#sbo-rt-content p.right{text-align:right;margin:0}#sbo-rt-content div.footnote{font-size:90%}#sbo-rt-content div.refnamediv h2,#sbo-rt-content div.refnamediv h3,#sbo-rt-content div.refsynopsisdiv h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refentry div.refsect1 h2{font-size:1.1em;color:#000;margin-top:15px !important;margin-bottom:0 !important}#sbo-rt-content div.refsect2 h3{font-size:1em;color:#000;margin-top:10px !important;margin-bottom:0 !important}#sbo-rt-content div.refnamediv p{margin-left:15px !important}#sbo-rt-content dt{padding-top:10px !important;padding-bottom:0 !important}#sbo-rt-content dt span.term{font-weight:bold;font-style:italic}#sbo-rt-content dt span.term code.literal{font-style:normal;font-weight:normal}#sbo-rt-content dd{margin-left:1.5em !important}#sbo-rt-content dd,#sbo-rt-content li{text-align:left}#sbo-rt-content ol{list-style-type:decimal;margin-top:8px !important;margin-bottom:8px !important;margin-left:20px !important;padding-left:25px !important}#sbo-rt-content ol ol{list-style-type:lower-alpha}#sbo-rt-content ol ol ol{list-style-type:lower-roman}#sbo-rt-content ul{list-style-type:square;margin-top:8px !important;margin-bottom:8px !important;margin-left:5px !important;padding-left:20px !important}#sbo-rt-content ul ul{list-style-type:none;padding-left:0 !important;margin-left:0 !important}#sbo-rt-content ol li,#sbo-rt-content ul li,#sbo-rt-content dd{margin-bottom:1em}#sbo-rt-content ul ul li p:before{content:"— "}#sbo-rt-content ul ul ul li p:before{content:""}#sbo-rt-content ul ul ul{list-style-type:square;margin-left:20px !important;padding-left:30px !important}#sbo-rt-content div.orderedlistalpha{list-style-type:upper-alpha}#sbo-rt-content table.simplelist{margin-left:20px !important;margin-bottom:10px}#sbo-rt-content table.simplelist td{border:none;font-size:90%}#sbo-rt-content table.simplelist tr{border-bottom:none}#sbo-rt-content table.simplelist tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.calloutlist p:first-child{margin-top:-25px !important}#sbo-rt-content div.calloutlist dd{padding-left:0;margin-top:-25px}#sbo-rt-content div.calloutlist img{padding:0}#sbo-rt-content a.co img{padding:0}#sbo-rt-content div.calloutlist>dl>dd>div.orderedlist{margin-top:25pt}#sbo-rt-content div.calloutlist>dl>dd>div.orderedlist>ol.orderedlist>li{margin-bottom:25pt}#sbo-rt-content div.toc ol{margin-top:8px !important;margin-bottom:8px !important;margin-left:0 !important;padding-left:0 !important}#sbo-rt-content div.toc ol ol{margin-left:30px !important;padding-left:0 !important}#sbo-rt-content div.toc ol li{list-style-type:none}#sbo-rt-content div.toc a{color:#8e0012}#sbo-rt-content div.toc ol a{font-size:1em;font-weight:bold}#sbo-rt-content div.toc ol>li>ol a{font-weight:bold;font-size:1em}#sbo-rt-content div.toc ol>li>ol>li>ol a{text-decoration:none;font-weight:normal;font-size:1em}#sbo-rt-content div.tip,#sbo-rt-content div.note,#sbo-rt-content div.warning,#sbo-rt-content div.caution,#sbo-rt-content div.important{margin:30px !important;-webkit-border-radius:5px;border-radius:5px;font-size:90%;padding:10px 8px 20px 8px !important;page-break-inside:avoid}#sbo-rt-content div.tip,#sbo-rt-content div.note,#sbo-rt-content div.important{border:1px solid #BEBEBE;background-color:transparent}#sbo-rt-content div.warning,#sbo-rt-content div.caution{border:1px solid #BC8F8F}#sbo-rt-content div.tip h3,#sbo-rt-content div.note h3,#sbo-rt-content div.warning h3,#sbo-rt-content div.caution h3,#sbo-rt-content div.important h3{font:bold 90%;font-family:sans-serif,"DejaVuSans";text-transform:uppercase;letter-spacing:1px;text-align:center;margin:4px 0 6px !important}#sbo-rt-content div.tip h3,#sbo-rt-content div.note h3,#sbo-rt-content div.important h3{color:#737373}#sbo-rt-content div.warning h3,#sbo-rt-content div.caution h3{color:#C67171}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note{background-color:transparent;margin:8px 0 0 !important;border:0 solid #BEBEBE;-webkit-border-radius:0;border-radius:0;font-size:100%;padding:0 !important;page-break-inside:avoid}#sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3{display:none}#sbo-rt-content div.table,#sbo-rt-content table{margin:15px auto 30px auto !important;max-width:95%;border-collapse:collapse;border-spacing:0}#sbo-rt-content div.table,#sbo-rt-content div.informaltable{page-break-inside:avoid}#sbo-rt-content tr{border-bottom:1px solid #c3c3c3}#sbo-rt-content thead td{border-bottom:#9d9d9d 1px solid !important;border-top:#9d9d9d 1px solid !important}#sbo-rt-content tr:nth-of-type(even){background-color:#f1f6fc}#sbo-rt-content thead{font-family:sans-serif,"DejaVuSans";color:#000;font-weight:bold}#sbo-rt-content td{padding:.3em;text-align:left;vertical-align:baseline;font-size:80%}#sbo-rt-content div.informaltable table{margin:10px auto !important}#sbo-rt-content div.informaltable table tr{border-bottom:none}#sbo-rt-content div.informaltable table tr:nth-of-type(even){background-color:transparent}#sbo-rt-content div.informaltable td,#sbo-rt-content div.informaltable th{border:#9d9d9d 1px solid}#sbo-rt-content div.table-title{font-weight:normal;font-style:italic;font-family:serif,"DejaVuSerif";margin:20px 0 0 0 !important;text-align:center;padding:0}#sbo-rt-content table code{font-size:smaller}#sbo-rt-content div.equation{margin:10px 0 15px 0 !important}#sbo-rt-content div.equation-title{font-style:italic;font-weight:normal;font-family:serif,"DejaVuSerif";margin:20px 0 10px 0 !important;page-break-after:avoid}#sbo-rt-content div.equation-contents{margin-left:20px}#sbo-rt-content span.inlinemediaobject{height:.85em;display:inline-block;margin-bottom:.2em}#sbo-rt-content span.inlinemediaobject img{margin:0;height:.85em}#sbo-rt-content div.informalequation{margin:20px 0 20px 20px;width:75%}#sbo-rt-content div.informalequation img{width:75%}#sbo-rt-content div.index{font-weight:bold}#sbo-rt-content div.index dt{line-height:140%}#sbo-rt-content div.index a.indexterm{color:#8e0012}#sbo-rt-content code.boolean,#sbo-rt-content .navy{color:rgb(0,0,128);}#sbo-rt-content code.character,#sbo-rt-content .olive{color:rgb(128,128,0);}#sbo-rt-content code.comment,#sbo-rt-content .blue{color:rgb(0,0,255);}#sbo-rt-content code.conditional,#sbo-rt-content .limegreen{color:rgb(50,205,50);}#sbo-rt-content code.constant,#sbo-rt-content .darkorange{color:rgb(255,140,0);}#sbo-rt-content code.debug,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.define,#sbo-rt-content .darkgoldenrod,#sbo-rt-content .gold{color:rgb(184,134,11);}#sbo-rt-content code.delimiter,#sbo-rt-content .dimgray{color:rgb(105,105,105);}#sbo-rt-content code.error,#sbo-rt-content .red{color:rgb(255,0,0);}#sbo-rt-content code.exception,#sbo-rt-content .salmon{color:rgb(250,128,11);}#sbo-rt-content code.float,#sbo-rt-content .steelblue{color:rgb(70,130,180);}#sbo-rt-content pre code.function,#sbo-rt-content .green{color:rgb(0,128,0);}#sbo-rt-content code.identifier,#sbo-rt-content .royalblue{color:rgb(65,105,225);}#sbo-rt-content code.ignore,#sbo-rt-content .gray{color:rgb(128,128,128);}#sbo-rt-content code.include,#sbo-rt-content .purple{color:rgb(128,0,128);}#sbo-rt-content code.keyword,#sbo-rt-content .sienna{color:rgb(160,82,45);}#sbo-rt-content code.label,#sbo-rt-content .deeppink{color:rgb(255,20,147);}#sbo-rt-content code.macro,#sbo-rt-content .orangered{color:rgb(255,69,0);}#sbo-rt-content code.number,#sbo-rt-content .brown{color:rgb(165,42,42);}#sbo-rt-content code.operator,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.preCondit,#sbo-rt-content .teal{color:rgb(0,128,128);}#sbo-rt-content code.preProc,#sbo-rt-content .fuschia{color:rgb(255,0,255);}#sbo-rt-content code.repeat,#sbo-rt-content .indigo{color:rgb(75,0,130);}#sbo-rt-content code.special,#sbo-rt-content .saddlebrown{color:rgb(139,69,19);}#sbo-rt-content code.specialchar,#sbo-rt-content .magenta{color:rgb(255,0,255);}#sbo-rt-content code.specialcomment,#sbo-rt-content .seagreen{color:rgb(46,139,87);}#sbo-rt-content code.statement,#sbo-rt-content .forestgreen{color:rgb(34,139,34);}#sbo-rt-content code.storageclass,#sbo-rt-content .plum{color:rgb(221,160,221);}#sbo-rt-content code.string,#sbo-rt-content .darkred{color:rgb(139,0,0);}#sbo-rt-content code.structure,#sbo-rt-content .chocolate{color:rgb(210,106,30);}#sbo-rt-content code.tag,#sbo-rt-content .darkcyan{color:rgb(0,139,139);}#sbo-rt-content code.todo,#sbo-rt-content .black{color:#000;}#sbo-rt-content code.type,#sbo-rt-content .mediumslateblue{color:rgb(123,104,238);}#sbo-rt-content code.typedef,#sbo-rt-content .darkgreen{color:rgb(0,100,0);}#sbo-rt-content code.underlined{text-decoration:underline;}#sbo-rt-content pre code.hll{background-color:#ffc}#sbo-rt-content pre code.c{color:#09F;font-style:italic}#sbo-rt-content pre code.err{color:#A00}#sbo-rt-content pre code.k{color:#069;font-weight:bold}#sbo-rt-content pre code.o{color:#555}#sbo-rt-content pre code.cm{color:#35586C;font-style:italic}#sbo-rt-content pre code.cp{color:#099}#sbo-rt-content pre code.c1{color:#35586C;font-style:italic}#sbo-rt-content pre code.cs{color:#35586C;font-weight:bold;font-style:italic}#sbo-rt-content pre code.gd{background-color:#FCC}#sbo-rt-content pre code.ge{font-style:italic}#sbo-rt-content pre code.gr{color:#F00}#sbo-rt-content pre code.gh{color:#030;font-weight:bold}#sbo-rt-content pre code.gi{background-color:#CFC}#sbo-rt-content pre code.go{color:#000}#sbo-rt-content pre code.gp{color:#009;font-weight:bold}#sbo-rt-content pre code.gs{font-weight:bold}#sbo-rt-content pre code.gu{color:#030;font-weight:bold}#sbo-rt-content pre code.gt{color:#9C6}#sbo-rt-content pre code.kc{color:#069;font-weight:bold}#sbo-rt-content pre code.kd{color:#069;font-weight:bold}#sbo-rt-content pre code.kn{color:#069;font-weight:bold}#sbo-rt-content pre code.kp{color:#069}#sbo-rt-content pre code.kr{color:#069;font-weight:bold}#sbo-rt-content pre code.kt{color:#078;font-weight:bold}#sbo-rt-content pre code.m{color:#F60}#sbo-rt-content pre code.s{color:#C30}#sbo-rt-content pre code.na{color:#309}#sbo-rt-content pre code.nb{color:#366}#sbo-rt-content pre code.nc{color:#0A8;font-weight:bold}#sbo-rt-content pre code.no{color:#360}#sbo-rt-content pre code.nd{color:#99F}#sbo-rt-content pre code.ni{color:#999;font-weight:bold}#sbo-rt-content pre code.ne{color:#C00;font-weight:bold}#sbo-rt-content pre code.nf{color:#C0F}#sbo-rt-content pre code.nl{color:#99F}#sbo-rt-content pre code.nn{color:#0CF;font-weight:bold}#sbo-rt-content pre code.nt{color:#309;font-weight:bold}#sbo-rt-content pre code.nv{color:#033}#sbo-rt-content pre code.ow{color:#000;font-weight:bold}#sbo-rt-content pre code.w{color:#bbb}#sbo-rt-content pre code.mf{color:#F60}#sbo-rt-content pre code.mh{color:#F60}#sbo-rt-content pre code.mi{color:#F60}#sbo-rt-content pre code.mo{color:#F60}#sbo-rt-content pre code.sb{color:#C30}#sbo-rt-content pre code.sc{color:#C30}#sbo-rt-content pre code.sd{color:#C30;font-style:italic}#sbo-rt-content pre code.s2{color:#C30}#sbo-rt-content pre code.se{color:#C30;font-weight:bold}#sbo-rt-content pre code.sh{color:#C30}#sbo-rt-content pre code.si{color:#A00}#sbo-rt-content pre code.sx{color:#C30}#sbo-rt-content pre code.sr{color:#3AA}#sbo-rt-content pre code.s1{color:#C30}#sbo-rt-content pre code.ss{color:#A60}#sbo-rt-content pre code.bp{color:#366}#sbo-rt-content pre code.vc{color:#033}#sbo-rt-content pre code.vg{color:#033}#sbo-rt-content pre code.vi{color:#033}#sbo-rt-content pre code.il{color:#F60}#sbo-rt-content pre code.g{color:#050}#sbo-rt-content pre code.l{color:#C60}#sbo-rt-content pre code.l{color:#F90}#sbo-rt-content pre code.n{color:#008}#sbo-rt-content pre code.nx{color:#008}#sbo-rt-content pre code.py{color:#96F}#sbo-rt-content pre code.p{color:#000}#sbo-rt-content pre code.x{color:#F06}#sbo-rt-content div.blockquote_sampler_toc{width:95%;margin:5px 5px 5px 10px !important}#sbo-rt-content div{font-family:serif,"DejaVuSerif";text-align:left}
    </style><link rel="canonical" href="/site/library/view/high-performance-python/9781449361747/ch09.html"><meta name="description" content="Chapter&nbsp;9.&nbsp;The multiprocessing module Questions you’ll be able to answer after this chapter What does the multiprocessing module offer? Processes vs Threads? How to choose the right size ... "><meta property="og:title" content="9. The multiprocessing module"><meta itemprop="isPartOf" content="/library/view/high-performance-python/9781449361747/"><meta itemprop="name" content="9. The multiprocessing module"><meta property="og:url" itemprop="url" content="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch09.html"><meta property="og:site_name" content="Safari"><meta property="og:image" itemprop="thumbnailUrl" content="https://www.safaribooksonline.com/library/cover/9781449361747/"><meta property="og:description" itemprop="description" content="Chapter&nbsp;9.&nbsp;The multiprocessing module Questions you’ll be able to answer after this chapter What does the multiprocessing module offer? Processes vs Threads? How to choose the right size ... "><meta itemprop="inLanguage" content="en"><meta itemprop="publisher" content="O'Reilly Media, Inc."><meta property="og:type" content="book"><meta property="og:book:isbn" itemprop="isbn" content="9781449361594"><meta property="og:book:author" itemprop="author" content="Ian Ozsvald"><meta property="og:book:author" itemprop="author" content="Micha Gorelick"><meta property="og:book:tag" itemprop="about" content="Python"><meta name="twitter:card" content="summary"><meta name="twitter:site" content="@safari"><style type="text/css" id="font-styles" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-size: <%= font_size %> !important; }"></style><style type="text/css" id="font-family" data-template="#sbo-rt-content, #sbo-rt-content p, #sbo-rt-content div { font-family: <%= font_family %> !important; }"></style><style type="text/css" id="column-width" data-template="#sbo-rt-content { max-width: <%= column_width %>% !important; margin: 0 auto !important; }"></style><!--[if lt IE 9]><script src="/static/js/src/respond.min.cf5c9b7980e5.js"></script><![endif]--><style id="annotator-dynamic-style">.annotator-adder, .annotator-outer, .annotator-notice {
  z-index: 100019;
}
.annotator-filter {
  z-index: 100009;
}</style></head>


<body class="reading sidenav nav-collapsed  scalefonts library" data-gr-c-s-loaded="true">

    
      <div class="hide working" role="status">
        <div class="working-image"></div>
      </div>
      <div class="sbo-site-nav">
        




<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#container" class="skip">Skip to content</a><header class="topbar t-topbar" style="display:None"><nav role="navigation" class="js-site-nav"><ul class="topnav"><li class="t-logo"><a href="https://www.safaribooksonline.com/home/" class="l0 None safari-home nav-icn js-keyboard-nav-home"><svg width="20" height="20" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>Safari Home Icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M4 9.9L4 9.9 4 18 16 18 16 9.9 10 4 4 9.9ZM2.6 8.1L2.6 8.1 8.7 1.9 10 0.5 11.3 1.9 17.4 8.1 18 8.7 18 9.5 18 18.1 18 20 16.1 20 3.9 20 2 20 2 18.1 2 9.5 2 8.7 2.6 8.1Z"></path><rect x="10" y="12" width="3" height="7"></rect><rect transform="translate(18.121320, 10.121320) rotate(-315.000000) translate(-18.121320, -10.121320) " x="16.1" y="9.1" width="4" height="2"></rect><rect transform="translate(2.121320, 10.121320) scale(-1, 1) rotate(-315.000000) translate(-2.121320, -10.121320) " x="0.1" y="9.1" width="4" height="2"></rect></g></svg><span>Safari Home</span></a></li><li><a href="https://www.safaribooksonline.com/r/" class="t-recommendations-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recommendations icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M50 25C50 18.2 44.9 12.5 38.3 11.7 37.5 5.1 31.8 0 25 0 18.2 0 12.5 5.1 11.7 11.7 5.1 12.5 0 18.2 0 25 0 31.8 5.1 37.5 11.7 38.3 12.5 44.9 18.2 50 25 50 31.8 50 37.5 44.9 38.3 38.3 44.9 37.5 50 31.8 50 25ZM25 3.1C29.7 3.1 33.6 6.9 34.4 11.8 30.4 12.4 26.9 15.1 25 18.8 23.1 15.1 19.6 12.4 15.6 11.8 16.4 6.9 20.3 3.1 25 3.1ZM34.4 15.6C33.6 19.3 30.7 22.2 27.1 22.9 27.8 19.2 30.7 16.3 34.4 15.6ZM22.9 22.9C19.2 22.2 16.3 19.3 15.6 15.6 19.3 16.3 22.2 19.2 22.9 22.9ZM3.1 25C3.1 20.3 6.9 16.4 11.8 15.6 12.4 19.6 15.1 23.1 18.8 25 15.1 26.9 12.4 30.4 11.8 34.4 6.9 33.6 3.1 29.7 3.1 25ZM22.9 27.1C22.2 30.7 19.3 33.6 15.6 34.4 16.3 30.7 19.2 27.8 22.9 27.1ZM25 46.9C20.3 46.9 16.4 43.1 15.6 38.2 19.6 37.6 23.1 34.9 25 31.3 26.9 34.9 30.4 37.6 34.4 38.2 33.6 43.1 29.7 46.9 25 46.9ZM27.1 27.1C30.7 27.8 33.6 30.7 34.4 34.4 30.7 33.6 27.8 30.7 27.1 27.1ZM38.2 34.4C37.6 30.4 34.9 26.9 31.3 25 34.9 23.1 37.6 19.6 38.2 15.6 43.1 16.4 46.9 20.3 46.9 25 46.9 29.7 43.1 33.6 38.2 34.4Z" fill="currentColor"></path></g></svg><span>Recommended</span></a></li><li><a href="https://www.safaribooksonline.com/s/" class="t-queue-nav l0 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>queue icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 29.2C25.4 29.2 25.8 29.1 26.1 28.9L48.7 16.8C49.5 16.4 50 15.5 50 14.6 50 13.7 49.5 12.8 48.7 12.4L26.1 0.3C25.4-0.1 24.6-0.1 23.9 0.3L1.3 12.4C0.5 12.8 0 13.7 0 14.6 0 15.5 0.5 16.4 1.3 16.8L23.9 28.9C24.2 29.1 24.6 29.2 25 29.2ZM7.3 14.6L25 5.2 42.7 14.6 25 24 7.3 14.6ZM48.7 22.4L47.7 21.9 25 34.2 2.3 21.9 1.3 22.4C0.5 22.9 0 23.7 0 24.7 0 25.6 0.5 26.5 1.3 26.9L23.9 39.3C24.2 39.5 24.6 39.6 25 39.6 25.4 39.6 25.8 39.5 26.1 39.3L48.7 26.9C49.5 26.5 50 25.6 50 24.7 50 23.7 49.5 22.9 48.7 22.4ZM48.7 32.8L47.7 32.3 25 44.6 2.3 32.3 1.3 32.8C0.5 33.3 0 34.1 0 35.1 0 36 0.5 36.9 1.3 37.3L23.9 49.7C24.2 49.9 24.6 50 25 50 25.4 50 25.8 49.9 26.1 49.7L48.7 37.3C49.5 36.9 50 36 50 35.1 50 34.1 49.5 33.3 48.7 32.8Z" fill="currentColor"></path></g></svg><span>Queue</span></a></li><li class="search"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="t-search-nav trigger nav-icn l0" data-dropdown-selector=".searchbox"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>search icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M31.3 0C20.9 0 12.5 8.4 12.5 18.8 12.5 22.5 13.6 25.9 15.4 28.8L1.2 42.9C-0.4 44.5-0.4 47.2 1.2 48.8 2 49.6 3.1 50 4.2 50 5.2 50 6.3 49.6 7.1 48.8L21.2 34.6C24.1 36.5 27.5 37.5 31.3 37.5 41.6 37.5 50 29.1 50 18.8 50 8.4 41.6 0 31.3 0ZM31.3 31.3C24.4 31.3 18.8 25.6 18.8 18.8 18.8 11.9 24.4 6.3 31.3 6.3 38.1 6.3 43.8 11.9 43.8 18.8 43.8 25.6 38.1 31.3 31.3 31.3Z" fill="currentColor"></path></g></svg><span>Search</span></a></li><li class="usermenu dropdown"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="trigger l0 nav-icn nav-dropdown"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>navigation arrow</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M0.1 12.5L9.7 3.1C9.8 3 9.9 3 10 3 10.1 3 10.2 3 10.3 3.1L19.9 12.5C20 12.5 20 12.6 20 12.8 20 12.9 20 13 19.9 13L17 15.9C16.9 16 16.8 16 16.7 16 16.5 16 16.4 16 16.4 15.9L10 9.7 3.6 15.9C3.6 16 3.5 16 3.3 16 3.2 16 3.1 16 3 15.9L0.1 13C0 12.9 0 12.8 0 12.7 0 12.7 0 12.6 0.1 12.5Z"></path></g></svg><span>Expand Nav</span></a><div class="drop-content"><ul><li><a href="https://www.safaribooksonline.com/history/" class="t-recent-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>recent items icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 0C11.2 0 0 11.2 0 25 0 38.8 11.2 50 25 50 38.8 50 50 38.8 50 25 50 11.2 38.8 0 25 0ZM6.3 25C6.3 14.6 14.6 6.3 25 6.3 35.4 6.3 43.8 14.6 43.8 25 43.8 35.4 35.4 43.8 25 43.8 14.6 43.8 6.3 35.4 6.3 25ZM31.8 31.5C32.5 30.5 32.4 29.2 31.6 28.3L27.1 23.8 27.1 12.8C27.1 11.5 26.2 10.4 25 10.4 23.9 10.4 22.9 11.5 22.9 12.8L22.9 25.7 28.8 31.7C29.2 32.1 29.7 32.3 30.2 32.3 30.8 32.3 31.3 32 31.8 31.5Z" fill="currentColor"></path></g></svg><span>History</span></a></li><li><a href="https://www.safaribooksonline.com/topics" class="t-topics-link l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 55" version="1.1" fill="#4A3C31"><desc>topics icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M25 55L50 41.262 50 13.762 25 0 0 13.762 0 41.262 25 55ZM8.333 37.032L8.333 17.968 25 8.462 41.667 17.968 41.667 37.032 25 46.538 8.333 37.032Z" fill="currentColor"></path></g></svg><span>Topics</span></a></li><li><a href="https://www.safaribooksonline.com/tutorials/" class="l1 nav-icn t-tutorials-nav js-toggle-menu-item None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 20 20" version="1.1" fill="#4A3C31"><desc>tutorials icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M15.8 18.2C15.8 18.2 15.9 18.2 16 18.2 16.1 18.2 16.2 18.2 16.4 18.2 16.5 18.2 16.7 18.1 16.9 18 17 17.9 17.1 17.8 17.2 17.7 17.3 17.6 17.4 17.5 17.4 17.4 17.5 17.2 17.6 16.9 17.6 16.7 17.6 16.6 17.6 16.5 17.6 16.4 17.5 16.2 17.5 16.1 17.4 15.9 17.3 15.8 17.2 15.6 17 15.5 16.8 15.3 16.6 15.3 16.4 15.2 16.2 15.2 16 15.2 15.8 15.2 15.7 15.2 15.5 15.3 15.3 15.4 15.2 15.4 15.1 15.5 15 15.7 14.9 15.8 14.8 15.9 14.7 16 14.7 16.1 14.6 16.3 14.6 16.4 14.6 16.5 14.6 16.6 14.6 16.6 14.6 16.7 14.6 16.9 14.6 17 14.6 17.1 14.7 17.3 14.7 17.4 14.8 17.6 15 17.7 15.1 17.9 15.2 18 15.3 18 15.5 18.1 15.5 18.1 15.6 18.2 15.7 18.2 15.7 18.2 15.7 18.2 15.8 18.2L15.8 18.2ZM9.4 11.5C9.5 11.5 9.5 11.5 9.6 11.5 9.7 11.5 9.9 11.5 10 11.5 10.2 11.5 10.3 11.4 10.5 11.3 10.6 11.2 10.8 11.1 10.9 11 10.9 10.9 11 10.8 11.1 10.7 11.2 10.5 11.2 10.2 11.2 10 11.2 9.9 11.2 9.8 11.2 9.7 11.2 9.5 11.1 9.4 11 9.2 10.9 9.1 10.8 8.9 10.6 8.8 10.5 8.7 10.3 8.6 10 8.5 9.9 8.5 9.7 8.5 9.5 8.5 9.3 8.5 9.1 8.6 9 8.7 8.8 8.7 8.7 8.8 8.6 9 8.5 9.1 8.4 9.2 8.4 9.3 8.2 9.5 8.2 9.8 8.2 10 8.2 10.1 8.2 10.2 8.2 10.3 8.2 10.5 8.3 10.6 8.4 10.7 8.5 10.9 8.6 11.1 8.7 11.2 8.9 11.3 9 11.4 9.1 11.4 9.2 11.4 9.3 11.5 9.3 11.5 9.3 11.5 9.4 11.5 9.4 11.5L9.4 11.5ZM3 4.8C3.1 4.8 3.1 4.8 3.2 4.8 3.4 4.8 3.5 4.8 3.7 4.8 3.8 4.8 4 4.7 4.1 4.6 4.3 4.5 4.4 4.4 4.5 4.3 4.6 4.2 4.6 4.1 4.7 4 4.8 3.8 4.8 3.5 4.8 3.3 4.8 3.1 4.8 3 4.8 2.9 4.7 2.8 4.7 2.6 4.6 2.5 4.5 2.3 4.4 2.2 4.2 2.1 4 1.9 3.8 1.9 3.6 1.8 3.5 1.8 3.3 1.8 3.1 1.8 2.9 1.8 2.7 1.9 2.6 2 2.4 2.1 2.3 2.2 2.2 2.3 2.1 2.4 2 2.5 2 2.6 1.8 2.8 1.8 3 1.8 3.3 1.8 3.4 1.8 3.5 1.8 3.6 1.8 3.8 1.9 3.9 2 4 2.1 4.2 2.2 4.4 2.4 4.5 2.5 4.6 2.6 4.7 2.7 4.7 2.8 4.7 2.9 4.8 2.9 4.8 3 4.8 3 4.8 3 4.8L3 4.8ZM13.1 15.2C13.2 15.1 13.2 15.1 13.2 15.1 13.3 14.9 13.4 14.7 13.6 14.5 13.8 14.2 14.1 14 14.4 13.8 14.7 13.6 15.1 13.5 15.5 13.4 15.9 13.4 16.3 13.4 16.7 13.5 17.2 13.5 17.6 13.7 17.9 13.9 18.2 14.1 18.5 14.4 18.7 14.7 18.9 15 19.1 15.3 19.2 15.6 19.3 15.9 19.4 16.1 19.4 16.4 19.4 17 19.3 17.5 19.1 18.1 19 18.3 18.9 18.5 18.7 18.7 18.5 19 18.3 19.2 18 19.4 17.7 19.6 17.3 19.8 16.9 19.9 16.6 20 16.3 20 16 20 15.8 20 15.6 20 15.4 19.9 15.4 19.9 15.4 19.9 15.4 19.9 15.2 19.9 15 19.8 14.9 19.8 14.8 19.7 14.7 19.7 14.6 19.7 14.4 19.6 14.3 19.5 14.1 19.3 13.7 19.1 13.4 18.7 13.2 18.4 13.1 18.1 12.9 17.8 12.9 17.5 12.8 17.3 12.8 17.1 12.8 16.9L3.5 14.9C3.3 14.9 3.1 14.8 3 14.8 2.7 14.7 2.4 14.5 2.1 14.3 1.7 14 1.4 13.7 1.2 13.3 1 13 0.9 12.6 0.8 12.3 0.7 12 0.7 11.7 0.7 11.4 0.7 11 0.8 10.5 1 10.1 1.1 9.8 1.3 9.5 1.6 9.2 1.8 8.9 2.1 8.7 2.4 8.5 2.8 8.3 3.2 8.1 3.6 8.1 3.9 8 4.2 8 4.5 8 4.6 8 4.8 8 4.9 8.1L6.8 8.5C6.8 8.4 6.8 8.4 6.8 8.4 6.9 8.2 7.1 8 7.2 7.8 7.5 7.5 7.7 7.3 8 7.1 8.4 6.9 8.7 6.8 9.1 6.7 9.5 6.7 10 6.7 10.4 6.8 10.8 6.8 11.2 7 11.5 7.2 11.8 7.5 12.1 7.7 12.4 8 12.6 8.3 12.7 8.6 12.8 8.9 12.9 9.2 13 9.4 13 9.7 13 9.7 13 9.8 13 9.8 13.6 9.9 14.2 10.1 14.9 10.2 15 10.2 15 10.2 15.1 10.2 15.3 10.2 15.4 10.2 15.6 10.2 15.8 10.1 16 10 16.2 9.9 16.4 9.8 16.5 9.6 16.6 9.5 16.8 9.2 16.9 8.8 16.9 8.5 16.9 8.3 16.9 8.2 16.8 8 16.8 7.8 16.7 7.7 16.6 7.5 16.5 7.3 16.3 7.2 16.2 7.1 16 7 15.9 6.9 15.8 6.9 15.7 6.9 15.6 6.8 15.5 6.8L6.2 4.8C6.2 5 6 5.2 5.9 5.3 5.7 5.6 5.5 5.8 5.3 6 4.9 6.2 4.5 6.4 4.1 6.5 3.8 6.6 3.5 6.6 3.2 6.6 3 6.6 2.8 6.6 2.7 6.6 2.6 6.6 2.6 6.5 2.6 6.5 2.5 6.5 2.3 6.5 2.1 6.4 1.8 6.3 1.6 6.1 1.3 6 1 5.7 0.7 5.4 0.5 5 0.3 4.7 0.2 4.4 0.1 4.1 0 3.8 0 3.6 0 3.3 0 2.8 0.1 2.2 0.4 1.7 0.5 1.5 0.7 1.3 0.8 1.1 1.1 0.8 1.3 0.6 1.6 0.5 2 0.3 2.3 0.1 2.7 0.1 3.1 0 3.6 0 4 0.1 4.4 0.2 4.8 0.3 5.1 0.5 5.5 0.8 5.7 1 6 1.3 6.2 1.6 6.3 1.9 6.4 2.3 6.5 2.5 6.6 2.7 6.6 3 6.6 3 6.6 3.1 6.6 3.1 9.7 3.8 12.8 4.4 15.9 5.1 16.1 5.1 16.2 5.2 16.4 5.2 16.7 5.3 16.9 5.5 17.2 5.6 17.5 5.9 17.8 6.2 18.1 6.5 18.3 6.8 18.4 7.2 18.6 7.5 18.6 7.9 18.7 8.2 18.7 8.6 18.7 9 18.6 9.4 18.4 9.8 18.3 10.1 18.2 10.3 18 10.6 17.8 10.9 17.5 11.1 17.3 11.3 16.9 11.6 16.5 11.8 16 11.9 15.7 12 15.3 12 15 12 14.8 12 14.7 12 14.5 11.9 13.9 11.8 13.3 11.7 12.6 11.5 12.5 11.7 12.4 11.9 12.3 12 12.1 12.3 11.9 12.5 11.7 12.7 11.3 12.9 10.9 13.1 10.5 13.2 10.2 13.3 9.9 13.3 9.6 13.3 9.4 13.3 9.2 13.3 9 13.2 9 13.2 9 13.2 9 13.2 8.8 13.2 8.7 13.2 8.5 13.1 8.2 13 8 12.8 7.7 12.6 7.4 12.4 7.1 12 6.8 11.7 6.7 11.4 6.6 11.1 6.5 10.8 6.4 10.6 6.4 10.4 6.4 10.2 5.8 10.1 5.2 9.9 4.5 9.8 4.4 9.8 4.4 9.8 4.3 9.8 4.1 9.8 4 9.8 3.8 9.8 3.6 9.9 3.4 10 3.2 10.1 3 10.2 2.9 10.4 2.8 10.5 2.6 10.8 2.5 11.1 2.5 11.5 2.5 11.6 2.5 11.8 2.6 12 2.6 12.1 2.7 12.3 2.8 12.5 2.9 12.6 3.1 12.8 3.2 12.9 3.3 13 3.5 13.1 3.6 13.1 3.7 13.1 3.8 13.2 3.9 13.2L13.1 15.2 13.1 15.2Z" fill="currentColor"></path></g></svg><span>Tutorials</span></a></li><li class="nav-offers flyout-parent"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" width="50" height="50" viewBox="0 0 50 50" version="1.1" fill="#4A3C31"><desc>offers icon</desc><path d="M10.8 43.7L0 39 0 10.2 13.6 4.6 23.3 8.7 11.5 13.5C11 13.6 10.8 13.9 10.8 14.3L10.8 43.7 10.8 43.7Z"></path><polygon points="12.3 44.4 25 50 38 44.3 38 14.7 25.2 9.4 12.3 14.7 12.3 44.4"></polygon><path d="M36.6 4.7L50 10.2 50 39 39.5 43.6 39.5 14.3C39.5 13.8 39.2 13.6 38.8 13.5L27 8.7 36.6 4.7 36.6 4.7Z"></path><polygon points="34.8 4 25 0 15.4 3.9 25.2 7.9 34.8 4"></polygon></svg><span>Offers</span></a><ul class="flyout"><li><a href="https://www.safaribooksonline.com/oreilly-conferences/" class="l2 nav-icn"><span>Conferences</span></a></li><li><a href="https://www.safaribooksonline.com/oreilly-newsletters/" class="l2 nav-icn"><span>Newsletter</span></a></li></ul></li><li class="nav-highlights"><a href="https://www.safaribooksonline.com/u/003o000000t5q9fAAA/" class="t-highlights-nav l1 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 35" version="1.1" fill="#4A3C31"><desc>highlights icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M13.325 18.071L8.036 18.071C8.036 11.335 12.36 7.146 22.5 5.594L22.5 0C6.37 1.113 0 10.632 0 22.113 0 29.406 3.477 35 10.403 35 15.545 35 19.578 31.485 19.578 26.184 19.578 21.556 17.211 18.891 13.325 18.071L13.325 18.071ZM40.825 18.071L35.565 18.071C35.565 11.335 39.86 7.146 50 5.594L50 0C33.899 1.113 27.5 10.632 27.5 22.113 27.5 29.406 30.977 35 37.932 35 43.045 35 47.078 31.485 47.078 26.184 47.078 21.556 44.74 18.891 40.825 18.071L40.825 18.071Z" fill="currentColor"></path></g></svg><span>Highlights</span></a></li><li><a href="https://www.safaribooksonline.com/u/" class="t-settings-nav l1 js-settings nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l1 no-icon">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l1 no-icon">Sign Out</a></li></ul><ul class="profile"><li><a href="https://www.safaribooksonline.com/u/" class="l2 nav-icn None"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 50 53" version="1.1" fill="#4A3C31"><desc>settings icon</desc><g stroke="none" stroke-width="1" fill-rule="evenodd"><path d="M44.6 29.6C44.7 28.6 44.8 27.5 44.8 26.5 44.8 25.5 44.7 24.4 44.6 23.4L49.6 19C50 18.8 50.1 18.3 49.9 17.9 48.9 14.7 47.1 11.7 44.9 9.1 44.6 8.8 44.2 8.7 43.8 8.8L37.4 11.1C35.8 9.8 34 8.7 32.1 8L30.9 1.4C30.8 0.9 30.4 0.6 30 0.5 26.7-0.2 23.3-0.2 20 0.5 19.6 0.6 19.2 0.9 19.1 1.4L17.9 8C16 8.7 14.1 9.8 12.6 11.1L6.2 8.8C5.8 8.7 5.4 8.8 5.1 9.1 2.9 11.7 1.1 14.7 0.1 17.9 -0.1 18.3 0 18.8 0.4 19L5.4 23.4C5.3 24.4 5.2 25.5 5.2 26.5 5.2 27.5 5.3 28.6 5.4 29.6L0.4 34C0 34.2-0.1 34.7 0.1 35.1 1.1 38.3 2.9 41.4 5.1 43.9 5.4 44.2 5.8 44.4 6.2 44.2L12.6 42C14.1 43.2 16 44.3 17.9 45L19.1 51.7C19.2 52.1 19.6 52.5 20 52.5 21.6 52.8 23.3 53 25 53 26.7 53 28.4 52.8 30 52.5 30.4 52.5 30.8 52.1 30.9 51.7L32.1 45C34 44.3 35.8 43.2 37.4 42L43.8 44.2C44.2 44.4 44.6 44.2 44.9 43.9 47.1 41.4 48.9 38.3 49.9 35.1 50.1 34.7 50 34.2 49.6 34L44.6 29.6ZM25 36.4C19.6 36.4 15.2 32 15.2 26.5 15.2 21 19.6 16.6 25 16.6 30.4 16.6 34.8 21 34.8 26.5 34.8 32 30.4 36.4 25 36.4Z" fill="currentColor"></path></g></svg><span>Settings</span></a></li><li><a href="https://community.safaribooksonline.com/" class="l2">Feedback</a></li><li><a href="https://www.safaribooksonline.com/accounts/logout/" class="l2">Sign Out</a></li></ul></div></li></ul></nav></header>


      </div>
      <div id="container" class="application">
        
          <div class="nav-container clearfix">
            


            
            
          </div>

          

  <div class="js-toc">
    
      <div class="sbo-reading-menu sbo-menu-top"><section class="sbo-toc-container toc-menu"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="sbo-toc-thumb"><span class="sbo-title ss-list"><h1><div class="visuallyhidden">Table of Contents for </div>
      
      High Performance Python
      
    </h1></span></a><div class="toc-contents"></div></section></div>

    

    <div class="interface-controls interface-controls-top">
      <ul class="interface-control-btns js-bitlist js-reader">
        <li class="js-search-in-archive search-in-archive t-search-in-archive"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" title="Search in archive" class="js-search-controls search-controls"><span class="icon">Search in book...</span></a><form class="search-archive-bar js-search-form"><input type="search" name="query" placeholder="Search inside this book..." autocomplete="off"></form><div class="search-archive-results"><div class="js-sitb-results-region"></div></div></li><li class="queue-control"><button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781449361747/chapter/ch09.html" data-for-analytics="9781449361747:ch09.html"><span>Add to Queue</span></button></li><li class="js-font-control-panel font-control-activator"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" data-push-state="false" id="font-controls" title="Change font size" aria-label="Change font size"><span class="icon">Toggle Font Controls</span></a></li><li class="dropdown sharing-controls"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="trigger" data-push-state="false" title="Share" aria-label="Share"><i class="fa fa-share"></i></a><ul class="social-sharing dropdown-menu"><li><a class="twitter share-button t-twitter" target="_blank" aria-label="Share this section on Twitter" title="Share this section on Twitter" href="https://twitter.com/share?url=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch09.html&amp;text=High%20Performance%20Python&amp;via=safari"><span>Twitter</span></a></li><li><a class="facebook share-button t-facebook" target="_blank" aria-label="Share this section on Facebook" title="Share this section on Facebook" href="https://www.facebook.com/sharer/sharer.php?u=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch09.html"><span>Facebook</span></a></li><li><a class="googleplus share-button t-googleplus" target="_blank" aria-label="Share this secton on Google Plus" title="Share this secton on Google Plus" href="https://plus.google.com/share?url=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch09.html"><span>Google Plus</span></a></li><li><a class="email share-button t-email" aria-label="Share this section via email" title="Share this section via email" href="mailto:?subject=Safari:%209.%20The%20multiprocessing%20module&amp;body=https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/ch09.html%0D%0Afrom%20High%20Performance%20Python%0D%0A"><span>Email</span></a></li></ul></li>
      </ul>
    </div>

    <section role="document">
      
	  <div class="t-sbo-prev sbo-prev sbo-nav-top">
  
    
      
        <a href="/site/library/view/high-performance-python/9781449361747/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. Concurrency</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-top">
  
    
      
        <a href="/site/library/view/high-performance-python/9781449361747/ch10.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">10. Clusters and Job Queues</div>
        </a>
    
  
  </div>



<div id="sbo-rt-content"><div class="annotator-wrapper"><section class="chapter" epub:type="chapter" id="multiprocessing"><div class="titlepage"><div><div><h2 class="title">Chapter&nbsp;9.&nbsp;The multiprocessing module</h2></div></div></div><div class="sidebar"><div class="sidebar-title">Questions you’ll be able to answer after this chapter</div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
What does the multiprocessing module offer?
</li><li class="listitem">
Processes vs Threads?
</li><li class="listitem">
How to choose the right size for a Process Pool?
</li><li class="listitem">
How to use non-persistent Queues for work processing?
</li><li class="listitem">
What are the costs and benefits of Inter Process Communication?
</li><li class="listitem">
How can we process numpy data with many CPUs  ?
</li><li class="listitem">
Why do we need Locking to avoid data loss?
</li></ul></div></div><p>CPython doesn’t use multiple CPUs by default. This is partly due to Python’s
design back in a single-core era and partly because parallelizing can actually
be quite difficult to do efficiently. Python gives us the tools to do it but
leaves us to make our own choices. It is painful to see your multi-core machine
using just 1 CPU on a long-running process so in this chapter we’ll review ways
of using all the machine’s cores at once.</p><p>It is important to note that I mentioned <span class="emphasis"><em>CPython</em></span> above (the common implementation that we all use), there’s nothing in the Python language that stops it from using multi-core systems. CPython’s implementation cannot efficiently use multiple cores but other implementations (e.g. PyPy with the forthcoming Software Transactional Memory) may not be bound by this restriction.</p><p>We live in a multi-core world - four cores are common in laptops, eight core
desktops configurations will be popular soon and 10, 12, and 15 core server CPUs
are available. If your job can be split to run on multiple CPUs <span class="emphasis"><em>without</em></span> too
much engineering effort then this is a wise direction to consider.</p><p>When used to parallelize a problem over a set of CPUs you can expect <span class="emphasis"><em>up to</em></span> an
n-times speed-up with n-cores. If you have a quad-core machine and you can use
all four cores for your task, it might run in 1/4 of the original run-time. You
are unlikely to see a greater than 4* speed-up, probably you’ll get a 3-4*
speed-up in practice.</p><p>This is because each additional process will increase the
communications overhead and decrease available RAM, so you rarely get a full n*
speed-up.  Depending on which problem you are solving, the communications
overhead can even get so large that you can see very significan slow-downs.
These sorts of problems are often where the complexity of any sort of parallel
programming and normally require a change in algorithm.  This is often why
parallel programming is considered an art.</p><p>If you’re not familiar with Amdahl’s Law <sup>[<a id="id814204" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814204" class="footnote">49</a>]</sup> then it is worth doing some background reading. The law shows that if only a small part of your code can be parallelized, it doesn’t matter how many CPUs you throw at it, overall you still won’t run much faster. Even if a large fraction of your run-time could be parallelized there’s a finite number of CPUs that can be used to efficiently make the overall process run faster, before you get to a point of diminishing returns.</p><p>The <code class="literal">multiprocessing</code> module lets you use process and thread based parallel
processing, share work over queues, and share data amongst processes. It is
mostly focused on single-machine multi-core parallelism (there are better
options for multi-machine parallelism). A very common use is to parallelize a
task over a set of processes for a CPU-bound problem. You might also use it to
parallelize an I/O-bound problem but there are better tools for this (e.g. the
new <code class="literal">asyncore</code> module in Python 3 and <code class="literal">gevent</code> or <code class="literal">tornado</code> in Python 2+).</p><p>To parallelize your task you have to think a little differently to the normal
way of writing a serial process. You must also accept that debugging a
parallelized task is <span class="emphasis"><em>harder</em></span> - often it can be very frustrating. I’d recommend
keeping the parallelizm as simple as possible (even if you’re not squeezing
every last drop of power from your machine) so that your development velocity is
kept high.</p><p>One particularly difficult topic is the sharing of state in a parallel system -
it “feels like it should be easy” but incurs lots of overheads and can be hard
to get right. There are many use cases, each with different trade-offs, so
there’s definitely no “one solution for everyone”. In
<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-verifying-primes-using-inter-process-communication">Verifying Primes using Inter Process Communication</a> we’ll go
through state sharing with an eye on the synchronization costs. Avoiding shared
state will make your life far easier.</p><p>In fact, an algorithm can be analysed to see how well it’ll perform in a
parallel environment almost entirely by how much state must be shared.  For
example, if we can have multiple python processes all solving the same problem
without communicating to eachother (a situation known as embarassingly
parallel), then the problem doesn’t incur much of a penalty as we add more and
more python processes.</p><p>On the other hand, if each processes needed to communicate to every other python
process then communications overhead will slowely overwhelm the process and slow
things down.  This would mean that as we add more python processes we could
actually slow down our overall performance.</p><p>As a result, sometimes some counter-intuitive algorithmic changes must occur to
efficiently solve a problem in parallel.  For example, when solving the diffusion
equation (<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch06.html">Chapter&nbsp;6</a>) in parallel, each process actually does some
redundant work that another process also does.  This redundancy reduces the
amount of communications and speeds up the overall calculation!</p><p>Here are some typical jobs for the <code class="literal">multiprocessing</code> module:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
Parallelize a CPU-bound task with <code class="literal">Process</code> or <code class="literal">Pool</code> objects
</li><li class="listitem">
Parallelize an I/O-bound task in a <code class="literal">Pool</code> with threads using the (oddly named) <code class="literal">dummy</code> module
</li><li class="listitem">
Share pickled work via a <code class="literal">Queue</code>
</li><li class="listitem">
Share state between parallelized workers including bytes, primitive datatypes, dictionaries and lists
</li></ul></div><p>If you come from a language where threads are used for CPU-bound tasks (e.g. C++
or Java) then you should know that whilst threads in Python are OS-native
(they’re not simulated, they are actual Operating System threads) they are bound
by the Global Interpreter Lock (GIL) so only one thread may interact with Python
objects at a time.</p><p>By using processes we run a number of Python interpreters in
parallel, each with a private memory space with their own GIL, and each runs in
series (so there’s no competition for each GIL) - this is the easiest way to
speed-up a CPU-bound task in Python. If we need to share state then we need to
add some communications overhead, we’ll explore that in
<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-verifying-primes-using-inter-process-communication">Verifying Primes using Inter Process Communication</a>.</p><p>If you work with <code class="literal">numpy</code> arrays you might wonder if you can create a larger
array (e.g. a large 2D matrix) and ask processes to work on segments of the
array in parallel. You can but it is hard to discover by trial and error so in
<a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-sharing-numpy-data-with-multiprocessing">Sharing numpy data with multiprocessing</a> we’ll work through
sharing a 6.4GB <code class="literal">numpy</code> array across four CPUs. Rather than sending partial
copies of the data (which would at least double the working size required in
RAM) and create a massive communications overhead, we share the underlying bytes
of the array amongst the processes. This is an ideal approach to sharing a large
array amongst local workers on one machine.</p><div class="note"><h3 class="title">Note</h3><p>Here we discuss <code class="literal">multiprocessing</code> on *nix based machines (this chapter is
written using Ubuntu, it should run unchanged on a Mac). For Windows machines
you should check the official documentation
<sup>[<a id="id814377" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814377" class="footnote">50</a>]</sup>.</p></div><p>In the following chapter I’m hardcoding the number of processes
(<code class="literal">NUM_PROCESSES=4</code>) to match the four physical cores on my laptop. By default
<code class="literal">multiprocessing</code> will use as many cores as it can see (my machine presents
eight - four CPUs and four HyperThreads). Normally you’d avoid hard-coding the
number of processes to create unless you were specifically managing your
resources.</p><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_an_overview_of_the_multiprocessing_module">An overview of the multiprocessing module</h2></div></div></div><p>The <code class="literal">multiprocessing</code> module was introduced in Python 2.6
<sup>[<a id="id814457" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814457" class="footnote">51</a>]</sup> by taking the existing
<code class="literal">pyProcessing</code> module and folding it into Python’s built-in library set. Its
main components are:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
<code class="literal">Process</code> - a forked copy of the current process, this creates a new Process Identifier and the task runs as an independent child process in the Operating System. You can start and query the state of the <code class="literal">Process</code> and provide it with a <code class="literal">target</code> method to run
</li><li class="listitem">
<code class="literal">Pool</code> - wraps the <code class="literal">Process</code> or <code class="literal">threading.Thread</code> API into a convenient pool of workers which share a chunk of work and return an aggregated result
</li><li class="listitem">
<code class="literal">Queue</code> - FIFO queue allowing multiple producers and consumers
</li><li class="listitem">
<code class="literal">Pipe</code> - uni- or bi-directional communication channel between two processes
</li><li class="listitem">
<code class="literal">Manager</code> - high level managed interface to share Python objects between processes
</li><li class="listitem">
<code class="literal">ctypes</code> - allows sharing primitive datatypes (e.g. integers, floats and bytes) between processes after they have forked
</li><li class="listitem">
Synchronization primitives - locks and semaphores to synchronize control flow between processes
</li></ul></div><div class="note"><h3 class="title">Note</h3><p>In Python 3.2 the <code class="literal">concurrent.futures</code> module was introduced (via PEP 3148 <sup>[<a id="id814567" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814567" class="footnote">52</a>]</sup>), this provides the core bahavior of <code class="literal">multiprocessing</code> with a simpler interface based on Java’s <code class="literal">java.util.concurrent</code>. It is available as a backport to earlier versions of Python <sup>[<a id="id814600" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814600" class="footnote">53</a>]</sup>. I don’t cover it here as it isn’t as flexible as <code class="literal">multiprocessing</code>. I suspect that with the growing adoption of Python 3+ we’ll see it replace <code class="literal">multiprocessing</code> over time.</p></div><p>In the rest of the chapter I’ll introduce a set of examples to demonstrate common ways of using this module.</p><p>We’ll estimate Pi using a Monte Carlo approach with a <code class="literal">Pool</code> of processes or threads, using normal Python and <code class="literal">numpy</code>. This is a simple problem with well-understodd complexity so it parallelizes easily, we can also see an unexpected result of using threads with <code class="literal">numpy</code>. Next we’ll search for Primes using the same <code class="literal">Pool</code> approach but we’ll investigate the non-predictable complexity of searching for Primes and look at how we can efficiently (and inefficiently!) split the workload to best use our computing resources. We’ll finish the Primes Search by switching to queues where we introduce <code class="literal">Process</code> objects in place of a <code class="literal">Pool</code> and use a list of work and poison pills to control the lifetime of workers.</p><p>Next we’ll tackle inter-process communication (IPC) to validate a small set of possible-Primes by splitting each number’s workload across multiple CPUs, we use IPC to end the search early if a factor is found so that we can significantly beat the speed of a single-CPU search process. We’ll cover shared Python objects, OS primitives and a Redis server to investigate the complexity and capability trade-offs of each approach.</p><p>We can share a 6.4GB <code class="literal">numpy</code> array across 4 CPUs to split a large workload
<span class="emphasis"><em>without</em></span> copying data.  If you have large arrays with parallelizable operations
then this technique should buy you a great speed-up since we have to allocate
less space in RAM and copy less data. Finally we’ll look at synchronizing access
to a file and a variable (as a <code class="literal">Value</code>) between processes to count without
corrupting data to illustrate how to correctly lock shared state.</p><div class="note"><h3 class="title">Note</h3><p>PyPy has full support of the <code class="literal">multiprocessing</code> library, the following CPython examples (though not the <code class="literal">numpy</code> examples at the time of writing) all run far quicker using PyPy. If you’re only using CPython code (no C extensions or more complex libraries) for parallel processing then PyPy might be a quick win for you.</p></div><p>OpenMP is a low level interface to multiple cores, you might wonder whether to focus on it rather than <code class="literal">multiprocessing</code>. We introduced it with Cython and Pythran back in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch07.html">Chapter&nbsp;7</a> and we don’t cover it in this chapter. <code class="literal">multiprocessing</code> works at a higher level sharing Python data structures whilst OpenMP works with C primitive objects (e.g. integers and floats) once you’ve compiled to C. It only makes sense to use it if you’re compiling your code, if you’re not compiling (e.g. you’re using efficient <code class="literal">numpy</code> code and you want to run on many cores) then sticking with <code class="literal">multiprocessing</code> is probably the right approach.</p><p>This chapter (and the entire book) focuses on Linux, Linux has a forking process to create new processes by cloning the parent process. Windows lacks <code class="literal">fork</code> and so the <code class="literal">multiprocessing</code> module imposes some Windows-specific restrictions <sup>[<a id="id814731" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814731" class="footnote">54</a>]</sup> which I urge you to review if you’re on Windows.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_estimating_pi_using_the_monte_carlo_method">Estimating Pi using the Monte Carlo method</h2></div></div></div><p>We can estimate Pi by throwing thousands of imaginary darts into a “dartboard” represented by a unit-circle. The relationship between the number of darts falling inside the circle’s edge and outside it will allow us to approximate Pi.</p><p>This is an ideal first problem as we can split the total workload evenly across a number of processes, each one running on a CPU. Each process will end at the same time as the workload for each is equal so we can investigate the speed-ups available as we add new CPUs and HyperThreads to the problem.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_estimate">Figure&nbsp;9-1</a> we throw 10,000 darts into the unit square, a percentage of them fall into the quarter of the unit circle that’s drawn. This estimate is rather bad - 10,000 dart throws does not reliably give us a 3 decimal place result. If you ran your own code you’d see this estimate vary between 3.0 and 3.2 on each run.</p><div class="figure"><a id="FIG-pi_monte_carlo_estimate"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_plot_monte_carlo_example.png" alt="Estimating Pi using the Monte Carlo method" width="800" height="800" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_plot_monte_carlo_example.png"></div></div><div class="figure-title">Figure&nbsp;9-1.&nbsp;Estimating Pi using the Monte Carlo method</div></div><p>To be confident of the first 3 decimal places we need to generate 10,000,000 random dart throws <sup>[<a id="id814788" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814788" class="footnote">55</a>]</sup>. This is inefficient (and better methods for Pi’s estimation exist) but it is rather convenient to demonstrate the benefits of parallelization using <code class="literal">multiprocessing</code>.</p><p>With the Monte Carlo method we use the Pythagorean Theorem <sup>[<a id="id814834" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id814834" class="footnote">56</a>]</sup> to test if a dart has landed inside our circle:</p><p><span class="inlinemediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/inleq_0901.png" alt="" width="144" height="29" data-mfp-src="/library/view/high-performance-python/9781449361747/inleq_0901.png"></span></p><p>As we’re using a unit circle we can optimize this by removing the square root operation as <span class="inlinemediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/inleq_0902.png" alt="" width="61" height="24" data-mfp-src="/library/view/high-performance-python/9781449361747/inleq_0902.png"></span> to leave a simplfied expression to implement:</p><p><span class="inlinemediaobject"><img src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/inleq_0903.png" alt="" width="113" height="29" data-mfp-src="/library/view/high-performance-python/9781449361747/inleq_0903.png"></span></p><p>We’ll look at a loop version of this in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-lists-calculation">Example&nbsp;9-1</a>. We’ll implement both a normal Python version and later a <code class="literal">numpy</code> version and we’ll use both Threads and Processes to parallelize the problem.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="multiprocessing-estimating-pi">Estimating Pi using Processes and Threads</h2></div></div></div><p>It is easier to understand a normal Python implementation so we’ll start with that in this section using float objects in a loop. We’ll parallelize this with Process to use all of our available CPUs and we’ll visualize the state of the machine as we use more CPUs.</p><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_python_objects">Using Python objects</h3></div></div></div><p>The Python implementation is easy to follow but it carries an overhead as each Python float object has to be managed, referenced and synchronized in turn. This overhead slows down our run-time but it has bought us thinking-time as the implementation was quick to put together. By parallelizing this version we get additional speed-ups for very little extra work.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_threads_and_processes_lists">Figure&nbsp;9-2</a> shows three implementations of the Python example:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
No use of <code class="literal">multiprocessing</code> (named Series)
</li><li class="listitem">
Using threads
</li><li class="listitem">
Using processes
</li></ul></div><p>When we use more than one thread or process we’re asking Python to calcualte the same total number of dart throws and to divide the work evenly between workers. If we want 100,000,000 dart throws in total using our Python implementation and we use 2 workers then we’ll be asking both threads or both processes to generate 50,000,000 dart throws per worker.</p><div class="figure"><a id="FIG-pi_monte_carlo_threads_and_processes_lists"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_graph_speed_tests_threaded_processes.png" alt="not set" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_graph_speed_tests_threaded_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-2.&nbsp;Working in series, Threaded and with Processes</div></div><p>Using 1 thread takes approximately 120 seconds. Using 2 or more threads takes <span class="emphasis"><em>longer</em></span>. By using 2 or more processes the run time gets <span class="emphasis"><em>shorter</em></span>. The cost of using no processes or threads (the Series implementation) is the same as running with 1 process.</p><p>By using processes I get a linear speed-up when using 2 or 4 cores on my laptop. For the 8 worker case we’re using Intel’s HyperThreading - my laptop only has 4 physical cores so we get barely additional speed-up by running 8 processes.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-lists-calculation">Example&nbsp;9-1</a> shows the Python version of our Pi estimator. If we’re using threads then each instruction is bound by the GIL so although each thread could run on a separate CPU it will only execute when no other threads are running. The process version is not bound by this restriction as each forked process has a private Python interpreter running as a single thread, so there’s no GIL contention as no objects are shared. We use Python’s built-in random number generator, see <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-random-numbers">Random Numbers in Parallel Systems</a> for some notes about the dangers of parallelized random number sequences.</p><div class="example"><a id="code-pi-lists-calculation"></a><div class="example-title">Example&nbsp;9-1.&nbsp;Estimating Pi using a loop in Python</div><div class="example-contents"><pre class="screen">def estimate_nbr_points_in_quarter_circle(nbr_estimates):
    nbr_trials_in_quarter_unit_circle = 0
    for step in xrange(int(nbr_estimates)):
        x = random.uniform(0, 1)
        y = random.uniform(0, 1)
        is_in_unit_circle = x * x + y * y &lt;= 1.0
        nbr_trials_in_quarter_unit_circle += is_in_unit_circle

    return nbr_trials_in_quarter_unit_circle</pre></div></div><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-lists-main">Example&nbsp;9-2</a> shows the <code class="literal">__main__</code> block and note that we build the <code class="literal">Pool</code> before we start the timer. Spawning threads is relatively instant, spawning processes involves a fork and this takes a measurable fraction of a second. We ignore this overhead in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_threads_and_processes_lists">Figure&nbsp;9-2</a> as this cost will be a tiny fraction of the overall execution time.</p><p>We create a list containing <code class="literal">nbr_estimates</code> divided by the number of workers, this new argument will be sent to each worker. After execution we’ll receive the same number of results back, we’ll sum these to estimate the number of darts in the unit circle.</p><p>We import the process-based <code class="literal">Pool</code> from <code class="literal">multiprocessing</code>. We could also <code class="literal">from multiprocessing.dummy import Pool</code> to get a threaded version - the “dummy” name is rather misleading (I confess to not understanding why it is named this way), it is simply a light wrapper around the <code class="literal">threading</code> module to present the same interface as the process-based <code class="literal">Pool</code>.</p><div class="example"><a id="code-pi-lists-main"></a><div class="example-title">Example&nbsp;9-2.&nbsp;main for estimating Pi using a loop</div><div class="example-contents"><pre class="screen">from multiprocessing import Pool
...

if __name__ == "__main__":
    nbr_samples_in_total = 1e8
    nbr_parallel_blocks = 4
    pool = Pool(processes=nbr_parallel_blocks)
    nbr_samples_per_worker = nbr_samples_in_total / nbr_parallel_blocks
    print "Making {} samples per worker".format(nbr_samples_per_worker)
    nbr_trials_per_process = [nbr_samples_per_worker] * nbr_parallel_blocks
    t1 = time.time()
    nbr_in_unit_circles = pool.map(calculate_pi, nbr_trials_per_process)
    pi_estimate = sum(nbr_in_unit_circles) * 4 / nbr_samples_in_total
    print "Estimated pi", pi_estimate
    print "Delta:", time.time() - t1</pre></div></div><div class="warning" epub:type="warning"><h3 class="title">Warning</h3><p>It is worth noting that each process we create consumes some RAM from the system. You can expect a forked process using the standard libraries to take on the order of 10-20MB of RAM, if you’re using many libraries and lots of data then you might expect each forked copy to take 100s of MB. On a system with a RAM constraint this might be a significant issue - if you run out of RAM and the system reverts to using the disk’s swap space then any parallelization advantage will be massively lost to the slow paging of RAM back and forth to disk!</p></div><p>The following figures plot the average CPU utilization of my 4 physical cores and their 4 associated HyperThreads (each HyperThread runs on unutilized silicon in a physical core). The data gathered for these figures <span class="emphasis"><em>includes</em></span> the startup time of the first Python process and the cost of starting sub-processes. The CPU sampler records the entire state of my laptop, not just the CPU time used by this tasks.</p><p>Note that the following diagrams are created using a different timing method with a slower sampling rate to <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_threads_and_processes_lists">Figure&nbsp;9-2</a> so the overall run time is a little longer.</p><p>The execution behavior in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_1_processes">Figure&nbsp;9-3</a> with 1 process in the pool (along with the parent process) shows some overhead in the first seconds as the pool is created and then a consistent clost-to-100% CPU utilization throughout the run. With 1 process we’re efficiently using one core.</p><div class="figure"><a id="FIG-pi_monte_carlo_lists_1_processes"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_1_processes.png" alt="Estimating Pi using lists and 1 process" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_1_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-3.&nbsp;Estimating Pi using Python objects and 1 process</div></div><p>Next we’ll add a second process, effectively saying <code class="literal">Pool(processes=2)</code>. By adding a second process <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_2_processes">Figure&nbsp;9-4</a> the execution time roughly halves to 56 seconds and two CPUs are fully occupied. This is the best result we can expect - we’ve efficiently used all the new computing resources and we’re not losing any speed to other overheads like communication, paging to disk or contention with competing processes who want to use the same CPUs.</p><div class="figure"><a id="FIG-pi_monte_carlo_lists_2_processes"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_2_processes.png" alt="Estimating Pi using lists and 2 processes" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_2_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-4.&nbsp;Estimating Pi using Python objects and 2 processes</div></div><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_4_processes">Figure&nbsp;9-5</a> we use all 4 physical CPUs, now we are using all of the raw power of this laptop. Execution time is roughly 1/4 of the single process version at 27 seconds.</p><div class="figure"><a id="FIG-pi_monte_carlo_lists_4_processes"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_4_processes.png" alt="Estimating Pi using lists and 4 processes" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_4_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-5.&nbsp;Estimating Pi using Python objects and 4 processes</div></div><p>By switching to 8 processes in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_8_processes">Figure&nbsp;9-6</a> we cannot achieve more than a tiny speed-up on the 4 process version. That is because the 4 HyperThreads are only able to squeeze a little extra processing power out of spare silicon on the CPUs and the 4 CPUs are already maximally utilized.</p><div class="figure"><a id="FIG-pi_monte_carlo_lists_8_processes"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_8_processes.png" alt="Estimating Pi using lists and 8 processes" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_8_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-6.&nbsp;Estimating Pi using Python objects and 8 processes with little additional gain</div></div><p>In each of the above diagrams we can see that we’re efficiently using more of the available CPU resources at each step and that the HyperThread resources are a poor addition. The biggest problem when using HyperThreads is that CPython is using a lot of RAM - it is not cache friendly so the spare resources on each chip are very poorly used. <code class="literal">numpy</code> in the following section makes better use of these resources.</p><div class="note"><h3 class="title">Note</h3><p>In my experience Hyperthreading can give up to a 30% performance gain <span class="emphasis"><em>if</em></span> ther are enough spare computing resources. This works if for example you have a mix of floating point and integer arithmatic rather than just the floating point operations we have here. By mixing the resource requirements the HyperThreads can schedule more of the CPU’s silicon to be working concurrently. Generally I see HyperThreads as an added bonus and not a resource to be optimized against as adding more CPUs is probably more economical than tuning your code which adds a support overhead.</p></div><p>Now we’ll switch to use threads in one process rather than multiple processes. The overhead around the “GIL battle” actually makes our code run <span class="emphasis"><em>more slowly</em></span>.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-davidbeazley-gil2threads">Figure&nbsp;9-7</a> shows two threads fighting on a dual core system with Python 2.6 (the same effect occurs with Python 2.7), this is the “GIL battle”. This image is taken with permissions from David Beazley’s blog post “The Python GIL Visualized” <sup>[<a id="id815432" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815432" class="footnote">57</a>]</sup>.</p><p>The darker red tone shows Python threads repeatedly trying to get the GIL but failing. The lighter green tone is a running thread. White shows the brief periods of a thread that is idle.  We can see that there’s an overhead when adding threads to a CPU-bound task in CPython. The context switching overhead actually adds to the overall runtime. David Beazley explains this in “Understanding the Python GIL” <sup>[<a id="id815442" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815442" class="footnote">58</a>]</sup>. Threads in Python are great for I/O bound tasks but they’re a poor choice for CPU-bound problems.</p><div class="figure"><a id="FIG-davidbeazley-gil2threads"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_davidbeazley_gil2threads.png" alt="notset" width="825" height="88" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_davidbeazley_gil2threads.png"></div></div><div class="figure-title">Figure&nbsp;9-7.&nbsp;Python threads fighting on a dual-core machine (thanks to David Beazley for the image)</div></div><p>Each time a thread wakes up and tries to acquire the GIL (whether it is available or not) it uses some system resources. If one thread is busy then the other will repeatedly awaken and try to acquire the GIL, these repeated attempts become expensive. David Beazley has an interactive set of plots which demonstrate the problem <sup>[<a id="id815426" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815426" class="footnote">59</a>]</sup>, you can zoom in to see every failed attempt at GIL acquisition for multiple threads on multiple CPUs. Note that this is only a problem with multiple threads running on a multi-core system - a single core system with multiple threads has no “GIL battle”. This is easily seen on this page <sup>[<a id="id815423" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815423" class="footnote">60</a>]</sup> on David’s site.</p><p>If the threads weren’t fighting for the GIL but were taking it efficiently then we shouldn’t see any of the dark red tone, instead we might expect the waiting thread to carry on waiting without consuming resources. By avoiding the battle for the GIL the overall runtime would be shorter (but still no faster than using a single thread due to the GIL). If there was no GIL then each thread could run in parallel without any waiting and so would use all of the system’s resources.</p><p>It is worth noting that the negative effect of threads on CPU bound problems is reasonably solved in Python 3.2+:</p><div class="blockquote"><blockquote class="blockquote"><p>“The mechanism for serializing execution of concurrently running Python threads (generally known as the GIL or Global Interpreter Lock) has been rewritten. Among the objectives were more predictable switching intervals and reduced overhead due to lock contention and the number of ensuing system calls. The notion of a “check interval” to allow thread switches has been abandoned and replaced by an absolute duration expressed in seconds.”</p><div class="attribution"><p>—<span class="attribution">
Raymond Hettinger
<em class="citetitle">http://docs.python.org/dev/whatsnew/3.2.html</em>
</span></p></div></blockquote></div><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_4_threads">Figure&nbsp;9-8</a> I run the same code that we used in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_4_processes">Figure&nbsp;9-5</a> but with threads in place of processes. Although a number of CPUs are being used, they each share the workload lightly. If each thread was running without the GIL then we’d see 100% CPU utilization on the four CPUs. Instead each CPU is partially utilized (due to the GIL) and in addition it is running slower than we’d like due to the “GIL battle”.</p><div class="figure"><a id="FIG-pi_monte_carlo_lists_4_threads"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_4.png" alt="Estimating Pi using lists and 4 threads" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_lists_parallel_py_4.png"></div></div><div class="figure-title">Figure&nbsp;9-8.&nbsp;Estimating Pi using Python objects and 4 threads</div></div><p>Compare this to <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_1_processes">Figure&nbsp;9-3</a> where 1 process executes the same job in approximately 120 seconds rather than 160 seconds.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="multiprocessing-random-numbers">Random Numbers in Parallel Systems</h3></div></div></div><p>Generating good random number sequences is a hard problem and it is easy to get it wrong if you try to do it yourself. Getting a good sequence quickly in parallel is even harder - suddenly you have to worry about whether you’ll get repeating or correlated sequences in the parallel processes.</p><p>We’ve just used Python’s built-in random number generator in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-lists-calculation">Example&nbsp;9-1</a> and we’ll use the <code class="literal">numpy</code> random number generator in the next section in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-numpy-calculation">Example&nbsp;9-3</a>. In both cases the random number generators are seeded in their forked process. For the Python <code class="literal">random</code> example the seeding is handled internally by <code class="literal">multiprocessing</code> - if during a fork it sees that <code class="literal">random</code> is in the namespace then it’ll force a call to seed the generators in each of the new processes.</p><p>In the forthcoming <code class="literal">numpy</code> example we have to do this explicitly. If you forget to seed the random number sequence with <code class="literal">numpy</code> then each of your forked processes will generate an identical sequence of random numbers.</p><p>If you care about the quality of the random numbers used in the parallel processes then we urge you to research this topic, we don’t discuss it here. <span class="emphasis"><em>Probably</em></span> the <code class="literal">numpy</code> and Python random number generators are good enough but if significant outcomes depend on the quality of the random sequences (e.g. for medical or financial systems) then you must read up on this area.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="multiprocessing-estimating-pi-using-numpy">Using numpy</h3></div></div></div><p>In this section we switch to using <code class="literal">numpy</code>. Our dart throwing problem is ideal for <code class="literal">numpy</code> vectorized operations - we generate the same estimate over 50 times faster than the Python examples before.</p><p>The main reason that <code class="literal">numpy</code> is faster when solving the same problem as our pure Python version is that it is creating and manipulating the same object types at a very low level in contiguous blocks of RAM, rather than creating many higher level Python objects which each require individual management and addressing.</p><p>As <code class="literal">numpy</code> is far more cache friendy we’ll also get a small speed boost when using the 4 HyperThreads. We didn’t get this in the pure Python version as caches aren’t used efficiently by larger Python objects.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_threads_and_processes_numpy">Figure&nbsp;9-9</a> we see three scenarios:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
No use of <code class="literal">multiprocessing</code> (named Series)
</li><li class="listitem">
Using threads
</li><li class="listitem">
Using processes
</li></ul></div><div class="figure"><a id="FIG-pi_monte_carlo_threads_and_processes_numpy"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_pi_numpy_graph_speed_tests_threaded_processes.png" alt="Working in series, Threaded and with Processes" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_pi_numpy_graph_speed_tests_threaded_processes.png"></div></div><div class="figure-title">Figure&nbsp;9-9.&nbsp;Working in series, Threaded and with Processes using numpy</div></div><p>The Serial and and single-worker versions execute at the same speed - there’s no overhead to using threads with <code class="literal">numpy</code> (and with only one worker there’s also no gain).</p><p>When using multiple processes we see a classic 100% utilization of each additional CPU, the result mirrors the plots shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_1_processes">Figure&nbsp;9-3</a>, <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_2_processes">Figure&nbsp;9-4</a>, <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_4_processes">Figure&nbsp;9-5</a> and <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-pi_monte_carlo_lists_8_processes">Figure&nbsp;9-6</a> but obviously runs much faster using <code class="literal">numpy</code>.</p><p>Interestingly the thread version runs <span class="emphasis"><em>faster</em></span> with more threads, this is the opposite behavior to the pure Python case where threads made the example run more slowly. As discussed on the SciPy wiki <sup>[<a id="id815781" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815781" class="footnote">61</a>]</sup> by working outside of the GIL <code class="literal">numpy</code> can achieve some level of additional speed-up around threads.</p><p>Using processes gives us a predictable speedup, just as it did in the pure Python example. A second CPU doubles the speed, using four CPUs quadruples the speed.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-pi-numpy-calculation">Example&nbsp;9-3</a> shows the vectorized form of our code. Note that the random number generator is seeded when this function is called. For the thread version this isn’t necessary as each thread shares the same random number generator and they access it in series.</p><p>For the process version as each new process is a fork, all the forked versions will share the <span class="emphasis"><em>same state</em></span> so the random number calls in each will return the same sequence! By calling <code class="literal">seed()</code> each of the forked processes should generate a unique sequence of random numbers. Look back at <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-random-numbers">Random Numbers in Parallel Systems</a> for some notes about the dangers of parallelized random number sequences.</p><div class="example"><a id="code-pi-numpy-calculation"></a><div class="example-title">Example&nbsp;9-3.&nbsp;Estimating Pi using numpy</div><div class="example-contents"><pre class="screen">def estimate_nbr_points_in_quarter_circle(nbr_samples):
    # set random seed for numpy in each new process
    # else the fork will mean they all share the same state
    np.random.seed()
    xs = np.random.uniform(0, 1, nbr_samples)
    ys = np.random.uniform(0, 1, nbr_samples)
    estimate_inside_quarter_unit_circle = (xs * xs + ys * ys) &lt;= 1
    nbr_trials_in_quarter_unit_circle = np.sum(estimate_inside_quarter_unit_circle)
    return nbr_trials_in_quarter_unit_circle</pre></div></div><p>A short code analysis shows that the calls to <code class="literal">random</code> run a little slower on this machine when executed with multiple threads and the call to <code class="literal">(xs * xs + ys * ys) &lt;= 1</code> parallelizes well. Calls to the random number generator are GIL-bound as the internal state variable is a Python object.</p><p>The process to understand this was basic but reliable:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Comment out all of the <code class="literal">numpy</code> lines, run with <span class="emphasis"><em>no</em></span> threads using the Serial version, run several times and record the execution time using <code class="literal">time.time()</code> in <code class="literal">__main__</code>
</li><li class="listitem">
Add a line back (first of all I add <code class="literal">xs = np.random.uniform(...</code>), run several times again record completion times
</li><li class="listitem">
Add the next line back (now adding <code class="literal">ys = ...</code>), run again, record completion time
</li><li class="listitem">
Repeat including the <code class="literal">nbr_trials_in_quarter_unit_circle = np.sum(...</code> line
</li><li class="listitem">
Repeat this process again but this time with four threads, repeat line by line
</li><li class="listitem">
Compare the difference in run-time at each step for no-threads and four-threads
</li></ol></div><p>Because we’re running code in parallel it becomes harder to use tools like <code class="literal">line_profiler</code> or <code class="literal">cProfile</code>. Recording the raw run time and observing the differences in behavior with different configurations takes some patience but gives solid evidence from which to draw conclusions.</p><div class="note"><h3 class="title">Note</h3><p>If you want to understand the serial behavior of the <code class="literal">uniform</code> call then take a look at the <code class="literal">mtrand</code> code in the <code class="literal">numpy</code> source <sup>[<a id="id815963" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id815963" class="footnote">62</a>]</sup> and follow the call to <code class="literal">uniform</code> in <code class="literal">mtrand.pyx</code>. This is a useful exercise if you haven’t looked at the <code class="literal">numpy</code> source code before.</p></div><p>The libraries used when building <code class="literal">numpy</code> are important for some of the parallelization opportunities, depending on the underlying libraries used when building <code class="literal">numpy</code> (e.g. whether Intel’s Math Kernel Library or OpenBLAS were included or not) you’ll see different speed-up behavior.</p><p>You can check your <code class="literal">numpy</code> configuration using <code class="literal">numpy.show_config()</code>. There are some example timings <sup>[<a id="id816013" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id816013" class="footnote">63</a>]</sup> on StackOverflow if you’re curious about the possibilities. Only some <code class="literal">numpy</code> calls will benefit from parallelization by external libraries.</p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_finding_prime_numbers">Finding Prime Numbers</h2></div></div></div><p><a id="multiprocessing-finding-prime-numbers"></a>Next we’ll look at testing for Prime Numbers over a large number range. This is
a different problem to estimating Pi as the workload varies depending on your
location in the number range and each individual number’s check has an
unpredictable complexity. We can create a serial routine that checks for
Primality and then pass sets of numbers across multiple processes for checking.
This problem is embarrassingly parallel which means there is no state that needs
to be shared.</p><p>The <code class="literal">multiprocessing</code> module makes it easy to control the workload so we shall investigate how we can tune the work queue to use (and mis-use!) our computing resources and investigate an easy way to use our resources slightly more efficiently. This means we’re looking at <span class="emphasis"><em>load balancing</em></span> to try to efficiently distribute our varying-complexity tasks to our fixed set of resources.</p><p>We’ll use a slightly improved algorithm from the one earlier in the book in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch01.html#understanding-performance-idealized-computing">Idealized computing vs Python VM</a> which exits early if we have an even number, see <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-primes-serial-generation-calculation">Example&nbsp;9-4</a>.</p><div class="example"><a id="code-primes-serial-generation-calculation"></a><div class="example-title">Example&nbsp;9-4.&nbsp;Finding Prime numbers using Python</div><div class="example-contents"><pre class="screen">def check_prime(n):
    if n % 2 == 0:
        return False
    from_i = 3
    to_i = math.sqrt(n) + 1
    for i in xrange(from_i, int(to_i), 2):
        if n % i == 0:
            return False
    return True</pre></div></div><p>How much variety in the work-load do we see when testing for a Prime with this approach? <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-cost-to-check-primality">Figure&nbsp;9-10</a> shows the increasing time cost to check for Primality as the possibly-Prime <code class="literal">n</code> increases between <code class="literal">10,000</code> to <code class="literal">1,000,000</code></p><div class="figure"><a id="FIG-primes-cost-to-check-primality"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_prime_time_cost_1e4to1e6.png" alt="Time to check primality" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_prime_time_cost_1e4to1e6.png"></div></div><div class="figure-title">Figure&nbsp;9-10.&nbsp;Time required to check primality as n increases</div></div><p>Most numbers are non-Prime, they’re drawn with a <code class="literal">o</code> and some can be cheap to check for, others require the checking of many factors. Primes are drawn with an <code class="literal">x</code> and form the thick darker band, they’re the most expensive to check for. The time cost of checking a number increases as <code class="literal">n</code> increases as the range of possible factors to check increases with the square root of <code class="literal">n</code>. The sequence of Primes is not predictable so we can’t determine the expected cost of a range of numbers (we could estimate it, but we can’t be sure of its complexity).</p><p>For the figure we test each <code class="literal">n</code> 20 times and take the fastest result to remove jitter from the results.</p><p>When we distribute work to a <code class="literal">Pool</code> of processes we can specify how much work is passed to each worker. We could divide all of the work evenly and aim for one pass or we could make many chunks of work which we pass out whenever a CPU is free. This is controlled using the <code class="literal">chunksize</code> parameter. Larger chunks of work mean less communications overhead, smaller chunks of work mean more control over how resources are allocated.</p><p>For our Prime finder a single piece of work is a number <code class="literal">n</code> that is checked by <code class="literal">check_prime</code>. A <code class="literal">chunksize</code> of 10 would mean that a list of 10 <code class="literal">n</code> integers are handled by one process, one at a time.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-pool-plot-chunksizetimes-type2">Figure&nbsp;9-11</a> we can see the effect of varying the <code class="literal">chunksize</code> from <code class="literal">1</code> (every job is a single piece of work) to <code class="literal">64</code> (every job is a list of 64 numbers). Although many tiny jobs gives us the greatest flexibility it also imposes the greatest communication overhead. All 4 CPUs will be utilized efficiently but the communication pipe will become a bottleneck as each job and result is passed through this single channel.</p><p>By doubling the <code class="literal">chunksize</code> to <code class="literal">2</code> our task is solved twice as quickly as we have less contention on the communication pipe. By increasing the <code class="literal">chunksize</code> we continue to improve the execution time, naively we might assume that if we continued to make the jobs bigger then we’d continue to see a speed increase.</p><div class="figure"><a id="FIG-primes-pool-plot-chunksizetimes-type2"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_plottype2.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_plottype2.png"></div></div><div class="figure-title">Figure&nbsp;9-11.&nbsp;Choosing a sensible chunksize value</div></div><p>We can continue to increase the <code class="literal">chunksize</code> until we start to see a worsening of behavior. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-pool-plot-chunksizetimes-type1">Figure&nbsp;9-12</a> we expand the range of chunk sizes, making them not just tiny but also huge. The worst result shown takes 1.3 seconds where we’ve asked for <code class="literal">chunksize</code> to be <code class="literal">50000</code> - this means our <code class="literal">100000</code> items are divided into two work chunks leaving 2 CPUs idle for that entire pass.</p><p>With a <code class="literal">chunksize</code> of <code class="literal">10000</code> items we are creating 10 chunks of work - this means that 4 chunks of work will run twice in parallel following by the 2 remaining chunks. This leaves two CPUs idle in the third round of work which is an inefficient usage of resources.</p><div class="figure"><a id="FIG-primes-pool-plot-chunksizetimes-type1"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_plottype1.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_plottype1.png"></div></div><div class="figure-title">Figure&nbsp;9-12.&nbsp;Choosing a sensible chunksize value (continued)</div></div><p>An optimal solution in this case is to divide the total number of jobs by the number of CPUs and this is the default behavior in <code class="literal">multiprocessing</code>, shown as the “default” black dot in the figure.</p><p>As a general rule the default behavior is sensible, only tune it if you expect to see a real gain and definitely confirm your hypothesis against the default behavior.</p><p>Unlike with the Monte Carlo Pi problem our Prime Testing calculation has a varying complexity - sometimes a job exits quickly (an even number is detected the fastest) and sometimes the number is large and a Prime and this takes a much longer time to check.</p><p>What happens if we randomize our job sequence? For this problem we squeeze out a 2% performance gain that you can see in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-pool-plot-chunksizetimes-type1-shuffled">Figure&nbsp;9-13</a>. By randomizing we reduce the likelihood of the problem that a final job in a sequence takes longer than the others, leaving all but one CPU active.</p><div class="figure"><a id="FIG-primes-pool-plot-chunksizetimes-type1-shuffled"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_shuffled_plottype1.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_1to50000_shuffled_plottype1.png"></div></div><div class="figure-title">Figure&nbsp;9-13.&nbsp;Randomzing the job sequence</div></div><p>We noted in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-pool-plot-chunksizetimes-type1">Figure&nbsp;9-12</a> that with a <code class="literal">chunksize</code> of <code class="literal">10000</code> we create three rounds of work, the first two rounds use 100% of the resources and the last round uses 50% of the resources. Misaligning the workload with the number of available resources leads to inefficiency.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-primes-pool-plot-chunksizetimes-by-nbrchunks-sawtoothpattern">Figure&nbsp;9-14</a> shows the odd effect that occurs when we mis-align the number of chunks of work against the number of processors. Mismatches will under-utilize the available resources. The slowest overall run time occurs when only 1 chunk of work is created, this leaves 3 unutilized CPUs. 2 work chunks leave 2 CPUs unutilized, once we have 4 work chunks we are using all of our resources. If we add a 5th work chunk then again we’re underutilizing our resources - 4 CPUs will work on their chunks and then 1 CPU will run to calculate the 5th chunk.</p><p>As we increase the number of chunks of work we can see that the inefficiencies decrease, the difference in run time between 29 and 32 work chunks is approximately 0.01 seconds. The general rule is to make lots of small jobs for efficient resource utilization if your jobs have varying run-times.</p><div class="figure"><a id="FIG-primes-pool-plot-chunksizetimes-by-nbrchunks-sawtoothpattern"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_by_nbrchunks_sawtoothpattern.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_primes_pool_plot_chunksizetimes_by_nbrchunks_sawtoothpattern.png"></div></div><div class="figure-title">Figure&nbsp;9-14.&nbsp;The danger of choosing an inappropriate number of chunks</div></div><p>Strategies for efficiently using multiprocessing for embarrassingly parallel problems:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
Split your jobs into independent units of work
</li><li class="listitem">
If you workers take varying amounts of time then consider randomizing the sequence of work (e.g. another example would be to process variable-sized files)
</li><li class="listitem">
Sorting your work-queue so slowest-jobs-go-first may be an equally useful strategy
</li><li class="listitem">
Use the default <code class="literal">chunksize</code> unless you have verified reasons for adjusting it
</li><li class="listitem">
Align the number of jobs with the number of physical CPUs (again the default <code class="literal">chunksize</code> takes care of this for you - although it’ll use any HyperThreads by default which may not offer any additional gain)
</li></ul></div><p>Note that by default <code class="literal">multiprocessing</code> will see HyperThreads as additional CPUS and so on my laptop it will allocate 8 processes when only 4 will really be running at 100% speed. The additional 4 processes could be taking up valuable RAM whilst barely offering any additional speed gain.</p><p>With a <code class="literal">Pool</code> we can split a chunk of pre-defined work up-front amongst CPUs. This is less helpful if we have dynamic workloads and particularly if we have workloads that arrive over time. For this sort of workload we might want to use a queue which I’ll introduce in the next section.</p><div class="note"><h3 class="title">Note</h3><p>If you’re working on long-running scientific problems where each job takes at least many seconds to run then you might want to review Gael Varoquaux’s <code class="literal">joblib</code> <sup>[<a id="id816594" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id816594" class="footnote">64</a>]</sup>. This tool supports lightweight pipelining, it sits on top of <code class="literal">multiprocessing</code> and offers an easier parallel interface, result caching and debugging features.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_queues_of_work">Queues of work</h3></div></div></div><p><code class="literal">multiprocessing.Queue</code> objects give us a non-persistent queue that can send any pickleable Python objects between processes. They carry an overhead as each object must be pickled to be sent and then unpickled in the consumer (along with some locking operations). If your workers are processing larger jobs then the communication overhead is probably acceptable. In the following example we’ll see that this cost is not negligible.</p><p>Working with the queues is fairly easy. In the following example we’ll check for Primes by consuming a list of candidate numbers and posting confirmed Primes back to a <code class="literal">definite_primes_queue</code>. We’ll run this with one, two, four and eight processes and confirm that these take longer than just running a single process that checks the same range.</p><p>A <code class="literal">Queue</code> gives us the ability to perform lots of inter-process communication using native Python objects, this can be useful it you’re passing around objects with lots of state. Since the <code class="literal">Queue</code> lacks persistence you probably don’t want to use them for jobs that might require robustness in the face of failure (e.g. if you lose power or a hard-drive gets corrupted).</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-queues-of-work-fullwork-check-prime">Example&nbsp;9-5</a> shows the <code class="literal">check_prime</code> function, we’re already familiar with the basic Primality test. We run in an infinite loop, blocking (waiting until work is available) on <code class="literal">possible_primes_queue.get()</code> to consume an item from the queue. Only one process can get an item at a time as the <code class="literal">Queue</code> object takes care of synchronizing the accesses. If there’s no work in the queue then the <code class="literal">.get()</code> blocks until a task is available.</p><p>When Primes are found they are <code class="literal">put</code> back on the <code class="literal">definite_primes_queue</code> for consumption by the parent process.</p><div class="example"><a id="code-queues-of-work-fullwork-check-prime"></a><div class="example-title">Example&nbsp;9-5.&nbsp;Using two Queues for IPC</div><div class="example-contents"><pre class="screen">FLAG_ALL_DONE = b"WORK_FINISHED"
FLAG_WORKER_FINISHED_PROCESSING = b"WORKER_FINISHED_PROCESSING"

def check_prime(possible_primes_queue, definite_primes_queue):
    while True:
        n = possible_primes_queue.get()
        if n == FLAG_ALL_DONE:
            # flag that our results have all been pushed to the results queue
            definite_primes_queue.put(FLAG_WORKER_FINISHED_PROCESSING)
            break
        else:
            if n % 2 == 0:
                continue
            for i in xrange(3, int(math.sqrt(n)) + 1, 2):
                if n % i == 0:
                    break
            else:
                definite_primes_queue.put(n)</pre></div></div><p>We define two flags, one is fed by the parent process as a poison pill to indicate that there is no more work available. The second is fed by the worker to confirm that it has seen the poison pill and has closed itself down.  The first poison pill is also known as a Sentinel <sup>[<a id="id816691" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id816691" class="footnote">65</a>]</sup> as it guarantees the termination of the processing loop.</p><p>When dealing with queues of work and remote workers it can be helpful to use flags like these to record that the poison pills were sent and to check that a response was sent from the children in a sensible time-window that they are shutting down. We don’t handle that process here but adding some timekeeping is a fairly simple addition to the code. The receipt of these flags can be logged or printed during debugging.</p><p>The <code class="literal">Queue</code> objects are created out of a <code class="literal">Manager</code> in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-queues-of-work-fullwork-main1">Example&nbsp;9-6</a>. We’ll use the familiar process of building a list of <code class="literal">Process</code> objects which each contain a forked process, the two queues are sent as arguments and <code class="literal">multiprocessing</code> handles their synchronization.</p><p>Having started the new processes we had a list of jobs to the <code class="literal">possible_primes_queue</code> and end with one poison pill per process. The jobs will be consumed in FIFO order leaving the poison pills for last.</p><p>In <code class="literal">check_prime</code> we use a blocking <code class="literal">.get()</code> as the new processes will have to wait for work to appear in the queue. Since we use flags we could add some work, deal with the results and then iterate by adding more work and signal the end of life of the workers by adding the poison pills later.</p><div class="example"><a id="code-queues-of-work-fullwork-main1"></a><div class="example-title">Example&nbsp;9-6.&nbsp;Building two Queues for IPC</div><div class="example-contents"><pre class="screen">if __name__ == "__main__":
    primes = []

    manager = multiprocessing.Manager()
    possible_primes_queue = manager.Queue()
    definite_primes_queue = manager.Queue()

    NBR_PROCESSES = 2
    pool = Pool(processes=NBR_PROCESSES)
    processes = []
    for _ in range(NBR_PROCESSES):
        p = multiprocessing.Process(target=check_prime,
                                    args=(possible_primes_queue, definite_primes_queue))
        processes.append(p)
        p.start()

    t1 = time.time()
    number_range = xrange(100000000, 101000000)

    # add jobs to the inbound work queue
    for possible_prime in number_range:
        possible_primes_queue.put(possible_prime)

    # add poison pills to stop the remote workers
    for n in xrange(NBR_PROCESSES):
        possible_primes_queue.put(FLAG_ALL_DONE)</pre></div></div><p>To consume the results we start another infinite loop in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-queues-of-work-fullwork-main2">Example&nbsp;9-7</a> using a blocking <code class="literal">.get()</code> on the <code class="literal">definite_primes_queue</code>. If the finished-processing flag is found then we take a count of the number of processes that have signalled their exit. If not then we have a new Prime and we add this to the <code class="literal">primes</code> list.</p><p>We exit the infinite loop when all of our processes have signalled their exit.</p><div class="example"><a id="code-queues-of-work-fullwork-main2"></a><div class="example-title">Example&nbsp;9-7.&nbsp;Using two Queues for IPC</div><div class="example-contents"><pre class="screen">    processors_indicating_they_have_finished = 0
    while True:
        new_result = definite_primes_queue.get()  # block whilst waiting for results
        if new_result == FLAG_WORKER_FINISHED_PROCESSING:
            processors_indicating_they_have_finished += 1
            if processors_indicating_they_have_finished == NBR_PROCESSES:
                break
        else:
            primes.append(new_result)
    assert processors_indicating_they_have_finished == NBR_PROCESSES

    print "Took:", time.time() - t1
    print len(primes), primes[:10], primes[-10:]</pre></div></div><p>There is quite an overhead to using a Queue due to the pickling and synchronization. As you can see in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-queues-of-work-fullwork">Figure&nbsp;9-15</a> using a Queue-less single process solution is significantly faster than using any number of processes. The reason in this case is because our workload is very light - the communication cost dominates the overall time for this task.</p><p>Two processes complete this example a little faster than using one process, four and eight processes are each slower.</p><div class="figure"><a id="FIG-queues-of-work-fullwork"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/multiprocessing_serial_vs_queue_times.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/multiprocessing_serial_vs_queue_times.png"></div></div><div class="figure-title">Figure&nbsp;9-15.&nbsp;Cost of using Queue objects</div></div><p>If your task has a long completion time (at least a sizeable fraction of a second) with a small amount of communication then a <code class="literal">Queue</code> approach might be the right answer. You will have to verify whether the communication cost makes this approach useful enough.</p><p>You might wonder what happens if we remove the redundant half of the job queue (all the even numbers - these are rejected very quickly in <code class="literal">check_prime</code>). By halving the size of the input queue our execution time halves in each case (and it still doesn’t beat the single-process non-Queue example!). This helps to illustrate that the communication cost is the dominating factor in this problem.</p><div class="sect3"><div class="titlepage"><div><div><h4 class="title" id="_asynchronously_adding_jobs_to_the_queue">Asynchronously adding jobs to the Queue</h4></div></div></div><p>By adding a <code class="literal">Thread</code> into the main process we can feed jobs asynchronously into the <code class="literal">possible_primes_queue</code>. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-queues-of-work-fullwork-jobs-feeder-thread">Example&nbsp;9-8</a> we define a <code class="literal">feed_new_jobs</code> function, it performs the same job as the job setup routine that we had in <code class="literal">__main__</code> before but it does it in a separate thread.</p><div class="example"><a id="code-queues-of-work-fullwork-jobs-feeder-thread"></a><div class="example-title">Example&nbsp;9-8.&nbsp;Asynchronous job feeding function</div><div class="example-contents"><pre class="screen">def feed_new_jobs(number_range, possible_primes_queue, nbr_poison_pills):
    for possible_prime in number_range:
        possible_primes_queue.put(possible_prime)
    # add poison pills to stop the remote workers
    for n in xrange(nbr_poison_pills):
        possible_primes_queue.put(FLAG_ALL_DONE)</pre></div></div><p>Now in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-queues-of-work-fullwork-main">Example&nbsp;9-9</a> our <code class="literal">__main__</code> will setup the <code class="literal">Thread</code> using the <code class="literal">possible_primes_queue</code> and then move on to the result-collection phase <span class="emphasis"><em>before</em></span> any work has been issued. The asynchronous job feeder could consume work from external sources (e.g. from a database or I/O bound communication) whilst the <code class="literal">__main__</code> thread handles each processed result. This means that the input sequence and output sequence do not need to be created in advance, they can both be handled on the fly.</p><div class="example"><a id="code-queues-of-work-fullwork-main"></a><div class="example-title">Example&nbsp;9-9.&nbsp;Using a Thread to setup an asyncronoush job feeder</div><div class="example-contents"><pre class="screen">if __name__ == "__main__":
    primes = []
    manager = multiprocessing.Manager()
    possible_primes_queue = manager.Queue()

    ...

    import threading
    thrd = threading.Thread(target=feed_new_jobs,
                            args=(number_range,
                                  possible_primes_queue,
                                  NBR_PROCESSES))
    thrd.start()

    # deal with the results</pre></div></div><p>It is <span class="emphasis"><em>very likely</em></span> that if you want robust asynchronous systems then you should probably look to an external library that is mature. <code class="literal">gevent</code>, <code class="literal">tornado</code> and <code class="literal">Twisted</code> are strong candidates, Python 3.4’s <code class="literal">tulip</code> is a new contender. The above examples will get you started but pragmatically they are more useful for simply systems and education than for production systems.</p><div class="informalexample"><a id="NOTE"></a><p>Another single-machine queue you might want to investigate is PyRes <sup>[<a id="id817027" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id817027" class="footnote">66</a>]</sup>. This module that uses Redis to store the queue’s state. I introduce Redis in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-using-redis-as-a-flag">Using Redis as a flag</a> later in this Chapter. Redis is a non-Python data storage system, a queue of data held in Redis is readable outside of Python (so you can inspect the queue’s state) and could be shared with non-Python systems.</p></div><p>Be <span class="emphasis"><em>very aware</em></span> that asynchronous systems require a special level of patience - you will end up tearing out your hair whilst you are debugging. We’d suggest:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
Apply the “Keep It Simple, Stupid” principle
</li><li class="listitem">
Probably avoid asynchronous self-contained systems (like the above example) if possible as they will grow in complexity and quickly will become hard to maintain
</li><li class="listitem">
Use mature libraries like <code class="literal">gevent</code> which give you tried-and-tested approaches to dealing with certain problem sets
</li></ul></div><p>We’d strongly suggest using an external queue system (e.g. <code class="literal">Gearman</code>, <code class="literal">0MQ</code>, <code class="literal">Celery</code>, <code class="literal">PyRes</code> or <code class="literal">HotQueue</code>) which gives you external visibility on the state of the queues. This requires more thought but is likely to save you time due to increased debug efficiency and better system visibility for production systems.</p></div></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_verifying_primes_using_inter_process_communication">Verifying Primes using Inter Process Communication</h2></div></div></div><p><a id="multiprocessing-verifying-primes-using-inter-process-communication"></a>Prime numbers have no factor other than themselves and one. It stands to reason that the most common factor is two (every even number cannot be a Prime). After that the low Prime numbers (e.g. 3, 5, 7) become factors of larger non-Primes (e.g. 9, 15, 21 respectively).</p><p>Let’s say that we’re given a large number and we’re asked to verify if it is Prime. We will probably have a large space of factors to search. <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-verifying-primes-count-of-factors-of-nonprimes">Figure&nbsp;9-16</a> shows the frequency of each factor for non-primes up to 10,000,000. Low factors are far more likely to occur than high factors but there’s no predictable pattern.</p><div class="figure"><a id="FIG-verifying-primes-count-of-factors-of-nonprimes"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/primes_validation_count_of_factors.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/primes_validation_count_of_factors.png"></div></div><div class="figure-title">Figure&nbsp;9-16.&nbsp;The frequency of factors of non-primes</div></div><p>Let’s define a new problem - we have a <span class="emphasis"><em>small</em></span> set of numbers and our task is to efficiently use our CPU resources to figure out if each number is a Prime, one number at a time. Possibly we’ll have just one large number to test. It no longer makes sense to use 1 CPU to do the check, we want to co-ordinate the work across many CPUs.</p><p>For this section we’ll look at some larger numbers, one of 15 digits and four with 18 digits:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
small non-prime: 112272535095295
</li><li class="listitem">
large non-prime 1: 100109100129100369
</li><li class="listitem">
large non-prime 2: 100109100129101027
</li><li class="listitem">
prime 1: 100109100129100151
</li><li class="listitem">
prime 2: 100109100129162907
</li></ol></div><p>By using a smaller non-prime and some larger non-primes we get to verify that our chosen process is not just faster at checking for primes but that it is also not getting slower at checking non-primes. We’ll assume that we don’t know the size or type of numbers that we’re being given so we want the fastest possible result for all our use-cases.</p><p>Co-operation comes at a cost - the cost of synchronizing data and checking the shared data can be quite high. We’ll work through several approaches here which can be used in different ways for task co-ordination. Note that I’m <span class="emphasis"><em>not</em></span> covering MPI (Message Passing Interface) here which is somewhat specialist, we’re lookinng at batteries-included modules and Redis (which is very common).</p><p>If you want to use MPI then I’m assuming you already know what you’re doing. The MPI4PY <sup>[<a id="id817243" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id817243" class="footnote">67</a>]</sup> project would be a good place to start. It is an ideal technology if you want to control latency when lots of processes are collaborating whether you have 1 or many machines.</p><p>For the following runs each test is performed 20 times and the minimum time is taken to show the fastest speed that is possible for that method. In these examples I’m using various techniques to share a flag (often as 1 byte). We could use a basic object like a <code class="literal">Lock</code> but then you could only share 1 bit of state. I’m choosing to show you how to share a primitive type so that more expressive state sharing is possible (even though we don’t need a more expressive state for this example).</p><p>I must emphasise that sharing state tends to make things <span class="emphasis"><em>get complicated</em></span> - you can easily end up in another hair-pulling state. Be careful and try to keep things as simple as they could be. It might be the case that less efficient resource usage is trumped by developer time spent on other challenges.</p><p>First we should discuss the results, then we’ll work through the code.</p><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-validating-primes-slower-results">Figure&nbsp;9-17</a> shows the first approaches to trying to use inter-process communication to test for Primality faster. The benchmark is the Serial version which does not use any inter-process communication, each attempt to speed-up our code must at least be faster than this.</p><p>The Less Naive Pool version has a predictable (and good) speed. It is good enough to be rather hard to beat. Don’t overlook the obvious in your search for high-speed solutions, sometimes a dumb and good-enough solution is all you need.</p><p>The approach for the Less Naive Pool solution is to take our number under test, divide its possible-factor range evenly amongst the available CPUs and then push the work out to each CPU. If any CPU finds a factor is will exit early but it won’t communicate this fact - the other CPUs will continue to work through their part of the range. This means for an 18 digit number (our four larger examples) the search time is the same whether it is Prime or non-Prime.</p><p>The Redis and Manager solutions are slower when it comes to testing a larger number of factors for Primality due to the communication overhead. They use a shared flag to indicate that a factor has been found and the search should be called off.</p><p>Redis lets you share state not just to other Python processes but also to other tools and other machines and even to expose that state over a web-browser interface (which might be useful for remote monitoring). The Manager is a part of <code class="literal">multiprocessing</code>, it provides a high-level synchronized set of Python objects (including primitives, the <code class="literal">list</code> and the <code class="literal">dict</code>).</p><p>For the larger non-Prime cases although there is a cost to checking the shared flag, this is dwarfed by the saving in search time by signalling early that a factor has been found.</p><p>For the Prime cases though there is no way to exit early as no factor will be found, so the cost of checking the shared flag will become the dominating cost.</p><div class="figure"><a id="FIG-validating-primes-slower-results"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/multiprocessing_plot_prime_validation_times_slower_results.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/multiprocessing_plot_prime_validation_times_slower_results.png"></div></div><div class="figure-title">Figure&nbsp;9-17.&nbsp;The slower ways to use IPC to validate Primality</div></div><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-validating-primes-faster-results">Figure&nbsp;9-18</a> shows that we can get a considerably faster result with a bit of effort. The Less naive Pool result is still our benchmark and the RawValue and MMap (memory-map) results are much faster than the previous Redis and Manager results. The real magic comes by taking the fasest solution and performing some less-obvious code manipulations to make a near-optimal MMap solution, this final version is faster than the Less naive Pool solution and almost as fast as it in the Primes case.</p><div class="figure"><a id="FIG-validating-primes-faster-results"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/multiprocessing_plot_prime_validation_times_faster_results.png" alt="notset" width="1000" height="727" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/multiprocessing_plot_prime_validation_times_faster_results.png"></div></div><div class="figure-title">Figure&nbsp;9-18.&nbsp;The faster ways to use IPC to validate Primality</div></div><p>In the following section we’ll work through various ways of using IPC in Python to solve our co-operative search problem. I hope you’ll see that IPC is fairly easy but generally comes with a cost.</p><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_serial_verification_is_inefficient">Serial verification is inefficient</h3></div></div></div><p>We’ll start with the same serial checking code that we used before, shown again in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-serial">Example&nbsp;9-10</a>. As noted above for any non-Prime with a large factor we could more efficiently search the space of factors in parallel. A serial sweep will give us a sensible baseline to work from.</p><div class="example"><a id="code-verifying-primes-serial"></a><div class="example-title">Example&nbsp;9-10.&nbsp;Serial verification</div><div class="example-contents"><pre class="screen">def check_prime(n):
    if n % 2 == 0:
        return False
    from_i = 3
    to_i = math.sqrt(n) + 1
    for i in xrange(from_i, int(to_i), 2):
        if n % i == 0:
            return False
    return True</pre></div></div></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_naive_pool_solution">Naive Pool solution</h3></div></div></div><p>The Naive Pool solution works with a <code class="literal">multiprocessing.Pool</code> as we’ve already seen in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-finding-prime-numbers">Finding Prime Numbers</a> and <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-estimating-pi">Estimating Pi using Processes and Threads</a> with four forked processes. We have a number to test for Primality, we divide the range of possible factors into four tuples of sub-ranges and send these into the <code class="literal">Pool</code>.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-pool1-1">Example&nbsp;9-11</a> we use a new method <code class="literal">create_range.create</code> (which I won’t show - it is quite boring) that splits the work space into equal sized regions, where each item in <code class="literal">ranges_to_check</code> is an pair of lower and upper bounds to search between. For the first 18 digit non-prime (100109100129100369) with 4 processes we’ll have the factor ranges <code class="literal">ranges_to_check == [(3, 79100057), (79100057, 158200111), (158200111, 237300165), (237300165, 316400222)]</code> (where 316400222 is the square root of 100109100129100369 plus one).</p><p>In <code class="literal">__main__</code> we establish a <code class="literal">Pool</code>, <code class="literal">check_prime</code> then splits the <code class="literal">ranges_to_check</code> for each possibly-Prime number <code class="literal">n</code> via a <code class="literal">map</code>. If the result is <code class="literal">False</code> then we have found a factor and we do not have a Prime.</p><div class="example"><a id="code-verifying-primes-pool1-1"></a><div class="example-title">Example&nbsp;9-11.&nbsp;Naive Pool solution</div><div class="example-contents"><pre class="screen">def check_prime(n, pool, nbr_processes):
    from_i = 3
    to_i = int(math.sqrt(n)) + 1
    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)
    ranges_to_check = zip(len(ranges_to_check) * [n], ranges_to_check)
    assert len(ranges_to_check) == nbr_processes
    results = pool.map(check_prime_in_range, ranges_to_check)
    if False in results:
        return False
    return True

if __name__ == "__main__":
    NBR_PROCESSES = 4
    pool = Pool(processes=NBR_PROCESSES)
    ...</pre></div></div><p>We modify the previous <code class="literal">check_prime</code> in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-pool1-2">Example&nbsp;9-12</a> to take a lower and upper bound for the range to check. There’s no value in passing a complete list of possible-factors to check so we save time and memory by passing just two numbers that define our range.</p><div class="example"><a id="code-verifying-primes-pool1-2"></a><div class="example-title">Example&nbsp;9-12.&nbsp;check_prime_in_range</div><div class="example-contents"><pre class="screen">def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    for i in xrange(from_i, int(to_i), 2):
        if n % i == 0:
            return False
    return True</pre></div></div><p>For the “small non-Prime” case the verification time via the Pool is 0.1 second, a significantly longer time than the original 0.000002 seconds in the Serial process. Accepting this one worse result the overall result is a speed-up across the board. We might accept that one slower result isn’t a problem - but what if we might get lots of smaller non-Primes to check? It turns out we can avoid this slow-down, we’ll see that next with the Less Naive Pool solution.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_a_less_naive_pool_solution">A Less Naive Pool solution</h3></div></div></div><p>The previous solution was inefficient at validating the smaller non-Prime. For any smaller (less than 18 digit) non-Prime it is likely to be slower than the serial method due to the overhead of sending out partitioned work and not knowing if a very small factor (which are the more likely factors) will be found. If a small factor is found then the process would have to wait for the other larger factor searches to complete.</p><p>We could start to signal between the processes that a small factor had been found but since they happen so frequently, this adds a lot of communications overhead. The solution presented in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-pool2-1">Example&nbsp;9-13</a> is a more pragmatic approach - a serial check is performed quickly for likely small factors and if none are found then a parallel search is started. Combining a serial pre-check before launching a relatively more expensive parallel operation is a common approach to avoiding some of the the costs of parallel computing.</p><p>The speed of this solution is equal to or better than the original serial search for each of our test numbers. This is our new benchmark.</p><p>Importantly this Pool approach gives us an optimal case for the Prime-checking situation. If we have a Prime then there’s no way to exit early, we have to manually check all possible factors before we can exit.</p><div class="example"><a id="code-verifying-primes-pool2-1"></a><div class="example-title">Example&nbsp;9-13.&nbsp;Improving the Naive Pool Solution for the small-non-Prime case</div><div class="example-contents"><pre class="screen">def check_prime(n, pool, nbr_processes):
    # cheaply check high probability set of possible factors
    from_i = 3
    to_i = 21
    if not check_prime_in_range((n, (from_i, to_i))):
        return False

    # continue to check for larger factors in parallel
    from_i = to_i
    to_i = int(math.sqrt(n)) + 1
    ranges_to_check = create_range.create(from_i, to_i, nbr_processes)
    ranges_to_check = zip(len(ranges_to_check) * [n], ranges_to_check)
    assert len(ranges_to_check) == nbr_processes
    results = pool.map(check_prime_in_range, ranges_to_check)
    if False in results:
        return False
    return True</pre></div></div><p>There’s no faster way to check though these factors as any approach that adds complexity will have more instructions and so the check-all-factors case will cause the most instructions to be executed. See the various <code class="literal">mmap</code> solutions at the end of this section for a discussion on how to get as close to this current result for Primes as possible.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_manager_value_as_a_flag">Using Manager.Value as a flag</h3></div></div></div><p>The <code class="literal">multiprocessing.Manager()</code> lets us share higher level Python objects between processes as managed shared objects, the lower-level objects are wrapped in proxy objects. The wrapping and safety has a speed-cost but also offers great flexibility. You can share both lower level objects (e.g. integers and floats) and also lists and dictionaries.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-managervalue1">Example&nbsp;9-14</a> we create a <code class="literal">Manager</code> and then create a 1 byte (character) <code class="literal">manager.Value(b"c", FLAG_CLEAR)</code> flag. You could create any of the <code class="literal">ctypes</code> primitives (which are the same as the <code class="literal">array.array</code> primitives) if you wanted to share strings or numbers.</p><p>Note that <code class="literal">FLAG_CLEAR</code> and <code class="literal">FLAG_SET</code> are assigned a byte (<code class="literal">b'0'</code> and <code class="literal">b'1'</code> respectively), I’ve chosen to use the leading <code class="literal">b</code> to be very explicit (it might default to a unicode or string object if left as an implicit string depending on your environment and Python version).</p><p>Now we can flag across all of our processes that a factor has been found so the search can be called off early. The difficulty is balancing the cost of reading the flag against the speed saving that is possible. Because the flag is synchronized we don’t want to check it too frequenly - this adds an additional overhead.</p><div class="example"><a id="code-verifying-primes-managervalue1"></a><div class="example-title">Example&nbsp;9-14.&nbsp;Passing a Manager.Value object as a flag</div><div class="example-contents"><pre class="screen">SERIAL_CHECK_CUTOFF = 21
CHECK_EVERY = 1000
FLAG_CLEAR = b'0'
FLAG_SET = b'1'
print "CHECK_EVERY", CHECK_EVERY

if __name__ == "__main__":
    NBR_PROCESSES = 4
    manager = multiprocessing.Manager()
    value = manager.Value(b'c', FLAG_CLEAR)  # 1 byte character
    ...</pre></div></div><p><code class="literal">check_prime_in_range</code> will now be aware of the shared flag. The routine will be checking to see if a Prime has been spotted by another process, even though we’ve yet to begin the parallel search we must clear the flag in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-managervalue2">Example&nbsp;9-15</a> before we start the serial check. Having completed the serial check if we haven’t found a factor then we know that the flag must still be false.</p><div class="example"><a id="code-verifying-primes-managervalue2"></a><div class="example-title">Example&nbsp;9-15.&nbsp;Clearing the flag with a Manager.Value</div><div class="example-contents"><pre class="screen">def check_prime(n, pool, nbr_processes, value):
    # cheaply check high probability set of possible factors
    from_i = 3
    to_i = SERIAL_CHECK_CUTOFF
    value.value = FLAG_CLEAR
    if not check_prime_in_range((n, (from_i, to_i), value)):
        return False

    from_i = to_i
    ...</pre></div></div><p>How frequently should we check the shared flag? Each check has a cost, both because we’re adding more instructions to our tight inner-loop and because checking requires a lock to be made on the shared variable which adds additional cost. The solution I’ve chosen is to check the flag every 1000 iterations. Every time we can check we look to see if <code class="literal">value.value</code> has been set to <code class="literal">FLAG_SET</code>, if so we exit the search. If in the search the process finds a factor then it sets <code class="literal">value.value = FLAG_SET</code> and exits.</p><p>The 1000 iteration check is performend using a <code class="literal">check_every</code> local counter. It turns out that this approach, although readable, is sub-optimal for speed. By the end of this section we’ll replace it with a less readable but significantly faster approach.</p><div class="example"><a id="code-verifying-primes-managervalue3"></a><div class="example-title">Example&nbsp;9-16.&nbsp;Passing a Manager.Value object as a flag</div><div class="example-contents"><pre class="screen">def check_prime_in_range((n, (from_i, to_i), value)):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    check_every = CHECK_EVERY
    for i in xrange(from_i, int(to_i), 2):
        check_every -= 1
        if not check_every:
            if value.value == FLAG_SET:
                return False
            check_every = CHECK_EVERY

        if n % i == 0:
            value.value = FLAG_SET
            return False
    return True</pre></div></div><p>You might be curious about the total number of times we check for the shared flag - in the case of the two large primes with four processes we check for the flag 316,405 times (we check it this many times in all of the following examples). Since each check has an overhead due to locking, this cost really adds up.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_redis_as_a_flag">Using Redis as a flag</h3></div></div></div><p><a id="multiprocessing-using-redis-as-a-flag"></a>Redis is a key-value in-memory storage engine. It provides its own locking and each operation is atomic so we don’t have to worry about using locks from inside Python (or from any other interfacing language).</p><p>By using Redis we make the data storage language-agnostic - any language or tool with an interface to Redis can share data in a compatible way. You could share data between Python, Ruby, C++ and PHP equally easily. You can share data on the local machine or over a network - you need to change the Redis default of sharing only on <code class="literal">localhost</code> if you want to share to other machines.</p><p>Redis lets you store:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
lists of strings
</li><li class="listitem">
sets of strings
</li><li class="listitem">
sorted sets of strings
</li><li class="listitem">
hashes of strings
</li></ul></div><p>Redis stores everything in RAM and snapshots to disk (optionally using journaling) and supports master-slave replication to a cluster of instances. One possibility with Redis is to use it to share a workload across a cluster, where other machines read and write state and Redis acts as a fast centralized data repository.</p><p>We can read and write a flag as a text string (all values in Redis are strings) in just the same way as we have been using Python flags previously. We create a <code class="literal">StrictRedis</code> interface as a global object, this talks to the external Redis server. We could create a new connection inside <code class="literal">check_prime_in_range</code> but this is slower and can exhaust the limited number of Redis handles that are available.</p><p>We talk to the Redis server using a dictionary-like access - we can set a value using <code class="literal">rds[SOME_KEY] = SOME_VALUE</code> and read the string back using <code class="literal">rds[SOME_KEY]</code>.</p><p>The example in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-redis1">Example&nbsp;9-17</a> is very similar to the previous <code class="literal">Manager</code> example - we’re using Redis as a substitute for the local Manager. It comes with a similar access cost.</p><p>You should note that Redis supports other (more complex) datastructures - it is a powerful storage engine that we’re using just to share a flag for this example. I encourage you to familiarize yourself with its features.</p><div class="example"><a id="code-verifying-primes-redis1"></a><div class="example-title">Example&nbsp;9-17.&nbsp;Using an external Redis server for our flag</div><div class="example-contents"><pre class="screen">FLAG_NAME = b'redis_primes_flag'
FLAG_CLEAR = b'0'
FLAG_SET = b'1'

rds = redis.StrictRedis()


def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    check_every = CHECK_EVERY
    for i in xrange(from_i, int(to_i), 2):
        check_every -= 1
        if not check_every:
            flag = rds[FLAG_NAME]
            if flag == FLAG_SET:
                return False
            check_every = CHECK_EVERY

        if n % i == 0:
            rds[FLAG_NAME] = FLAG_SET
            return False
    return True


def check_prime(n, pool, nbr_processes):
    # cheaply check high probability set of possible factors
    from_i = 3
    to_i = SERIAL_CHECK_CUTOFF
    rds[FLAG_NAME] = FLAG_CLEAR
    if not check_prime_in_range((n, (from_i, to_i))):
        return False

    ...
    if False in results:
        return False
    return True</pre></div></div><p>To confirm that the data is stored outside of these Python instances we can invoke the <code class="literal">redis-cli</code> at the command line in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-redis-cli">Redis-cli example</a> and get the value stored in the key <code class="literal">redis_primes_flag</code>. You’ll note that the returned item is a string (not an integer). All values returned from Redis are strings so if you want to manipulate them in Python, you’ll have to convert them to an appropriate data type first.</p><p><a id="code-08-redis-cli"></a><span class="title"><strong>Redis-cli example</strong></span>.&nbsp;
</p><pre class="screen">====
$ redis-cli
redis 127.0.0.1:6379&gt; GET "redis_primes_flag"
"0"
====</pre><p>
</p><p>One powerful argument in favour of the use of Redis for data sharing is that it lives outside of the Python world - non-Python developers on your team will understand it and many tools exist for it. They’ll be able to look at its state whilst reading (but not necessarily running and debugging) your code and follow what’s happening. From a team-velocity perspective this might be a big win for you, despite the communication overhead of using Redis. Whilst Redis is an additional dependency on your project you should note that it is a very commonly deployed tool, it is well debugged and well understood. Consider it a powerful tool to add to your armory.</p><p>Redis has many configuration options. By default it uses a TCP interface (that’s what I’m using), their benchmarks documentation notes that sockets might be much faster. TCP/IP lets you share data over a network between different types of OS, other configuration options are likely to be faster and also to limit your communication options.</p><div class="blockquote"><blockquote class="blockquote"><p>“When the server and client benchmark programs run on the same box, both the TCP/IP loopback and unix domain sockets can be used. It depends on the platform, but unix domain sockets can achieve around 50% more throughput than the TCP/IP loopback (on Linux for instance). The default behavior of redis-benchmark is to use the TCP/IP loopback.
The performance benefit of unix domain sockets compared to TCP/IP loopback tends to decrease when pipelining is heavily used (i.e. long pipelines).”</p><div class="attribution"><p>—<span class="attribution">
Redis documentation
<em class="citetitle">http://redis.io/topics/benchmarks</em>
</span></p></div></blockquote></div></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_rawvalue_as_a_flag">Using RawValue as a flag</h3></div></div></div><p>The <code class="literal">multiprocessing.RawValue</code> is a thin wrapper around a <code class="literal">ctypes</code> block of bytes. It lacks synchronization primitives and so there’s little to get in our way in our search for the fastest way to set a flag between processes. It will be almost as fast as the following <code class="literal">mmap</code> example (it is only slower because a few more instructions get in the way).</p><p>Again we could use any <code class="literal">ctypes</code> primitive, there’s also a <code class="literal">RawArray</code> option for sharing an array of primitive objects (which will behave similarly to <code class="literal">array.array</code>). <code class="literal">RawValue</code> avoids any locking - it is faster to use but you don’t get atomic operations.</p><p>Generally if you avoid the synchronization that Python provides during IPC then you’ll come unstuck (once again - back to that pulling-your-hair-out situation). <span class="emphasis"><em>However</em></span> in this problem it doesn’t matter if one or more processes set the flag at the same time - the flag only gets switched in one direction and every other time it is read it is just to learn if the search can be called off.</p><p>Because we never reset the state of the flag during the parallel search we don’t need synchronization. Be aware that this may not apply to your problem. If you avoid synchronization - please make sure you are doing it for the right reasons.</p><p>If you want to do things like update a shared counter then look at the documentation for the <code class="literal">Value</code> and use a context manager with <code class="literal">value.get_lock()</code> <sup>[<a id="id818012" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id818012" class="footnote">68</a>]</sup> as the implicit locking on a <code class="literal">Value</code> doesn’t allow for atomic operations.</p><p>This example looks very similar to the <code class="literal">Manager</code> example before, the only difference is that in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-value1">Example&nbsp;9-18</a> we create the <code class="literal">RawValue</code> as a 1 character (byte) flag.</p><div class="example"><a id="code-verifying-primes-value1"></a><div class="example-title">Example&nbsp;9-18.&nbsp;Creating and passing a RawValue</div><div class="example-contents"><pre class="screen">if __name__ == "__main__":
    NBR_PROCESSES = 4
    value = multiprocessing.RawValue(b'c', FLAG_CLEAR)  # 1 byte character
    pool = Pool(processes=NBR_PROCESSES)
    ...</pre></div></div><p>The flexibility to use managed and raw values is a benefit of the clean design for data sharing in <code class="literal">multiprocessing</code>.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_mmap_as_a_flag">Using mmap as a flag</h3></div></div></div><p>Finally we get to the fastest way of sharing bytes - <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-mmap1">Example&nbsp;9-19</a> shows a memory mapped (shared memory) solution using the <code class="literal">mmap</code> module. The bytes in a shared memory block are not synchronized and they come with very little overhead. They act like a file - in this case they are a block of memory with a file-like interface. We have to <code class="literal">seek</code> to a location and read or write sequentially.</p><p>Typically this is used to give a short (memory-mapped) view into a larger file, in our case rather than specifying a file number as the first argument we instead pass <code class="literal">-1</code> to indicate that we want an anonymous block of memory. We could also specify whether we want read-only or write-only access (we want both which is the default).</p><div class="example"><a id="code-verifying-primes-mmap1"></a><div class="example-title">Example&nbsp;9-19.&nbsp;Using a shared memory flag via mmap</div><div class="example-contents"><pre class="screen">sh_mem = mmap.mmap(-1, 1)  # memory map 1 byte as a flag


def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    check_every = CHECK_EVERY
    for i in xrange(from_i, int(to_i), 2):
        check_every -= 1
        if not check_every:
            sh_mem.seek(0)
            flag = sh_mem.read_byte()
            if flag == FLAG_SET:
                return False
            check_every = CHECK_EVERY

        if n % i == 0:
            sh_mem.seek(0)
            sh_mem.write_byte(FLAG_SET)
            return False
    return True


def check_prime(n, pool, nbr_processes):
    # cheaply check high probability set of possible factors
    from_i = 3
    to_i = SERIAL_CHECK_CUTOFF
    sh_mem.seek(0)
    sh_mem.write_byte(FLAG_CLEAR)
    if not check_prime_in_range((n, (from_i, to_i))):
        return False

    ...
    if False in results:
        return False
    return True</pre></div></div><p>A <code class="literal">mmap</code> has a number of methods to move around the file that it would normally represent (including <code class="literal">find</code>, <code class="literal">readline</code>, <code class="literal">write</code>). We are using it in the most basic way - we <code class="literal">seek</code> to the start of the memory block before each read or write and since we’re sharing just 1 byte, we use <code class="literal">read_byte</code> and <code class="literal">write_byte</code> to be explict.</p><p>This is no Python overhead for locking and no interpretation of the data, we’re dealing with bytes directly with the Operating System so this is our fastest communication method.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_using_mmap_as_a_flag_redux">Using mmap as a flag redux</h3></div></div></div><p>Whilst the previous <code class="literal">mmap</code> result was the best overall, I couldn’t help but think that we should be able to get back to the naive <code class="literal">Pool</code> result for the most expensive case of having Primes. The goal is to accept that there is no early exit from the inner loop and to minimize the cost of anything extraneous.</p><p>I present a slightly more complex solution below, the same changes can be made to the other flag-based approaches above although this <code class="literal">mmap</code> result will still be fastest.</p><p>In our previous example we’ve used <code class="literal">CHECK_EVERY</code>, this means we have the <code class="literal">check_next</code> local variable to track, decrement and use in Boolean tests - each operations adds a bit of extra time to every iteration. In the case of validating a large Prime this extra management occurs over 300,000 times.</p><p>The first optimization, shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-mmap2-1">Example&nbsp;9-20</a>, is to realize that we can replace the decremented counter with a look-ahead value and then we only have to do a Boolean comparison on the inner loop.</p><p>This removes a decrement which, due to Python’s interpreted style, is quite slow. This optimization works in this test in CPython 2.7, it is unlikely to offer any benefit in a smarter compiler (e.g. PyPy or Cython).</p><p>This step saves 0.7 seconds when checking one of our large Primes.</p><div class="example"><a id="code-verifying-primes-mmap2-1"></a><div class="example-title">Example&nbsp;9-20.&nbsp;Starting to optimize away our expensive logic</div><div class="example-contents"><pre class="screen">def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    check_next = from_i + CHECK_EVERY
    for i in xrange(from_i, int(to_i), 2):
        if check_next == i:
            sh_mem.seek(0)
            flag = sh_mem.read_byte()
            if flag == FLAG_SET:
                return False
            check_next += CHECK_EVERY

        if n % i == 0:
            sh_mem.seek(0)
            sh_mem.write_byte(FLAG_SET)
            return False
    return True</pre></div></div><p>We can also entirely replace the logic that the counter represents in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-verifying-primes-mmap3-1">Example&nbsp;9-21</a> by unrolling our loop into a two stage process. First - the outer loop covers the expected range but in steps on <code class="literal">CHECK_EVERY</code>.</p><p>Second - a new inner loop replaces the <code class="literal">check_every</code> logic - it checks the local range of factors and then finishes, this is equivalent to the <code class="literal">if not check_every:</code> test. We follow this with the previous <code class="literal">sh_mem</code> logic to check the early-exit flag.</p><p>The speed impact is dramatic. Our non-Prime case improves even further but more importantly our Prime-checking case is very nearly as fast as the Naive Pool version (it is now just 0.05 seconds slower). Given that we’re doing a lot of extra work with inter-process communication this is a very interesting result. Do note that it is specific to CPython and unlikely to offer any gains when run through a compiler.</p><div class="example"><a id="code-verifying-primes-mmap3-1"></a><div class="example-title">Example&nbsp;9-21.&nbsp;Optimizing away our expensive logic</div><div class="example-contents"><pre class="screen">def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    for outer_counter in xrange(from_i, int(to_i), CHECK_EVERY):
        upper_bound = min(int(to_i), outer_counter + CHECK_EVERY)
        for i in xrange(outer_counter, upper_bound, 2):
            if n % i == 0:
                sh_mem.seek(0)
                sh_mem.write_byte(FLAG_SET)
                return False
        sh_mem.seek(0)
        flag = sh_mem.read_byte()
        if flag == FLAG_SET:
            return False
    return True</pre></div></div><p>We can go even further (but frankly - this is a bit foolish). Look-ups for variables that aren’t declared in the local scope are a little expensive. We can create local references to the global <code class="literal">FLAG_SET</code> and the frequencly used <code class="literal">.seek()</code> and <code class="literal">.read_byte()</code> methods to avoid their more expensive look-ups. The resulting code is even less readable than before and I really recommend that you <span class="emphasis"><em>do not do this</em></span>.</p><p>This final result is 1.5% slower than the Naive Pool version when checking the larger Primes. Given that we’re 4.8* faster for the non-Prime cases I think we’ve taken this example about as far as it could (and should!) go.</p><div class="example"><a id="code-verifying-primes-mmap4-1"></a><div class="example-title">Example&nbsp;9-22.&nbsp;Breaking the “don’t hurt team velocity” rule to eek out an extra speed-up</div><div class="example-contents"><pre class="screen">def check_prime_in_range((n, (from_i, to_i))):
    if n % 2 == 0:
        return False
    assert from_i % 2 != 0
    FLAG_SET_LOCAL = FLAG_SET
    sh_seek = sh_mem.seek
    sh_read_byte = sh_mem.read_byte
    for outer_counter in xrange(from_i, int(to_i), CHECK_EVERY):
        upper_bound = min(int(to_i), outer_counter + CHECK_EVERY)
        for i in xrange(outer_counter, upper_bound, 2):
            if n % i == 0:
                sh_seek(0)
                sh_mem.write_byte(FLAG_SET)
                return False
        sh_seek(0)
        if sh_read_byte() == FLAG_SET_LOCAL:
            return False
    return True</pre></div></div><p>The behavior above with manual loop unrolling and creating local references to global objects is foolish. Overall it is bound to lower team velocity by making the code harder to understand and really this is the job of a compiler (e.g. a JIT like PyPy or a static compiler like Cython).</p><p>Humans shouldn’t be doing this sort of manipulation because it’ll be very brittle. I haven’t tested this optimization approach in Python 3+ and I don’t want to - I don’t really expect that these incremental improvements will work in another version of Python (and certainly not in a different implementation like PyPy or IronPython).</p><p>I’m showing you so you know that it <span class="emphasis"><em>might</em></span> be possible and I’m warning you that to keep your sanity you really should let compilers take care of this sort of work for you.</p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_sharing_numpy_data_with_multiprocessing">Sharing numpy data with multiprocessing</h2></div></div></div><p><a id="multiprocessing-sharing-numpy-data-with-multiprocessing"></a>When working with large <code class="literal">numpy</code> arrays you’re bound to wonder if you can share the data for read and write access, without a copy, between processes. It is possible though a little fiddly. I’d like to acknowledge StackOverlfow user <code class="literal">pv</code> for the inspiration for this demo <sup>[<a id="id818496" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id818496" class="footnote">69</a>]</sup>.</p><p>Do not use this method to recreate the behaviors of BLAS, MKL, Accelerate and ATLAS. These libraries all have multithreading support in their primitives and it is likely that they are better debugged than a new routine that you create. These libraries can require some configuration to enable multithreading support, it would be wise to see if these libraries give you free speed-ups before investing time (and losing time to debugging!) as you write your own.</p><p>When you have a large matrix which could be shared between processes you’ll have several benefits:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
only one copy means no wasted RAM
</li><li class="listitem">
no time is wasted copying large blocks of RAM
</li><li class="listitem">
you gain the possibility of sharing partial results between the processes
</li></ul></div><p>Thinking back to the Pi estimation demo using <code class="literal">numpy</code> in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-estimating-pi-using-numpy">Using numpy</a> we had the problem that the random number generation was a serial process. Here we could imagine forking processes that shared one large array, each one using a differently seeded random number generator to fill in a section of the array with random numbers, therefore completing the generation of a large random block faster than possible with a single process.</p><p>To verify this I modified the forthcoming demo to create a random large matrix (10,000 by 80,000 elements) as a serial process and by splitting the matrix into four segments where <code class="literal">random</code> is called in parallel (in both cases one row at a time). The serial process took 15 seconds, the parallel version took 4 seconds. Refer back to <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#multiprocessing-random-numbers">Random Numbers in Parallel Systems</a> to understand some of the dangers of parallelized random number generation.</p><p>For the rest of this section we’ll use a simplified demo that illustrates the point whilst remaining easy to verify.</p><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#FIG-numpy-shared-multiprocessing-htop">Figure&nbsp;9-19</a> you can see <code class="literal">htop</code> on my laptop, it shows four child processes of the parent (with PID 11268) where all five processes are sharing a single 10,000 by 80,000 element <code class="literal">numpy</code> array of doubles. One copy of this array costs 6.4GB and my laptop has 8GB, you can see in <code class="literal">htop</code> by the process meters that the <code class="literal">Mem</code> reading shows a maximum of 7,941MB RAM.</p><div class="figure"><a id="FIG-numpy-shared-multiprocessing-htop"></a><div class="figure-contents"><div class="mediaobject"><img style="width: 378; " src="https://www.safaribooksonline.com/library/view/high-performance-python/9781449361747/figures/08_80000rows.png" alt="notset" width="1000" height="514" data-mfp-src="/library/view/high-performance-python/9781449361747/figures/08_80000rows.png"></div></div><div class="figure-title">Figure&nbsp;9-19.&nbsp;htop showing RAM and swap usage</div></div><p>To understand this demo we’ll first walk through the console output and then we’ll look at the code. In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-run-setup">Example&nbsp;9-23</a> we start the parent process, it allocates a 6.4GB double array of dimension 10,000 by 80,000 filled with the value zero. The 10,000 rows will be passed out as indices to the worker function and the worker will operate on each column of 80,000 items in turn.</p><p>Having allocated the array we fill it with the answer to life, the universe and everything (<code class="literal">42</code>!). We can test in the worker function that we’re receiving this modified array and not a filled-with-0s version to confirm that this code is behaving as expected.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-run-setup"></a><div class="example-title">Example&nbsp;9-23.&nbsp;Setting up the shared array</div><div class="example-contents"><pre class="screen">$ python np_shared.py
Created shared array with 6,400,000,000 nbytes
Shared array id is 20255664 in PID 11268
Starting with an array of 0 values:
[[ 0.  0.  0. ...,  0.  0.  0.]
 ...,
 [ 0.  0.  0. ...,  0.  0.  0.]]

Original array filled with value 42:
[[ 42.  42.  42. ...,  42.  42.  42.]
 ...,
 [ 42.  42.  42. ...,  42.  42.  42.]]
Press a key to start workers using multiprocessing...</pre></div></div><p>Now in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-run-worker-fn">Example&nbsp;9-24</a> we’ve started four processes working on this shared array. No copy of the array was made, each process is looking at the same large block of memory and each process has a different set of indices to work from. Every few thousand lines the worker outputs the current index and its PID so we can observe its behavior.</p><p>The worker’s job is trivial - it will check that the current element is still set to the default (so we know that no other process has modified it already) and then it will overwrite this value with the current PID.</p><p>Once the workers have completed we return to the parent process and print the array again - this time we see that it is filled with PIDs rather than <code class="literal">42</code>.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-run-worker-fn"></a><div class="example-title">Example&nbsp;9-24.&nbsp;Running worker_fn on the shared array</div><div class="example-contents"><pre class="screen"> worker_fn: with idx 0
  id of shared_array is 20255664 in PID 11288
 worker_fn: with idx 2000
  id of shared_array is 20255664 in PID 11291
 worker_fn: with idx 1000
  id of shared_array is 20255664 in PID 11289
 ...
 worker_fn: with idx 8000
  id of shared_array is 20255664 in PID 11290

The default value has been over-written with worker_fn's result:
[[ 11288.  11288.  11288. ...,  11288.  11288.  11288.]
 ...,
 [ 11291.  11291.  11291. ...,  11291.  11291.  11291.]]</pre></div></div><p>Finally in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-run-verify">Example&nbsp;9-25</a> we use a <code class="literal">Counter</code> to confirm the frequency of each PID in the array. As the work was evenly divided we expect to see each of the four PIDs represented an equal number of times. In our 800,000,000 element array we see four sets of 200,000,000 PIDs. The table output is presented using PrettyTable <sup>[<a id="id818716" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id818716" class="footnote">70</a>]</sup>.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-run-verify"></a><div class="example-title">Example&nbsp;9-25.&nbsp;Verifying the result on the shared array</div><div class="example-contents"><pre class="screen">Verification - extracting unique values from 800,000,000 items
in the numpy array (this might be slow)...
Unique values in shared_array:
+---------+-----------+
|   PID   |   Count   |
+---------+-----------+
| 11288.0 | 200000000 |
| 11289.0 | 200000000 |
| 11290.0 | 200000000 |
| 11291.0 | 200000000 |
+---------+-----------+
Press a key to exit...</pre></div></div><p>Having completed the program now exits and the array is deleted.</p><p>We can take a peak inside each process under Linux using <code class="literal">ps</code> and <code class="literal">pmap</code>. <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-run-pmap">Example&nbsp;9-26</a> shows the result of calling <code class="literal">ps</code>:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem">
<code class="literal">ps</code> tells us about the process
</li><li class="listitem">
<code class="literal">-A</code> lists all processes
</li><li class="listitem">
<code class="literal">-o pid,size,vsize,cmd</code> outputs the PID, size information and the command name
</li><li class="listitem">
<code class="literal">grep</code> is used to filter all other results and leave only the lines for our demo
</li></ul></div><p>The parent process (PID 11268) and its four forked children are shown, the result in similar to what we saw in <code class="literal">htop</code>.</p><p>We can use <code class="literal">pmap</code> to look at the memory map of each process, requesting extended output with <code class="literal">-x</code>. We grep for the pattern <code class="literal">s-</code> to list blocks of memory that are marked as being shared. In the parent process and the child processes we see a 6250000 kB block (6.2GB) that is shared between them.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-run-pmap"></a><div class="example-title">Example&nbsp;9-26.&nbsp;pmap and ps to investigate the Operating System’s view of the processes</div><div class="example-contents"><pre class="screen">$ ps -A -o pid,size,vsize,cmd | grep np_shared
11268 232464 6564988 python np_shared.py
11288 11232 6343756 python np_shared.py
11289 11228 6343752 python np_shared.py
11290 11228 6343752 python np_shared.py
11291 11228 6343752 python np_shared.py

ian@ian-Latitude-E6420 $ pmap -x 11268 | grep s-
Address           Kbytes     RSS   Dirty Mode   Mapping
00007f1953663000 6250000 6250000 6250000 rw-s-  zero (deleted)
...
ian@ian-Latitude-E6420 $ pmap -x 11288 | grep s-
Address           Kbytes     RSS   Dirty Mode   Mapping
00007f1953663000 6250000 1562512 1562512 rw-s-  zero (deleted)
...</pre></div></div><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-preamble1">Example&nbsp;9-27</a> shows the important steps to share this array. We use a <code class="literal">multprocessing.Array</code> to allocate a shared block of memory as a 1D array. We then instantiate a <code class="literal">numpy</code> array from this object and reshape it back to a 2D array. Now we have a <code class="literal">numpy</code> wrapped block of memory that can be shared between processes and addressed as though it were a normal <code class="literal">numpy</code> array. <code class="literal">numpy</code> is not managing the RAM, <code class="literal">multiprocessing.Array</code> is managing it.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-preamble1"></a><div class="example-title">Example&nbsp;9-27.&nbsp;Sharing the numpy array using multiprocessing</div><div class="example-contents"><pre class="screen">import os
import multiprocessing
from collections import Counter
import ctypes
import numpy as np
from prettytable import PrettyTable

SIZE_A, SIZE_B = 10000, 80000  # 6.2GB - starts to use swap (maximal RAM usage)</pre></div></div><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-worker-fn">Example&nbsp;9-28</a> you’ll see that each forked process has access to a global <code class="literal">main_nparray</code>. Whilst the forked process has a copy of the <code class="literal">numpy</code> object, the underlying bytes that the object accesses are stored as shared memory. Our <code class="literal">worker_fn</code> will overwrite a chosen row (via <code class="literal">idx</code>) with the current process identifier.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-worker-fn"></a><div class="example-title">Example&nbsp;9-28.&nbsp;worker_fn for sharing numpy arrays using multiprocessing</div><div class="example-contents"><pre class="screen">def worker_fn(idx):
    """Do some work on the shared np array on row idx"""
    # confirm that no other process has modified this value already
    assert main_nparray[idx, 0] == DEFAULT_VALUE
    # inside the subprocess print the PID and id of the array
    # to check we don't have a copy
    if idx % 1000 == 0:
        print " {}: with idx {}\n  id of local_nparray_in_process is {} in PID {}"\
            .format(worker_fn.__name__, idx, id(main_nparray), os.getpid())
    # we can do any work on the array, here we set every item in this row to
    # have the value of the process id for this process
    main_nparray[idx, :] = os.getpid()</pre></div></div><p>In our <code class="literal">__main__</code> in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-main1">Example&nbsp;9-29</a> we’ll work through three major stages:</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
Build a shared <code class="literal">multiprocessing.Array</code> and convert it into a <code class="literal">numpy</code> array
</li><li class="listitem">
Set a default value into the array, spawn four processes to work on the array in parallel
</li><li class="listitem">
Verify the array’s contents after the processes return
</li></ol></div><p>Typically you’d setup a <code class="literal">numpy</code> array and work on it in a single process, you’d probably do something like <code class="literal">arr = np.array((100, 5), dtype=np.float_)</code>. This is fine in a single process but you can’t share this data across processes for both reading and writing.</p><p>The trick is to make a shared block of bytes, one way is to create a <code class="literal">multiprocessing.Array</code>. By default the <code class="literal">Array</code> is wrapped in a lock to prevent concurrent edits, we don’t need this lock as we’ll be careful about our access patterns. To communicate this clearly to other team members it is worth being explicit and setting <code class="literal">lock=False</code>.</p><p>If you don’t set <code class="literal">lock=False</code> then you’ll have an object rather than a reference to the bytes, you’ll need to call <code class="literal">.get_obj()</code> to get to the bytes. By calling <code class="literal">.get_obj()</code> you bypass the lock, so there’s no value in not being explicit about this in the first place.</p><p>Next we take this block of shareable bytes and wrap a <code class="literal">numpy</code> array around them using <code class="literal">frombuffer</code>. The <code class="literal">dtype</code> is optional but since we’re passing bytes around it is always sensible to be explicit. We <code class="literal">reshape</code> so we can address the bytes as a 2D array. By default the array values are set to <code class="literal">0</code>.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-main1"></a><div class="example-title">Example&nbsp;9-29.&nbsp;main to setup numpy arrays for sharing</div><div class="example-contents"><pre class="screen">if __name__ == '__main__':
    DEFAULT_VALUE = 42
    NBR_OF_PROCESSES = 4

    # create a block of bytes, reshape into a local numpy array
    NBR_ITEMS_IN_ARRAY = SIZE_A * SIZE_B
    shared_array_base = multiprocessing.Array(ctypes.c_double,
                                              NBR_ITEMS_IN_ARRAY, lock=False)
    main_nparray = np.frombuffer(shared_array_base, dtype=ctypes.c_double)
    main_nparray = main_nparray.reshape(SIZE_A, SIZE_B)
    # Assert no copy was made
    assert main_nparray.base.base is shared_array_base
    print "Created shared array with {:,} nbytes".format(main_nparray.nbytes)
    print "Shared array id is {} in PID {}".format(id(main_nparray), os.getpid())
    print "Starting with an array of 0 values:"
    print main_nparray
    print</pre></div></div><p>To confirm that our processes are operating on the same block of data that we started with we’ll set each item to a new <code class="literal">DEFAULT_VALUE</code>, you’ll see that at the top of <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-sharing-numpy-array-using-multiprocessing-main2">Example&nbsp;9-30</a> (we use the answer to life, the universe and everything). Next we build a <code class="literal">Pool</code> of processes (four in this case) and then send batches of row indices via the call to <code class="literal">map</code>.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-main2"></a><div class="example-title">Example&nbsp;9-30.&nbsp;main for sharing numpy arrays using multiprocessing</div><div class="example-contents"><pre class="screen">    # Modify the data via our local numpy array
    main_nparray.fill(DEFAULT_VALUE)
    print "Original array filled with value {}:".format(DEFAULT_VALUE)
    print main_nparray

    raw_input("Press a key to start workers using multiprocessing...")
    print

    # create a pool of processes that will share the memory block
    # of the global numpy array, share the reference to the underlying
    # block of data so we can build a numpy array wrapper in the new processes
    pool = multiprocessing.Pool(processes=NBR_OF_PROCESSES)
    # perform a map where each row index is passed as a parameter to the
    # worker_fn
    pool.map(worker_fn, xrange(SIZE_A))</pre></div></div><p>Once we’ve completed the parallel processing we return to the parent process to verify the result. The verification step runs through a flattened view on the array (note that the view does <span class="emphasis"><em>not</em></span> make a copy, it just creates a 1D iterable view on the 2D array) counting the frequency of each PID. Finally we perform some <code class="literal">assert</code> checks to make sure we have the expected counts.</p><div class="example"><a id="code-sharing-numpy-array-using-multiprocessing-main3"></a><div class="example-title">Example&nbsp;9-31.&nbsp;main to verify the shared result</div><div class="example-contents"><pre class="screen">    print "Verification - extracting unique values from {:,} items\nin the numpy array (this might be slow)...".format(NBR_ITEMS_IN_ARRAY)
    # main_nparray.flat iterates over the contents of the array, it doesn't
    # make a copy
    counter = Counter(main_nparray.flat)
    print "Unique values in main_nparray:"
    tbl = PrettyTable(["PID", "Count"])
    for pid, count in counter.items():
        tbl.add_row([pid, count])
    print tbl

    total_items_set_in_array = sum(counter.values())

    # check that we have set every item in the array away from DEFAULT_VALUE
    assert DEFAULT_VALUE not in counter.keys()
    # check that we have accounted for every item in the array
    assert total_items_set_in_array == NBR_ITEMS_IN_ARRAY
    # check that we have NBR_OF_PROCESSES of unique keys to confirm that every
    # process did some of the work
    assert len(counter) == NBR_OF_PROCESSES

    raw_input("Press a key to exit...")</pre></div></div><p>We’ve just created a 1D array of bytes, converted it into a 2D array, shared the array amongst four processes and allowed them to process concurrently on the same block of memory. This recipe will help you parallize over many cores. Be careful with concurrent access to the <span class="emphasis"><em>same</em></span> data points - you’ll have to use the locks in <code class="literal">multiprocessing</code> if you want to avoid synchronizatoin problems and this will slow down your code.</p></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_synchronizing_file_and_variable_access">Synchronizing File and Variable Access</h2></div></div></div><p>In the following examples we’ll look at multiple processes sharing and manipulating a state - in this case four processes incrementing a shared counter a set number of times. Without a synchronization process the counting is incorrect. If you’re sharing data in a coherent way you’ll always need a method to synchronize the reading and writing of data otherwise you’ll end up with errors.</p><p>Typically the synchronization methods are specific to the OS you’re using and often specific to the language you use. Here we look at file based synchronization using a Python library and sharing an integer object between Python processes.</p><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_file_locking">File locking</h3></div></div></div><p>Reading and writing to a file will be the slowest example of data sharing in this section.</p><p>You can see our first <code class="literal">work</code> function in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-filelocking-nolock-1-work">Example&nbsp;9-32</a>. It will iterate over a local counter, in each iteration it opens a file and reads the existing value, increments it by one and then writes the new value over the old one. On the first iteration the file will be empty or won’t exist so it will catch an exception and assume the value should be zero.</p><div class="example"><a id="code-08-syncronization-filelocking-nolock-1-work"></a><div class="example-title">Example&nbsp;9-32.&nbsp;work function without a lock</div><div class="example-contents"><pre class="screen">def work(filename, max_count):
    for n in range(max_count):
        f = open(filename, "r")
        try:
            nbr = int(f.read())
        except ValueError as err:
            print "File is empty, starting to count from 0, error: " + str(err)
            nbr = 0
        f = open(filename, "w")
        f.write(str(nbr + 1) + '\n')
        f.close()</pre></div></div><p>Let’s run this example with one process, you can see the output in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-filelocking-nolock-1-console1">Example&nbsp;9-33</a>. <code class="literal">work</code> is called 1000 times, as expected it correctly counts without losing any data.</p><p>On the first read it sees and empty file, this raises the <code class="literal">invalid literal for int()</code> error for <code class="literal">int()</code> (as <code class="literal">int()</code> is called on an empty string). This error only occurs once, afterwards we always have a valid value to read and convert into an integer.</p><div class="example"><a id="code-08-syncronization-filelocking-nolock-1-console1"></a><div class="example-title">Example&nbsp;9-33.&nbsp;Timing of file based counting without a lock and 1 process</div><div class="example-contents"><pre class="screen">$ python ex1_nolock.py
Starting 1 process(es) to count to 1000
File is empty, starting to count from 0,
error: invalid literal for int() with base 10: ''
Expecting to see a count of 1000
count.txt contains:
1000</pre></div></div><p>Now we’ll run the same <code class="literal">work</code> function with four concurrent processes.  We don’t have any locking code so we’ll expect some odd results.</p><div class="tip"><h3 class="title">Tip</h3><p>BEFORE you look at the following code - what <span class="emphasis"><em>two</em></span> types of error can you expect to see when two processes read or write simultaneously to the same file? Think about the two main states of the code - the start of execution for each process and the normal running state of each process.</p></div><p>Take a look at <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-filelocking-nolock-1-console2">Example&nbsp;9-34</a> to see the problems. First - when each process starts the file is empty, so they each try to start counting from zero. Second - as one process writes the other can read a partially written result which can’t be parsed, this causes an exception and a zero will be written back. This causes our counter to keep getting reset!</p><p>Can you see how <code class="literal">\n</code> and two values have been written by two concurrent processes to the same open file, causing an invalid entry to be read by a third process?</p><div class="example"><a id="code-08-syncronization-filelocking-nolock-1-console2"></a><div class="example-title">Example&nbsp;9-34.&nbsp;Timing of file based counting without a lock and 4 processes</div><div class="example-contents"><pre class="screen">Starting 4 process(es) to count to 4000
File is empty, starting to count from 0,
error: invalid literal for int() with base 10: ''
File is empty, starting to count from 0,
error: invalid literal for int() with base 10: '1\n7\n'
# many errors like these
Expecting to see a count of 4000
count.txt contains:
629
$ python -m timeit -s "import ex1_nolock" "ex1_nolock.run_workers()"
10 loops, best of 3: 125 msec per loop</pre></div></div><p><a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-filelocking-nolock-1-main">Example&nbsp;9-35</a> shows the <code class="literal">multiprocessing</code> code that calls <code class="literal">work</code> with four processes. Note that rather than using a <code class="literal">map</code> instead we’re building a list of <code class="literal">Process</code> objects. Although we don’t use the functionality here the <code class="literal">Process</code> object gives us the power to introspect the state of each <code class="literal">Process</code>. I encourage you to read the documentation to learn about why you might want to use a <code class="literal">Process</code> <sup>[<a id="id819369" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819369" class="footnote">71</a>]</sup>.</p><div class="example"><a id="code-08-syncronization-filelocking-nolock-1-main"></a><div class="example-title">Example&nbsp;9-35.&nbsp;run_workers setting up 4 processes</div><div class="example-contents"><pre class="screen">import multiprocessing
import os

...
MAX_COUNT_PER_PROCESS = 1000
FILENAME = "count.txt"
...

def run_workers():
    NBR_PROCESSES = 4
    total_expected_count = NBR_PROCESSES * MAX_COUNT_PER_PROCESS
    print "Starting {} process(es) to count to {}".format(NBR_PROCESSES, total_expected_count)
    # reset counter
    f = open(FILENAME, "w")
    f.close()

    processes = []
    for process_nbr in range(NBR_PROCESSES):
        p = multiprocessing.Process(target=work, args=(FILENAME, MAX_COUNT_PER_PROCESS))
        p.start()
        processes.append(p)

    for p in processes:
        p.join()

    print "Expecting to see a count of {}".format(total_expected_count)
    print "{} contains:".format(FILENAME)
    os.system('more ' + FILENAME)


if __name__ == "__main__":
    run_workers()</pre></div></div><p>Using the <code class="literal">lockfile</code> <sup>[<a id="id819448" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819448" class="footnote">72</a>]</sup> module we can introduce a syncoronization method so only 1 process get to write at a time and the others each await their turn. The overal process therefore runs more slowly but it doesn’t make mistakes. You can see the correct output in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-filelocking-lock-1-console">Example&nbsp;9-36</a>. You’ll find full documentation online <sup>[<a id="id819468" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819468" class="footnote">73</a>]</sup>. Be aware that the locking mechanism is specific to Python so other processes that are looking at this file will <span class="emphasis"><em>not</em></span> care about the “locked” nature of this file.</p><div class="example"><a id="code-08-syncronization-filelocking-lock-1-console"></a><div class="example-title">Example&nbsp;9-36.&nbsp;Timing of file based counting with a lock and 4 processes</div><div class="example-contents"><pre class="screen">$ python ex1_lock.py
Starting 4 process(es) to count to 4000
File is empty, starting to count from 0,
error: invalid literal for int() with base 10: ''
Expecting to see a count of 4000
count.txt contains:
4000

$ python -m timeit -s "import ex1_lock" "ex1_lock.run_workers()"
10 loops, best of 3: 401 msec per loop</pre></div></div><p>Using <code class="literal">lockfile</code> adds just a couple of lines of code. First we create a <code class="literal">FileLock</code> object, the filename can be anything but using the same name as the file you want to lock probably makes debugging from the command line easier. When you ask to <code class="literal">acquire</code> the lock the <code class="literal">FileLock</code> opens a new file with named as filename with <code class="literal">.lock</code> appended.</p><p><code class="literal">acquire</code> without any arguments will block forever, until the lock is available. Once you have the lock you can do your processing without any danger of a conflict and then <code class="literal">release</code> the lock once you’ve finished writing.</p><div class="example"><a id="code-08-syncronization-filelocking-lock-1"></a><div class="example-title">Example&nbsp;9-37.&nbsp;work function with a lock</div><div class="example-contents"><pre class="screen">def work(filename, max_count):
    lock = lockfile.FileLock(filename)
    for n in range(max_count):
        lock.acquire()
        f = open(filename, "r")
        try:
            nbr = int(f.read())
        except ValueError as err:
            print "File is empty, starting to count from 0, error: " + str(err)
            nbr = 0
        f = open(filename, "w")
        f.write(str(nbr + 1) + '\n')
        f.close()
        lock.release()</pre></div></div><p>You could use a context manager, you replace <code class="literal">acquire</code> and <code class="literal">release</code> with <code class="literal">with lock:</code>. This adds a small overhead to the runtime but it also makes the code a little easier to read. Clarity usually beats execution speed.</p><p>You can also ask to <code class="literal">acquire</code> the lock with a timeout, check for an existing lock and break an existing lock. Several locking mechanisms are provided, sensible default choices for each platform are hidden behind the <code class="literal">FileLock</code> interface.</p></div><div class="sect2"><div class="titlepage"><div><div><h3 class="title" id="_locking_a_value">Locking a Value</h3></div></div></div><p>The <code class="literal">multiprocessing</code> module offers several options to share Python objects between processes. We can share primitive objects with a low communication overhead, we can also share higher level Python objects (e.g. dictionaries and lists) using a <code class="literal">Manager</code> (but note that the syncronization cost will significantly slow down the data sharing).</p><p>Here we’ll use a <code class="literal">multiprocessing.Value</code> <sup>[<a id="id819608" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819608" class="footnote">74</a>]</sup> object to share an integer between processes. Whilst a <code class="literal">Value</code> has a lock, the lock doesn’t do quite what you might expect - it prevent simultaneous reads or writes but does <span class="emphasis"><em>not</em></span> provide an atomic increment. Let’s illustrate this in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-nolock-1-console">Example&nbsp;9-38</a>, you can see that we end up with an incorrect count, this is similar to the file-based unsynchronized example before.</p><div class="example"><a id="code-08-syncronization-valuelocking-nolock-1-console"></a><div class="example-title">Example&nbsp;9-38.&nbsp;No locking leads to an incorrect count</div><div class="example-contents"><pre class="screen">$ python ex2_nolock.py
Expecting to see a count of 4000
We have counted to 2340
$ python -m timeit -s "import ex2_nolock" "ex2_nolock.run_workers()"
100 loops, best of 3: 12.6 msec per loop</pre></div></div><p>No corruption occurs to the data but we do miss some of the updates. This approach might be suitable if you’re writing to a <code class="literal">Value</code> from one process and consuming (but not modifying) that <code class="literal">Value</code> in other processes. The code to share the <code class="literal">Value</code> is shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-nolock-1">Example&nbsp;9-39</a>.</p><p>We have to specify a datatype and an initialization value, using <code class="literal">Value("i", 0)</code> we request a signed-integer with a default value of 0. This is passed as a regular argument to our <code class="literal">Process</code> object and behind the scenes it takes care of sharing the same block of bytes between processes.</p><p>To access the primitive object held by our <code class="literal">Value</code> we use <code class="literal">.value</code>. Note that I’m asking for an in-place addition, we’d expect this to be an atomic operation but that’s not supported by <code class="literal">Value</code> so our final count is lower than expected.</p><div class="example"><a id="code-08-syncronization-valuelocking-nolock-1"></a><div class="example-title">Example&nbsp;9-39.&nbsp;The counting code without a Lock</div><div class="example-contents"><pre class="screen">import multiprocessing

def work(value, max_count):
    for n in range(max_count):
        value.value += 1

def run_workers():
...
    value = multiprocessing.Value('i', 0)
    for process_nbr in range(NBR_PROCESSES):
        p = multiprocessing.Process(target=work, args=(value, MAX_COUNT_PER_PROCESS))
        p.start()
        processes.append(p)
...</pre></div></div><p>We can add a <code class="literal">Lock</code> <sup>[<a id="id819722" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819722" class="footnote">75</a>]</sup>, it works very similarly to the <code class="literal">FileLock</code> example before. You can see the correctly synchronized count in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-lock-1-console">Example&nbsp;9-40</a>.</p><div class="example"><a id="code-08-syncronization-valuelocking-lock-1-console"></a><div class="example-title">Example&nbsp;9-40.&nbsp;Using a Lock to synchronize writes to a Value</div><div class="example-contents"><pre class="screen"># lock on the update but this isn't atomic
$ python ex2_lock.py
Expecting to see a count of 4000
We have counted to 4000
python -m timeit -s "import ex2_lock" "ex2_lock.run_workers()"
10 loops, best of 3: 22.2 msec per loop</pre></div></div><p>In <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-lock-1">Example&nbsp;9-41</a> I’ve used a context manager (<code class="literal">with Lock</code>) to acquire the lock, as for the <code class="literal">FileLock</code> example before it waits indefinitely to acquire the lock.</p><div class="example"><a id="code-08-syncronization-valuelocking-lock-1"></a><div class="example-title">Example&nbsp;9-41.&nbsp;Acquiring a Lock using a context manager</div><div class="example-contents"><pre class="screen">import multiprocessing


def work(value, max_count, lock):
    for n in range(max_count):
        with lock:
            value.value += 1


def run_workers():
...
    processes = []
    lock = multiprocessing.Lock()
    value = multiprocessing.Value('i', 0)
    for process_nbr in range(NBR_PROCESSES):
        p = multiprocessing.Process(target=work, args=(value, MAX_COUNT_PER_PROCESS, lock))
        p.start()
        processes.append(p)
...</pre></div></div><p>As noted in the <code class="literal">FileLock</code> example it is a little quicker to avoid using the context manager, the snippet in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-lock-1-inline-lock">Example&nbsp;9-42</a> shows how to <code class="literal">acquire</code> and <code class="literal">release</code> the <code class="literal">Lock</code> object.</p><div class="example"><a id="code-08-syncronization-valuelocking-lock-1-inline-lock"></a><div class="example-title">Example&nbsp;9-42.&nbsp;In-line Locking rather than using a Conext Manager</div><div class="example-contents"><pre class="screen">lock.acquire()
value.value += 1
lock.release()</pre></div></div><p>Since a <code class="literal">Lock</code> doesn’t give us the level of granularity that we’re after, the basic locking that it provides waste a bit of time unnecessarily. We can replace the <code class="literal">Value</code> with a <code class="literal">RawValue</code> <sup>[<a id="id819844" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819844" class="footnote">76</a>]</sup> in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-lock-rawvalue-1-console">Example&nbsp;9-43</a> and we achieve an incremental speed-up. If you’re interested in seeing the bytecode behind this change then read Eli Bendersky’s blogpost <sup>[<a id="id819850" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#ftn.id819850" class="footnote">77</a>]</sup> on the subject.</p><div class="example"><a id="code-08-syncronization-valuelocking-lock-rawvalue-1-console"></a><div class="example-title">Example&nbsp;9-43.&nbsp;Console output showing the faster RawValue and Lock approach</div><div class="example-contents"><pre class="screen"># RawValue hsa no lock on it
$ python ex2_lock_rawvalue.py
Expecting to see a count of 4000
We have counted to 4000
$ python -m timeit -s "import ex2_lock_rawvalue" "ex2_lock_rawvalue.run_workers()"
100 loops, best of 3: 12.6 msec per loop</pre></div></div><p>To use a <code class="literal">RawValue</code> just swap it for a <code class="literal">Value</code> as shown in <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#code-08-syncronization-valuelocking-lock-rawvalue-1">Example&nbsp;9-44</a>.</p><div class="example"><a id="code-08-syncronization-valuelocking-lock-rawvalue-1"></a><div class="example-title">Example&nbsp;9-44.&nbsp;Example of using a RawValue integer</div><div class="example-contents"><pre class="screen">...
def run_workers():
...
    lock = multiprocessing.Lock()
    value = multiprocessing.RawValue('i', 0)
    for process_nbr in range(NBR_PROCESSES):
        p = multiprocessing.Process(target=work, args=(value, MAX_COUNT_PER_PROCESS, lock))
        p.start()
        processes.append(p)</pre></div></div><p>We could also use a <code class="literal">RawArray</code> in place of <code class="literal">multiprocessing.Array</code> if we were sharing an array of primitive objects.</p><p>Now we’ve looked at various ways of diving up work on a single machine between multiple processes along with sharing a flag and synchronizing data sharing between these processes.</p><p>Remember that sharing data can lead to headaches - try to avoid it if possible. Making a machine deal with all the edge cases of state sharing is hard, the first time you have to debug the interactions of multiple processes you’ll realise why the wisdom is to avoid this situation if possible.</p><p>Do consider writing code that runs slower but is more likely to be understood by your team. Using an external tool like Redis to share state leads to a system that can be inspected at run-time by people <span class="emphasis"><em>other</em></span> than the developers - this is a powerful way to enable your team to keep on top of what’s happening in your parallel systems.</p><p>Definitely bare in mind that tweaked performant Python code is less likely to be understood by more junior members of your team - they’ll either be scared of it or they’ll break it. Avoid this problem (and accept a sacrifice in speed) to keep team velocity high.</p></div></div><div class="sect1"><div class="titlepage"><div><div><h2 class="title" style="clear: both" id="_summary">Summary</h2></div></div></div><p>We’ve covered a lot in this chapter. First we looked at two embarrassingly parallel problems where one has predictable complexity and the other has non-predictable complexity. We’ll use these examples again shortly on multiple machines when we discuss <a class="xref" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch10.html">Chapter&nbsp;10</a>.</p><p>Next we looked at Queue support in <code class="literal">multiprocessing</code> and its overheads, in general I recommend using an external queue library so that the state of the queue is more transparent. Preferably you should use an easy to read job format so that it is easy to debug, rather than pickled data.</p><p>The Inter-Process Communication discussion should have impressed upon you how difficult it is to use IPC efficiently and how it can make sense just to use a naive parallel solution (without IPC). Buying a faster computer with more cores might be a far more pragmatic solution than trying to use IPC to exploit an existing machine.</p><p>Sharing <code class="literal">numpy</code> matrices in parallel without making copies is important for a small set of problems (but when it counts, it’ll really count). It takes a few extra lines of code and requires some sanity checking to make sure that you’re really not copying the data between processes.</p><p>Finally we looked at using file and memory locks to avoid corrupting data - this is a source of subtle and hard to track errors, this section shows you robust and lightweight solutions.</p><p>In the next chapter we’ll look at Clustering using Python. With a Cluster we can move beyond single-machine parallelism and utilize the CPUs on a group of machines. This introduces a new world of debugging pain - not only can your code have errors but the other machines can have errors (either from bad configuration or failing hardware). We’ll show how to parallelize the Pi estimation demo using the <code class="literal">parallelpython</code> module and how to run research code inside IPython using <code class="literal">IPython cluster</code>.</p></div><div class="footnotes" epub:type="footnotes"><br><hr style="width: 100; align: left;"><div class="footnote" id="ftn.id814204"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814204" class="simpara">49</a>] </sup><a class="ulink" href="http://en.wikipedia.org/wiki/Amdahl%27s_law" target="_top">http://en.wikipedia.org/wiki/Amdahl%27s_law</a></p></div><div class="footnote" id="ftn.id814377"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814377" class="simpara">50</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#windows" target="_top">http://docs.python.org/2/library/multiprocessing.html#windows</a></p></div><div class="footnote" id="ftn.id814457"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814457" class="simpara">51</a>] </sup><a class="ulink" href="http://legacy.python.org/dev/peps/pep-0371/" target="_top">http://legacy.python.org/dev/peps/pep-0371/</a></p></div><div class="footnote" id="ftn.id814567"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814567" class="simpara">52</a>] </sup><a class="ulink" href="http://legacy.python.org/dev/peps/pep-3148/" target="_top">http://legacy.python.org/dev/peps/pep-3148/</a></p></div><div class="footnote" id="ftn.id814600"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814600" class="simpara">53</a>] </sup><a class="ulink" href="https://pypi.python.org/pypi/futures" target="_top">https://pypi.python.org/pypi/futures</a></p></div><div class="footnote" id="ftn.id814731"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814731" class="simpara">54</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#windows" target="_top">http://docs.python.org/2/library/multiprocessing.html#windows</a></p></div><div class="footnote" id="ftn.id814788"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814788" class="simpara">55</a>] </sup><a class="ulink" href="http://math.missouristate.edu/assets/Math/brett.pptx" target="_top">http://math.missouristate.edu/assets/Math/brett.pptx</a></p></div><div class="footnote" id="ftn.id814834"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id814834" class="simpara">56</a>] </sup><a class="ulink" href="http://en.wikipedia.org/wiki/Pythagorean_theorem" target="_top">http://en.wikipedia.org/wiki/Pythagorean_theorem</a></p></div><div class="footnote" id="ftn.id815432"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815432" class="simpara">57</a>] </sup><a class="ulink" href="http://dabeaz.blogspot.co.uk/2010/01/python-gil-visualized.html" target="_top">http://dabeaz.blogspot.co.uk/2010/01/python-gil-visualized.html</a></p></div><div class="footnote" id="ftn.id815442"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815442" class="simpara">58</a>] </sup><a class="ulink" href="http://www.dabeaz.com/GIL/" target="_top">http://www.dabeaz.com/GIL/</a></p></div><div class="footnote" id="ftn.id815426"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815426" class="simpara">59</a>] </sup><a class="ulink" href="http://www.dabeaz.com/GIL/gilvis/index.html" target="_top">http://www.dabeaz.com/GIL/gilvis/index.html</a></p></div><div class="footnote" id="ftn.id815423"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815423" class="simpara">60</a>] </sup><a class="ulink" href="http://www.dabeaz.com/GIL/gilvis/fourthread.html" target="_top">http://www.dabeaz.com/GIL/gilvis/fourthread.html</a></p></div><div class="footnote" id="ftn.id815781"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815781" class="simpara">61</a>] </sup><a class="ulink" href="http://wiki.scipy.org/ParallelProgramming" target="_top">http://wiki.scipy.org/ParallelProgramming</a></p></div><div class="footnote" id="ftn.id815963"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id815963" class="simpara">62</a>] </sup><a class="ulink" href="https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/" target="_top">https://github.com/numpy/numpy/blob/master/numpy/random/mtrand/</a></p></div><div class="footnote" id="ftn.id816013"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id816013" class="simpara">63</a>] </sup><a class="ulink" href="http://stackoverflow.com/questions/7596612/benchmarking-python-vs-c-using-blas-and-numpy" target="_top">http://stackoverflow.com/questions/7596612/benchmarking-python-vs-c-using-blas-and-numpy</a></p></div><div class="footnote" id="ftn.id816594"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id816594" class="simpara">64</a>] </sup><a class="ulink" href="https://pythonhosted.org/joblib/" target="_top">https://pythonhosted.org/joblib/</a></p></div><div class="footnote" id="ftn.id816691"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id816691" class="simpara">65</a>] </sup><a class="ulink" href="http://en.wikipedia.org/wiki/Sentinel_value" target="_top">http://en.wikipedia.org/wiki/Sentinel_value</a></p></div><div class="footnote" id="ftn.id817027"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id817027" class="simpara">66</a>] </sup><a class="ulink" href="https://github.com/binarydud/pyres" target="_top">https://github.com/binarydud/pyres</a></p></div><div class="footnote" id="ftn.id817243"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id817243" class="simpara">67</a>] </sup><a class="ulink" href="https://pypi.python.org/pypi/mpi4py" target="_top">https://pypi.python.org/pypi/mpi4py</a></p></div><div class="footnote" id="ftn.id818012"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id818012" class="simpara">68</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Value" target="_top">http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Value</a></p></div><div class="footnote" id="ftn.id818496"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id818496" class="simpara">69</a>] </sup><a class="ulink" href="http://stackoverflow.com/questions/5549190/is-shared-readonly-data-copied-to-different-processes-for-python-multiprocessing/5550156#5550156" target="_top">http://stackoverflow.com/questions/5549190/is-shared-readonly-data-copied-to-different-processes-for-python-multiprocessing/5550156#5550156</a></p></div><div class="footnote" id="ftn.id818716"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id818716" class="simpara">70</a>] </sup><a class="ulink" href="https://pypi.python.org/pypi/PrettyTable" target="_top">https://pypi.python.org/pypi/PrettyTable</a></p></div><div class="footnote" id="ftn.id819369"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819369" class="simpara">71</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#process-and-exceptions" target="_top">http://docs.python.org/2/library/multiprocessing.html#process-and-exceptions</a></p></div><div class="footnote" id="ftn.id819448"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819448" class="simpara">72</a>] </sup><a class="ulink" href="https://pypi.python.org/pypi/lockfile" target="_top">https://pypi.python.org/pypi/lockfile</a></p></div><div class="footnote" id="ftn.id819468"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819468" class="simpara">73</a>] </sup><a class="ulink" href="http://pythonhosted.org//lockfile/" target="_top">http://pythonhosted.org//lockfile/</a></p></div><div class="footnote" id="ftn.id819608"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819608" class="simpara">74</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Value" target="_top">http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Value</a></p></div><div class="footnote" id="ftn.id819722"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819722" class="simpara">75</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Lock" target="_top">http://docs.python.org/2/library/multiprocessing.html#multiprocessing.Lock</a></p></div><div class="footnote" id="ftn.id819844"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819844" class="simpara">76</a>] </sup><a class="ulink" href="http://docs.python.org/2/library/multiprocessing.html#multiprocessing.sharedctypes.RawValue" target="_top">http://docs.python.org/2/library/multiprocessing.html#multiprocessing.sharedctypes.RawValue</a></p></div><div class="footnote" id="ftn.id819850"><p><sup>[<a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#id819850" class="simpara">77</a>] </sup><a class="ulink" href="http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/" target="_top">http://eli.thegreenplace.net/2012/01/04/shared-counter-with-pythons-multiprocessing/</a></p></div></div></section><div class="annotator-outer annotator-viewer viewer annotator-hide">
  <ul class="annotator-widget annotator-listing"></ul>
</div><div class="annotator-modal-wrapper annotator-editor-modal annotator-editor annotator-hide">
	<div class="annotator-outer editor">
		<h2 class="title">Highlight</h2>
		<form class="annotator-widget">
			<ul class="annotator-listing">
			<li class="annotator-item"><textarea id="annotator-field-0" placeholder="Add a note using markdown (optional)" class="js-editor" maxlength="750"></textarea></li></ul>
			<div class="annotator-controls">
				<a class="link-to-markdown" href="https://daringfireball.net/projects/markdown/basics" target="_blank">?</a>
				<ul>
					<li class="delete annotator-hide"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#delete" class="annotator-delete-note button positive">Delete Note</a></li>
					<li class="save"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#save" class="annotator-save annotator-focus button positive">Save Note</a></li>
					<li class="cancel"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#cancel" class="annotator-cancel button">Cancel</a></li>
				</ul>
			</div>
		</form>
	</div>
</div><div class="annotator-modal-wrapper annotator-delete-confirm-modal" style="display: none;">
  <div class="annotator-outer">
    <h2 class="title">Highlight</h2>
      <a class="js-close-delete-confirm annotator-cancel close" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#close">Close</a>
      <div class="annotator-widget">
         <div class="delete-confirm">
            Are you sure you want to permanently delete this note?
         </div>
         <div class="annotator-controls">
            <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#cancel" class="annotator-cancel button js-cancel-delete-confirm">No, I changed my mind</a>
            <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#delete" class="annotator-delete button positive js-delete-confirm">Yes, delete it</a>
         </div>
       </div>
   </div>
</div><div class="annotator-adder" style="display: none;">
	<ul class="adders ">
		
		<li class="copy"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#">Copy</a></li>
		
		<li class="add-highlight"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#">Add Highlight</a></li>
		<li class="add-note"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#">
			
				Add Note
			
		</a></li>
	</ul>
</div></div></div>



  <div class="t-sbo-prev sbo-prev sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/high-performance-python/9781449361747/ch08.html" class="prev nav-link">
      
          <span aria-hidden="true" class="pagination-label t-prev-label">Prev</span>
          <span class="visuallyhidden">Previous Chapter</span>
          <div class="pagination-title t-prev-title">8. Concurrency</div>
        </a>
    
  
  </div>

  <div class="t-sbo-next sbo-next sbo-nav-bottom">
  
    
      
        <a href="/site/library/view/high-performance-python/9781449361747/ch10.html" class="next nav-link">
      
          <span aria-hidden="true" class="pagination-label t-next-label">Next</span>
          <span class="visuallyhidden">Next Chapter</span>
          <div class="pagination-title t-next-title">10. Clusters and Job Queues</div>
        </a>
    
  
  </div>


      
    </section>
    <div class="reading-controls-bottom">
      <ul class="interface-controls js-bitlist">
        <li class="queue-control">
            <button type="button" class="rec-fav ss-queue js-queue js-current-chapter-queue" data-queue-endpoint="/api/v1/book/9781449361747/chapter/ch09.html" data-for-analytics="9781449361747:ch09.html">
      <span>Add to Queue</span>
  </button>
        </li>
      </ul>
    </div>
  </div>
<section class="sbo-saved-archives"></section>



          
          
  





    
    



        
      </div>
      



  <footer class="pagefoot t-pagefoot">
    <a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" class="icon-up"><div class="visuallyhidden">Back to top</div></a>
    <ul class="js-footer-nav">
      
        <li><a class="t-recommendations-footer" href="https://www.safaribooksonline.com/r/">Recommended</a></li>
      
      <li><a class="t-queue-footer" href="https://www.safaribooksonline.com/s/">Queue</a></li>
      
        <li><a class="t-recent-footer" href="https://www.safaribooksonline.com/history/">History</a></li>
        <li><a class="t-topics-footer" href="https://www.safaribooksonline.com/topics?q=*&amp;limit=21">Topics</a></li>
      
      
        <li><a class="t-tutorials-footer" href="https://www.safaribooksonline.com/tutorials/">Tutorials</a></li>
      
      <li><a class="t-settings-footer js-settings" href="https://www.safaribooksonline.com/u/">Settings</a></li>
      <li><a href="https://www.safaribooksonline.com/blog/">Blog</a></li>
      <li class="full-support"><a href="https://www.safaribooksonline.com/public/support">Support</a></li>
      <li><a href="https://community.safaribooksonline.com/">Feedback</a></li>
      <li><a href="https://www.safaribooksonline.com/apps/">Get the App</a></li>
      <li><a href="https://www.safaribooksonline.com/accounts/logout/">Sign Out</a></li>
    </ul>
    <span class="copyright">© 2016 <a href="https://www.safaribooksonline.com/" target="_blank">Safari</a>.</span>
    <a href="https://www.safaribooksonline.com/terms/">Terms of Service</a> /
    <a href="https://www.safaribooksonline.com/privacy/">Privacy Policy</a>
  </footer>




    

    
    
  

<div class="annotator-notice"></div><div class="font-flyout"><div class="font-controls-panel">
	<div class="nightmodes">
		<ul>
			<li class="day"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" id="day-mode" title="Day Mode">
				<i class="fa fa-sun-o"></i>
				<span>Day Mode</span></a></li>
			<li class="cloudy"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" id="cloudy-mode" title="Cloudy Mode">
				<i class="fa fa-cloud"></i>
				<span>Cloud Mode</span>
			</a></li>
			<li class="night"><a href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#" id="night-mode" title="Night Mode">
				<i class="fa fa-moon-o"></i>
				<span>Night Mode</span>
			</a></li>
		</ul>
	</div>

	<div class="font-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-font left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-font-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-font right"></i>
		</div>
	</div>

	<div class="column-resizer resizer">
		<div class="draggable-containment-wrapper">
			<i class="fa fa-compress left"></i>
			<span class="filler" style="width: 50%;"></span>
			<div id="js-column-size-draggable" class="draggable ui-widget-content ui-draggable ui-draggable-handle" style="position: relative; left: 80px;"></div>
			<i class="fa fa-expand right"></i>
		</div>
	</div>

	<a id="reset" class="button" href="https://www.safaribooksonline.com//library/view/high-performance-python/9781449361747/ch09.html#">Reset</a>
</div>
</div></body><span class="gr__tooltip"><span class="gr__tooltip-content"></span><i class="gr__tooltip-logo"></i><span class="gr__triangle"></span></span></html>